ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.10 Atlas Data Lake

## Introduction

**Atlas Data Lake** transforme votre stockage cloud (S3, Azure Blob, GCS) en source de donnÃ©es interrogeable via le langage MongoDB. Plus besoin d'ETL complexe ou de duplication : interrogez directement vos donnÃ©es archivÃ©es, logs historiques, ou datasets massifs avec la syntaxe MongoDB familiÃ¨re. C'est la solution idÃ©ale pour l'analytics sur donnÃ©es froides, l'archivage Ã©conomique, et les requÃªtes fÃ©dÃ©rÃ©es (MongoDB + S3).

### ğŸ¯ Objectifs de cette Section

- Comprendre l'architecture Data Lake
- Configurer un Data Lake avec S3/Azure/GCS
- MaÃ®triser les requÃªtes fÃ©dÃ©rÃ©es (MongoDB + S3)
- ImplÃ©menter une stratÃ©gie de data tiering (hot/cold)
- Optimiser les coÃ»ts avec cold storage
- Utiliser Data Lake pour analytics et compliance

---

## ğŸ—ï¸ Architecture Atlas Data Lake

### Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ATLAS DATA LAKE ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚   APPLICATION                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  MongoDB Query (Standard Syntax):                                â”‚ â”‚
â”‚   â”‚  db.orders.find({ createdAt: { $gte: ISODate("2023-01-01") } })  â”‚ â”‚
â”‚   â”‚                                                                  â”‚ â”‚
â”‚   â”‚  Single connection string for both:                              â”‚ â”‚
â”‚   â”‚  â€¢ Hot data (MongoDB Atlas cluster)                              â”‚ â”‚
â”‚   â”‚  â€¢ Cold data (S3/Azure/GCS)                                      â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                   â”‚
â”‚                                    â–¼                                   â”‚
â”‚   ATLAS DATA LAKE SERVICE (MongoDB-managed)                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  Query Router & Optimizer                                        â”‚ â”‚
â”‚   â”‚  â€¢ Parses MongoDB queries                                        â”‚ â”‚
â”‚   â”‚  â€¢ Routes to appropriate storage                                 â”‚ â”‚
â”‚   â”‚  â€¢ Optimizes execution plan                                      â”‚ â”‚
â”‚   â”‚  â€¢ Merges results from multiple sources                          â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                â”‚                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚         â–¼                  â–¼              â–¼                  â–¼         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ MONGODB  â”‚      â”‚ MONGODB  â”‚    â”‚   AWS    â”‚      â”‚  AZURE   â”‚     â”‚
â”‚   â”‚ CLUSTER  â”‚      â”‚ CLUSTER  â”‚    â”‚    S3    â”‚      â”‚   BLOB   â”‚     â”‚
â”‚   â”‚  (Hot)   â”‚      â”‚  (Warm)  â”‚    â”‚  (Cold)  â”‚      â”‚  (Cold)  â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚   Recent data       Last 90 days     Archives         Compliance       â”‚
â”‚   < 30 days         Real-time        Historical       Long-term        â”‚
â”‚   High IOPS         queries          analytics        retention        â”‚
â”‚                                                                        â”‚
â”‚   SUPPORTED FORMATS IN S3/AZURE/GCS:                                   â”‚
â”‚   â€¢ JSON (plain text or gzip)                                          â”‚
â”‚   â€¢ BSON (MongoDB native)                                              â”‚
â”‚   â€¢ Parquet (columnar, optimal for analytics)                          â”‚
â”‚   â€¢ CSV / TSV                                                          â”‚
â”‚   â€¢ Avro                                                               â”‚
â”‚                                                                        â”‚
â”‚   KEY BENEFITS:                                                        â”‚
â”‚   âœ… Query S3 with MongoDB syntax (no Athena/Spark needed)             â”‚
â”‚   âœ… Federated queries (join MongoDB + S3 data)                        â”‚
â”‚   âœ… Cost optimization: S3 ~$0.023/GB vs MongoDB ~$0.25/GB             â”‚
â”‚   âœ… No data duplication                                               â”‚
â”‚   âœ… Automatic schema inference                                        â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Lake vs Alternatives

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ATLAS DATA LAKE vs ALTERNATIVES (Analytics on S3)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  FEATURE           DATA LAKE    ATHENA      SPARK        REDSHIFT      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Query Language    MongoDB      SQL         SQL/Scala    SQL           â”‚
â”‚  Setup Time        Minutes      Minutes     Hours        Hours         â”‚
â”‚  Infrastructure    Zero         Zero        Cluster      Cluster       â”‚
â”‚  MongoDB Native    âœ… Yes       âŒ No       âš ï¸ Via       âŒ No         â”‚
â”‚                                             Connector                  â”‚
â”‚  Federated Query   âœ… Yes       âŒ No       âœ… Yes       âš ï¸ Limited    â”‚
â”‚  (MongoDB+S3)                                                          â”‚
â”‚                                                                        â”‚
â”‚  JSON Support      âœ… Native    âš ï¸ Limited  âœ… Good      âš ï¸ Limited    â”‚
â”‚  Nested Docs       âœ… Native    âŒ No       âš ï¸ Complex   âŒ No         â”‚
â”‚  Aggregations      âœ… Full      âš ï¸ Basic    âœ… Full      âœ… Full       â”‚
â”‚                                                                        â”‚
â”‚  Cost Model        Compute      Scanned     Cluster      Cluster       â”‚
â”‚                    hours        data        hours        hours         â”‚
â”‚  Typical Cost      $2-5/TB      $5/TB       $15-30/TB    $25+/TB       â”‚
â”‚                                                                        â”‚
â”‚  Best For          MongoDB      Ad-hoc SQL  Complex      High-perf     â”‚
â”‚                    users        queries     ETL          BI/DW         â”‚
â”‚                                                                        â”‚
â”‚  RECOMMENDATION:                                                       â”‚
â”‚  â€¢ Data Lake: If using MongoDB, want unified query experience          â”‚
â”‚  â€¢ Athena: SQL team, simple ad-hoc queries                             â”‚
â”‚  â€¢ Spark: Complex transformations, ML pipelines                        â”‚
â”‚  â€¢ Redshift: Traditional data warehouse, heavy BI workloads            â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### 1. CrÃ©ation Data Lake

```bash
# Via Atlas UI:
# 1. Atlas Dashboard â†’ Data Federation (left menu)
# 2. Create Data Lake
# 3. Configure:
#    - Data Lake name
#    - Cloud provider (AWS/Azure/GCP)
#    - Region (same as cluster for best perf)
#    - S3 bucket or Azure container

# Via Atlas CLI
atlas dataLakes create \
  --projectId <PROJECT_ID> \
  --name production-datalake \
  --cloudProviderConfig \
    provider=AWS \
    roleId=<IAM_ROLE_ARN> \
    testS3Bucket=my-bucket

# Via Terraform
resource "mongodbatlas_data_lake" "production" {
  project_id = var.atlas_project_id
  name       = "production-datalake"

  cloud_provider_config {
    aws {
      role_id        = aws_iam_role.atlas_data_lake.arn
      test_s3_bucket = aws_s3_bucket.datalake.id
    }
  }
}
```

### 2. Configuration IAM (AWS)

```hcl
# Terraform: IAM Role for Data Lake to access S3
resource "aws_iam_role" "atlas_data_lake" {
  name = "atlas-data-lake-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          AWS = "arn:aws:iam::${var.atlas_aws_account_id}:root"
        }
        Action = "sts:AssumeRole"
        Condition = {
          StringEquals = {
            "sts:ExternalId" = var.atlas_external_id
          }
        }
      }
    ]
  })
}

# IAM Policy: Read access to S3 bucket
resource "aws_iam_role_policy" "atlas_data_lake_s3" {
  name = "atlas-data-lake-s3-policy"
  role = aws_iam_role.atlas_data_lake.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:ListBucket",
          "s3:GetBucketLocation"
        ]
        Resource = [
          aws_s3_bucket.datalake.arn,
          "${aws_s3_bucket.datalake.arn}/*"
        ]
      }
    ]
  })
}

# S3 Bucket for Data Lake
resource "aws_s3_bucket" "datalake" {
  bucket = "company-mongodb-datalake"

  versioning {
    enabled = true
  }

  lifecycle_rule {
    enabled = true

    transition {
      days          = 90
      storage_class = "GLACIER"
    }
  }
}
```

### 3. DÃ©finition du Virtual Database

```javascript
// Virtual database configuration
// Maps S3 paths to MongoDB collections

// Option 1: Via Atlas UI
// Data Federation â†’ Databases â†’ Add Database
// Configure paths and mappings

// Option 2: Via API
{
  "databases": [
    {
      "name": "analytics",
      "collections": [
        {
          "name": "orders",
          "dataSources": [
            {
              "storeName": "s3Store",
              "path": "/orders/{year}-{month}-{day}/*.json",
              "defaultFormat": "json"
            }
          ]
        },
        {
          "name": "logs",
          "dataSources": [
            {
              "storeName": "s3Store",
              "path": "/logs/{year}/{month}/{day}/*.parquet",
              "defaultFormat": "parquet"
            }
          ]
        }
      ]
    }
  ],
  "stores": [
    {
      "name": "s3Store",
      "provider": "s3",
      "region": "us-east-1",
      "bucket": "company-mongodb-datalake"
    }
  ]
}
```

### 4. Connection String

```javascript
// Data Lake connection string (separate from cluster)
const dataLakeUri = "mongodb://user:password@data-lake-0.mongodb.net/analytics";

// OR unified connection string (federated)
const federatedUri = "mongodb+srv://user:password@cluster0.mongodb.net/";

// Connect
const client = new MongoClient(dataLakeUri);
await client.connect();

// Query S3 data with MongoDB syntax
const db = client.db('analytics');
const orders = await db.collection('orders').find({
  createdAt: { $gte: new Date('2023-01-01') }
}).toArray();

console.log(`Found ${orders.length} orders from 2023`);
```

---

## ğŸ” RequÃªtes Data Lake

### RequÃªtes Basiques

```javascript
// Standard MongoDB queries work on S3 data
const db = client.db('analytics');

// 1. Find documents
const recentOrders = await db.collection('orders').find({
  createdAt: { $gte: new Date('2024-01-01') },
  status: 'completed'
}).toArray();

// 2. Aggregation pipeline
const salesByMonth = await db.collection('orders').aggregate([
  {
    $match: {
      createdAt: { $gte: new Date('2024-01-01') }
    }
  },
  {
    $group: {
      _id: {
        year: { $year: "$createdAt" },
        month: { $month: "$createdAt" }
      },
      totalSales: { $sum: "$amount" },
      orderCount: { $count: {} }
    }
  },
  {
    $sort: { "_id.year": 1, "_id.month": 1 }
  }
]).toArray();

// 3. Complex aggregations
const topProducts = await db.collection('orders').aggregate([
  { $unwind: "$items" },
  {
    $group: {
      _id: "$items.productId",
      totalQuantity: { $sum: "$items.quantity" },
      totalRevenue: { $sum: { $multiply: ["$items.quantity", "$items.price"] } }
    }
  },
  { $sort: { totalRevenue: -1 } },
  { $limit: 10 }
]).toArray();
```

### Federated Queries (MongoDB + S3)

```javascript
// Query that joins hot data (MongoDB) with cold data (S3)

// Scenario: Recent orders in MongoDB, historical in S3
// Query last 30 days + historical data together

const allOrders = await db.collection('orders').aggregate([
  {
    $unionWith: {
      coll: "orders_archive",  // S3-backed collection
      pipeline: [
        {
          $match: {
            createdAt: {
              $gte: new Date('2023-01-01'),
              $lt: new Date('2024-01-01')
            }
          }
        }
      ]
    }
  },
  {
    $match: {
      customerId: "12345"
    }
  },
  {
    $sort: { createdAt: -1 }
  }
]).toArray();

// This query:
// 1. Searches recent orders in MongoDB cluster
// 2. Searches historical orders in S3
// 3. Merges results
// 4. Filters by customerId
// 5. Sorts by date
```

### Partitioning pour Performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    S3 PARTITIONING STRATEGY                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  PARTITIONING BY DATE (Most Common)                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  s3://bucket/orders/                                             â”‚ â”‚
â”‚  â”‚    â”œâ”€â”€ year=2024/                                                â”‚ â”‚
â”‚  â”‚    â”‚   â”œâ”€â”€ month=01/                                             â”‚ â”‚
â”‚  â”‚    â”‚   â”‚   â”œâ”€â”€ day=01/                                           â”‚ â”‚
â”‚  â”‚    â”‚   â”‚   â”‚   â””â”€â”€ data.parquet                                  â”‚ â”‚
â”‚  â”‚    â”‚   â”‚   â”œâ”€â”€ day=02/                                           â”‚ â”‚
â”‚  â”‚    â”‚   â”‚   â”‚   â””â”€â”€ data.parquet                                  â”‚ â”‚
â”‚  â”‚    â”‚   â”‚   â””â”€â”€ ...                                               â”‚ â”‚
â”‚  â”‚    â”‚   â”œâ”€â”€ month=02/                                             â”‚ â”‚
â”‚  â”‚    â”‚   â”‚   â””â”€â”€ ...                                               â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Virtual collection mapping:                                     â”‚ â”‚
â”‚  â”‚  path: "/orders/year={year}/month={month}/day={day}/*.parquet"   â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Query with partition pruning:                                   â”‚ â”‚
â”‚  â”‚  db.orders.find({                                                â”‚ â”‚
â”‚  â”‚    createdAt: {                                                  â”‚ â”‚
â”‚  â”‚      $gte: ISODate("2024-01-15"),                                â”‚ â”‚
â”‚  â”‚      $lt: ISODate("2024-01-20")                                  â”‚ â”‚
â”‚  â”‚    }                                                             â”‚ â”‚
â”‚  â”‚  })                                                              â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Only scans:                                                     â”‚ â”‚
â”‚  â”‚  â€¢ year=2024/month=01/day=15/                                    â”‚ â”‚
â”‚  â”‚  â€¢ year=2024/month=01/day=16/                                    â”‚ â”‚
â”‚  â”‚  â€¢ ...                                                           â”‚ â”‚
â”‚  â”‚  â€¢ year=2024/month=01/day=19/                                    â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  BENEFITS:                                                            â”‚
â”‚  âœ… Partition pruning: Only scan relevant directories                 â”‚
â”‚  âœ… Faster queries: 10-100x faster with proper partitioning           â”‚
â”‚  âœ… Lower cost: Less data scanned = lower compute cost                â”‚
â”‚  âœ… Parallel processing: Multiple partitions read in parallel         â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ Data Tiering Strategies

### Hot/Warm/Cold Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DATA TIERING ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  HOT TIER (MongoDB Atlas Cluster)                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Age:      0-30 days                                         â”‚ â”‚
â”‚  â”‚ Access:        Very frequent (100s/sec)                          â”‚ â”‚
â”‚  â”‚ Latency:       < 10ms                                            â”‚ â”‚
â”‚  â”‚ Size:          50 GB                                             â”‚ â”‚
â”‚  â”‚ Cost:          $0.25/GB-month = $12.50/month                     â”‚ â”‚
â”‚  â”‚ Use Cases:     â€¢ Real-time queries                               â”‚ â”‚
â”‚  â”‚                â€¢ Transactional operations                        â”‚ â”‚
â”‚  â”‚                â€¢ User-facing features                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                â†“                                      â”‚
â”‚                         TTL Index (30 days)                           â”‚
â”‚                         Auto-archive to S3                            â”‚
â”‚                                â†“                                      â”‚
â”‚  WARM TIER (S3 Standard)                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Age:      30-90 days                                        â”‚ â”‚
â”‚  â”‚ Access:        Occasional (10s/day)                              â”‚ â”‚
â”‚  â”‚ Latency:       100-500ms                                         â”‚ â”‚
â”‚  â”‚ Size:          200 GB                                            â”‚ â”‚
â”‚  â”‚ Cost:          $0.023/GB-month = $4.60/month                     â”‚ â”‚
â”‚  â”‚ Use Cases:     â€¢ Recent analytics                                â”‚ â”‚
â”‚  â”‚                â€¢ Reporting                                       â”‚ â”‚
â”‚  â”‚                â€¢ Audit queries                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                â†“                                      â”‚
â”‚                      S3 Lifecycle (90 days)                           â”‚
â”‚                      Transition to Glacier                            â”‚
â”‚                                â†“                                      â”‚
â”‚  COLD TIER (S3 Glacier)                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Age:      > 90 days                                         â”‚ â”‚
â”‚  â”‚ Access:        Rare (few/month)                                  â”‚ â”‚
â”‚  â”‚ Latency:       Minutes-hours (retrieval)                         â”‚ â”‚
â”‚  â”‚ Size:          2 TB                                              â”‚ â”‚
â”‚  â”‚ Cost:          $0.004/GB-month = $8.20/month                     â”‚ â”‚
â”‚  â”‚ Use Cases:     â€¢ Compliance                                      â”‚ â”‚
â”‚  â”‚                â€¢ Historical analysis                             â”‚ â”‚
â”‚  â”‚                â€¢ Long-term archival                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  TOTAL COST ANALYSIS (2.25 TB total):                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Scenario A: All in MongoDB                                           â”‚
â”‚  2.25 TB Ã— $0.25/GB = $576/month                                      â”‚
â”‚                                                                       â”‚
â”‚  Scenario B: Tiered (Hot/Warm/Cold)                                   â”‚
â”‚  Hot:   50 GB Ã— $0.25  = $12.50/month                                 â”‚
â”‚  Warm: 200 GB Ã— $0.023 = $4.60/month                                  â”‚
â”‚  Cold:   2 TB Ã— $0.004 = $8.20/month                                  â”‚
â”‚  Total: $25.30/month                                                  â”‚
â”‚                                                                       â”‚
â”‚  SAVINGS: $550.70/month (96% reduction!)                              â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Automated Archiving

```javascript
// Automated archiving strategy using TTL + Change Streams

// 1. TTL Index on MongoDB (auto-delete after 30 days)
db.orders.createIndex(
  { createdAt: 1 },
  { expireAfterSeconds: 2592000 }  // 30 days
);

// 2. Change Stream to capture deletions and archive to S3
const changeStream = db.orders.watch([
  { $match: { operationType: 'delete' } }
]);

changeStream.on('change', async (change) => {
  // Archive deleted document to S3
  const doc = change.fullDocument;

  const year = doc.createdAt.getFullYear();
  const month = String(doc.createdAt.getMonth() + 1).padStart(2, '0');
  const day = String(doc.createdAt.getDate()).padStart(2, '0');

  const s3Key = `orders/year=${year}/month=${month}/day=${day}/${doc._id}.json`;

  await s3Client.putObject({
    Bucket: 'company-mongodb-datalake',
    Key: s3Key,
    Body: JSON.stringify(doc),
    ContentType: 'application/json'
  });

  console.log(`Archived order ${doc._id} to S3`);
});

// 3. Or use Atlas Scheduled Trigger for batch archiving
// Atlas UI â†’ Triggers â†’ Scheduled â†’ Run daily at 2 AM
exports = async function() {
  const collection = context.services.get("mongodb-atlas")
    .db("production").collection("orders");

  const thirtyDaysAgo = new Date();
  thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);

  const oldOrders = await collection.find({
    createdAt: { $lt: thirtyDaysAgo }
  }).toArray();

  // Archive to S3
  for (const order of oldOrders) {
    await archiveToS3(order);
  }

  // Delete from MongoDB
  await collection.deleteMany({
    createdAt: { $lt: thirtyDaysAgo }
  });

  return `Archived ${oldOrders.length} orders`;
};
```

---

## ğŸ“Š Cas d'Usage

### 1. Historical Analytics

```javascript
// Use case: Analyze sales trends over 3 years (mostly in S3)

const salesTrend = await db.collection('orders').aggregate([
  {
    $match: {
      createdAt: {
        $gte: new Date('2022-01-01'),
        $lt: new Date('2025-01-01')
      }
    }
  },
  {
    $group: {
      _id: {
        year: { $year: "$createdAt" },
        month: { $month: "$createdAt" }
      },
      totalSales: { $sum: "$amount" },
      orderCount: { $count: {} },
      avgOrderValue: { $avg: "$amount" }
    }
  },
  {
    $sort: { "_id.year": 1, "_id.month": 1 }
  }
]).toArray();

// This query:
// - Scans 3 years of data
// - Most data in S3 (cost-effective)
// - Returns monthly aggregates
// - No need to move data to MongoDB
```

### 2. Customer 360 View

```javascript
// Combine recent orders (MongoDB) with historical (S3)

async function getCustomer360(customerId) {
  // Recent orders from MongoDB
  const recentOrders = await mongoDb.collection('orders').find({
    customerId: customerId,
    createdAt: { $gte: thirtyDaysAgo }
  }).toArray();

  // Historical orders from Data Lake (S3)
  const historicalOrders = await dataLakeDb.collection('orders_archive').find({
    customerId: customerId,
    createdAt: { $lt: thirtyDaysAgo }
  }).toArray();

  // Combine
  const allOrders = [...recentOrders, ...historicalOrders];

  // Calculate metrics
  const totalSpent = allOrders.reduce((sum, order) => sum + order.amount, 0);
  const orderCount = allOrders.length;
  const avgOrderValue = totalSpent / orderCount;

  return {
    customerId,
    totalSpent,
    orderCount,
    avgOrderValue,
    firstOrderDate: allOrders[allOrders.length - 1].createdAt,
    lastOrderDate: allOrders[0].createdAt
  };
}
```

### 3. Compliance & Audit

```javascript
// Use case: Query 7 years of data for compliance audit

const auditQuery = await db.collection('transactions').aggregate([
  {
    $match: {
      userId: "suspicious-user-123",
      createdAt: {
        $gte: new Date('2018-01-01'),  // 7 years ago
        $lt: new Date('2025-01-01')
      }
    }
  },
  {
    $group: {
      _id: {
        year: { $year: "$createdAt" }
      },
      transactionCount: { $count: {} },
      totalAmount: { $sum: "$amount" },
      avgAmount: { $avg: "$amount" }
    }
  }
]).toArray();

// Benefits:
// - 7 years of data queried seamlessly
// - Most data in Glacier (pennies per GB)
// - Meets compliance requirements
// - No need to restore from backups
```

### 4. Log Analytics

```javascript
// Use case: Analyze application logs stored in S3 as JSON

// S3 structure:
// s3://logs/app-logs/year=2024/month=12/day=08/hour=14/*.json

const errorAnalysis = await db.collection('app_logs').aggregate([
  {
    $match: {
      timestamp: {
        $gte: new Date('2024-12-01'),
        $lt: new Date('2024-12-08')
      },
      level: 'ERROR'
    }
  },
  {
    $group: {
      _id: {
        errorType: "$error.type",
        errorMessage: "$error.message"
      },
      count: { $count: {} },
      firstOccurrence: { $min: "$timestamp" },
      lastOccurrence: { $max: "$timestamp" }
    }
  },
  {
    $sort: { count: -1 }
  },
  {
    $limit: 20
  }
]).toArray();

// Analyze millions of log entries
// Stored cheaply in S3
// Query with familiar MongoDB syntax
// No need for ELK stack
```

---

## ğŸš€ Performance et Optimisation

### File Formats

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FILE FORMAT COMPARISON                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  FORMAT      SIZE    READ PERF    WRITE PERF    USE CASE               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  JSON        â­      â­â­         â­â­â­         â€¢ Human-readable
â”‚              Large   Slow        Fast          â€¢ Compatibility         â”‚
â”‚                                                â€¢ Simple use cases      â”‚
â”‚                                                                        â”‚
â”‚  BSON        â­â­     â­â­â­        â­â­â­         â€¢ MongoDB native
â”‚              Medium  Fast        Fast          â€¢ Binary efficiency     â”‚
â”‚                                                â€¢ Nested docs           â”‚
â”‚                                                                        â”‚
â”‚  Parquet     â­â­â­   â­â­â­â­â­     â­            â€¢ Analytics (BEST)
â”‚              Small   Very fast   Slow          â€¢ Columnar storage      â”‚
â”‚              (10x)   (100x)                    â€¢ Compression           â”‚
â”‚                                                â€¢ Large datasets        â”‚
â”‚                                                                        â”‚
â”‚  CSV          â­â­    â­â­         â­â­â­         â€¢ Flat data
â”‚              Small   Medium      Fast          â€¢ Excel compatibility   â”‚
â”‚                                                â€¢ Simple structure      â”‚
â”‚                                                                        â”‚
â”‚  RECOMMENDATION FOR DATA LAKE:                                         â”‚
â”‚  âœ… Parquet for analytics workloads (10-100x better performance)       â”‚
â”‚  âœ… BSON for MongoDB-native data                                       â”‚
â”‚  âœ… JSON for human-readable archives                                   â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Optimization Tips

```javascript
// âŒ BAD: Full collection scan
db.orders.find({
  amount: { $gt: 1000 }
}).toArray();
// Scans all files in S3

// âœ… GOOD: Partition pruning with date filter
db.orders.find({
  createdAt: {
    $gte: new Date('2024-12-01'),
    $lt: new Date('2024-12-08')
  },
  amount: { $gt: 1000 }
}).toArray();
// Only scans relevant date partitions

// âŒ BAD: Projection after full document fetch
db.orders.find({}).toArray().then(docs =>
  docs.map(d => ({ _id: d._id, amount: d.amount }))
);

// âœ… GOOD: Projection in query (especially for Parquet)
db.orders.find(
  {},
  { projection: { _id: 1, amount: 1 } }
).toArray();
// Columnar format reads only needed columns

// âœ… BEST: Combine partition pruning + projection + filter
db.orders.find(
  {
    createdAt: { $gte: new Date('2024-12-01') },
    status: 'completed',
    amount: { $gt: 1000 }
  },
  {
    projection: { _id: 1, customerId: 1, amount: 1 }
  }
).toArray();
```

### Costs Monitoring

```javascript
// Data Lake costs are based on:
// 1. Compute time (processing)
// 2. Data scanned (S3 reads)

// Example cost calculation:
const costs = {
  // Compute: $5 per 1M document-reads
  compute: {
    documentsRead: 10_000_000,  // 10M docs
    costPerMillion: 5,
    total: (10_000_000 / 1_000_000) * 5  // $50
  },

  // Data transfer: $0.09/GB (if crossing regions)
  dataTransfer: {
    gigabytesTransferred: 100,  // 100GB
    costPerGB: 0.09,
    total: 100 * 0.09  // $9
  },

  // S3 storage: $0.023/GB-month
  storage: {
    gigabytesStored: 1000,  // 1TB
    costPerGB: 0.023,
    total: 1000 * 0.023  // $23
  }
};

const totalMonthlyCost =
  costs.compute.total +
  costs.dataTransfer.total +
  costs.storage.total;

console.log(`Estimated monthly cost: $${totalMonthlyCost}`);
// Output: Estimated monthly cost: $82
```

---

## ğŸ“‹ Best Practices

### Data Lake Checklist

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA LAKE BEST PRACTICES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  DATA ORGANIZATION                                                    â”‚
â”‚  â˜ Partition by date (year/month/day) for time-series data            â”‚
â”‚  â˜ Use Parquet format for analytics workloads                         â”‚
â”‚  â˜ Consistent naming conventions (lowercase, hyphens)                 â”‚
â”‚  â˜ Compress files (gzip for JSON, snappy for Parquet)                 â”‚
â”‚  â˜ Optimal file sizes: 128MB - 1GB per file                           â”‚
â”‚                                                                       â”‚
â”‚  QUERY OPTIMIZATION                                                   â”‚
â”‚  â˜ Always include partition keys in queries                           â”‚
â”‚  â˜ Use projection to limit columns (especially Parquet)               â”‚
â”‚  â˜ Limit result sets with $limit                                      â”‚
â”‚  â˜ Test queries on small date ranges first                            â”‚
â”‚  â˜ Monitor query performance in Atlas UI                              â”‚
â”‚                                                                       â”‚
â”‚  DATA LIFECYCLE                                                       â”‚
â”‚  â˜ Implement TTL indexes on hot data                                  â”‚
â”‚  â˜ Automate archiving to S3                                           â”‚
â”‚  â˜ Use S3 lifecycle policies (Standard â†’ Glacier)                     â”‚
â”‚  â˜ Test restore procedures for Glacier data                           â”‚
â”‚  â˜ Document retention policies                                        â”‚
â”‚                                                                       â”‚
â”‚  COST MANAGEMENT                                                      â”‚
â”‚  â˜ Monitor Data Lake compute costs                                    â”‚
â”‚  â˜ Optimize queries to minimize data scanned                          â”‚
â”‚  â˜ Use same region for cluster and S3 (avoid transfer)                â”‚
â”‚  â˜ Archive rarely-accessed data to Glacier                            â”‚
â”‚  â˜ Review and delete unused virtual collections                       â”‚
â”‚                                                                       â”‚
â”‚  SECURITY                                                             â”‚
â”‚  â˜ Use IAM roles (not access keys) for S3 access                      â”‚
â”‚  â˜ Encrypt data at rest (S3 encryption)                               â”‚
â”‚  â˜ Encrypt data in transit (TLS)                                      â”‚
â”‚  â˜ Implement least privilege access                                   â”‚
â”‚  â˜ Enable S3 versioning for data protection                           â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Common Pitfalls

```
âŒ DATA LAKE ANTI-PATTERNS:

1. No Partitioning
   â€¢ Storing all data in single directory
   â€¢ Every query scans entire dataset
   â†’ Solution: Partition by date or other key dimension

2. Too Many Small Files
   â€¢ Millions of < 1MB files
   â€¢ High overhead, slow queries
   â†’ Solution: Combine into 128MB-1GB files

3. Wrong File Format
   â€¢ Using JSON for analytics (10-100x slower than Parquet)
   â†’ Solution: Convert to Parquet for analytics

4. No Projection
   â€¢ Fetching full documents when only need few fields
   â€¢ Wastes bandwidth and compute
   â†’ Solution: Always use projection

5. Cross-Region Data Transfer
   â€¢ Cluster in us-east-1, S3 in eu-west-1
   â€¢ High transfer costs ($0.09/GB)
   â†’ Solution: Same region for cluster and S3

6. No Lifecycle Management
   â€¢ Keeping all data in S3 Standard
   â€¢ 5x more expensive than Glacier
   â†’ Solution: Lifecycle policy to Glacier after 90 days
```

---

## ğŸ RÃ©sumÃ©

### Points ClÃ©s

1. **Architecture**
   - Query S3/Azure/GCS with MongoDB syntax
   - Federated queries (MongoDB + cloud storage)
   - Near real-time sync
   - No data duplication

2. **File Formats**
   - Parquet: Best for analytics (10-100x faster)
   - BSON: MongoDB native
   - JSON: Human-readable, compatibility
   - CSV: Flat data, Excel

3. **Data Tiering**
   - Hot (0-30d): MongoDB ($0.25/GB)
   - Warm (30-90d): S3 Standard ($0.023/GB)
   - Cold (>90d): S3 Glacier ($0.004/GB)
   - 96% cost savings possible

4. **Performance**
   - Partition by date (year/month/day)
   - Use Parquet for analytics
   - Always include partition keys in queries
   - Use projection for columnar formats

5. **Use Cases**
   - Historical analytics
   - Compliance (7+ years retention)
   - Log analytics
   - Customer 360 views

### Configuration Minimale Production

```javascript
// 1. S3 bucket with partitioning
// s3://bucket/collection/year=2024/month=12/day=08/*.parquet

// 2. Virtual database config
{
  "databases": [{
    "name": "analytics",
    "collections": [{
      "name": "orders",
      "dataSources": [{
        "storeName": "s3Store",
        "path": "/orders/year={year}/month={month}/day={day}/*.parquet",
        "defaultFormat": "parquet"
      }]
    }]
  }]
}

// 3. Optimized query pattern
db.orders.find(
  {
    createdAt: {
      $gte: new Date('2024-12-01'),  // Partition pruning
      $lt: new Date('2024-12-08')
    },
    status: 'completed'
  },
  {
    projection: { _id: 1, amount: 1, customerId: 1 }  // Column pruning
  }
).limit(1000)  // Limit results
```

### Cost Example

```
Scenario: 10TB total data, 1 year retention

Option A: All in MongoDB
10,000 GB Ã— $0.25/GB = $2,500/month

Option B: Data Lake (Hot/Warm/Cold)
Hot (50GB, 30d):    50 Ã— $0.25 = $12.50
Warm (500GB, 60d): 500 Ã— $0.023 = $11.50
Cold (9.5TB, 9m): 9,500 Ã— $0.004 = $38.00
Total: $62/month

Savings: $2,438/month (97%)
```

---


â­ï¸ [Atlas Charts (visualisation)](/14-mongodb-atlas/11-atlas-charts.md)
