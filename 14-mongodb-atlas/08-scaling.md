ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.8 Scaling (Vertical et Horizontal)

## Introduction

Le scaling est l'art de **faire Ã©voluer votre infrastructure** pour rÃ©pondre Ã  la demande croissante sans sacrifier les performances ni exploser les coÃ»ts. Atlas offre plusieurs stratÃ©gies de scaling : vertical (augmenter les ressources par nÅ“ud), horizontal (ajouter des nÅ“uds via sharding), et auto-scaling (ajustement automatique). Cette section guide les Ã©quipes DevOps dans la planification de capacitÃ©, l'implÃ©mentation du scaling, et l'optimisation des coÃ»ts.

### ğŸ¯ Objectifs de cette Section

- Comprendre les diffÃ©rents types de scaling
- MaÃ®triser le scaling vertical (tier changes)
- ImplÃ©menter le sharding pour le scaling horizontal
- Configurer l'auto-scaling intelligent
- Planifier la capacitÃ© Ã  long terme
- Optimiser le ratio performance/coÃ»t

---

## ğŸ“Š Types de Scaling

### Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SCALING STRATEGIES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  1. VERTICAL SCALING (Scale Up/Down)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  M30 (8GB RAM)      â†’    M40 (16GB RAM)     â†’    M50 (32GB RAM)  â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚  â”‚  Node   â”‚            â”‚  Node   â”‚            â”‚  Node   â”‚       â”‚ â”‚
â”‚  â”‚  â”‚ 2 vCPU  â”‚            â”‚ 4 vCPU  â”‚            â”‚ 8 vCPU  â”‚       â”‚ â”‚
â”‚  â”‚  â”‚  8GB    â”‚            â”‚  16GB   â”‚            â”‚  32GB   â”‚       â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Pros: Simple, no code changes                                   â”‚ â”‚
â”‚  â”‚  Cons: Limits (max M700), brief downtime                         â”‚ â”‚
â”‚  â”‚  Cost: Linear increase                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  2. HORIZONTAL SCALING - READ REPLICAS (Scale Out)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  3-node RS          â†’        5-node RS         â†’     7-node RS   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚ Primary â”‚              â”‚ Primary â”‚              â”‚ Primary â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚    â”Œâ”€â”€â”´â”€â”€â”                  â”Œâ”€â”€â”´â”€â”€â”                  â”Œâ”€â”€â”´â”€â”€â”     â”‚ â”‚
â”‚  â”‚  â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”              â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”              â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚S1 â”‚ â”‚S2 â”‚              â”‚S1 â”‚ â”‚S2 â”‚              â”‚S1 â”‚ â”‚S2 â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜              â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜              â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜   â”‚ â”‚
â”‚  â”‚                           â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”              â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”   â”‚ â”‚
â”‚  â”‚                           â”‚S3 â”‚ â”‚S4 â”‚              â”‚S3 â”‚ â”‚S4 â”‚   â”‚ â”‚
â”‚  â”‚                           â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜              â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜   â”‚ â”‚
â”‚  â”‚                                                    â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”   â”‚ â”‚
â”‚  â”‚                                                    â”‚S5 â”‚ â”‚S6 â”‚   â”‚ â”‚
â”‚  â”‚                                                    â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Pros: Increase read capacity, better availability               â”‚ â”‚
â”‚  â”‚  Cons: No write scaling, storage still limited                   â”‚ â”‚
â”‚  â”‚  Cost: Linear per node                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  3. HORIZONTAL SCALING - SHARDING (Scale Out)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  1 Shard              â†’         2 Shards        â†’    4 Shards    â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚ â”‚
â”‚  â”‚  â”‚ Shard 0 â”‚                â”‚ Shard 0 â”‚ Shard 1 â”‚                â”‚ â”‚
â”‚  â”‚  â”‚ 100GB   â”‚                â”‚  50GB   â”‚  50GB   â”‚                â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚ â”‚
â”‚  â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚                              â”‚Shard0â”‚Shard1â”‚Shard2â”‚Shard3â”‚       â”‚ â”‚
â”‚  â”‚                              â”‚ 25GB â”‚ 25GB â”‚ 25GB â”‚ 25GB â”‚       â”‚ â”‚
â”‚  â”‚                              â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Pros: Unlimited scaling, write scaling, data distribution       â”‚ â”‚
â”‚  â”‚  Cons: Complex, shard key crucial, rebalancing overhead          â”‚ â”‚
â”‚  â”‚  Cost: Linear per shard                                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  4. AUTO-SCALING (Automated)                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Normal Load    â†’    Peak Load    â†’    Normal Load               â”‚ â”‚
â”‚  â”‚  M40 (16GB)         M50 (32GB)        M40 (16GB)                 â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Automatic adjustment based on:                                  â”‚ â”‚
â”‚  â”‚  â€¢ CPU utilization                                               â”‚ â”‚
â”‚  â”‚  â€¢ Memory pressure                                               â”‚ â”‚
â”‚  â”‚  â€¢ Disk usage                                                    â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Pros: Responsive, cost-optimized, hands-off                     â”‚ â”‚
â”‚  â”‚  Cons: Potential brief downtime, cost variability                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Matrice de DÃ©cision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALING DECISION MATRIX                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  SCENARIO                           SOLUTION                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  CPU constantly > 80%               Vertical scale up                  â”‚
â”‚  Memory constantly > 85%            Vertical scale up                  â”‚
â”‚  Disk > 80% full                    Auto-scale storage OR vertical up  â”‚
â”‚                                                                        â”‚
â”‚  Read-heavy workload                Add read replicas                  â”‚
â”‚  Analytics slowing production       Add analytics node                 â”‚
â”‚                                                                        â”‚
â”‚  Data > 2TB                         Consider sharding                  â”‚
â”‚  Write throughput maxed out         Implement sharding                 â”‚
â”‚  Single collection > 500GB          Shard that collection              â”‚
â”‚                                                                        â”‚
â”‚  Unpredictable traffic spikes       Enable auto-scaling                â”‚
â”‚  Daily/weekly traffic patterns      Enable auto-scaling                â”‚
â”‚                                                                        â”‚
â”‚  Budget constraints                 Right-size + auto-scale down       â”‚
â”‚  Steady predictable load            Reserved capacity                  â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â¬†ï¸ Scaling Vertical

### Process de Scaling Vertical

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERTICAL SCALING WORKFLOW                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  PHASE 1: ASSESSMENT (5 minutes)                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Current metrics: CPU, RAM, disk utilization                    â”‚ â”‚
â”‚  â”‚ â€¢ Workload trends over past 30 days                              â”‚ â”‚
â”‚  â”‚ â€¢ Peak usage patterns                                            â”‚ â”‚
â”‚  â”‚ â€¢ Growth projection                                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â–¼                                        â”‚
â”‚  PHASE 2: TIER SELECTION (2 minutes)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Current: M40 (16GB RAM, 4 vCPU)                                  â”‚ â”‚
â”‚  â”‚ Target:  M50 (32GB RAM, 8 vCPU)                                  â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚ Validation:                                                      â”‚ â”‚
â”‚  â”‚ âœ… Target RAM > Working Set Size Ã— 1.5                           â”‚ â”‚
â”‚  â”‚ âœ… Target CPU handles peak + 30% headroom                        â”‚ â”‚
â”‚  â”‚ âœ… Target storage > current usage Ã— 1.3                          â”‚ â”‚
â”‚  â”‚ âœ… Cost increase acceptable ($630 â†’ $1,525/month)                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â–¼                                        â”‚
â”‚  PHASE 3: PLANNING (10 minutes)                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Schedule maintenance window (low traffic period)               â”‚ â”‚
â”‚  â”‚ â€¢ Notify stakeholders                                            â”‚ â”‚
â”‚  â”‚ â€¢ Prepare rollback plan                                          â”‚ â”‚
â”‚  â”‚ â€¢ Update capacity planning docs                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â–¼                                        â”‚
â”‚  PHASE 4: EXECUTION (10-20 minutes)                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ For each node in replica set:                                    â”‚ â”‚
â”‚  â”‚   1. Secondary 1: Scale, wait for healthy                        â”‚ â”‚
â”‚  â”‚   2. Secondary 2: Scale, wait for healthy                        â”‚ â”‚
â”‚  â”‚   3. Primary: Trigger election, then scale                       â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚ Brief downtime: ~30-60 seconds during primary election           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â–¼                                        â”‚
â”‚  PHASE 5: VALIDATION (5 minutes)                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ âœ… All nodes healthy                                             â”‚ â”‚
â”‚  â”‚ âœ… Replication lag < 1s                                          â”‚ â”‚
â”‚  â”‚ âœ… Application connections restored                              â”‚ â”‚
â”‚  â”‚ âœ… No errors in logs                                             â”‚ â”‚
â”‚  â”‚ âœ… Metrics showing improved capacity                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  TOTAL TIME: ~30-40 minutes                                           â”‚
â”‚  DOWNTIME:   ~30-60 seconds                                           â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration via Terraform

```hcl
# Terraform: Vertical Scaling
resource "mongodbatlas_cluster" "production" {
  project_id = var.atlas_project_id
  name       = "production-cluster"

  # Current tier
  provider_instance_size_name = "M40"

  # To scale up to M50, simply update:
  # provider_instance_size_name = "M50"
  # terraform apply

  provider_name = "AWS"
  provider_region_name = "US_EAST_1"

  # Replica set configuration
  replication_specs {
    num_shards = 1
    regions_config {
      region_name     = "US_EAST_1"
      electable_nodes = 3
      priority        = 7
      read_only_nodes = 0
    }
  }
}

# Alternative: Use auto-scaling (recommended)
resource "mongodbatlas_cluster" "production_autoscale" {
  project_id = var.atlas_project_id
  name       = "production-cluster"

  provider_instance_size_name = "M40"

  # Auto-scaling configuration
  auto_scaling_compute_enabled = true
  auto_scaling_compute_scale_down_enabled = true

  provider_auto_scaling_compute_min_instance_size = "M40"
  provider_auto_scaling_compute_max_instance_size = "M60"
}
```

### Limites du Scaling Vertical

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 VERTICAL SCALING LIMITATIONS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  LIMIT              CONSTRAINT                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  Maximum Tier       M700+ (custom tiers available)                     â”‚
â”‚                     Typical max: M400 (512GB RAM)                      â”‚
â”‚                                                                        â”‚
â”‚  Cost               Exponential growth at high tiers                   â”‚
â”‚                     M200: $14,400/month                                â”‚
â”‚                     M400: ~$30,000/month                               â”‚
â”‚                                                                        â”‚
â”‚  Downtime           30-60 seconds per scaling operation                â”‚
â”‚                     (primary election)                                 â”‚
â”‚                                                                        â”‚
â”‚  Single Point       All data on one logical cluster                    â”‚
â”‚                     Cannot distribute writes                           â”‚
â”‚                                                                        â”‚
â”‚  Hardware           Physical limits of single machines                 â”‚
â”‚                     Max ~1TB RAM per node in practice                  â”‚
â”‚                                                                        â”‚
â”‚  WHEN TO STOP VERTICAL SCALING:                                        â”‚
â”‚  â€¢ Tier > M200 (consider sharding)                                     â”‚
â”‚  â€¢ Single collection > 500GB (must shard)                              â”‚
â”‚  â€¢ Write throughput maxed (need write distribution)                    â”‚
â”‚  â€¢ Cost > $10K/month (evaluate sharding economics)                     â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â¡ï¸ Scaling Horizontal : Sharding

### Architecture Sharded Cluster

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SHARDED CLUSTER ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚                           APPLICATION                                 â”‚
â”‚                                 â”‚                                     â”‚
â”‚                                 â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      MONGOS (Query Router)                      â”‚  â”‚
â”‚  â”‚  â€¢ Routes queries to appropriate shards                         â”‚  â”‚
â”‚  â”‚  â€¢ Merges results from multiple shards                          â”‚  â”‚
â”‚  â”‚  â€¢ Atlas-managed (automatic load balancing)                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                 â”‚                 â”‚                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â–¼                â–¼   â–¼                 â–¼   â–¼                 â–¼    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ SHARD 0â”‚      â”‚ SHARD 1â”‚          â”‚ SHARD 2â”‚           â”‚ SHARD 3â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚Primary â”‚      â”‚Primary â”‚          â”‚Primary â”‚           â”‚Primary â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”´â”     â”‚   â”Œâ”€â”€â”€â”€â”´â”         â”‚   â”Œâ”€â”€â”€â”€â”´â”          â”‚   â”Œâ”€â”€â”€â”€â”´â” â”‚
â”‚  â”‚   â”‚Sec 1â”‚     â”‚   â”‚Sec 1â”‚         â”‚   â”‚Sec 1â”‚          â”‚   â”‚Sec 1â”‚ â”‚
â”‚  â”‚   â”‚Sec 2â”‚     â”‚   â”‚Sec 2â”‚         â”‚   â”‚Sec 2â”‚          â”‚   â”‚Sec 2â”‚ â”‚
â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  Data Range:     Data Range:        Data Range:         Data Range:   â”‚
â”‚  userId:         userId:            userId:             userId:       â”‚
â”‚  0-250K          250K-500K          500K-750K           750K-1M       â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              CONFIG SERVERS (3-node replica set)                â”‚  â”‚
â”‚  â”‚  â€¢ Store cluster metadata                                       â”‚  â”‚
â”‚  â”‚  â€¢ Shard key ranges                                             â”‚  â”‚
â”‚  â”‚  â€¢ Chunk distribution                                           â”‚  â”‚
â”‚  â”‚  â€¢ Atlas-managed                                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Choix du Shard Key

Le **shard key** est la dÃ©cision la plus critique du sharding. Il dÃ©termine comment les donnÃ©es sont distribuÃ©es.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SHARD KEY SELECTION GUIDE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  CRITERIA FOR GOOD SHARD KEY:                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚                                                                        â”‚
â”‚  1. HIGH CARDINALITY                                                   â”‚
â”‚     â€¢ Many distinct values                                             â”‚
â”‚     â€¢ Example: userId (millions of users)                              â”‚
â”‚     âœ… Good: userId (1M+ unique values)                                â”‚
â”‚     âŒ Bad: country (200 values) â†’ Hot spots                           â”‚
â”‚                                                                        â”‚
â”‚  2. EVEN DISTRIBUTION                                                  â”‚
â”‚     â€¢ Values spread evenly across range                                â”‚
â”‚     â€¢ No skew towards certain values                                   â”‚
â”‚     âœ… Good: UUID, hashed field                                        â”‚
â”‚     âŒ Bad: timestamp (monotonic) â†’ All writes to one shard            â”‚
â”‚                                                                        â”‚
â”‚  3. QUERY TARGETING                                                    â”‚
â”‚     â€¢ Most queries include shard key                                   â”‚
â”‚     â€¢ Avoid scatter-gather queries                                     â”‚
â”‚     âœ… Good: Query by userId (targets 1 shard)                         â”‚
â”‚     âŒ Bad: Query without shard key (hits all shards)                  â”‚
â”‚                                                                        â”‚
â”‚  4. WRITE DISTRIBUTION                                                 â”‚
â”‚     â€¢ Writes spread across all shards                                  â”‚
â”‚     â€¢ No single "hot" shard                                            â”‚
â”‚     âœ… Good: Hashed _id                                                â”‚
â”‚     âŒ Bad: Incrementing orderId                                       â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Exemples de Shard Keys

```javascript
// EXAMPLE 1: E-commerce Order System
// =====================================

// âŒ BAD: Monotonic orderId
{
  shardKey: { orderId: 1 }
  // Problem: All new orders go to last shard
  // Result: Hot shard, poor write distribution
}

// âš ï¸ MEDIUM: Country
{
  shardKey: { country: 1 }
  // Problem: Low cardinality (200 values)
  // Some shards have much more data (USA, China)
  // Result: Uneven distribution
}

// âœ… GOOD: Hashed userId
{
  shardKey: { userId: "hashed" }
  // Pros: Even distribution, good write scaling
  // Cons: Range queries on userId hit all shards
  // Use case: When most queries are equality (userId == X)
}

// âœ… BEST: Compound key (userId + orderId)
{
  shardKey: { userId: 1, orderId: 1 }
  // Pros:
  // - Targets single shard per user
  // - Range queries within user efficient
  // - Good write distribution (many users)
  // Use case: Query orders by user
}

// EXAMPLE 2: IoT Time-Series Data
// =================================

// âŒ BAD: timestamp only
{
  shardKey: { timestamp: 1 }
  // All writes go to last chunk (latest time)
}

// âœ… GOOD: deviceId + timestamp
{
  shardKey: { deviceId: 1, timestamp: 1 }
  // Distributes writes across devices
  // Time-range queries per device efficient
}

// âœ… ALTERNATIVE: Hashed deviceId
{
  shardKey: { deviceId: "hashed" }
  // Random distribution, maximum write throughput
  // Trade-off: Range queries less efficient
}

// EXAMPLE 3: Social Media Platform
// ==================================

// âŒ BAD: createdAt
{
  shardKey: { createdAt: 1 }
  // All new posts to one shard
}

// âœ… GOOD: authorId + createdAt
{
  shardKey: { authorId: 1, createdAt: -1 }
  // Posts distributed by author
  // Query "author's recent posts" hits one shard
}

// EXAMPLE 4: Multi-tenant SaaS
// ==============================

// âœ… EXCELLENT: tenantId + documentId
{
  shardKey: { tenantId: 1, documentId: 1 }
  // Complete tenant isolation per shard (if possible)
  // All queries for tenant hit single shard
  // Crucial for performance and data isolation
}
```

### ImplÃ©mentation du Sharding

```javascript
// Step 1: Enable sharding on database
sh.enableSharding("mydb")

// Step 2: Create index on shard key
db.orders.createIndex({ userId: 1, orderId: 1 })

// Step 3: Shard the collection
sh.shardCollection("mydb.orders", { userId: 1, orderId: 1 })

// Step 4: Pre-split chunks (optional, for large collections)
// Prevents initial balancing overhead
for (let i = 0; i < 1000000; i += 100000) {
  sh.splitAt("mydb.orders", { userId: i, orderId: MinKey })
}

// Step 5: Monitor shard distribution
db.orders.getShardDistribution()
// Output:
// Shard shard0000 at shard0000/...
//  data : 25GiB docs : 5000000 chunks : 250
// Shard shard0001 at shard0001/...
//  data : 25GiB docs : 5000000 chunks : 250
// Shard shard0002 at shard0002/...
//  data : 25GiB docs : 5000000 chunks : 250
// Shard shard0003 at shard0003/...
//  data : 25GiB docs : 5000000 chunks : 250
//
// Totals
//  data : 100GiB docs : 20000000 chunks : 1000
```

### Configuration Atlas Sharding

```hcl
# Terraform: Sharded Cluster Configuration
resource "mongodbatlas_cluster" "production_sharded" {
  project_id = var.atlas_project_id
  name       = "production-sharded"

  cluster_type = "SHARDED"

  # Each shard is an M40 replica set
  provider_instance_size_name = "M40"
  provider_name               = "AWS"

  # Sharding configuration
  replication_specs {
    num_shards = 4  # Start with 4 shards

    regions_config {
      region_name     = "US_EAST_1"
      electable_nodes = 3  # 3 nodes per shard (HA)
      priority        = 7
      read_only_nodes = 0
    }
  }

  # Auto-scaling per shard
  auto_scaling_compute_enabled            = true
  auto_scaling_compute_scale_down_enabled = true
  provider_auto_scaling_compute_min_instance_size = "M40"
  provider_auto_scaling_compute_max_instance_size = "M60"
}

# Output monthly cost calculation
output "sharded_cluster_cost" {
  value = "Estimated: 4 shards Ã— $630 (M40) = $2,520/month base"
}
```

### Ajout de Shards

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADDING SHARDS WORKFLOW                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  INITIAL STATE: 2 Shards                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚ Shard 0  â”‚ Shard 1  â”‚                                               â”‚
â”‚  â”‚ 100GB    â”‚ 100GB    â”‚                                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚                                                                        â”‚
â”‚  STEP 1: Add 2 New Shards (via Atlas UI or Terraform)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Shard 0  â”‚ Shard 1  â”‚ Shard 2  â”‚ Shard 3  â”‚                         â”‚
â”‚  â”‚ 100GB    â”‚ 100GB    â”‚  Empty   â”‚  Empty   â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                        â”‚
â”‚  STEP 2: Balancer Redistributes Chunks (Automatic)                     â”‚
â”‚  Time: 2-24 hours depending on data size                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Shard 0  â”‚ Shard 1  â”‚ Shard 2  â”‚ Shard 3  â”‚                         â”‚
â”‚  â”‚ 75GB     â”‚ 75GB     â”‚ 25GB     â”‚ 25GB     â”‚  (Progressive)          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                 â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ Shard 0  â”‚ Shard 1  â”‚ Shard 2  â”‚ Shard 3  â”‚                         â”‚
â”‚  â”‚ 50GB     â”‚ 50GB     â”‚ 50GB     â”‚ 50GB     â”‚  (Balanced)             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                        â”‚
â”‚  IMPACT DURING REBALANCING:                                            â”‚
â”‚  â€¢ Increased network traffic                                           â”‚
â”‚  â€¢ Slight performance impact (queries still work)                      â”‚
â”‚  â€¢ Automatic throttling to minimize disruption                         â”‚
â”‚                                                                        â”‚
â”‚  COST IMPACT:                                                          â”‚
â”‚  Before: 2 shards Ã— $630 = $1,260/month                                â”‚
â”‚  After:  4 shards Ã— $630 = $2,520/month                                â”‚
â”‚  Increase: 100% cost for 100% capacity                                 â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Auto-Scaling

### Configuration Auto-Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AUTO-SCALING CONFIGURATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  COMPUTE AUTO-SCALING                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Triggers:                                                       â”‚ â”‚
â”‚  â”‚  â€¢ CPU > 75% sustained for 1 hour      â†’ Scale up                â”‚ â”‚
â”‚  â”‚  â€¢ Memory > 75% sustained for 1 hour   â†’ Scale up                â”‚ â”‚
â”‚  â”‚  â€¢ CPU < 50% sustained for 24 hours    â†’ Scale down              â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Configuration:                                                  â”‚ â”‚
â”‚  â”‚  â€¢ Min instance size: M40 (16GB)                                 â”‚ â”‚
â”‚  â”‚  â€¢ Max instance size: M80 (128GB)                                â”‚ â”‚
â”‚  â”‚  â€¢ Scale down enabled: Yes                                       â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Behavior:                                                       â”‚ â”‚
â”‚  â”‚  â€¢ Scales up: Within 1-2 hours of threshold                      â”‚ â”‚
â”‚  â”‚  â€¢ Scales down: After 24h below threshold (conservative)         â”‚ â”‚
â”‚  â”‚  â€¢ Brief downtime: ~30-60s per scale event                       â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  STORAGE AUTO-SCALING                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Triggers:                                                       â”‚ â”‚
â”‚  â”‚  â€¢ Disk usage > 90%                    â†’ Add storage             â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Behavior:                                                       â”‚ â”‚
â”‚  â”‚  â€¢ Adds 10-20% storage increment                                 â”‚ â”‚
â”‚  â”‚  â€¢ No downtime                                                   â”‚ â”‚
â”‚  â”‚  â€¢ Max limit: 4TB per node (configurable)                        â”‚ â”‚
â”‚  â”‚  â€¢ Cooldown: 6 hours between expansions                          â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â”‚  Cost:                                                           â”‚ â”‚
â”‚  â”‚  â€¢ $0.25/GB-month (standard)                                     â”‚ â”‚
â”‚  â”‚  â€¢ Example: +100GB = +$25/month                                  â”‚ â”‚
â”‚  â”‚                                                                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Terraform Configuration

```hcl
# Terraform: Auto-Scaling Configuration
resource "mongodbatlas_cluster" "production" {
  project_id = var.atlas_project_id
  name       = "production-autoscale"

  provider_instance_size_name = "M40"

  # Compute Auto-Scaling
  auto_scaling_compute_enabled            = true
  auto_scaling_compute_scale_down_enabled = true

  provider_auto_scaling_compute_min_instance_size = "M40"
  provider_auto_scaling_compute_max_instance_size = "M80"

  # Storage Auto-Scaling
  auto_scaling_disk_gb_enabled = true

  # Disk size will auto-expand when > 90% full
  # Starts at disk_size_gb, can grow to max
  disk_size_gb = 100  # Starting size

  # Additional settings
  provider_name         = "AWS"
  provider_region_name  = "US_EAST_1"

  replication_specs {
    num_shards = 1
    regions_config {
      region_name     = "US_EAST_1"
      electable_nodes = 3
      priority        = 7
    }
  }
}
```

### Auto-Scaling en Action

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AUTO-SCALING SCENARIO: BLACK FRIDAY                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  Timeline: 24-hour period                                             â”‚
â”‚                                                                       â”‚
â”‚  00:00 - Normal Load                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Cluster: M40 (16GB RAM)                                          â”‚ â”‚
â”‚  â”‚ CPU: 45%    Memory: 60%    Cost: $630/month ($0.88/hour)         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  08:00 - Traffic Increases                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CPU: 68%    Memory: 72%                                          â”‚ â”‚
â”‚  â”‚ Status: Monitoring, approaching threshold                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  09:00 - Threshold Exceeded                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CPU: 78%    Memory: 76%    (sustained for 1 hour)                â”‚ â”‚
â”‚  â”‚ ğŸ”” AUTO-SCALE TRIGGERED: M40 â†’ M50                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  10:00 - Scaled Up                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Cluster: M50 (32GB RAM)                                          â”‚ â”‚
â”‚  â”‚ CPU: 45%    Memory: 48%    Cost: $1,525/month ($2.12/hour)       â”‚ â”‚
â”‚  â”‚ Performance: Smooth, handling increased load                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  12:00 - Peak Black Friday Traffic                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CPU: 72%    Memory: 68%                                          â”‚ â”‚
â”‚  â”‚ Status: Within acceptable range on M50                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  18:00 - Traffic Subsiding                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CPU: 38%    Memory: 45%                                          â”‚ â”‚
â”‚  â”‚ Status: Below threshold, monitoring for 24h before scale down    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  Next Day 19:00 - Scale Down                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CPU < 50% for 24+ hours                                          â”‚ â”‚
â”‚  â”‚ ğŸ”” AUTO-SCALE TRIGGERED: M50 â†’ M40                               â”‚ â”‚
â”‚  â”‚ Back to baseline configuration                                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â”‚  COST ANALYSIS:                                                       â”‚
â”‚  â€¢ M40 baseline: 22 hours Ã— $0.88 = $19.36                            â”‚
â”‚  â€¢ M50 peak: 34 hours Ã— $2.12 = $72.08                                â”‚
â”‚  â€¢ Total: $91.44 for 56-hour period                                   â”‚
â”‚                                                                       â”‚
â”‚  vs. Always M50: 56 hours Ã— $2.12 = $118.72                           â”‚
â”‚  Savings: $27.28 (23% cost reduction)                                 â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Capacity Planning

### Sizing Formula

```python
# Capacity Planning Calculator
def calculate_required_tier(
    working_set_size_gb,
    peak_ops_per_sec,
    peak_connections,
    growth_rate_monthly=0.10,
    planning_horizon_months=12
):
    """
    Calculate required Atlas tier based on workload characteristics
    """
    # 1. Project future data size
    future_data_size = working_set_size_gb * (1 + growth_rate_monthly) ** planning_horizon_months

    # 2. RAM requirement (WSS + 50% headroom)
    required_ram = future_data_size * 1.5

    # 3. Operations requirement
    # Rule of thumb: M40 handles ~10K reads/s, 5K writes/s
    ops_factor = peak_ops_per_sec / 10000

    # 4. Connections requirement
    # Each tier supports different max connections
    connection_factor = peak_connections / 1000

    # 5. Select tier
    tiers = {
        "M10": {"ram": 2, "cost": 57},
        "M20": {"ram": 4, "cost": 140},
        "M30": {"ram": 8, "cost": 285},
        "M40": {"ram": 16, "cost": 630},
        "M50": {"ram": 32, "cost": 1525},
        "M60": {"ram": 64, "cost": 3050},
        "M80": {"ram": 128, "cost": 6480},
        "M140": {"ram": 192, "cost": 10800},
        "M200": {"ram": 256, "cost": 14400},
    }

    # Find minimum tier that satisfies requirements
    for tier_name, specs in tiers.items():
        if specs["ram"] >= required_ram:
            return {
                "recommended_tier": tier_name,
                "ram_gb": specs["ram"],
                "monthly_cost": specs["cost"],
                "future_data_size_gb": round(future_data_size, 2),
                "utilization": round(required_ram / specs["ram"] * 100, 1),
            }

    return {
        "recommended_tier": "M200+ or Sharding",
        "reason": "Data size exceeds single cluster capacity"
    }

# Example usage
result = calculate_required_tier(
    working_set_size_gb=50,      # Current: 50GB working set
    peak_ops_per_sec=5000,       # Peak: 5K ops/second
    peak_connections=500,         # Peak: 500 connections
    growth_rate_monthly=0.10,    # Growth: 10% per month
    planning_horizon_months=12   # Planning: 1 year ahead
)

print(f"Recommended Tier: {result['recommended_tier']}")
print(f"RAM: {result['ram_gb']}GB")
print(f"Monthly Cost: ${result['monthly_cost']}")
print(f"Projected Data (1yr): {result['future_data_size_gb']}GB")
print(f"Utilization: {result['utilization']}%")

# Output:
# Recommended Tier: M80
# RAM: 128GB
# Monthly Cost: $6480
# Projected Data (1yr): 157.69GB
# Utilization: 61.8%
```

### Growth Projection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GROWTH PROJECTION MODEL                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  Current State: M40 cluster, 50GB data                                 â”‚
â”‚  Growth Rate: 10% monthly                                              â”‚
â”‚                                                                        â”‚
â”‚  PROJECTION (12 MONTHS):                                               â”‚
â”‚                                                                        â”‚
â”‚  Month    Data Size    Tier Needed    Monthly Cost    Action           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  0 (Now)  50 GB        M40            $630            Current          â”‚
â”‚  3        66 GB        M50            $1,525          Scale in Q1      â”‚
â”‚  6        89 GB        M50            $1,525          OK               â”‚
â”‚  9        118 GB       M60            $3,050          Scale in Q3      â”‚
â”‚  12       157 GB       M80            $6,480          Scale in Q4      â”‚
â”‚                                                                        â”‚
â”‚  ALTERNATIVE STRATEGY: Enable Auto-Scaling                             â”‚
â”‚  â€¢ Min: M40                                                            â”‚
â”‚  â€¢ Max: M80                                                            â”‚
â”‚  â€¢ Let Atlas handle scaling automatically                              â”‚
â”‚  â€¢ Average cost: ~$3,500/month (vs. $6,480 always-on M80)              â”‚
â”‚                                                                        â”‚
â”‚  ALTERNATIVE STRATEGY 2: Shard at M60                                  â”‚
â”‚  â€¢ At month 9, implement 2-shard cluster                               â”‚
â”‚  â€¢ Cost: 2 Ã— M40 = $1,260/month (cheaper than M60!)                    â”‚
â”‚  â€¢ Better scalability path                                             â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ StratÃ©gies de Scaling

### Par Type d'Application

```yaml
# Scaling Strategies by Application Type

# 1. E-COMMERCE
e_commerce:
  characteristics:
    - High read/write ratio: 10:1
    - Spiky traffic (sales, Black Friday)
    - User-centric queries

  strategy:
    initial: "M30-M40"
    scaling_approach: "Vertical + Auto-scaling"
    auto_scale:
      min: "M30"
      max: "M60"

    shard_when: "Data > 500GB OR Tier > M60"
    shard_key: "{ userId: 1, orderId: 1 }"

  expected_cost:
    baseline: "$285-630/month"
    peak: "$3,050/month (auto-scaled to M60)"
    average: "$1,200/month"

# 2. SOCIAL MEDIA / CONTENT PLATFORM
social_media:
  characteristics:
    - Very high read ratio: 100:1
    - Large media metadata
    - User timeline queries

  strategy:
    initial: "M40"
    scaling_approach: "Read replicas + Sharding"
    read_replicas: 5  # Add analytics nodes

    shard_when: "Early (proactive)"
    shard_key: "{ authorId: 1, createdAt: -1 }"

  expected_cost:
    baseline: "$630/month (M40)"
    with_replicas: "$3,150/month (5 nodes)"
    sharded: "$2,520/month (4 shards M40)"

# 3. IOT / TIME-SERIES
iot_timeseries:
  characteristics:
    - Very high write ratio: 1:100
    - Time-based queries
    - Predictable growth

  strategy:
    initial: "M20-M30"
    scaling_approach: "Horizontal sharding (early)"

    shard_immediately: true
    shard_key: "{ deviceId: 1, timestamp: 1 }"
    num_shards: 4  # Start with 4

    data_lifecycle:
      hot_data: "7 days (Atlas cluster)"
      warm_data: "30 days (Atlas Data Lake)"
      cold_data: "1+ year (S3 Glacier)"

  expected_cost:
    baseline: "$1,140/month (4 Ã— M30)"
    with_tiering: "$600/month (reduced hot data)"

# 4. ANALYTICS / BI
analytics:
  characteristics:
    - Read-only (mostly)
    - Complex aggregations
    - Large scans

  strategy:
    initial: "M40"
    scaling_approach: "Dedicated analytics nodes"

    analytics_nodes: 2-3
    read_preference: "secondary"

    consider: "Atlas Data Lake for historical data"

  expected_cost:
    baseline: "$630/month (M40)"
    with_analytics: "$1,890/month (3 nodes M40)"

# 5. MULTI-TENANT SAAS
multi_tenant:
  characteristics:
    - Tenant isolation critical
    - Variable per-tenant usage
    - Compliance requirements

  strategy:
    initial: "M30-M40"
    scaling_approach: "Shard by tenant"

    shard_key: "{ tenantId: 1, documentId: 1 }"

    large_tenants: "Dedicated clusters"
    small_tenants: "Shared sharded cluster"

  expected_cost:
    shared_cluster: "$630-1,260/month"
    per_large_tenant: "$285-630/month"
```

---

## ğŸ“‹ Best Practices

### Scaling Checklist

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SCALING BEST PRACTICES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  MONITORING & ALERTS                                                   â”‚
â”‚  â˜ Set up alerts for CPU > 70%, Memory > 75%                           â”‚
â”‚  â˜ Monitor trends, not just absolutes                                  â”‚
â”‚  â˜ Track growth rate (weekly/monthly)                                  â”‚
â”‚  â˜ Dashboard with capacity planning metrics                            â”‚
â”‚                                                                        â”‚
â”‚  VERTICAL SCALING                                                      â”‚
â”‚  â˜ Scale proactively, not reactively                                   â”‚
â”‚  â˜ Keep 20-30% headroom for unexpected spikes                          â”‚
â”‚  â˜ Schedule scaling during low-traffic windows                         â”‚
â”‚  â˜ Test application resilience to brief disconnects                    â”‚
â”‚  â˜ Enable auto-scaling for production clusters                         â”‚
â”‚                                                                        â”‚
â”‚  HORIZONTAL SCALING (SHARDING)                                         â”‚
â”‚  â˜ Choose shard key carefully (cannot change easily)                   â”‚
â”‚  â˜ Test shard key with production query patterns                       â”‚
â”‚  â˜ Validate even data distribution before full migration               â”‚
â”‚  â˜ Start with 2-4 shards (not 1, not 10)                               â”‚
â”‚  â˜ Plan for future shard additions                                     â”‚
â”‚  â˜ Monitor balancer impact during rebalancing                          â”‚
â”‚                                                                        â”‚
â”‚  CAPACITY PLANNING                                                     â”‚
â”‚  â˜ Review capacity quarterly                                           â”‚
â”‚  â˜ Project growth 6-12 months ahead                                    â”‚
â”‚  â˜ Budget for scaling costs                                            â”‚
â”‚  â˜ Consider sharding at Tier > M60                                     â”‚
â”‚  â˜ Evaluate multi-region for DR + capacity                             â”‚
â”‚                                                                        â”‚
â”‚  COST OPTIMIZATION                                                     â”‚
â”‚  â˜ Right-size clusters (don't over-provision)                          â”‚
â”‚  â˜ Use auto-scaling to handle variability                              â”‚
â”‚  â˜ Consider reserved capacity for stable workloads                     â”‚
â”‚  â˜ Archive old data to Data Lake                                       â”‚
â”‚  â˜ Review and remove unused indexes                                    â”‚
â”‚                                                                        â”‚
â”‚  TESTING                                                               â”‚
â”‚  â˜ Load test before scaling events                                     â”‚
â”‚  â˜ Validate application handles scaling gracefully                     â”‚
â”‚  â˜ Test failover scenarios                                             â”‚
â”‚  â˜ Document scaling procedures                                         â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Anti-Patterns

```
âŒ SCALING ANTI-PATTERNS:

1. Reactive Scaling
   â€¢ Waiting until CPU hits 100%
   â€¢ Scaling during peak traffic
   â€¢ No capacity planning
   â†’ Solution: Proactive monitoring, scale at 70-80%

2. Over-Provisioning
   â€¢ Running M200 with 20% utilization
   â€¢ "Just in case" mentality
   â†’ Solution: Use auto-scaling, right-size

3. Poor Shard Key Choice
   â€¢ Using monotonic fields (timestamp, auto-increment)
   â€¢ Low cardinality fields (country, status)
   â†’ Solution: Research, test, validate distribution

4. Too Many Shards
   â€¢ Starting with 10+ shards for 100GB data
   â€¢ Each shard < 50GB
   â†’ Solution: Start small (2-4 shards), grow as needed

5. Ignoring Query Patterns
   â€¢ Sharding without analyzing queries
   â€¢ Scatter-gather on every query
   â†’ Solution: Analyze slow query log, optimize for common patterns

6. No Testing
   â€¢ Scaling in production without testing
   â€¢ Assuming application handles disconnects
   â†’ Solution: Test in staging first
```

---

## ğŸ RÃ©sumÃ©

### Points ClÃ©s

1. **Types de Scaling**
   - Vertical : Augmenter les ressources par nÅ“ud (simple, limitÃ©)
   - Horizontal : Ajouter des nÅ“uds/shards (complexe, illimitÃ©)
   - Auto-scaling : Ajustement automatique (recommandÃ©)

2. **Vertical Scaling**
   - Simple : Update tier dans Terraform
   - Downtime : ~30-60s (primary election)
   - Limite : M700 pratique max
   - CoÃ»t : Exponentiel aux tiers Ã©levÃ©s

3. **Sharding**
   - Shard key = dÃ©cision critique
   - Cardinality, distribution, query targeting
   - Start small : 2-4 shards
   - Ajouter shards = 2x capacity, 2x cost

4. **Auto-Scaling**
   - Compute : M40-M80 range recommandÃ©
   - Storage : Auto-expand Ã  90%
   - Scale up : 1h aprÃ¨s threshold
   - Scale down : 24h aprÃ¨s threshold

5. **Capacity Planning**
   - Projeter 6-12 mois
   - WSS Ã— 1.5 = RAM requis
   - ConsidÃ©rer sharding Ã  M60+
   - Budget pour croissance

### Configuration RecommandÃ©e Production

```hcl
resource "mongodbatlas_cluster" "production" {
  project_id = var.project_id
  name       = "production"

  # Start with appropriate tier
  provider_instance_size_name = "M40"

  # Enable auto-scaling
  auto_scaling_compute_enabled            = true
  auto_scaling_compute_scale_down_enabled = true
  provider_auto_scaling_compute_min_instance_size = "M40"
  provider_auto_scaling_compute_max_instance_size = "M80"

  # Storage auto-scaling
  auto_scaling_disk_gb_enabled = true
  disk_size_gb                 = 100

  # 3-node replica set (HA)
  replication_specs {
    num_shards = 1
    regions_config {
      region_name     = "US_EAST_1"
      electable_nodes = 3
      priority        = 7
    }
  }
}
```

### Decision Tree

```
Start
  â”‚
  â”œâ”€ Data < 500GB AND Tier < M60?
  â”‚  â””â”€ YES â†’ Use vertical scaling + auto-scaling
  â”‚  â””â”€ NO  â†’ â†“
  â”‚
  â”œâ”€ Write-heavy OR Data > 2TB?
  â”‚  â””â”€ YES â†’ Implement sharding
  â”‚  â””â”€ NO  â†’ â†“
  â”‚
  â”œâ”€ Read-heavy analytics?
  â”‚  â””â”€ YES â†’ Add analytics nodes OR Data Lake
  â”‚  â””â”€ NO  â†’ â†“
  â”‚
  â””â”€ Variable traffic patterns?
     â””â”€ YES â†’ Enable auto-scaling
     â””â”€ NO  â†’ Reserved capacity
```

---


â­ï¸ [Atlas Search](/14-mongodb-atlas/09-atlas-search.md)
