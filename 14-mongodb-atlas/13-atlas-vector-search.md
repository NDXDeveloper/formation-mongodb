ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 14.13 Atlas Vector Search

## Introduction

**Atlas Vector Search** transforme MongoDB en **vector database** pour les applications d'IA gÃ©nÃ©rative. Stockez des embeddings vectoriels, effectuez des recherches de similaritÃ© sÃ©mantique, et implÃ©mentez des patterns RAG (Retrieval Augmented Generation) directement dans votre base MongoDB. Plus besoin de Pinecone, Weaviate ou Qdrant : vos donnÃ©es et vos vecteurs cohabitent dans la mÃªme base, avec la mÃªme sÃ©curitÃ© et scalabilitÃ©.

### ğŸ¯ Objectifs de cette Section

- Comprendre les embeddings et la recherche vectorielle
- Configurer un index vectoriel dans Atlas
- Effectuer des recherches de similaritÃ© sÃ©mantique
- ImplÃ©menter le pattern RAG (Retrieval Augmented Generation)
- IntÃ©grer avec OpenAI, HuggingFace, Cohere
- Optimiser les performances vectorielles
- Construire des applications GenAI production-ready

---

## ğŸ§  Concepts Fondamentaux

### Qu'est-ce qu'un Embedding ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDINGS EXPLIQUÃ‰S                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  TEXTE â†’ VECTEUR NUMÃ‰RIQUE (Embedding)                                 â”‚
â”‚                                                                        â”‚
â”‚  Input:  "Le chat dort sur le canapÃ©"                                  â”‚
â”‚  â†“                                                                     â”‚
â”‚  Embedding Model (ex: OpenAI text-embedding-3-small)                   â”‚
â”‚  â†“                                                                     â”‚
â”‚  Output: [0.023, -0.891, 0.445, ..., 0.234]  (1536 dimensions)         â”‚
â”‚           â†‘                                                            â”‚
â”‚           Vecteur numÃ©rique qui capture le sens sÃ©mantique             â”‚
â”‚                                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                        â”‚
â”‚  PROPRIÃ‰TÃ‰ CLÃ‰: SimilaritÃ© SÃ©mantique                                  â”‚
â”‚                                                                        â”‚
â”‚  Textes similaires â†’ Vecteurs proches                                  â”‚
â”‚                                                                        â”‚
â”‚  "Le chat dort"           â†’ [0.02, -0.89, 0.45, ...]                   â”‚
â”‚  "Un fÃ©lin qui sommeille" â†’ [0.03, -0.88, 0.46, ...]  â† Proche!        â”‚
â”‚  "J'achÃ¨te une voiture"   â†’ [-0.65, 0.12, -0.33, ...] â† Loin           â”‚
â”‚                                                                        â”‚
â”‚  Distance vectorielle (cosinus, euclidienne) = SimilaritÃ© sÃ©mantique   â”‚
â”‚                                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                        â”‚
â”‚  DIMENSIONS TYPIQUES:                                                  â”‚
â”‚  â€¢ OpenAI text-embedding-3-small: 1536 dimensions                      â”‚
â”‚  â€¢ OpenAI text-embedding-3-large: 3072 dimensions                      â”‚
â”‚  â€¢ HuggingFace all-MiniLM-L6-v2: 384 dimensions                        â”‚
â”‚  â€¢ Cohere embed-english-v3.0: 1024 dimensions                          â”‚
â”‚                                                                        â”‚
â”‚  Plus de dimensions = Plus de nuances capturÃ©es (mais plus cher)       â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Vector Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ATLAS VECTOR SEARCH ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚
â”‚  APPLICATION
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  1. User Query: "Comment cuisiner des pÃ¢tes carbonara?"
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                               â”‚
â”‚                               â–¼
â”‚  EMBEDDING SERVICE (OpenAI, HuggingFace, etc.)
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  2. Convert query to embedding vector
â”‚  â”‚     Query â†’ [0.123, -0.456, 0.789, ..., 0.234]
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                               â”‚
â”‚                               â–¼
â”‚  ATLAS VECTOR SEARCH
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  3. Vector similarity search (k-NN)
â”‚  â”‚     Find k nearest neighbors to query vector
â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  â”‚ Vector Index (HNSW algorithm)
â”‚  â”‚  â”‚ â€¢ Hierarchical graph structure
â”‚  â”‚  â”‚ â€¢ Sub-linear search time O(log n)
â”‚  â”‚  â”‚ â€¢ Cosine similarity metric
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                               â”‚
â”‚                               â–¼
â”‚  MONGODB ATLAS CLUSTER
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚  4. Results: Documents with highest similarity
â”‚  â”‚
â”‚  â”‚  Document 1: "Recette carbonara authentique" (score: 0.92)
â”‚  â”‚  Document 2: "PÃ¢tes Ã  la carbonara rapide" (score: 0.89)
â”‚  â”‚  Document 3: "Carbonara : astuces de chef" (score: 0.85)
â”‚  â”‚
â”‚  â”‚  Each document contains:
â”‚  â”‚  â€¢ Content (text)
â”‚  â”‚  â€¢ Embedding vector (1536 dims)
â”‚  â”‚  â€¢ Metadata (category, date, etc.)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”‚  KEY ADVANTAGES:
â”‚  âœ… Unified storage (vectors + data + metadata)
â”‚  âœ… Single database (no separate vector DB)
â”‚  âœ… Hybrid search (vector + metadata filters)
â”‚  âœ… ACID transactions
â”‚  âœ… Same security, scaling, backups as MongoDB
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration Vector Index

### Structure de Document

```javascript
// Collection: articles
// Chaque document contient texte + embedding

{
  "_id": ObjectId("..."),

  // Contenu original
  "title": "Guide complet des pÃ¢tes italiennes",
  "content": "Les pÃ¢tes sont un pilier de la cuisine italienne...",
  "category": "cuisine",
  "author": "Chef Mario",
  "publishedAt": ISODate("2024-11-15"),

  // Embedding vectoriel (gÃ©nÃ©rÃ© par OpenAI, HuggingFace, etc.)
  "embedding": [
    0.0234, -0.8912, 0.4456, 0.7823, -0.2341, 0.5678,
    // ... 1536 dimensions au total
    0.8901, 0.2345, -0.6789, 0.1234
  ],

  // MÃ©tadonnÃ©es pour filtrage hybride
  "tags": ["italien", "recette", "pÃ¢tes"],
  "difficulty": "facile",
  "preparationTime": 30
}
```

### CrÃ©ation d'Index Vectoriel

```javascript
// Configuration Vector Search Index via Atlas UI ou API

{
  "name": "vector_index",
  "type": "vectorSearch",

  "fields": [
    {
      "type": "vector",
      "path": "embedding",           // Champ contenant le vecteur
      "numDimensions": 1536,          // Dimensions (OpenAI small = 1536)
      "similarity": "cosine"          // MÃ©trique: cosine, euclidean, dotProduct
    },

    // Champs supplÃ©mentaires pour filtrage hybride (optionnel)
    {
      "type": "filter",
      "path": "category"
    },
    {
      "type": "filter",
      "path": "publishedAt"
    }
  ]
}
```

### Configuration via MongoDB Shell

```javascript
// CrÃ©er l'index vectoriel

db.articles.createSearchIndex(
  "vector_index",
  "vectorSearch",
  {
    fields: [
      {
        type: "vector",
        path: "embedding",
        numDimensions: 1536,
        similarity: "cosine"
      }
    ]
  }
);

// VÃ©rifier la crÃ©ation
db.articles.listSearchIndexes();
```

---

## ğŸ” RequÃªtes Vector Search

### Recherche Basique de SimilaritÃ©

```javascript
// Ã‰TAPE 1: GÃ©nÃ©rer embedding pour la requÃªte
const queryText = "Comment faire des pÃ¢tes carbonara?";

// Appel API OpenAI (ou autre service)
const embeddingResponse = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: queryText
});

const queryVector = embeddingResponse.data[0].embedding;
// queryVector = [0.123, -0.456, ..., 0.789] (1536 dimensions)

// Ã‰TAPE 2: Recherche vectorielle dans MongoDB
const results = await db.articles.aggregate([
  {
    $vectorSearch: {
      index: "vector_index",           // Nom de l'index
      path: "embedding",                // Champ vectoriel
      queryVector: queryVector,         // Vecteur de la requÃªte
      numCandidates: 100,               // Candidates Ã  examiner
      limit: 10                         // Top 10 rÃ©sultats
    }
  },
  {
    $project: {
      title: 1,
      content: 1,
      category: 1,
      score: { $meta: "vectorSearchScore" }  // Score de similaritÃ©
    }
  }
]).toArray();

// RÃ©sultats triÃ©s par similaritÃ©
console.log(results);
/*
[
  {
    _id: ObjectId("..."),
    title: "Carbonara authentique: recette italienne",
    content: "La vraie carbonara se fait avec...",
    category: "cuisine",
    score: 0.923  // TrÃ¨s similaire!
  },
  {
    _id: ObjectId("..."),
    title: "PÃ¢tes italiennes: le guide complet",
    content: "Les pÃ¢tes sont un pilier...",
    category: "cuisine",
    score: 0.887
  },
  ...
]
*/
```

### Recherche Hybride (Vector + Filtres)

```javascript
// Combinaison: SimilaritÃ© vectorielle + Filtres mÃ©tadonnÃ©es

const results = await db.articles.aggregate([
  {
    $vectorSearch: {
      index: "vector_index",
      path: "embedding",
      queryVector: queryVector,
      numCandidates: 200,
      limit: 10,

      // Filtres additionnels (pre-filter)
      filter: {
        category: "cuisine",
        difficulty: "facile",
        preparationTime: { $lte: 30 }
      }
    }
  },
  {
    $project: {
      title: 1,
      content: 1,
      difficulty: 1,
      preparationTime: 1,
      score: { $meta: "vectorSearchScore" }
    }
  }
]).toArray();

// RÃ©sultats: Recettes de pÃ¢tes SIMILAIRES + Faciles + Rapides
```

### Fonction Helper RÃ©utilisable

```javascript
// Helper function pour recherche vectorielle

async function vectorSearch(
  collection,
  queryText,
  options = {}
) {
  const {
    indexName = "vector_index",
    limit = 10,
    numCandidates = 100,
    filter = {},
    embeddingModel = "text-embedding-3-small"
  } = options;

  // 1. GÃ©nÃ©rer embedding
  const embeddingResponse = await openai.embeddings.create({
    model: embeddingModel,
    input: queryText
  });

  const queryVector = embeddingResponse.data[0].embedding;

  // 2. Recherche vectorielle
  const pipeline = [
    {
      $vectorSearch: {
        index: indexName,
        path: "embedding",
        queryVector: queryVector,
        numCandidates: numCandidates,
        limit: limit,
        ...(Object.keys(filter).length > 0 && { filter })
      }
    },
    {
      $project: {
        _id: 1,
        title: 1,
        content: 1,
        metadata: 1,
        score: { $meta: "vectorSearchScore" }
      }
    }
  ];

  return await collection.aggregate(pipeline).toArray();
}

// Usage
const results = await vectorSearch(
  db.collection("articles"),
  "recettes italiennes rapides",
  {
    limit: 5,
    filter: { category: "cuisine", difficulty: "facile" }
  }
);
```

---

## ğŸ¤– Pattern RAG (Retrieval Augmented Generation)

### Architecture RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RAG PATTERN ARCHITECTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚
â”‚  RAG = Retrieval Augmented Generation
â”‚  AmÃ©liore les LLMs avec votre propre base de connaissances
â”‚
â”‚  WORKFLOW:
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
â”‚  1. USER QUERY
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ "Quel est le processus de remboursement de votre entreprise?"    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                               â”‚
â”‚                               â–¼
â”‚  2. VECTOR SEARCH (Retrieval)
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ Query â†’ Embedding â†’ Vector Search
â”‚  â”‚
â”‚  â”‚ Top 3 documents pertinents:
â”‚  â”‚ â€¢ "Politique de remboursement" (score: 0.94)
â”‚  â”‚ â€¢ "Guide remboursement employÃ©s" (score: 0.89)
â”‚  â”‚ â€¢ "FAQ finance" (score: 0.82)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                               â”‚
â”‚                               â–¼
â”‚  3. CONTEXT ASSEMBLY
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ Construct prompt avec contexte:
â”‚  â”‚
â”‚  â”‚ System: "Tu es un assistant qui rÃ©pond en te basant
â”‚  â”‚          uniquement sur le contexte fourni."
â”‚  â”‚
â”‚  â”‚ Context: [Documents retrieved]
â”‚  â”‚
â”‚  â”‚ Question: [User query]
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                               â”‚
â”‚                               â–¼
â”‚  4. LLM GENERATION
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ OpenAI GPT-4 / Claude / etc.
â”‚  â”‚
â”‚  â”‚ GÃ©nÃ¨re rÃ©ponse basÃ©e sur le contexte rÃ©cupÃ©rÃ©
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                               â”‚
â”‚                               â–¼
â”‚  5. RESPONSE
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ "Pour obtenir un remboursement, suivez ces Ã©tapes:
â”‚  â”‚  1. Soumettez une demande via le portail employÃ©
â”‚  â”‚  2. Joignez les reÃ§us justificatifs
â”‚  â”‚  3. Validation sous 48h par votre manager
â”‚  â”‚  4. Paiement sous 5 jours ouvrÃ©s
â”‚  â”‚
â”‚  â”‚  Source: Politique de remboursement (page 12)"
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”‚  AVANTAGES RAG:
â”‚  âœ… RÃ©ponses basÃ©es sur VOS donnÃ©es (pas hallucinations)
â”‚  âœ… Toujours Ã  jour (nouveaux docs = nouvelles rÃ©ponses)
â”‚  âœ… Sources traÃ§ables (citations)
â”‚  âœ… Pas de fine-tuning LLM requis
â”‚  âœ… Cost-effective
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ImplÃ©mentation RAG ComplÃ¨te

```javascript
// RAG Implementation avec Atlas Vector Search + OpenAI

import { MongoClient } from "mongodb";
import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const client = new MongoClient(process.env.MONGODB_URI);

async function ragQuery(userQuestion) {
  try {
    await client.connect();
    const db = client.db("knowledge_base");
    const collection = db.collection("documents");

    // Ã‰TAPE 1: Generate embedding pour la question
    console.log("Generating query embedding...");
    const embeddingResponse = await openai.embeddings.create({
      model: "text-embedding-3-small",
      input: userQuestion
    });
    const queryVector = embeddingResponse.data[0].embedding;

    // Ã‰TAPE 2: Vector search pour documents pertinents
    console.log("Searching for relevant documents...");
    const relevantDocs = await collection.aggregate([
      {
        $vectorSearch: {
          index: "vector_index",
          path: "embedding",
          queryVector: queryVector,
          numCandidates: 100,
          limit: 3  // Top 3 documents les plus pertinents
        }
      },
      {
        $project: {
          title: 1,
          content: 1,
          metadata: 1,
          score: { $meta: "vectorSearchScore" }
        }
      }
    ]).toArray();

    console.log(`Found ${relevantDocs.length} relevant documents`);

    // Ã‰TAPE 3: Construire contexte
    const context = relevantDocs
      .map((doc, i) => `Document ${i + 1}: ${doc.title}\n${doc.content}`)
      .join("\n\n---\n\n");

    // Ã‰TAPE 4: Construire prompt pour LLM
    const systemPrompt = `Tu es un assistant intelligent qui rÃ©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.
Si la rÃ©ponse n'est pas dans le contexte, dis "Je ne trouve pas cette information dans les documents disponibles."
Cite toujours tes sources en mentionnant le numÃ©ro du document.`;

    const userPrompt = `Contexte:
${context}

Question: ${userQuestion}

RÃ©ponds de maniÃ¨re claire et concise, en citant tes sources.`;

    // Ã‰TAPE 5: Appel LLM
    console.log("Generating answer with LLM...");
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt }
      ],
      temperature: 0.3  // Bas = plus factuel
    });

    const answer = completion.choices[0].message.content;

    // Ã‰TAPE 6: Retourner rÃ©ponse + sources
    return {
      answer: answer,
      sources: relevantDocs.map(doc => ({
        title: doc.title,
        score: doc.score,
        metadata: doc.metadata
      }))
    };

  } finally {
    await client.close();
  }
}

// Usage
const result = await ragQuery(
  "Quelle est la politique de remboursement de l'entreprise?"
);

console.log("Answer:", result.answer);
console.log("\nSources:");
result.sources.forEach((source, i) => {
  console.log(`${i + 1}. ${source.title} (relevance: ${source.score.toFixed(2)})`);
});

/* Output exemple:
Answer: Pour obtenir un remboursement, vous devez suivre ces Ã©tapes selon le Document 1 (Politique de remboursement):
1. Soumettez une demande via le portail employÃ© avec justificatifs
2. Validation par votre manager sous 48h
3. Paiement sous 5 jours ouvrÃ©s maximum

Les plafonds sont dÃ©taillÃ©s dans le Document 2, avec 50â‚¬/jour pour les repas et 100â‚¬/nuit pour l'hÃ©bergement.

Sources:
1. Politique de remboursement employÃ©s (relevance: 0.94)
2. Guide frais professionnels (relevance: 0.89)
3. FAQ finance (relevance: 0.82)
*/
```

---

## ğŸ¯ Cas d'Usage AvancÃ©s

### 1. Semantic Search (Moteur de Recherche)

```javascript
// Use case: Moteur de recherche intelligent pour documentation

async function semanticSearch(query, filters = {}) {
  // Contrairement Ã  la recherche textuelle classique,
  // semantic search comprend l'INTENTION

  // Query: "rÃ©parer bug serveur crashÃ©"
  // Trouve aussi: "rÃ©soudre problÃ¨me dÃ©marrage", "serveur ne rÃ©pond pas"
  // Car sÃ©mantiquement similaire!

  const embedding = await generateEmbedding(query);

  return await db.documentation.aggregate([
    {
      $vectorSearch: {
        index: "semantic_index",
        path: "content_embedding",
        queryVector: embedding,
        numCandidates: 200,
        limit: 20,
        filter: filters
      }
    },
    {
      $project: {
        title: 1,
        summary: 1,
        url: 1,
        category: 1,
        score: { $meta: "vectorSearchScore" },
        // Highlight du contenu pertinent
        snippet: { $substr: ["$content", 0, 200] }
      }
    }
  ]).toArray();
}
```

### 2. Recommendation System

```javascript
// Use case: Recommandations de produits basÃ©es sur similaritÃ©

async function recommendSimilarProducts(productId, limit = 5) {
  // Trouver le produit source
  const product = await db.products.findOne({ _id: productId });

  if (!product || !product.embedding) {
    throw new Error("Product not found or missing embedding");
  }

  // Chercher produits similaires
  const similar = await db.products.aggregate([
    {
      $vectorSearch: {
        index: "product_vector_index",
        path: "embedding",
        queryVector: product.embedding,
        numCandidates: 100,
        limit: limit + 1  // +1 car le produit lui-mÃªme sera dans les rÃ©sultats
      }
    },
    {
      $match: {
        _id: { $ne: productId }  // Exclure le produit source
      }
    },
    {
      $project: {
        name: 1,
        description: 1,
        price: 1,
        category: 1,
        image: 1,
        similarity: { $meta: "vectorSearchScore" }
      }
    },
    {
      $limit: limit
    }
  ]).toArray();

  return similar;
}

// Frontend usage
const recommendations = await recommendSimilarProducts("prod_12345", 4);
// Affiche "Produits similaires" sur la page produit
```

### 3. Chatbot avec MÃ©moire Conversationnelle

```javascript
// Use case: Chatbot intelligent avec contexte

class RAGChatbot {
  constructor(db, knowledgeCollection) {
    this.db = db;
    this.collection = knowledgeCollection;
    this.conversationHistory = [];
  }

  async chat(userMessage) {
    // Ajouter message utilisateur Ã  l'historique
    this.conversationHistory.push({
      role: "user",
      content: userMessage
    });

    // Chercher contexte pertinent
    const embedding = await generateEmbedding(userMessage);
    const relevantDocs = await this.collection.aggregate([
      {
        $vectorSearch: {
          index: "knowledge_index",
          path: "embedding",
          queryVector: embedding,
          numCandidates: 50,
          limit: 2
        }
      }
    ]).toArray();

    // Construire prompt avec historique + contexte
    const context = relevantDocs
      .map(doc => doc.content)
      .join("\n\n");

    const messages = [
      {
        role: "system",
        content: `Tu es un assistant qui aide les utilisateurs.
Contexte disponible:
${context}

RÃ©ponds en te basant sur ce contexte et l'historique de conversation.`
      },
      ...this.conversationHistory
    ];

    // Appel LLM
    const completion = await openai.chat.completions.create({
      model: "gpt-4o",
      messages: messages
    });

    const assistantMessage = completion.choices[0].message.content;

    // Ajouter rÃ©ponse Ã  l'historique
    this.conversationHistory.push({
      role: "assistant",
      content: assistantMessage
    });

    return {
      message: assistantMessage,
      sources: relevantDocs.map(d => d.title)
    };
  }

  clearHistory() {
    this.conversationHistory = [];
  }
}

// Usage
const chatbot = new RAGChatbot(db, db.collection("knowledge"));

await chatbot.chat("Comment fonctionne votre API?");
// â†’ RÃ©ponse avec contexte

await chatbot.chat("Et quel est le rate limit?");
// â†’ Comprend que "rate limit" se rÃ©fÃ¨re Ã  l'API
//   grÃ¢ce Ã  l'historique conversationnel
```

### 4. Duplicate Detection

```javascript
// Use case: DÃ©tection de doublons sÃ©mantiques

async function findDuplicates(newDocument, threshold = 0.95) {
  // GÃ©nÃ©rer embedding du nouveau document
  const embedding = await generateEmbedding(newDocument.content);

  // Chercher documents trÃ¨s similaires
  const potentialDuplicates = await db.articles.aggregate([
    {
      $vectorSearch: {
        index: "content_vector_index",
        path: "embedding",
        queryVector: embedding,
        numCandidates: 100,
        limit: 10
      }
    },
    {
      $match: {
        score: { $gte: threshold }  // SimilaritÃ© > 95%
      }
    },
    {
      $project: {
        title: 1,
        content: 1,
        publishedAt: 1,
        similarity: { $meta: "vectorSearchScore" }
      }
    }
  ]).toArray();

  if (potentialDuplicates.length > 0) {
    console.log(`âš ï¸ Warning: Found ${potentialDuplicates.length} potential duplicates`);
    return potentialDuplicates;
  }

  return [];
}

// Avant d'insÃ©rer un nouvel article
const duplicates = await findDuplicates(newArticle);
if (duplicates.length > 0) {
  // Alerter l'Ã©diteur ou fusionner automatiquement
}
```

---

## ğŸš€ Performance et Optimisation

### Choix du Nombre de Dimensions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EMBEDDING DIMENSIONS TRADE-OFFS
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
â”‚  MODEL                         DIMS    QUALITY   SPEED    COST
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  text-embedding-3-small        1536   â­â­â­     â­â­â­â­   $
â”‚  text-embedding-3-large        3072   â­â­â­â­â­   â­â­â­     $$
â”‚  all-MiniLM-L6-v2 (HF)         384    â­â­       â­â­â­â­â­ Free
â”‚  Cohere embed-english-v3       1024   â­â­â­â­    â­â­â­     $
â”‚
â”‚  RECOMMANDATIONS:
â”‚  â€¢ Prototype/MVP: all-MiniLM-L6-v2 (gratuit, rapide)
â”‚  â€¢ Production gÃ©nÃ©rale: text-embedding-3-small (bon Ã©quilibre)
â”‚  â€¢ Haute prÃ©cision: text-embedding-3-large (meilleure qualitÃ©)
â”‚
â”‚  IMPACT STORAGE:
â”‚  â€¢ 1 million docs Ã— 1536 dims Ã— 4 bytes = ~6 GB
â”‚  â€¢ 1 million docs Ã— 3072 dims Ã— 4 bytes = ~12 GB
â”‚  â€¢ 1 million docs Ã— 384 dims Ã— 4 bytes = ~1.5 GB
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Optimisation de numCandidates

```javascript
// numCandidates = nombre de candidats examinÃ©s avant sÃ©lection finale

// âŒ MAUVAIS: Trop bas (qualitÃ© mÃ©diocre)
{
  $vectorSearch: {
    queryVector: embedding,
    numCandidates: 20,  // âŒ Trop peu
    limit: 10
  }
}
// ProblÃ¨me: Peut manquer de bons rÃ©sultats

// âœ… BON: Ã‰quilibre qualitÃ©/performance
{
  $vectorSearch: {
    queryVector: embedding,
    numCandidates: 100,  // âœ… Good: 10x limit
    limit: 10
  }
}

// âš ï¸ ACCEPTABLE: Plus de qualitÃ©, moins de perf
{
  $vectorSearch: {
    queryVector: embedding,
    numCandidates: 500,  // âš ï¸ 50x limit = plus lent
    limit: 10
  }
}

// RÃˆGLE GÃ‰NÃ‰RALE:
// numCandidates = limit Ã— 10 (bon Ã©quilibre)
// numCandidates = limit Ã— 50 (haute prÃ©cision)
```

### StratÃ©gies de Chunking

```javascript
// Pour longs documents: dÃ©couper en chunks

function chunkDocument(document, chunkSize = 500, overlap = 50) {
  const words = document.content.split(' ');
  const chunks = [];

  for (let i = 0; i < words.length; i += chunkSize - overlap) {
    const chunk = words.slice(i, i + chunkSize).join(' ');

    chunks.push({
      documentId: document._id,
      chunkIndex: chunks.length,
      content: chunk,
      metadata: {
        title: document.title,
        author: document.author,
        // ... mÃ©tadonnÃ©es hÃ©ritÃ©es
      }
    });
  }

  return chunks;
}

// GÃ©nÃ©rer embeddings pour chaque chunk
async function embedChunks(chunks) {
  const embeddings = await Promise.all(
    chunks.map(chunk => generateEmbedding(chunk.content))
  );

  return chunks.map((chunk, i) => ({
    ...chunk,
    embedding: embeddings[i]
  }));
}

// InsÃ©rer chunks avec embeddings
const chunks = chunkDocument(longDocument);
const chunksWithEmbeddings = await embedChunks(chunks);
await db.document_chunks.insertMany(chunksWithEmbeddings);

// AVANTAGES:
// âœ… Meilleure granularitÃ©
// âœ… Recherche plus prÃ©cise
// âœ… Moins de tokens LLM (context plus ciblÃ©)
```

---

## ğŸ“‹ Best Practices

### Checklist Production

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VECTOR SEARCH PRODUCTION CHECKLIST                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  INDEX CONFIGURATION                                                  â”‚
â”‚  â˜ Choisir nombre de dimensions appropriÃ© (balance qualitÃ©/coÃ»t)      â”‚
â”‚  â˜ MÃ©trique de similaritÃ©: cosine (gÃ©nÃ©ral), euclidean (numÃ©rique)    â”‚
â”‚  â˜ CrÃ©er index sur champs mÃ©tadonnÃ©es pour filtrage hybride           â”‚
â”‚  â˜ Tester index avec donnÃ©es production-scale                         â”‚
â”‚                                                                       â”‚
â”‚  EMBEDDINGS GENERATION                                                â”‚
â”‚  â˜ Utiliser mÃªme modÃ¨le pour indexation et recherche                  â”‚
â”‚  â˜ Normaliser texte avant embedding (lowercase, trim)                 â”‚
â”‚  â˜ ImplÃ©menter rate limiting pour API embeddings                      â”‚
â”‚  â˜ Cacher embeddings (Ã©viter regÃ©nÃ©ration)                            â”‚
â”‚  â˜ Batch processing pour gÃ©nÃ©ration bulk                              â”‚
â”‚                                                                       â”‚
â”‚  QUERIES                                                              â”‚
â”‚  â˜ numCandidates â‰¥ limit Ã— 10 (balance qualitÃ©/perf)                  â”‚
â”‚  â˜ Utiliser filtres mÃ©tadonnÃ©es pour rÃ©duire espace recherche         â”‚
â”‚  â˜ ImplÃ©menter retry logic (API failures)                             â”‚
â”‚  â˜ Monitorer latence queries (target < 200ms)                         â”‚
â”‚                                                                       â”‚
â”‚  RAG IMPLEMENTATION                                                   â”‚
â”‚  â˜ Valider contexte avant envoi au LLM (pertinence)                   â”‚
â”‚  â˜ Limiter longueur contexte (Ã©viter dÃ©passement token limit)         â”‚
â”‚  â˜ Citer sources dans rÃ©ponses (traÃ§abilitÃ©)                          â”‚
â”‚  â˜ ImplÃ©menter fallback si aucun document pertinent                   â”‚
â”‚  â˜ Logger queries pour amÃ©lioration continue                          â”‚
â”‚                                                                       â”‚
â”‚  SECURITY                                                             â”‚
â”‚  â˜ Ne pas exposer embeddings cÃ´tÃ© client                              â”‚
â”‚  â˜ Filtrer rÃ©sultats selon permissions utilisateur                    â”‚
â”‚  â˜ Valider/sanitizer inputs                                           â”‚
â”‚  â˜ Rate limiting sur endpoints                                        â”‚
â”‚                                                                       â”‚
â”‚  MONITORING                                                           â”‚
â”‚  â˜ Tracker coÃ»ts API embeddings                                       â”‚
â”‚  â˜ Monitorer latence vector search                                    â”‚
â”‚  â˜ Analyser qualitÃ© rÃ©sultats (user feedback)                         â”‚
â”‚  â˜ Alertes sur erreurs embeddings/search                              â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ RÃ©sumÃ©

### Points ClÃ©s

1. **Embeddings**
   - Vecteurs numÃ©riques capturant sens sÃ©mantique
   - 384-3072 dimensions selon modÃ¨le
   - SimilaritÃ© vectorielle = similaritÃ© sÃ©mantique

2. **Vector Search**
   - Recherche k-NN (k plus proches voisins)
   - Algorithme HNSW (sub-linear performance)
   - MÃ©triques: cosine, euclidean, dotProduct

3. **RAG Pattern**
   - Retrieval (vector search) + Generation (LLM)
   - RÃ©ponses basÃ©es sur VOS donnÃ©es
   - Pas d'hallucinations
   - Sources traÃ§ables

4. **Use Cases**
   - Semantic search (moteurs recherche)
   - Recommendations (produits similaires)
   - Chatbots intelligents
   - Duplicate detection

5. **Performance**
   - numCandidates = limit Ã— 10 (Ã©quilibre)
   - Chunking pour longs documents
   - Filtrage hybride (vector + metadata)
   - Caching embeddings

### Configuration Minimale

```javascript
// 1. Structure document
{
  content: "Votre texte...",
  embedding: [0.023, -0.891, ...],  // 1536 dims
  metadata: { category: "X" }
}

// 2. Index vectoriel
db.collection.createSearchIndex("vector_index", "vectorSearch", {
  fields: [{
    type: "vector",
    path: "embedding",
    numDimensions: 1536,
    similarity: "cosine"
  }]
});

// 3. Recherche
const embedding = await generateEmbedding(query);
db.collection.aggregate([
  {
    $vectorSearch: {
      index: "vector_index",
      path: "embedding",
      queryVector: embedding,
      numCandidates: 100,
      limit: 10
    }
  }
]);
```

### Ressources

- [Atlas Vector Search Docs](https://www.mongodb.com/docs/atlas/atlas-vector-search/)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [RAG Best Practices](https://www.mongodb.com/developer/products/atlas/rag-best-practices/)

---


â­ï¸ [Triggers et fonctions serverless](/14-mongodb-atlas/14-triggers-fonctions-serverless.md)
