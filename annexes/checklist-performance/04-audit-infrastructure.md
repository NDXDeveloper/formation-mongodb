üîù Retour au [Sommaire](/SOMMAIRE.md)

# E.4 - Audit d'Infrastructure

## Introduction

L'audit d'infrastructure √©value les **ressources mat√©rielles, la configuration syst√®me et l'architecture** de votre d√©ploiement MongoDB. Une infrastructure bien dimensionn√©e et configur√©e est essentielle pour des performances optimales et une haute disponibilit√©.

### üéØ Objectif

V√©rifier que l'infrastructure supporte efficacement la charge actuelle et future, et qu'elle est configur√©e selon les meilleures pratiques MongoDB.

### ‚è±Ô∏è Dur√©e estim√©e
- Audit rapide : 45 minutes - 1 heure
- Audit complet : 3-5 heures

---

## Ressources Syst√®me

### üíª CPU (Processeur)

#### ‚úÖ Checklist CPU

| Point de v√©rification | Valeur Cible | Priorit√© | Action |
|----------------------|--------------|----------|--------|
| Utilisation moyenne | < 70% | üü† | Dimensionnement |
| Utilisation en pointe | < 85% | üü† | Planifier scaling |
| I/O Wait | < 10% | üî¥ | Probl√®me disque |
| Steal (VM) | < 5% | üü† | Ressources partag√©es |
| Nombre de c≈ìurs | ‚â• 4 (prod) | üü° | Performance |

**Commandes de v√©rification** :
```bash
# Utilisation CPU en temps r√©el
top -b -n 1 | head -20

# Vue d√©taill√©e
htop

# Utilisation moyenne sur 1 minute
uptime

# Statistiques CPU d√©taill√©es
mpstat 1 5

# I/O Wait
iostat -x 1 5

# Pour VM : v√©rifier steal time
top
# Regarder la ligne "%Cpu(s)" -> "st" (steal time)
```

**Analyse** :
```bash
# Script de monitoring CPU
#!/bin/bash
echo "=== Audit CPU ==="
echo "Load Average:"
uptime | awk -F'load average:' '{print $2}'

echo -e "\nCPU Usage:"
top -bn1 | grep "Cpu(s)" | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print "CPU Usage: " 100 - $1"%"}'

echo -e "\nI/O Wait:"
iostat -c | awk 'NR==4 {print "I/O Wait: "$4"%"}'

echo -e "\nCores:"
nproc
```

**Sympt√¥mes de probl√®me** :
```markdown
‚ö†Ô∏è CPU > 80% constant ‚Üí Sous-dimensionnement
‚ö†Ô∏è I/O Wait > 20% ‚Üí Disque trop lent
‚ö†Ô∏è Load average > 2x nombre de c≈ìurs ‚Üí Surcharge
‚ö†Ô∏è Steal time > 10% (VM) ‚Üí Contention ressources host
```

**Actions correctives** :
- Scale vertical (plus de CPU)
- Scale horizontal (sharding)
- Optimiser les requ√™tes lentes
- V√©rifier les process non-MongoDB

---

### üíæ RAM (M√©moire)

#### ‚úÖ Checklist RAM

| Point de v√©rification | Valeur Cible | Priorit√© | Action |
|----------------------|--------------|----------|--------|
| RAM totale | ‚â• 8 GB (prod) | üî¥ | Minimum absolu |
| Utilisation RAM | < 80% | üü† | √âviter swap |
| Working Set < RAM | Oui | üî¥ | Performance critique |
| Index size < RAM | Oui | üî¥ | Performance critique |
| Swap utilis√© | 0 ou minimal | üî¥ | Performance |
| Cache WiredTiger | 50% RAM ou config | üü° | Optimisation |

**Commandes de v√©rification** :
```bash
# RAM totale et disponible
free -h

# D√©tail de l'utilisation
cat /proc/meminfo | grep -E 'MemTotal|MemFree|MemAvailable|Cached|Buffers|SwapTotal|SwapFree'

# Swap actif
swapon -s

# Monitoring continu
vmstat 1 5
```

**V√©rifications MongoDB** :
```javascript
// Statistiques m√©moire MongoDB
db.serverStatus().mem

// R√©sultat :
{
  bits: 64,
  resident: 2048,     // RAM utilis√©e par MongoDB (Mo)
  virtual: 4096,      // M√©moire virtuelle
  supported: true
}

// Working Set
db.serverStatus().wiredTiger.cache

// Taille des index
db.collection.stats().indexSizes

// Total index de la DB
let totalIndexSize = 0;
db.getCollectionNames().forEach(coll => {
  totalIndexSize += db[coll].stats().totalIndexSize;
});
print("Total index size: " + (totalIndexSize / 1024 / 1024).toFixed(2) + " Mo");
```

**Calcul du Working Set** :
```javascript
function calculateWorkingSet() {
  const stats = db.serverStatus().wiredTiger.cache;

  print("=== Working Set Analysis ===");
  print("Bytes in cache: " + (stats["bytes currently in the cache"] / 1024 / 1024).toFixed(2) + " Mo");
  print("Max cache size: " + (stats["maximum bytes configured"] / 1024 / 1024).toFixed(2) + " Mo");
  print("Pages read into cache: " + stats["pages read into cache"]);
  print("Pages written from cache: " + stats["pages written from cache"]);

  const ratio = (stats["bytes currently in the cache"] / stats["maximum bytes configured"]) * 100;
  print("Cache utilization: " + ratio.toFixed(1) + "%");

  if (ratio > 95) {
    print("‚ö†Ô∏è  Cache presque plein - Working Set > RAM");
  }
}

calculateWorkingSet();
```

**Sympt√¥mes de probl√®me** :
```markdown
‚ö†Ô∏è Swap utilis√© > 100 Mo ‚Üí Performance d√©grad√©e
‚ö†Ô∏è RAM disponible < 20% ‚Üí Risque de swap
‚ö†Ô∏è Cache hit ratio < 90% ‚Üí Working Set > RAM
‚ö†Ô∏è Page faults fr√©quents ‚Üí Donn√©es swapp√©es
```

**Actions correctives** :
- Augmenter la RAM
- R√©duire la taille du working set (archivage)
- Optimiser les index
- D√©sactiver le swap (`swapoff -a`)
- Ajuster cache WiredTiger

---

### üíø Disque (Stockage)

#### ‚úÖ Checklist Disque

| Point de v√©rification | Valeur Cible | Priorit√© | Action |
|----------------------|--------------|----------|--------|
| Type de disque | SSD/NVMe | üü† | Performance |
| IOPS disponibles | > 3000 (SSD) | üü† | Charge I/O |
| Latence disque | < 10ms | üü† | Performance |
| Espace libre | > 20% | üî¥ | √âviter saturation |
| Syst√®me de fichiers | XFS ou ext4 | üü° | Recommand√© |
| Point de montage s√©par√© | Oui | üü° | Isolation |

**Commandes de v√©rification** :
```bash
# Espace disque
df -h

# Inodes (important)
df -i

# Type de disque et partitions
lsblk
fdisk -l

# Syst√®me de fichiers
mount | grep mongo

# Performance I/O
iostat -x 1 5
# Regarder : %util, await, r/s, w/s

# Test performance disque
fio --name=random-write --ioengine=libaio --iodepth=32 --rw=randwrite --bs=4k --direct=1 --size=1G --numjobs=4 --runtime=60 --time_based --group_reporting

# Latence disque
ioping /path/to/mongodb/data
```

**Analyse I/O MongoDB** :
```javascript
// Statistiques I/O
db.serverStatus().wiredTiger.concurrentTransactions

// Background flush
db.serverStatus().wiredTiger.log

// Data file statistics
db.serverStatus().wiredTiger["data-handle"]
```

**Calcul de l'espace n√©cessaire** :
```javascript
function estimateStorage() {
  let total = 0;

  db.getCollectionNames().forEach(coll => {
    const stats = db[coll].stats();
    total += stats.size + stats.totalIndexSize;
  });

  const current = total / 1024 / 1024 / 1024;  // Go
  const withJournal = current * 1.1;  // +10% pour journal
  const withOplog = withJournal * 1.05;  // +5% pour oplog
  const recommended = withOplog * 1.5;  // +50% buffer

  print("=== Estimation Stockage ===");
  print("Donn√©es actuelles: " + current.toFixed(2) + " Go");
  print("Avec journal: " + withJournal.toFixed(2) + " Go");
  print("Avec oplog: " + withOplog.toFixed(2) + " Go");
  print("Recommand√© (buffer 50%): " + recommended.toFixed(2) + " Go");
}

estimateStorage();
```

**Sympt√¥mes de probl√®me** :
```markdown
‚ö†Ô∏è Espace libre < 10% ‚Üí Critique
‚ö†Ô∏è %util > 80% constant ‚Üí Disque satur√©
‚ö†Ô∏è await > 50ms ‚Üí Latence √©lev√©e
‚ö†Ô∏è IOPS < 1000 (HDD) ‚Üí Trop lent
‚ö†Ô∏è Inodes < 10% libre ‚Üí Risque saturation
```

**Actions correctives** :
- Migrer vers SSD/NVMe
- Ajouter du stockage
- Archiver anciennes donn√©es
- Compression WiredTiger
- Sharding pour distribuer

**Configuration recommand√©e** :
```bash
# XFS (recommand√©)
mkfs.xfs -f /dev/sdb1
mount -o noatime,nodiratime /dev/sdb1 /var/lib/mongodb

# Ajout dans /etc/fstab
/dev/sdb1 /var/lib/mongodb xfs noatime,nodiratime 0 0

# D√©sactiver le readahead
blockdev --setra 32 /dev/sdb1

# V√©rifier
blockdev --getra /dev/sdb1
```

---

### üåê R√©seau

#### ‚úÖ Checklist R√©seau

| Point de v√©rification | Valeur Cible | Priorit√© | Action |
|----------------------|--------------|----------|--------|
| Latence intra-cluster | < 1ms | üü† | M√™me datacenter |
| Bande passante | ‚â• 1 Gbps | üü† | Performance |
| Connexions actives | < 80% max | üü† | Pool sizing |
| Firewall configur√© | Oui | üî¥ | S√©curit√© |
| Latence client-serveur | < 50ms | üü° | UX |

**Commandes de v√©rification** :
```bash
# Latence r√©seau
ping -c 10 mongodb-server

# Entre membres replica set
ping -c 10 replica-member-2

# Bande passante
iftop
# ou
nload

# Connexions MongoDB
netstat -an | grep 27017 | wc -l

# Connexions √©tablies
ss -s

# Test latence avec nc
time echo "test" | nc mongodb-server 27017

# MTU (taille paquet)
ip link show | grep mtu
```

**V√©rifications MongoDB** :
```javascript
// Connexions actives
db.serverStatus().connections

// R√©sultat :
{
  current: 52,        // Connexions actuelles
  available: 51148,   // Connexions disponibles
  totalCreated: 1234  // Total cr√©√© depuis d√©marrage
}

// Limite de connexions
db.adminCommand({ getParameter: 1, maxIncomingConnections: 1 })

// Statistiques r√©seau
db.serverStatus().network
{
  bytesIn: NumberLong("..."),
  bytesOut: NumberLong("..."),
  numRequests: NumberLong("...")
}
```

**Configuration des connexions** :
```yaml
# mongod.conf
net:
  port: 27017
  bindIp: 0.0.0.0  # ‚ö†Ô∏è S√©curiser avec firewall
  maxIncomingConnections: 65536

  # Compression r√©seau (MongoDB 3.4+)
  compression:
    compressors: snappy,zlib,zstd
```

**Connection Pooling (application)** :
```javascript
// Node.js example
const client = new MongoClient(uri, {
  maxPoolSize: 100,        // Max connexions dans le pool
  minPoolSize: 10,         // Min connexions maintenues
  maxIdleTimeMS: 30000,    // Timeout connexion idle
  waitQueueTimeoutMS: 5000 // Timeout attente connexion
});
```

**Sympt√¥mes de probl√®me** :
```markdown
‚ö†Ô∏è Latence > 10ms intra-cluster ‚Üí Probl√®me r√©seau
‚ö†Ô∏è Connexions > 90% du max ‚Üí Augmenter limite
‚ö†Ô∏è Connexions croissantes ‚Üí Fuite connexions app
‚ö†Ô∏è Bytes In/Out d√©s√©quilibr√©s ‚Üí V√©rifier pattern
‚ö†Ô∏è Packet loss > 0.1% ‚Üí Probl√®me r√©seau
```

**Actions correctives** :
- Augmenter maxIncomingConnections
- Optimiser connection pooling app
- V√©rifier configuration r√©seau
- Utiliser compression r√©seau
- Placer membres dans m√™me zone

---

## Configuration MongoDB

### ‚öôÔ∏è Configuration WiredTiger

#### ‚úÖ Checklist WiredTiger

| Param√®tre | Valeur Recommand√©e | Description |
|-----------|-------------------|-------------|
| **cacheSizeGB** | 50-60% RAM | Cache m√©moire |
| **directoryForIndexes** | true | Index s√©par√©s |
| **journalCompressor** | snappy | Compression journal |
| **collectionConfig.blockCompressor** | snappy | Compression donn√©es |

**Configuration fichier** :
```yaml
# mongod.conf
storage:
  dbPath: /var/lib/mongodb
  journal:
    enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 2  # 50% de 4GB RAM
      journalCompressor: snappy
      directoryForIndexes: true
    collectionConfig:
      blockCompressor: snappy
    indexConfig:
      prefixCompression: true
```

**V√©rification runtime** :
```javascript
// Configuration actuelle
db.serverStatus().wiredTiger.cache

// Ajuster le cache (sans red√©marrage)
db.adminCommand({
  setParameter: 1,
  wiredTigerEngineRuntimeConfig: "cache_size=2GB"
})

// Statistiques de compression
db.collection.stats().wiredTiger
```

**Calcul cache optimal** :
```javascript
function recommendCacheSize() {
  const totalRAM = db.serverStatus().mem.resident;
  const indexSize = /* calculer total des index */;

  // Formule : 50% RAM ou (Index Size + Working Set), au plus grand
  const fiftyPercent = totalRAM * 0.5;
  const recommended = Math.max(fiftyPercent, indexSize * 1.2);

  print("RAM totale: " + totalRAM + " Mo");
  print("50% RAM: " + fiftyPercent + " Mo");
  print("Taille index: " + indexSize + " Mo");
  print("Cache recommand√©: " + recommended + " Mo");
}
```

---

### ‚öôÔ∏è Param√®tres Syst√®me (OS)

#### ‚úÖ Checklist OS

| Param√®tre | Valeur Recommand√©e | Priorit√© |
|-----------|-------------------|----------|
| **Transparent Huge Pages** | disabled | üî¥ |
| **NUMA** | interleave (si multi-socket) | üü† |
| **ulimit (files)** | ‚â• 64000 | üî¥ |
| **ulimit (processes)** | ‚â• 64000 | üî¥ |
| **Swappiness** | 1 | üü† |
| **TCP keepalive** | Configur√© | üü° |

**V√©rifications** :
```bash
# Transparent Huge Pages (doit √™tre disabled)
cat /sys/kernel/mm/transparent_hugepage/enabled
cat /sys/kernel/mm/transparent_hugepage/defrag

# D√©sactiver THP
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/defrag

# Permanent (ajouter √† /etc/rc.local)
cat << 'EOF' > /etc/init.d/disable-transparent-hugepages
#!/bin/bash
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
EOF

chmod 755 /etc/init.d/disable-transparent-hugepages
update-rc.d disable-transparent-hugepages defaults

# ulimit
ulimit -a

# Configurer dans /etc/security/limits.conf
mongod soft nofile 64000
mongod hard nofile 64000
mongod soft nproc 64000
mongod hard nproc 64000

# Swappiness
cat /proc/sys/vm/swappiness
# D√©finir √† 1
echo 1 > /proc/sys/vm/swappiness
# Permanent dans /etc/sysctl.conf
vm.swappiness = 1

# TCP keepalive
cat /proc/sys/net/ipv4/tcp_keepalive_time
# Recommand√© : 120
sysctl -w net.ipv4.tcp_keepalive_time=120
```

**Script de v√©rification** :
```bash
#!/bin/bash
echo "=== MongoDB System Configuration Audit ==="

echo -e "\n1. Transparent Huge Pages:"
cat /sys/kernel/mm/transparent_hugepage/enabled | grep -o "\[.*\]"

echo -e "\n2. ulimit (nofile):"
ulimit -n

echo -e "\n3. Swappiness:"
cat /proc/sys/vm/swappiness

echo -e "\n4. TCP Keepalive:"
cat /proc/sys/net/ipv4/tcp_keepalive_time

echo -e "\n5. NUMA (si applicable):"
numactl --hardware 2>/dev/null || echo "Non applicable"

echo -e "\n=== Recommendations ==="
[[ $(cat /sys/kernel/mm/transparent_hugepage/enabled) != *"[never]"* ]] && echo "‚ö†Ô∏è  Disable Transparent Huge Pages"
[[ $(ulimit -n) -lt 64000 ]] && echo "‚ö†Ô∏è  Increase ulimit nofile to 64000"
[[ $(cat /proc/sys/vm/swappiness) -gt 1 ]] && echo "‚ö†Ô∏è  Set swappiness to 1"
```

---

## Architecture Distribu√©e

### üîÑ Replica Set

#### ‚úÖ Checklist Replica Set

| Point de v√©rification | Recommandation | Priorit√© |
|----------------------|----------------|----------|
| Nombre de membres | 3 (min) ou 5 | üî¥ |
| Nombre impair | Oui (ou arbiter) | üî¥ |
| Distribution g√©ographique | Multi-AZ/Multi-DC | üü† |
| Priority configur√©es | Oui | üü° |
| Hidden members | Pour backup/analytics | üü° |
| Oplog size | ‚â• 5% du dataset | üü† |

**V√©rifications** :
```javascript
// Status du replica set
rs.status()

// Configuration
rs.conf()

// Membre actuel
db.isMaster()

// Oplog
db.getReplicationInfo()
{
  logSizeMB: 1024,
  usedMB: 512,
  timeDiff: 3600,  // Secondes de donn√©es dans oplog
  tFirst: ISODate("..."),
  tLast: ISODate("..."),
  now: ISODate("...")
}

// Replication lag
rs.printSlaveReplicationInfo()
```

**Oplog sizing** :
```javascript
// V√©rifier la taille actuelle
db.getReplicationInfo()

// Changer la taille (n√©cessite red√©marrage du membre)
// 1. Red√©marrer en standalone
// 2. Changer la taille
db.adminCommand({ replSetResizeOplog: 1, size: 2048 })  // 2 Go
```

**Configuration optimale** :
```javascript
// Configuration type pour 3 membres
cfg = rs.conf()
cfg.members = [
  {
    _id: 0,
    host: "mongo1.example.com:27017",
    priority: 2  // Primary pr√©f√©r√©
  },
  {
    _id: 1,
    host: "mongo2.example.com:27017",
    priority: 1
  },
  {
    _id: 2,
    host: "mongo3.example.com:27017",
    priority: 1
  }
]
cfg.settings = {
  chainingAllowed: true,
  heartbeatTimeoutSecs: 10,
  electionTimeoutMillis: 10000
}
rs.reconfig(cfg)
```

**Sympt√¥mes de probl√®me** :
```markdown
‚ö†Ô∏è Lag secondaires > 10 secondes ‚Üí Performance
‚ö†Ô∏è Oplog < 1 heure de donn√©es ‚Üí Trop petit
‚ö†Ô∏è √âlections fr√©quentes ‚Üí Probl√®me r√©seau
‚ö†Ô∏è Membres DOWN ‚Üí Haute disponibilit√© compromise
‚ö†Ô∏è Oplog wrapping ‚Üí Resync n√©cessaire
```

---

### üìä Sharded Cluster

#### ‚úÖ Checklist Sharding

| Point de v√©rification | Recommandation | Priorit√© |
|----------------------|----------------|----------|
| Shard key choisie | Cardinalit√© √©lev√©e | üî¥ |
| Distribution √©quilibr√©e | Oui | üü† |
| Jumbo chunks | Aucun | üî¥ |
| Config servers | 3 (CSRS) | üî¥ |
| Mongos instances | ‚â• 2 | üü† |
| Balancer actif | Oui (fen√™tre d√©finie) | üü° |

**V√©rifications** :
```javascript
// Status g√©n√©ral
sh.status()

// Distribution des chunks
db.getSiblingDB("config").chunks.aggregate([
  { $group: { _id: "$shard", count: { $sum: 1 } } },
  { $sort: { count: -1 } }
])

// Jumbo chunks
db.getSiblingDB("config").chunks.find({ jumbo: true })

// Balancer status
sh.getBalancerState()
sh.isBalancerRunning()

// Statistiques par shard
db.printShardingStatus()

// Config servers
sh.status().configsvr
```

**Analyse de la shard key** :
```javascript
function analyzeShardKey(namespace) {
  const [dbName, collName] = namespace.split('.');
  const coll = db.getSiblingDB(dbName).getCollection(collName);

  print("=== Analyse Shard Key : " + namespace + " ===");

  // R√©cup√©rer la shard key
  const collInfo = db.getSiblingDB("config").collections.findOne({
    _id: namespace
  });

  if (!collInfo) {
    print("Collection non shard√©e");
    return;
  }

  const shardKey = collInfo.key;
  print("Shard Key: " + JSON.stringify(shardKey));

  // Distribution
  const chunks = db.getSiblingDB("config").chunks.aggregate([
    { $match: { ns: namespace } },
    { $group: {
        _id: "$shard",
        chunks: { $sum: 1 },
        jumbo: { $sum: { $cond: ["$jumbo", 1, 0] } }
      }
    }
  ]).toArray();

  print("\nDistribution des chunks:");
  chunks.forEach(s => {
    print("  " + s._id + ": " + s.chunks + " chunks" +
          (s.jumbo > 0 ? " (‚ö†Ô∏è " + s.jumbo + " jumbo)" : ""));
  });

  // Cardinalit√©
  const keyField = Object.keys(shardKey)[0];
  const cardinality = coll.distinct(keyField).length;
  print("\nCardinalit√© (" + keyField + "): " + cardinality);
}

analyzeShardKey("mydb.users");
```

**Configuration du balancer** :
```javascript
// D√©sactiver pendant maintenance
sh.stopBalancer()

// R√©activer
sh.startBalancer()

// D√©finir fen√™tre de balancing
db.getSiblingDB("config").settings.update(
  { _id: "balancer" },
  {
    $set: {
      activeWindow: {
        start: "23:00",  // 11 PM
        stop: "06:00"    // 6 AM
      }
    }
  },
  { upsert: true }
)

// V√©rifier
sh.getBalancerWindow()
```

**Sympt√¥mes de probl√®me** :
```markdown
‚ö†Ô∏è Distribution d√©s√©quilibr√©e (ratio > 2x) ‚Üí Mauvaise shard key
‚ö†Ô∏è Jumbo chunks pr√©sents ‚Üí Split manuel n√©cessaire
‚ö†Ô∏è Balancer constamment actif ‚Üí Shard key probl√©matique
‚ö†Ô∏è Migrations lentes ‚Üí Jumbo chunks ou r√©seau
‚ö†Ô∏è Shard unique surcharg√© ‚Üí Redistribution n√©cessaire
```

---

## Monitoring et Alertes

### üìà M√©triques √† Surveiller

#### ‚úÖ Checklist Monitoring

| Cat√©gorie | M√©triques | Fr√©quence | Outil |
|-----------|-----------|-----------|-------|
| **Syst√®me** | CPU, RAM, Disque, R√©seau | 1 min | Prometheus, Datadog |
| **MongoDB** | Ops/sec, Latence, Connexions | 1 min | mongostat, Atlas |
| **R√©plication** | Lag, Oplog, Elections | 5 min | rs.status() |
| **Sharding** | Distribution, Migrations | 10 min | sh.status() |
| **S√©curit√©** | Auth failures, Connections | 5 min | Logs |

**Outils essentiels** :
```bash
# mongostat - Vue d'ensemble
mongostat --host localhost:27017 --rowcount 10

# mongotop - Top collections
mongotop --host localhost:27017 10

# Logs en temps r√©el
tail -f /var/log/mongodb/mongod.log

# Avec grep pour erreurs
tail -f /var/log/mongodb/mongod.log | grep -i error
```

**Configuration Prometheus** :
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongodb-exporter:9216']

# mongodb-exporter
docker run -d \
  --name mongodb-exporter \
  -p 9216:9216 \
  percona/mongodb_exporter:0.40 \
  --mongodb.uri=mongodb://user:pass@localhost:27017
```

**Grafana Dashboard** :
```markdown
Dashboards recommand√©s :
- MongoDB Overview (ID: 2583)
- MongoDB Exporter (ID: 7353)
- WiredTiger (ID: 12079)
```

---

### üö® Alertes Critiques

#### Configuration des Seuils

```yaml
# Exemple AlertManager
groups:
  - name: mongodb_alerts
    rules:
      # CPU
      - alert: HighCPU
        expr: cpu_usage > 80
        for: 5m
        annotations:
          summary: "High CPU on {{ $labels.instance }}"

      # RAM
      - alert: LowMemory
        expr: memory_available_percent < 20
        for: 5m

      # Disque
      - alert: DiskSpaceLow
        expr: disk_free_percent < 20
        for: 5m

      # R√©plication lag
      - alert: ReplicationLag
        expr: mongodb_replset_member_replication_lag > 10
        for: 5m

      # Connexions
      - alert: HighConnections
        expr: mongodb_connections_current / mongodb_connections_available > 0.8
        for: 5m
```

---

## Dimensionnement

### üìä Calcul des Ressources

#### RAM Sizing

```
RAM n√©cessaire = Working Set + Index Size + Buffer

Formule d√©taill√©e :
RAM = (Donn√©es actives * 1.2) + (Total index * 1.1) + 2 GB (OS + buffer)

Minimum absolu : 8 GB
Recommand√© production : 16-32 GB
Enterprise / Volume √©lev√© : 64-128 GB+
```

#### CPU Sizing

```
Calcul bas√© sur :
- Ops/seconde attendues
- Complexit√© des requ√™tes
- Agr√©gations

Guide :
< 1000 ops/sec : 4 cores
1000-5000 ops/sec : 8 cores
5000-20000 ops/sec : 16 cores
> 20000 ops/sec : 32+ cores ou sharding
```

#### Disque Sizing

```
Capacit√© = (Donn√©es + Index) * 1.5 * Croissance

Exemple :
Donn√©es actuelles : 100 GB
Index : 20 GB
Croissance 12 mois : 2x
Buffer s√©curit√© : 50%

Calcul : (100 + 20) * 2 * 1.5 = 360 GB
```

**Script de recommandation** :
```javascript
function recommendInfrastructure() {
  // Collecter les stats
  let totalData = 0;
  let totalIndex = 0;

  db.getCollectionNames().forEach(coll => {
    const stats = db[coll].stats();
    totalData += stats.size;
    totalIndex += stats.totalIndexSize;
  });

  const dataGB = totalData / 1024 / 1024 / 1024;
  const indexGB = totalIndex / 1024 / 1024 / 1024;
  const ops = db.serverStatus().opcounters;

  print("=== Recommandations Infrastructure ===");
  print("\nDonn√©es actuelles: " + dataGB.toFixed(2) + " GB");
  print("Index: " + indexGB.toFixed(2) + " GB");

  // RAM
  const workingSet = (dataGB * 0.3) + indexGB;  // 30% donn√©es actives
  const ramRec = Math.max(16, workingSet * 1.2);
  print("\nRAM recommand√©e: " + Math.ceil(ramRec) + " GB");

  // CPU
  const totalOps = ops.insert + ops.query + ops.update + ops.delete;
  let cpuRec = 4;
  if (totalOps > 1000) cpuRec = 8;
  if (totalOps > 5000) cpuRec = 16;
  if (totalOps > 20000) cpuRec = 32;
  print("CPU recommand√©: " + cpuRec + " cores");

  // Disque
  const diskRec = (dataGB + indexGB) * 2 * 1.5;  // 2x croissance, 1.5x buffer
  print("Disque recommand√©: " + Math.ceil(diskRec) + " GB SSD");
}

recommendInfrastructure();
```

---

## Checklist par Environnement

### üß™ D√©veloppement

```markdown
‚ñ° Ressources minimales (2-4 GB RAM, 2 cores)
‚ñ° Standalone ou RS 1 membre
‚ñ° Pas de haute disponibilit√© requise
‚ñ° D√©sactiver authentication (optionnel)
‚ñ° Logging verbose pour debug
‚ñ° Profiler niveau 2 acceptable
```

### üß™ Staging/QA

```markdown
‚ñ° Configuration proche de production
‚ñ° Replica Set 3 membres
‚ñ° M√™me version MongoDB que prod
‚ñ° Donn√©es anonymis√©es de production
‚ñ° Monitoring basique
‚ñ° Backup r√©guliers
```

### üè≠ Production

```markdown
‚ñ° Replica Set 3-5 membres (minimum)
‚ñ° Multi-AZ ou Multi-DC
‚ñ° Authentication activ√©e
‚ñ° TLS/SSL configur√©
‚ñ° Monitoring complet + alertes
‚ñ° Backup automatis√©s + tests restauration
‚ñ° Oplog ‚â• 24h de donn√©es
‚ñ° Ressources dimensionn√©es + 50% buffer
‚ñ° Documentation √† jour
‚ñ° Proc√©dures disaster recovery
```

---

## Actions Prioritaires

### üî¥ Critique - √Ä corriger imm√©diatement

```markdown
‚ñ° Transparent Huge Pages activ√©
‚ñ° RAM < 8 GB en production
‚ñ° Disque plein > 90%
‚ñ° Swap utilis√© > 1 GB
‚ñ° Replica Set < 3 membres en prod
‚ñ° Pas de monitoring
‚ñ° Pas de backup
‚ñ° S√©curit√© non configur√©e
```

### üü† Important - √Ä planifier sous 2 semaines

```markdown
‚ñ° CPU > 80% constant
‚ñ° RAM utilis√©e > 80%
‚ñ° Disque > 80%
‚ñ° I/O Wait > 20%
‚ñ° Replication lag > 30 secondes
‚ñ° Jumbo chunks pr√©sents
‚ñ° ulimits non configur√©s
‚ñ° Distribution sharding d√©s√©quilibr√©e (ratio > 2x)
```

### üü° Mod√©r√© - √Ä am√©liorer progressivement

```markdown
‚ñ° CPU > 70%
‚ñ° Pas de members hidden pour backup
‚ñ° Oplog < 5% du dataset
‚ñ° Balancer sans fen√™tre d√©finie
‚ñ° Configuration syst√®me non optimis√©e
‚ñ° Monitoring partiel
‚ñ° Documentation incompl√®te
```

---

## Template de Rapport d'Audit

```markdown
# Rapport d'Audit d'Infrastructure
**Date** : [DATE]
**Environnement** : [DEV/STAGING/PROD]
**Auditeur** : [NOM]

## R√©sum√© Ex√©cutif
- Architecture : [Standalone/RS/Sharded]
- Membres : [NOMBRE]
- Probl√®mes critiques : X
- Recommandations prioritaires : X

## Ressources Syst√®me

### Serveur 1 : [HOSTNAME]
| Ressource | Actuel | Recommand√© | Statut |
|-----------|--------|------------|--------|
| CPU | X cores, Y% util | Z cores | üü¢/üü°/üî¥ |
| RAM | X GB (Y% util) | Z GB | üü¢/üü°/üî¥ |
| Disque | X GB (Y% libre) | Z GB | üü¢/üü°/üî¥ |
| IOPS | X | Y | üü¢/üü°/üî¥ |

[R√©p√©ter pour chaque serveur]

## Configuration MongoDB

### WiredTiger
- Cache Size : X GB (Y% RAM)
- Compression : [TYPE]
- Journal : [ENABLED/DISABLED]

### Replica Set
- Membres : X
- Oplog Size : X GB (Y heures)
- Replication Lag : X secondes

### Sharding (si applicable)
- Shards : X
- Distribution : [BALANCED/UNBALANCED]
- Jumbo Chunks : X

## Probl√®mes Identifi√©s

### Critiques üî¥
1. [PROBL√àME]
   - Impact : [DESCRIPTION]
   - Action : [IMM√âDIATE]

### Importants üü†
1. [PROBL√àME]
   - Impact : [DESCRIPTION]
   - Action : [SOUS 2 SEMAINES]

## Recommandations

### Court terme (< 1 semaine)
1. D√©sactiver Transparent Huge Pages
2. Augmenter RAM de X √† Y GB
3. Configurer monitoring et alertes

### Moyen terme (1-4 semaines)
1. Migrer vers SSD
2. Ajouter membre hidden
3. Optimiser configuration OS

### Long terme (> 1 mois)
1. Planifier sharding
2. Multi-r√©gion pour DR
3. Migration Atlas

## Plan d'Action
| Action | Priorit√© | Responsable | Deadline |
|--------|----------|-------------|----------|
| [ACTION 1] | üî¥ | [NOM] | [DATE] |
| [ACTION 2] | üü† | [NOM] | [DATE] |

## Co√ªts Estim√©s
- Infrastructure : [MONTANT]
- Migration : [MONTANT]
- Formation : [MONTANT]
**Total** : [MONTANT]

## Annexes
- Captures screenshots
- Outputs de commandes
- Scripts utilis√©s
```

---

## Scripts Utilitaires

### Script Complet d'Audit

```bash
#!/bin/bash
# audit_infrastructure.sh

echo "========================================="
echo "AUDIT INFRASTRUCTURE MONGODB"
echo "Date: $(date)"
echo "========================================="

echo -e "\n### SYST√àME ###"
echo "OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)"
echo "Kernel: $(uname -r)"
echo "Uptime: $(uptime -p)"

echo -e "\n### CPU ###"
echo "Cores: $(nproc)"
echo "Load: $(uptime | awk -F'load average:' '{print $2}')"
top -bn1 | head -5

echo -e "\n### RAM ###"
free -h

echo -e "\n### DISQUE ###"
df -h | grep -v tmpfs

echo -e "\n### R√âSEAU ###"
echo "Connections MongoDB:"
netstat -an | grep 27017 | wc -l

echo -e "\n### CONFIGURATION OS ###"
echo "Transparent Huge Pages:"
cat /sys/kernel/mm/transparent_hugepage/enabled
echo "Swappiness:"
cat /proc/sys/vm/swappiness
echo "ulimit:"
ulimit -n

echo -e "\n### MONGODB ###"
mongo --quiet --eval "
  print('Version: ' + db.version());
  print('Uptime: ' + db.serverStatus().uptime + 's');
  var mem = db.serverStatus().mem;
  print('Memory - Resident: ' + mem.resident + 'MB');
  print('Connections: ' + db.serverStatus().connections.current + '/' + db.serverStatus().connections.available);
  if (rs.status().ok) {
    print('Replica Set: ' + rs.status().set);
    print('Members: ' + rs.status().members.length);
  }
"

echo -e "\n========================================="
echo "FIN AUDIT"
echo "========================================="
```

---

## Ressources Compl√©mentaires

### Documentation Officielle
- [Production Notes](https://www.mongodb.com/docs/manual/administration/production-notes/)
- [Production Checklist](https://www.mongodb.com/docs/manual/administration/production-checklist-operations/)
- [Hardware and OS Configuration](https://www.mongodb.com/docs/manual/administration/production-notes/#hardware-considerations)

### Guides Avanc√©s
- [Performance Best Practices](https://www.mongodb.com/docs/manual/administration/analyzing-mongodb-performance/)
- [Capacity Planning](https://www.mongodb.com/docs/manual/core/capacity-planning/)

### Outils
- **MongoDB Atlas** : Monitoring et alertes int√©gr√©s
- **Ops Manager** : Monitoring on-premise
- **Prometheus + Grafana** : Monitoring open-source
- **Percona Monitoring** : Suite compl√®te

---


‚è≠Ô∏è [Docker et Docker Compose](/annexes/docker-compose/README.md)
