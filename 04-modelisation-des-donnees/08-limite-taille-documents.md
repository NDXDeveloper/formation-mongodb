üîù Retour au [Sommaire](/SOMMAIRE.md)

# 4.8 Limite de taille des documents (16 Mo)

## Introduction

L'une des contraintes fondamentales de MongoDB est la **limite de 16 Mo par document**. Cette limite, loin d'√™tre arbitraire, est une d√©cision d'architecture qui influence profond√©ment la fa√ßon dont vous devez mod√©liser vos donn√©es.

Comprendre cette limite, savoir pourquoi elle existe, comment la mesurer et comment concevoir vos sch√©mas pour l'√©viter est essentiel pour construire des applications MongoDB robustes et performantes.

Dans ce chapitre, nous allons explorer en d√©tail cette contrainte, ses implications pratiques et les strat√©gies pour la g√©rer efficacement.

---

## La limite de 16 Mo : qu'est-ce que c'est ?

### D√©finition pr√©cise

MongoDB impose une **taille maximale de 16 777 216 octets (16 Mo)** pour un document BSON unique. Cette limite s'applique √† :

- **Le document complet** incluant tous ses champs et sous-documents
- **Les tableaux imbriqu√©s** avec tous leurs √©l√©ments
- **Les m√©tadonn√©es BSON** (noms de champs, types, etc.)

**Important :** Cette limite concerne la repr√©sentation **BSON** (Binary JSON) du document, pas la repr√©sentation JSON. Le BSON ajoute des m√©tadonn√©es qui augmentent l√©g√®rement la taille.

### Conversion des unit√©s

Pour mieux comprendre :

```
16 Mo = 16 777 216 octets
     = 16 384 Ko
     = 16 Mo
```

**En pratique :**
- Environ **16 millions de caract√®res** en texte brut
- Environ **2 000 √† 4 000 pages** de texte
- Environ **500 √† 1 000 documents Word** moyens
- Environ **50 000 √† 100 000 petits objets JSON**

---

## Pourquoi cette limite existe-t-elle ?

### 1. Performance de la m√©moire

**Raison principale :** MongoDB charge les documents complets en m√©moire RAM pour les traiter.

```
Document de 16 Mo √ó 1000 requ√™tes simultan√©es = 16 Go de RAM
```

Si les documents √©taient illimit√©s, quelques requ√™tes simultan√©es pourraient **saturer toute la m√©moire** du serveur.

### 2. Performance du r√©seau

**Transfert de donn√©es :**

```
Document de 100 Mo √ó r√©seau 1 Gbps = 0.8 secondes de transfert minimum
Document de 16 Mo √ó r√©seau 1 Gbps = 0.13 secondes de transfert
```

Des documents plus grands :
- Augmentent la **latence r√©seau**
- Consomment plus de **bande passante**
- Ralentissent les **op√©rations distribu√©es** (r√©plication, sharding)

### 3. Atomicit√© et performances d'√©criture

MongoDB garantit l'**atomicit√© au niveau du document**. Chaque modification de document doit :

1. √ätre √©crite dans le journal (write-ahead log)
2. √ätre r√©pliqu√©e vers les secondaires
3. Potentiellement r√©organiser le document sur le disque

Plus le document est grand :
- Plus ces op√©rations sont **co√ªteuses**
- Plus le risque de **fragmentation** est √©lev√©
- Plus la **journalisation** est volumineuse

### 4. Design encourag√©

Cette limite **encourage les bonnes pratiques** :
- Normalisation quand n√©cessaire
- Utilisation de r√©f√©rences pour les relations one-to-many volumineuses
- S√©paration des pr√©occupations
- √âviter l'accumulation infinie de donn√©es

---

## Comment mesurer la taille d'un document ?

### M√©thode 1 : Object.bsonsize() dans mongosh

```javascript
// V√©rifier la taille d'un document
const doc = db.articles.findOne({ _id: ObjectId("...") })
const tailleBSON = Object.bsonsize(doc)

print(`Taille du document : ${tailleBSON} octets`)
print(`Taille en Ko : ${(tailleBSON / 1024).toFixed(2)} Ko`)
print(`Taille en Mo : ${(tailleBSON / 1024 / 1024).toFixed(2)} Mo`)

// Calculer le pourcentage de la limite
const pourcentage = (tailleBSON / 16777216 * 100).toFixed(2)
print(`Utilisation : ${pourcentage}% de la limite`)
```

**Exemple de sortie :**
```
Taille du document : 2457600 octets
Taille en Ko : 2400.00 Ko
Taille en Mo : 2.34 Mo
Utilisation : 14.65% de la limite
```

### M√©thode 2 : Trouver les plus gros documents

```javascript
// Trouver les 10 documents les plus volumineux
db.articles.find().forEach(doc => {
  const size = Object.bsonsize(doc)
  if (size > 1024 * 1024) {  // Plus de 1 Mo
    print(`Document ${doc._id} : ${(size / 1024 / 1024).toFixed(2)} Mo`)
  }
})
```

### M√©thode 3 : Agr√©gation pour analyser

```javascript
// Statistiques sur la taille des documents
db.articles.aggregate([
  {
    $project: {
      taille: { $bsonSize: "$$ROOT" }  // Taille du document complet
    }
  },
  {
    $group: {
      _id: null,
      tailleMoyenne: { $avg: "$taille" },
      tailleMax: { $max: "$taille" },
      tailleMin: { $min: "$taille" }
    }
  }
])
```

**R√©sultat exemple :**
```javascript
{
  _id: null,
  tailleMoyenne: 125430,      // ~122 Ko en moyenne
  tailleMax: 8945231,         // ~8.5 Mo maximum
  tailleMin: 512              // 512 octets minimum
}
```

### M√©thode 4 : Analyser un champ sp√©cifique

```javascript
// Taille d'un tableau sp√©cifique
db.articles.aggregate([
  {
    $project: {
      titre: 1,
      nombreCommentaires: { $size: "$commentaires" },
      tailleCommentaires: { $bsonSize: "$commentaires" }
    }
  },
  { $sort: { tailleCommentaires: -1 } },
  { $limit: 10 }
])
```

---

## Quand la limite devient-elle un probl√®me ?

### Sc√©narios √† risque

#### 1. Collections avec tableaux croissants

**Probl√®me typique :** Tableaux qui accumulent sans limite.

```javascript
// ‚ö†Ô∏è DANGER : Tableau de commentaires qui grandit ind√©finiment
{
  _id: ObjectId("..."),
  titre: "Article viral",
  contenu: "...",
  commentaires: [
    // Commence avec 10 commentaires (5 Ko)
    // Apr√®s 1 an : 5 000 commentaires (2.5 Mo)
    // Apr√®s 5 ans : 25 000 commentaires (12.5 Mo) ‚Üê Proche de la limite !
    // Apr√®s 7 ans : 35 000 commentaires ‚Üí ‚ùå ERREUR 16 Mo !
  ]
}
```

**Signes d'alerte :**
- Tableaux dans `posts`, `commentaires`, `likes`, `vues`, `historique`
- Collections d'√©v√©nements temporels sans limite
- Logs ou m√©triques imbriqu√©s

#### 2. Donn√©es binaires volumineuses

**Probl√®me :** Stocker directement des fichiers dans les documents.

```javascript
// ‚ö†Ô∏è DANGER : Image encod√©e en base64
{
  _id: ObjectId("..."),
  titre: "Photo de profil",
  imageBase64: "iVBORw0KGgoAAAANSUhEUgAA..."  // ‚Üê 5 Mo en base64 !
}

// ‚ö†Ô∏è DANGER : Document PDF
{
  _id: ObjectId("..."),
  titre: "Contrat",
  pdfData: BinData(0, "JVBERi0xLjQKJ...")  // ‚Üê 20 Mo ‚Üí IMPOSSIBLE !
}
```

**R√®gle :** Ne **jamais** stocker de fichiers volumineux directement dans les documents.

#### 3. Historiques complets

**Probl√®me :** Conserver tout l'historique dans le document.

```javascript
// ‚ö†Ô∏è DANGER : Historique de modifications illimit√©
{
  _id: ObjectId("..."),
  titre: "Document collaboratif",
  contenu: "Contenu actuel...",
  historique: [
    { date: ISODate("2020-01-01"), auteur: "user1", contenu: "Version 1..." },
    { date: ISODate("2020-01-02"), auteur: "user2", contenu: "Version 2..." },
    // ... 10 000 r√©visions plus tard ‚Üí 14 Mo !
  ]
}
```

#### 4. Imbrication profonde

**Probl√®me :** Structures tr√®s imbriqu√©es avec beaucoup de donn√©es.

```javascript
// ‚ö†Ô∏è DANGER : Catalogue produit avec toutes les variantes
{
  _id: ObjectId("..."),
  nom: "T-shirt",
  variantes: [
    {
      taille: "S",
      couleurs: [
        {
          nom: "Rouge",
          images: [ /* 50 images haute r√©solution */ ],
          stock: { /* d√©tails par entrep√¥t */ }
        }
        // √ó 20 couleurs √ó 10 tailles = 200 variantes d√©taill√©es
      ]
    }
  ]
}
```

---

## Solutions pour g√©rer les documents volumineux

### Solution 1 : Utiliser des r√©f√©rences (Child-Referencing)

**Au lieu de tout imbriquer, s√©parer en collections.**

#### Avant (anti-pattern) :

```javascript
{
  _id: ObjectId("..."),
  titre: "Article",
  commentaires: [ /* 50 000 commentaires ‚Üí 15 Mo */ ]
}
```

#### Apr√®s (solution) :

```javascript
// Collection "articles"
{
  _id: ObjectId("..."),
  titre: "Article",
  nombreCommentaires: 50000,  // Compteur pour affichage
  datePublication: ISODate("2024-01-15")
}

// Collection "commentaires" (s√©par√©e)
{
  _id: ObjectId("..."),
  articleId: ObjectId("..."),
  auteur: "Sophie",
  texte: "Excellent article !",
  date: ISODate("2024-01-16")
}

// Index pour performance
db.commentaires.createIndex({ articleId: 1, date: -1 })
```

**Avantages :**
- ‚úÖ Pas de limite sur le nombre de commentaires
- ‚úÖ Pagination facile
- ‚úÖ Performance maintenue

---

### Solution 2 : Pattern Subset (Top N)

**Imbriquer seulement les N √©l√©ments les plus importants.**

```javascript
{
  _id: ObjectId("..."),
  titre: "Produit",
  avisRecents: [  // ‚Üê Seulement les 10 derniers avis
    { auteur: "Jean", note: 5, texte: "Excellent !" },
    { auteur: "Marie", note: 4, texte: "Tr√®s bien" }
    // ... 8 autres avis
  ],
  nombreAvisTotal: 5234,
  statistiques: {
    noteMoyenne: 4.3,
    distribution: { 5: 3200, 4: 1500, 3: 400, 2: 100, 1: 34 }
  }
}

// Collection "avis" (tous les avis)
{
  _id: ObjectId("..."),
  produitId: ObjectId("..."),
  auteur: "Jean",
  note: 5,
  texte: "Excellent produit !",
  date: ISODate("2024-01-15")
}
```

**Avantages :**
- ‚úÖ Affichage rapide avec les avis les plus pertinents
- ‚úÖ Document principal reste petit
- ‚úÖ Tous les avis accessibles via la collection s√©par√©e

---

### Solution 3 : Pattern Bucket (Regroupement)

**Regrouper les √©l√©ments en "seaux" de taille fixe.**

#### Avant (anti-pattern) :

```javascript
// Un document par mesure IoT ‚Üí 1 million de documents/heure !
{
  _id: ObjectId("..."),
  capteurId: "SENSOR-001",
  temperature: 22.5,
  timestamp: ISODate("2024-01-15T10:00:00Z")
}
```

#### Apr√®s (solution) :

```javascript
// Regroupement par heure (bucket)
{
  _id: ObjectId("..."),
  capteurId: "SENSOR-001",
  date: ISODate("2024-01-15T10:00:00Z"),
  mesures: [
    { timestamp: ISODate("2024-01-15T10:00:00Z"), temperature: 22.5 },
    { timestamp: ISODate("2024-01-15T10:01:00Z"), temperature: 22.6 },
    { timestamp: ISODate("2024-01-15T10:02:00Z"), temperature: 22.4 }
    // ... 60 mesures max (une par minute)
  ],
  statistiques: {
    nombreMesures: 60,
    temperatureMoyenne: 22.5,
    temperatureMin: 22.1,
    temperatureMax: 22.9
  }
}
```

**Avantages :**
- ‚úÖ 60 mesures par document au lieu de 60 documents
- ‚úÖ R√©duction de 98% du nombre de documents
- ‚úÖ Meilleure performance d'indexation
- ‚úÖ Document ne d√©passera jamais ~50 Ko

---

### Solution 4 : GridFS pour fichiers volumineux

**GridFS** divise les fichiers en chunks de 255 Ko.

#### Utilisation de GridFS :

```javascript
// Stocker un fichier avec GridFS
const bucket = new GridFSBucket(db)

// Upload
const uploadStream = bucket.openUploadStream('mon-fichier.pdf', {
  metadata: {
    type: 'document',
    utilisateur: 'sophie@example.com',
    dateUpload: new Date()
  }
})

fs.createReadStream('fichier.pdf').pipe(uploadStream)

uploadStream.on('finish', () => {
  console.log(`Fichier upload√© : ${uploadStream.id}`)
})

// Document principal r√©f√©rence le fichier GridFS
{
  _id: ObjectId("..."),
  titre: "Contrat de service",
  description: "Contrat sign√© avec le client ABC",
  fichierGridFSId: uploadStream.id,  // ‚Üê R√©f√©rence
  tailleFichier: 5242880,  // 5 Mo
  dateCreation: ISODate("2024-01-15")
}

// Download
const downloadStream = bucket.openDownloadStream(fileId)
downloadStream.pipe(fs.createWriteStream('fichier-telecharge.pdf'))
```

**Quand utiliser GridFS :**
- Fichiers > 16 Mo (obligatoire)
- Fichiers entre 1 Mo et 16 Mo (recommand√©)
- Images, PDFs, vid√©os, archives

**Avantages :**
- ‚úÖ Pas de limite de taille (fichiers de plusieurs Go possibles)
- ‚úÖ Streaming efficace
- ‚úÖ M√©tadonn√©es associ√©es au fichier
- ‚úÖ R√©plication automatique avec MongoDB

---

### Solution 5 : Compression des donn√©es

**Compresser les champs texte volumineux.**

```javascript
// Avec compression (exemple conceptuel - n√©cessite une biblioth√®que)
const contenuCompresse = compresser(contenuLong)

{
  _id: ObjectId("..."),
  titre: "Article",
  contenuCompresse: BinData(0, contenuCompresse),  // ‚Üê Compress√©
  tailleOriginale: 5242880,
  tailleCompresse: 1048576,
  compression: "gzip"
}

// D√©compression lors de la lecture
const contenu = decompresser(document.contenuCompresse)
```

**Cas d'usage :**
- Logs textuels volumineux
- Contenu riche (HTML, Markdown)
- Donn√©es JSON imbriqu√©es

**Important :** La compression ajoute de la complexit√©. √Ä utiliser seulement si n√©cessaire.

---

### Solution 6 : Archivage et Time-To-Live (TTL)

**Supprimer automatiquement les vieilles donn√©es.**

```javascript
// Index TTL : suppression automatique apr√®s 30 jours
db.logs.createIndex(
  { createdAt: 1 },
  { expireAfterSeconds: 2592000 }  // 30 jours
)

// Document se supprime automatiquement
{
  _id: ObjectId("..."),
  message: "Log entry",
  niveau: "INFO",
  createdAt: ISODate("2024-01-15T10:00:00Z")
  // ‚Üê Supprim√© automatiquement le 2024-02-14
}
```

**Ou archivage manuel vers collection historique :**

```javascript
// Archiver les donn√©es > 1 an dans collection s√©par√©e
const unAnAuparavant = new Date()
unAnAuparavant.setFullYear(unAnAuparavant.getFullYear() - 1)

// Copier vers archive
const anciensDocuments = db.articles.find({
  datePublication: { $lt: unAnAuparavant }
})

anciensDocuments.forEach(doc => {
  db.articlesArchives.insertOne(doc)
})

// Supprimer de la collection principale
db.articles.deleteMany({
  datePublication: { $lt: unAnAuparavant }
})
```

---

### Solution 7 : Stockage externe (S3, Cloud Storage)

**Stocker les fichiers dans un service de stockage externe.**

```javascript
// Document MongoDB (m√©tadonn√©es seulement)
{
  _id: ObjectId("..."),
  titre: "Vid√©o de pr√©sentation",
  description: "Pr√©sentation produit 2024",
  urlS3: "https://mon-bucket.s3.amazonaws.com/videos/presentation-2024.mp4",
  tailleFichier: 524288000,  // 500 Mo
  duree: 600,  // 10 minutes
  format: "mp4",
  dateUpload: ISODate("2024-01-15")
}
```

**Avantages :**
- ‚úÖ Pas de limite de taille
- ‚úÖ CDN pour diffusion rapide
- ‚úÖ Co√ªt optimis√© pour le stockage
- ‚úÖ Sp√©cialis√© pour les fichiers statiques

**Services courants :**
- AWS S3
- Google Cloud Storage
- Azure Blob Storage
- Cloudinary (images/vid√©os)

---

## Strat√©gies d'optimisation

### 1. Analyser et identifier les gros documents

**Script de monitoring :**

```javascript
// Fonction pour trouver les documents volumineux
function trouverDocumentsVolumineux(collection, seuilMo) {
  const seuilOctets = seuilMo * 1024 * 1024
  const resultats = []

  db[collection].find().forEach(doc => {
    const taille = Object.bsonsize(doc)
    if (taille > seuilOctets) {
      resultats.push({
        _id: doc._id,
        taille: taille,
        tailleMo: (taille / 1024 / 1024).toFixed(2),
        pourcentageLimit: ((taille / 16777216) * 100).toFixed(2)
      })
    }
  })

  return resultats.sort((a, b) => b.taille - a.taille)
}

// Utilisation
const grosDocuments = trouverDocumentsVolumineux("articles", 1)
printjson(grosDocuments)
```

### 2. Identifier les champs volumineux

```javascript
// Analyser quel champ prend le plus de place
db.articles.aggregate([
  { $limit: 100 },  // √âchantillon
  {
    $project: {
      tailleCommentaires: { $bsonSize: "$commentaires" },
      tailleContenu: { $bsonSize: "$contenu" },
      tailleMetadonnees: { $bsonSize: "$metadonnees" },
      tailleTotal: { $bsonSize: "$$ROOT" }
    }
  }
])
```

### 3. √âtablir des alertes

```javascript
// Script de monitoring quotidien
function verifierTailleDocuments() {
  const seuil = 10 * 1024 * 1024  // 10 Mo

  db.articles.find().forEach(doc => {
    const taille = Object.bsonsize(doc)

    if (taille > seuil) {
      // Logger ou envoyer une alerte
      print(`‚ö†Ô∏è  ALERTE : Document ${doc._id} = ${(taille / 1024 / 1024).toFixed(2)} Mo`)

      // Potentiellement d√©clencher une action automatique
      // (archivage, notification, etc.)
    }
  })
}

// Ex√©cuter quotidiennement
verifierTailleDocuments()
```

---

## Erreurs li√©es √† la limite de 16 Mo

### Message d'erreur typique

```javascript
MongoServerError: BSONObj size: 17825792 (0x10FE000) is invalid.
Size must be between 0 and 16793600(16MB)
First element: _id: ObjectId('...')
```

### Que faire quand vous rencontrez cette erreur ?

#### 1. Identifier le document probl√©matique

```javascript
// L'erreur vous donne l'_id, r√©cup√©rez le document
const doc = db.articles.findOne({ _id: ObjectId("...") })

// V√©rifier sa taille
const taille = Object.bsonsize(doc)
print(`Taille : ${(taille / 1024 / 1024).toFixed(2)} Mo`)

// Identifier les gros champs
print(`Commentaires : ${Object.bsonsize(doc.commentaires)} octets`)
print(`Contenu : ${Object.bsonsize(doc.contenu)} octets`)
```

#### 2. Solutions d'urgence

**Option A : Extraire les donn√©es vers une collection s√©par√©e**

```javascript
// Sauvegarder les commentaires ailleurs
doc.commentaires.forEach(commentaire => {
  db.commentairesArticle.insertOne({
    articleId: doc._id,
    ...commentaire
  })
})

// Vider le tableau dans le document original
db.articles.updateOne(
  { _id: doc._id },
  {
    $set: { commentaires: [] },
    $inc: { version: 1 }
  }
)
```

**Option B : Archiver une partie des donn√©es**

```javascript
// Garder seulement les 100 derniers commentaires
const commentairesRecents = doc.commentaires.slice(-100)
const commentairesArchives = doc.commentaires.slice(0, -100)

// Archiver les anciens
db.commentairesArchives.insertOne({
  articleId: doc._id,
  commentaires: commentairesArchives,
  dateArchivage: new Date()
})

// Mettre √† jour le document
db.articles.updateOne(
  { _id: doc._id },
  { $set: { commentaires: commentairesRecents } }
)
```

---

## Bonnes pratiques

### ‚úÖ √Ä faire

1. **Anticiper la croissance**
   - Calculer la taille maximale th√©orique d'un document
   - Pr√©voir l'√©volution sur 1 an, 5 ans, 10 ans

2. **Monitorer r√©guli√®rement**
   - Script quotidien pour identifier les documents > 5 Mo
   - Alertes si documents > 10 Mo

3. **Limiter les tableaux**
   - Imposer une limite applicative (ex : max 100 √©l√©ments)
   - Utiliser des r√©f√©rences au-del√† de cette limite

4. **Documenter les choix**
   - Expliquer pourquoi tel champ est imbriqu√©
   - Documenter les strat√©gies de croissance

5. **Tester avec des donn√©es r√©alistes**
   - Ne pas tester qu'avec 10 documents
   - Simuler 1 an, 5 ans de donn√©es

### ‚ö†Ô∏è √Ä √©viter

1. **Assumer que "√ßa ira"**
   - Un tableau de 10 √©l√©ments aujourd'hui ‚Üí 10 000 dans 5 ans

2. **Ignorer les alertes**
   - Document √† 8 Mo ‚Üí sera √† 16 Mo bient√¥t

3. **Stocker des fichiers directement**
   - Toujours utiliser GridFS ou stockage externe

4. **Imbrication sans limite**
   - Tout imbriquer "parce que c'est MongoDB"

5. **Ne pas mesurer**
   - "Je pense que c'est petit" ‚Üí Mesurer !

---

## Tableau r√©capitulatif des solutions

| Probl√®me | Solution recommand√©e | Complexit√© | Performance |
|----------|---------------------|------------|-------------|
| Commentaires illimit√©s | Child-Referencing | Faible | Excellente |
| Top N + total | Pattern Subset | Faible | Excellente |
| Donn√©es temporelles | Pattern Bucket | Moyenne | Tr√®s bonne |
| Fichiers > 16 Mo | GridFS | Moyenne | Bonne |
| Fichiers 1-16 Mo | GridFS ou S3 | Faible-Moyenne | Excellente |
| Historique complet | Collection s√©par√©e | Faible | Bonne |
| Logs volumineux | TTL + Archivage | Faible | Bonne |
| Images/Vid√©os | Stockage externe (S3) | Faible | Excellente |
| Contenu texte long | Compression | √âlev√©e | Moyenne |

---

## Outils de diagnostic

### MongoDB Compass

MongoDB Compass affiche la taille des documents visuellement :

```
Documents > Schema > Analyze
‚Üí Document size distribution
‚Üí Field size analysis
```

### Scripts de monitoring

```javascript
// Script complet de diagnostic
function diagnosticTailleDocuments(nomCollection) {
  print(`\n=== Diagnostic : ${nomCollection} ===\n`)

  const stats = db[nomCollection].stats()
  print(`Nombre de documents : ${stats.count}`)
  print(`Taille moyenne : ${(stats.avgObjSize / 1024).toFixed(2)} Ko`)
  print(`Taille totale : ${(stats.size / 1024 / 1024).toFixed(2)} Mo\n`)

  // Top 10 des plus gros documents
  print("Top 10 documents les plus volumineux :")

  const gros = []
  db[nomCollection].find().limit(1000).forEach(doc => {
    gros.push({
      _id: doc._id,
      taille: Object.bsonsize(doc)
    })
  })

  gros.sort((a, b) => b.taille - a.taille)
  gros.slice(0, 10).forEach((item, index) => {
    const tailleMo = (item.taille / 1024 / 1024).toFixed(2)
    const pourcentage = ((item.taille / 16777216) * 100).toFixed(2)
    print(`${index + 1}. ${item._id} : ${tailleMo} Mo (${pourcentage}%)`)
  })
}

// Utilisation
diagnosticTailleDocuments("articles")
```

---

## Conclusion

La **limite de 16 Mo** est une contrainte fondamentale de MongoDB qui :

- ‚úÖ **Encourage** les bonnes pratiques de mod√©lisation
- ‚úÖ **Prot√®ge** les performances et la stabilit√© du syst√®me
- ‚úÖ **Force** √† penser scalabilit√© d√®s la conception

**R√®gles d'or :**

1. üìè **Mesurez** : Utilisez `Object.bsonsize()` r√©guli√®rement
2. üìè **Anticipez** : Calculez la taille maximale th√©orique
3. üìè **Limitez** : Imposez des limites applicatives sur les tableaux
4. üìè **S√©parez** : Utilisez des r√©f√©rences pour les donn√©es volumineuses
5. üìè **Surveillez** : √âtablissez des alertes √† 10 Mo
6. üìè **GridFS** : Pour tous les fichiers
7. üìè **Documentez** : Expliquez vos choix de conception

En respectant ces principes, la limite de 16 Mo ne sera jamais un obstacle, mais un guide pour concevoir des sch√©mas MongoDB efficaces et performants.

---

**Points cl√©s √† retenir :**

- ‚úÖ Limite stricte de 16 Mo (16 777 216 octets) par document
- ‚úÖ Cette limite existe pour prot√©ger les performances
- ‚úÖ Utilisez Object.bsonsize() pour mesurer la taille
- ‚úÖ Les tableaux illimit√©s sont le pi√®ge le plus courant
- ‚úÖ Pattern Subset = solution √©l√©gante pour top N + total
- ‚úÖ GridFS = solution pour fichiers > 16 Mo
- ‚úÖ Stockage externe (S3) = id√©al pour m√©dias
- ‚úÖ √âtablissez des alertes √† 10 Mo pour anticiper

---


‚è≠Ô∏è [Conception pour la performance](/04-modelisation-des-donnees/09-conception-performance.md)
