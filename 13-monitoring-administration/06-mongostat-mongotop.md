ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 13.6 mongostat et mongotop

## Introduction

**mongostat** et **mongotop** sont les deux utilitaires de monitoring en temps rÃ©el essentiels pour les SRE et administrateurs MongoDB. Contrairement aux outils de monitoring Ã  long terme comme Prometheus ou aux outils d'analyse post-mortem comme le profiler, ces utilitaires fournissent une **vue immÃ©diate et continue** de l'Ã©tat du serveur, permettant un diagnostic rapide et une intervention en temps rÃ©el.

Ces outils sont particuliÃ¨rement critiques pour :
- **Diagnostic immÃ©diat** lors d'incidents de performance
- **Validation** des changements de configuration ou de code
- **DÃ©tection proactive** de patterns anormaux
- **Baseline** des performances en conditions normales
- **Investigation** de problÃ¨mes transitoires difficiles Ã  capturer

Dans l'arsenal d'un SRE, mongostat et mongotop sont souvent les **premiers outils** invoquÃ©s lors d'une alerte de performance, avant mÃªme d'examiner les logs ou les dashboards.

---

## mongostat - Monitoring Global du Serveur

### Architecture et Fonctionnement

mongostat collecte et affiche les statistiques du serveur en exÃ©cutant pÃ©riodiquement la commande `serverStatus()` et en calculant les deltas entre les snapshots successifs.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        mongostat                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   Snapshot   â”‚â”€â”€â”€â”€â–¶â”‚   Calculate  â”‚â”€â”€â”€â”€â–¶â”‚   Display    â”‚    â”‚
â”‚  â”‚ serverStatus â”‚     â”‚    Deltas    â”‚     â”‚   Metrics    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                                          â”‚           â”‚
â”‚         â”‚                                          â”‚           â”‚
â”‚         â–¼                                          â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            MongoDB Server (mongod/mongos)                â”‚  â”‚
â”‚  â”‚  â€¢ serverStatus()                                        â”‚  â”‚
â”‚  â”‚  â€¢ replSetGetStatus()                                    â”‚  â”‚
â”‚  â”‚  â€¢ shardingStatistics()                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Installation et PrÃ©requis

mongostat est inclus dans le package MongoDB Database Tools :

```bash
# VÃ©rifier l'installation
mongostat --version

# Output attendu
mongostat version: 100.9.4
git version: e22e7e2bc564646d65b2959c98fa01afd004f935
Go version: go1.20.12
os: linux
arch: amd64
compiler: gc
```

**Permissions requises** :
- Lecture sur `admin.system.version`
- Action `serverStatus` sur le cluster
- Pour replica sets : `replSetGetStatus`
- Pour sharding : `shardingState`

```javascript
// CrÃ©er un utilisateur dÃ©diÃ© au monitoring
db.getSiblingDB('admin').createUser({
  user: "monitoring",
  pwd: "securePassword",
  roles: [
    { role: "clusterMonitor", db: "admin" },
    { role: "read", db: "local" }
  ]
})
```

### Syntaxe de Base

```bash
mongostat [options] [polling-interval]
```

**Exemple simple** :
```bash
# RafraÃ®chissement toutes les secondes (dÃ©faut)
mongostat --uri="mongodb://monitoring:password@localhost:27017/?authSource=admin"

# RafraÃ®chissement toutes les 5 secondes
mongostat --uri="mongodb://localhost:27017" 5

# Nombre limitÃ© d'itÃ©rations (10 snapshots)
mongostat --uri="mongodb://localhost:27017" --rowcount=10
```

---

## MÃ©triques mongostat - RÃ©fÃ©rence ComplÃ¨te

### Vue d'Ensemble des Colonnes

Sortie standard de mongostat :

```
insert query update delete getmore command dirty used flushes vsize  res qrw arw net_in net_out conn                time
   *45   *89    *23     *5       8    15|2  3.2% 42.1%       0 1.6G 1.1G 0|0 1|0  15.2k   38.4k   87 Dec 08 16:45:23.456
    52    95     28      7      12    18|3  3.5% 43.2%       0 1.6G 1.1G 0|0 1|0  16.8k   42.1k   89 Dec 08 16:45:24.456
```

### 1. OpÃ©rations par Seconde

#### insert, query, update, delete

**Description** : Nombre d'opÃ©rations par seconde pour chaque type.

**Signification** :
- `*` prÃ©fixant la valeur : au moins une opÃ©ration a pris plus de 1ms
- Valeur simple : nombre d'opÃ©rations/seconde

**InterprÃ©tation** :

| Valeur | Niveau | Action |
|--------|--------|--------|
| < 100 | Normal | Aucune |
| 100-1000 | ModÃ©rÃ© | Surveiller |
| 1000-5000 | Ã‰levÃ© | VÃ©rifier indexes |
| > 5000 | TrÃ¨s Ã©levÃ© | Investigation requise |

**Patterns d'alerte** :

```bash
# Spike soudain d'insertions
insert: 45 â†’ 5234 â†’ 5678 â†’ 4892
â†’ Possible batch insert ou import en cours

# Queries Ã©levÃ©es continues avec *
query: *8234 *8456 *8123 *8345
â†’ RequÃªtes lentes, manque d'index probable

# Deletes Ã©levÃ©s
delete: 2345 2456 2389 2412
â†’ OpÃ©ration de purge, vÃ©rifier l'impact
```

**Exemple d'analyse** :
```bash
# Capturer 60 secondes de statistiques
mongostat --uri="$MONGO_URI" --rowcount=60 > mongostat_output.txt

# Analyser les pics
awk '{if ($2 > 1000) print $21, $2}' mongostat_output.txt
```

#### getmore

**Description** : Nombre d'opÃ©rations getMore par seconde (continuations de curseurs).

**Signification** : RÃ©sultats de requÃªtes retournÃ©s par batch. Valeur Ã©levÃ©e = beaucoup de grands curseurs actifs.

**Seuils** :
- < 50 : Normal
- 50-200 : Surveillance
- > 200 : Attention - possibles curseurs non fermÃ©s ou scans lourds

**Diagnostic** :
```javascript
// Identifier les curseurs ouverts
db.getSiblingDB('admin').aggregate([
  { $currentOp: { idleCursors: true } },
  { $match: { type: "idleCursor" } },
  { $group: {
      _id: "$ns",
      count: { $sum: 1 },
      totalTime: { $sum: "$microsecs_running" }
  }},
  { $sort: { count: -1 } },
  { $limit: 10 }
])
```

#### command

**Description** : Nombre de commandes par seconde, format `read|write`.

**Exemples de commandes** :
- Read : find, aggregate, count, distinct
- Write : insert, update, delete, findAndModify
- Admin : replSetGetStatus, serverStatus, ping

**Analyse** :
```
command: 45|12
â†’ 45 commandes de lecture/sec, 12 d'Ã©criture/sec

command: 2345|1234
â†’ Charge trÃ¨s Ã©levÃ©e, vÃ©rifier les sources
```

### 2. MÃ©triques Cache WiredTiger

#### dirty

**Description** : Pourcentage du cache WiredTiger contenant des donnÃ©es modifiÃ©es non encore Ã©crites sur disque.

**Seuils critiques** :

| Valeur | Ã‰tat | Impact | Action |
|--------|------|--------|--------|
| < 5% | Normal | Aucun | - |
| 5-15% | SurveillÃ© | Faible | Monitoring |
| 15-30% | Attention | ModÃ©rÃ© | VÃ©rifier I/O |
| 30-50% | Critique | Ã‰levÃ© | Action immÃ©diate |
| > 50% | Urgence | Critique | Intervention |

**Signification** :
- Cache dirty Ã©levÃ© â†’ Ã‰critures plus rapides que la capacitÃ© du disque Ã  absorber
- Peut causer des checkpoints lents
- Risque de ralentissement brutal quand limite atteinte

**Pattern problÃ©matique** :
```
dirty: 3.2% â†’ 8.5% â†’ 15.2% â†’ 28.4% â†’ 42.1% â†’ 51.3%
â†’ Progression constante = surcharge d'Ã©critures
```

**Actions correctives** :

```yaml
# 1. VÃ©rifier les I/O disque
iostat -x 1 10

# 2. Analyser les Ã©critures
db.serverStatus().wiredTiger.transaction

# 3. Configuration WiredTiger (mongod.conf)
storage:
  wiredTiger:
    engineConfig:
      cacheSizeGB: 8  # Augmenter si possible
    collectionConfig:
      blockCompressor: snappy  # VÃ©rifier compression
    indexConfig:
      prefixCompression: true

# 4. Ajuster le checkpoint interval
storage:
  syncPeriodSecs: 60  # DÃ©faut: 60s, augmenter si nÃ©cessaire
```

#### used

**Description** : Pourcentage du cache WiredTiger actuellement utilisÃ©.

**Seuils** :

| Valeur | Ã‰tat | Action |
|--------|------|--------|
| < 60% | Normal | Aucune |
| 60-80% | Optimal | Surveiller |
| 80-95% | Haute utilisation | Planifier augmentation |
| > 95% | Saturation | Action requise |

**Comportement normal** :
```
used: 42.1% â†’ 45.3% â†’ 48.7% â†’ 52.1%
â†’ Croissance progressive = working set grandit

used: 78.2% â†’ 79.1% â†’ 78.5% â†’ 79.3%
â†’ Stabilisation haute = working set correspond au cache
```

**Comportement problÃ©matique** :
```
used: 95.2% â†’ 96.8% â†’ 97.1% â†’ 95.4% â†’ 96.9%
â†’ Oscillation haute = cache trop petit, evictions frÃ©quentes
```

**Calcul du cache optimal** :
```javascript
// Obtenir le working set
var stats = db.serverStatus();
var workingSet = stats.wiredTiger.cache["bytes currently in the cache"];
var cacheSize = stats.wiredTiger.cache["maximum bytes configured"];

print("Working Set: " + (workingSet / 1024 / 1024 / 1024).toFixed(2) + " GB");
print("Cache Size: " + (cacheSize / 1024 / 1024 / 1024).toFixed(2) + " GB");
print("Utilization: " + ((workingSet / cacheSize) * 100).toFixed(2) + "%");

// Recommandation
if (workingSet / cacheSize > 0.8) {
  var recommended = (workingSet / 0.7) / 1024 / 1024 / 1024;
  print("Recommended cache: " + recommended.toFixed(2) + " GB");
}
```

### 3. MÃ©triques I/O et MÃ©moire

#### flushes

**Description** : Nombre de checkpoints WiredTiger par seconde.

**Valeurs normales** :
- 0 : Aucun checkpoint dans l'intervalle (normal si < 60s)
- 1 : Un checkpoint (attendu toutes les 60s par dÃ©faut)
- > 1 : Multiple checkpoints (anormal, investigation requise)

**Configuration checkpoint** :
```javascript
// VÃ©rifier la configuration
db.serverStatus().wiredTiger.transaction

// Sortie exemple
{
  "transaction checkpoint currently running": false,
  "transaction checkpoint generation": 1234,
  "transaction checkpoint max time (msecs)": 127,
  "transaction checkpoint min time (msecs)": 45,
  "transaction checkpoint most recent time (msecs)": 87
}
```

**Checkpoint trop lent** :
```
Si "most recent time" > 30000 ms (30s) :
â†’ I/O disque saturÃ©
â†’ Cache dirty trop Ã©levÃ©
â†’ Volume de donnÃ©es Ã  Ã©crire trop important
```

#### vsize (Virtual Memory Size)

**Description** : Taille totale de la mÃ©moire virtuelle utilisÃ©e par MongoDB.

**InterprÃ©tation** :
- Inclut : mÃ©moire mappÃ©e, cache, heap, stack
- Croissance normale : progressive avec le dataset
- Croissance anormale : fuite mÃ©moire potentielle

**Monitoring** :
```bash
# Suivre l'Ã©volution
mongostat --uri="$MONGO_URI" --rowcount=60 | awk '{print $21, $10}'

# Comparer avec la mÃ©moire systÃ¨me
free -h
```

#### res (Resident Memory)

**Description** : MÃ©moire rÃ©sidente (RAM physique utilisÃ©e).

**Calcul optimal** :
```
Optimal RES = Working Set + Overhead
- Working Set : donnÃ©es frÃ©quemment accÃ©dÃ©es
- Overhead : ~1-2 GB (structures internes, connexions)
```

**Seuils d'alerte** :

| Condition | Signification | Action |
|-----------|--------------|--------|
| res > 90% RAM totale | Risque swap | Ajouter RAM ou rÃ©duire cache |
| res croÃ®t constamment | Fuite mÃ©moire possible | Investigation |
| res >> cache configurÃ© | Overhead Ã©levÃ© | Analyser connexions |

**Exemple d'analyse** :
```bash
#!/bin/bash
# memory_analysis.sh

TOTAL_RAM=$(free -g | awk '/Mem:/ {print $2}')
MONGO_RES=$(mongostat --uri="$MONGO_URI" --rowcount=1 | tail -1 | awk '{print $11}' | sed 's/G//')

USAGE_PERCENT=$(echo "scale=2; ($MONGO_RES / $TOTAL_RAM) * 100" | bc)

echo "Total RAM: ${TOTAL_RAM}GB"
echo "MongoDB RES: ${MONGO_RES}GB"
echo "Usage: ${USAGE_PERCENT}%"

if (( $(echo "$USAGE_PERCENT > 85" | bc -l) )); then
  echo "WARNING: High memory usage!"
fi
```

### 4. MÃ©triques de Queue et Concurrence

#### qrw (Queue Read|Write)

**Description** : Nombre d'opÃ©rations en attente de traitement (queue), format `read|write`.

**Signification** : Indique la saturation du systÃ¨me - les requÃªtes arrivent plus vite qu'elles ne peuvent Ãªtre traitÃ©es.

**Seuils critiques** :

| Valeur | Ã‰tat | Impact Utilisateur | Action |
|--------|------|-------------------|--------|
| 0\|0 | Normal | Aucun | - |
| < 10\|10 | Faible | Latence +50ms | Surveiller |
| 10-50\|10-50 | ModÃ©rÃ© | Latence +200ms | Investigation |
| > 50\|> 50 | Critique | Latence +1s+ | Action immÃ©diate |

**Patterns d'analyse** :

```bash
# Pattern 1 : Queue lectures Ã©levÃ©e
qrw: 45|2 48|3 52|2 47|1
â†’ Goulot en lecture (queries lentes, index manquants)

# Pattern 2 : Queue Ã©critures Ã©levÃ©e
qrw: 2|34 1|38 3|42 2|39
â†’ Goulot en Ã©criture (I/O disque lent, write concern Ã©levÃ©)

# Pattern 3 : Queue mixte Ã©levÃ©e
qrw: 67|45 72|48 68|52 71|47
â†’ Surcharge gÃ©nÃ©rale du serveur
```

**Diagnostic des queues Ã©levÃ©es** :
```javascript
// 1. Identifier les opÃ©rations en attente
db.currentOp({
  "$or": [
    { "waitingForLock": true },
    { "locks": { "$exists": true } }
  ]
}).inprog.forEach(function(op) {
  if (op.secs_running > 5) {
    printjson(op);
  }
})

// 2. Analyser les locks
db.serverStatus().locks

// 3. VÃ©rifier les requÃªtes lentes en cours
db.currentOp({
  "active": true,
  "secs_running": { "$gt": 3 },
  "op": { "$in": ["query", "getmore"] }
})
```

#### arw (Active Read|Write)

**Description** : Nombre d'opÃ©rations actuellement en cours d'exÃ©cution, format `read|write`.

**CapacitÃ© thÃ©orique** :
- Nombre de CPU cores disponibles
- MongoDB utilise un pool de threads (dÃ©faut : 128)

**InterprÃ©tation** :

```bash
# SystÃ¨me 8 cores
arw: 4|2  â†’ Utilisation normale (< cores)
arw: 12|8 â†’ Utilisation Ã©levÃ©e (> cores, mais normal si temporaire)
arw: 45|32 â†’ Surcharge (nombreuses requÃªtes lentes concurrentes)

# Ratio queue/active Ã©levÃ©
qrw: 89|45, arw: 8|4
â†’ ProblÃ¨me : queue grandit plus vite que le traitement
â†’ Cause probable : requÃªtes lentes monopolisant les threads
```

**Configuration du pool de threads** :
```yaml
# mongod.conf
net:
  serviceExecutor: synchronous  # ou adaptive (dÃ©faut)

# Note : adaptive ajuste dynamiquement le pool
```

### 5. MÃ©triques RÃ©seau

#### net_in, net_out

**Description** : Trafic rÃ©seau entrant/sortant en octets par seconde.

**Valeurs typiques** :

| Type d'Application | net_in | net_out |
|-------------------|--------|---------|
| OLTP (transactionnel) | 10-100 KB/s | 50-500 KB/s |
| Analytics (reporting) | 1-10 KB/s | 1-50 MB/s |
| Bulk operations | 1-100 MB/s | 100 KB-10 MB/s |

**Patterns d'analyse** :

```bash
# Pattern 1 : net_out Ã©levÃ©
net_out: 45.2 MB/s
â†’ RequÃªtes retournant beaucoup de donnÃ©es
â†’ VÃ©rifier projections et limites

# Pattern 2 : net_in Ã©levÃ©
net_in: 23.4 MB/s
â†’ Bulk inserts ou imports en cours
â†’ Normal si planifiÃ©

# Pattern 3 : Spike soudain
net_out: 125 KB/s â†’ 45.2 MB/s â†’ 47.8 MB/s
â†’ RequÃªte retournant dataset complet (missing limit ?)
```

**Analyse du trafic** :
```javascript
// Identifier les opÃ©rations volumineuses
db.currentOp({
  "active": true,
  "responseLength": { "$gt": 10000000 }  // > 10 MB
}).inprog.forEach(function(op) {
  print("Client: " + op.client);
  print("Response size: " + (op.responseLength / 1024 / 1024).toFixed(2) + " MB");
  printjson(op.command);
  print("---");
})
```

### 6. MÃ©triques de Connexion

#### conn

**Description** : Nombre total de connexions actives au serveur.

**Limites** :
```javascript
// VÃ©rifier les limites
db.serverStatus().connections

// Sortie exemple
{
  "current": 87,
  "available": 51113,
  "totalCreated": 2345
}
```

**Seuils** :

| Condition | Ã‰tat | Action |
|-----------|------|--------|
| < 100 | Normal | - |
| 100-500 | ModÃ©rÃ© | Surveiller |
| 500-1000 | Ã‰levÃ© | VÃ©rifier connection pooling |
| > 1000 | Critique | Investigation requise |
| Proche de maxIncomingConnections | Urgence | Augmenter limite ou rÃ©soudre fuite |

**Diagnostic des connexions** :
```javascript
// Par IP source
db.getSiblingDB('admin').aggregate([
  { $currentOp: { allUsers: true } },
  { $group: {
      _id: "$client",
      count: { $sum: 1 }
  }},
  { $sort: { count: -1 } },
  { $limit: 10 }
])

// Par type d'opÃ©ration
db.getSiblingDB('admin').aggregate([
  { $currentOp: { allUsers: true } },
  { $group: {
      _id: "$op",
      count: { $sum: 1 }
  }},
  { $sort: { count: -1 } }
])
```

**Configuration** :
```yaml
# mongod.conf
net:
  maxIncomingConnections: 2000  # Augmenter si nÃ©cessaire

# CÃ´tÃ© application (exemple Node.js)
const client = new MongoClient(uri, {
  maxPoolSize: 100,
  minPoolSize: 10,
  maxIdleTimeMS: 30000
});
```

---

## Options AvancÃ©es de mongostat

### Output PersonnalisÃ©

```bash
# Colonnes spÃ©cifiques (-O)
mongostat --uri="$MONGO_URI" \
  -O='host,insert,query,update,delete,qrw,arw,dirty,used,conn,time'

# Output JSON pour parsing
mongostat --uri="$MONGO_URI" -o=json --rowcount=10

# Output exemple JSON
[{
  "host": "localhost:27017",
  "insert": 45,
  "query": 89,
  "update": 23,
  "delete": 5,
  "qr": 0,
  "qw": 0,
  "ar": 1,
  "aw": 0,
  "dirty": 3.2,
  "used": 42.1,
  "conn": 87,
  "time": "2024-12-08T16:45:23Z"
}]
```

### Monitoring de Replica Set

```bash
# DÃ©couverte automatique des membres
mongostat --uri="mongodb://rs0-primary:27017,rs0-secondary1:27017,rs0-secondary2:27017/?replicaSet=rs0" \
  --discover

# Output
               host insert query update delete getmore command dirty  used flushes vsize  res qrw arw net_in net_out conn repl                time
rs0-primary:27017     45    89     23      5       8    15|2  3.2% 42.1%       0 1.6G 1.1G 0|0 1|0  15.2k   38.4k   87  PRI Dec 08 16:45:23
rs0-sec1:27017         0    23      0      0      12     5|0  2.1% 38.7%       0 1.5G 1.0G 0|0 0|0   8.4k   12.1k   45  SEC Dec 08 16:45:23
rs0-sec2:27017         0    19      0      0       8     4|0  1.8% 37.2%       0 1.5G 1.0G 0|0 0|0   7.8k   11.3k   42  SEC Dec 08 16:45:23
```

**Colonne `repl`** :
- PRI : Primary
- SEC : Secondary
- REC : Recovering
- ARB : Arbiter
- UNK : Unknown

**Analyse comparative** :
```bash
# Script d'analyse des membres
#!/bin/bash

mongostat --uri="$RS_URI" --discover --rowcount=1 -o=json | \
  jq -r '.[] |
    [.host, .repl, .qr, .qw, .ar, .aw, .conn] |
    @tsv' | \
  column -t
```

### Monitoring de Cluster Sharded

```bash
# Connecter au mongos
mongostat --uri="mongodb://mongos:27017" --discover

# Output inclut les shards
               host insert query update delete getmore command dirty  used flushes vsize  res qrw arw net_in net_out conn                time
mongos1:27017        52    95     28      7      12    18|3     -     -        -    -    -  0|0 1|0  16.8k   42.1k   89 Dec 08 16:45:24
shard01:27017        23    42     12      3       5     8|1  3.1% 41.5%       0 1.6G 1.1G 0|0 1|0   7.2k   18.3k   34 Dec 08 16:45:24
shard02:27017        29    53     16      4       7    10|2  3.8% 43.2%       0 1.6G 1.2G 0|0 1|0   9.6k   23.9k   55 Dec 08 16:45:24
```

**Note** : mongos ne gÃ¨re pas de cache, donc dirty/used sont vides.

### Mode Interactif

```bash
# DÃ©marrer en mode interactif
mongostat --uri="$MONGO_URI" --interactive

# Commandes disponibles dans le mode interactif :
# - Espace : Pause/Resume
# - q : Quit
# - + : Augmenter refresh rate
# - - : Diminuer refresh rate
```

---

## mongotop - Monitoring par Collection

### Architecture et Fonctionnement

mongotop mesure le temps passÃ© en lecture/Ã©criture par namespace (base.collection), permettant d'identifier rapidement les "hot collections".

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       mongotop                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Collect    â”‚â”€â”€â”€â”€â–¶â”‚  Aggregate   â”‚â”€â”€â”€â”€â–¶â”‚  Display   â”‚  â”‚
â”‚  â”‚   top()      â”‚     â”‚  by NS       â”‚     â”‚   Report   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                                  â”‚
â”‚         â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            MongoDB Server                            â”‚  â”‚
â”‚  â”‚  â€¢ top command                                       â”‚  â”‚
â”‚  â”‚  â€¢ Collection-level stats                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Syntaxe et Options

```bash
# Syntaxe de base
mongotop [options] [polling-interval]

# Exemples
mongotop --uri="mongodb://localhost:27017"  # DÃ©faut : 1 seconde
mongotop --uri="mongodb://localhost:27017" 5  # Toutes les 5 secondes
mongotop --uri="mongodb://localhost:27017" --rowcount=20  # 20 snapshots
```

### InterprÃ©tation de la Sortie

```bash
# Sortie standard
                      ns    total    read    write    2024-12-08T16:45:23Z
         mydb.orders    4567ms   1234ms    3333ms
          mydb.users     892ms    856ms      36ms
       mydb.products     234ms    234ms       0ms
         mydb.logs       89ms     12ms      77ms
  mydb.system.profile     45ms     45ms       0ms
```

**Colonnes** :
- **ns** : Namespace (database.collection)
- **total** : Temps total en millisecondes
- **read** : Temps passÃ© en lectures
- **write** : Temps passÃ© en Ã©critures
- **timestamp** : Horodatage

**Signification** :
```
mydb.orders: total 4567ms, read 1234ms, write 3333ms
â†’ Sur l'intervalle (1s), MongoDB a passÃ© 4.567s en opÃ©rations sur cette collection
â†’ 1.234s en lectures, 3.333s en Ã©critures
â†’ Ratio write/read Ã©levÃ© = collection Ã  forte Ã©criture
```

### Analyse des Patterns

#### Pattern 1 : Collection Lecture-Intensive

```
mydb.analytics    12.3s    12.1s    0.2s
â†’ Presque exclusivement des lectures
â†’ Candidat pour :
  - Index de couverture
  - Read preference sur secondaries
  - Caching applicatif
```

**Actions** :
```javascript
// 1. VÃ©rifier les requÃªtes
db.system.profile.find({ ns: "mydb.analytics" }).sort({ ts: -1 }).limit(5)

// 2. Analyser les index
db.analytics.getIndexes()

// 3. VÃ©rifier si covered queries possible
db.analytics.find(
  { category: "electronics" },
  { _id: 0, name: 1, price: 1 }
).explain("executionStats")
```

#### Pattern 2 : Collection Ã‰criture-Intensive

```
mydb.logs    8.7s    0.3s    8.4s
â†’ Presque exclusivement des Ã©critures
â†’ Candidat pour :
  - Capped collection (si logs)
  - Bulk inserts
  - Write concern ajustÃ©
  - Sharding si volume Ã©levÃ©
```

**Actions** :
```javascript
// 1. Convertir en capped collection (si applicable)
db.runCommand({
  convertToCapped: "logs",
  size: 1073741824  // 1 GB
})

// 2. Utiliser bulk operations
var bulk = db.logs.initializeUnorderedBulkOp();
for (var i = 0; i < 1000; i++) {
  bulk.insert({ timestamp: new Date(), message: "log" + i });
}
bulk.execute({ w: 0 });  // Pas d'attente de confirmation

// 3. VÃ©rifier write concern
db.runCommand({ getLastError: 1 })
```

#### Pattern 3 : Collection Ã‰quilibrÃ©e mais Volumineuse

```
mydb.orders    15.2s    7.8s    7.4s
â†’ Volume trÃ¨s Ã©levÃ© (> 15s sur 1s d'intervalle)
â†’ Multiple operations concurrentes
â†’ Probable hot collection
```

**Actions** :
```javascript
// 1. Identifier les requÃªtes concurrentes
db.currentOp({
  "active": true,
  "ns": "mydb.orders"
}).inprog.length

// 2. VÃ©rifier si sharding nÃ©cessaire
db.orders.stats().count / db.orders.stats().storageSize

// 3. Analyser la distribution de la charge
db.orders.aggregate([
  { $sample: { size: 10000 } },
  { $group: {
      _id: { $dateToString: { format: "%Y-%m-%d", date: "$createdAt" } },
      count: { $sum: 1 }
  }},
  { $sort: { _id: 1 } }
])
```

### Options AvancÃ©es

#### Output JSON

```bash
mongotop --uri="mongodb://localhost:27017" -o=json

# Output
{
  "totals": [
    {
      "ns": "mydb.orders",
      "total": { "time": 4567, "count": 1234 },
      "read": { "time": 1234, "count": 456 },
      "write": { "time": 3333, "count": 778 }
    }
  ],
  "ok": 1
}
```

#### Filtrage et Tri

```bash
# Afficher uniquement certaines bases
mongotop --uri="mongodb://localhost:27017" --json | \
  jq '.totals[] | select(.ns | startswith("mydb.")) | {ns: .ns, total: .total.time}'

# Top 5 collections par temps total
mongotop --uri="mongodb://localhost:27017" --rowcount=1 --json | \
  jq '.totals | sort_by(.total.time) | reverse | .[0:5]'
```

#### Monitoring de Locks

```bash
# Inclure les informations de locks
mongotop --uri="mongodb://localhost:27017" --locks

# Output additionnel
Lock type    time
global read  123ms
global write  45ms
database      89ms
collection    234ms
```

---

## Cas d'Usage AvancÃ©s

### 1. Diagnostic d'Incident en Temps RÃ©el

**ScÃ©nario** : Alerte de latence Ã©levÃ©e en production.

```bash
#!/bin/bash
# incident_diagnosis.sh

MONGO_URI="mongodb://monitoring:password@prod.example.com:27017/?authSource=admin"
OUTPUT_DIR="/tmp/incident_$(date +%Y%m%d_%H%M%S)"
mkdir -p $OUTPUT_DIR

echo "=== INCIDENT DIAGNOSIS STARTED ==="

# 1. Capturer mongostat pendant 2 minutes
echo "Capturing mongostat..."
mongostat --uri="$MONGO_URI" --rowcount=120 > $OUTPUT_DIR/mongostat.txt &
MONGOSTAT_PID=$!

# 2. Capturer mongotop pendant 2 minutes
echo "Capturing mongotop..."
mongotop --uri="$MONGO_URI" --rowcount=120 > $OUTPUT_DIR/mongotop.txt &
MONGOTOP_PID=$!

# 3. Snapshot des opÃ©rations en cours
echo "Capturing currentOp..."
mongo "$MONGO_URI" --quiet --eval "
  printjson(db.currentOp({
    'active': true,
    'secs_running': { '\$gt': 1 }
  }))
" > $OUTPUT_DIR/currentop.json

# 4. Attendre la fin des captures
wait $MONGOSTAT_PID
wait $MONGOTOP_PID

# 5. Analyse rapide
echo ""
echo "=== QUICK ANALYSIS ==="

# Queues moyennes
echo "Average queues:"
awk 'NR>1 {
  split($12, qrw, "|");
  qr+=$qrw[0]; qw+=$qrw[1]; count++
}
END {
  print "  qr: " qr/count;
  print "  qw: " qw/count
}' $OUTPUT_DIR/mongostat.txt

# Cache dirty moyen
echo "Average dirty cache:"
awk 'NR>1 {
  gsub(/%/, "", $7);
  dirty+=$7; count++
}
END {
  print "  " dirty/count "%"
}' $OUTPUT_DIR/mongostat.txt

# Top 3 collections
echo "Top 3 hot collections:"
awk 'NR>1 {
  ns=$1; total=$2;
  if (ns != "") totals[ns]+=total
}
END {
  for (ns in totals) print totals[ns], ns
}' $OUTPUT_DIR/mongotop.txt | sort -rn | head -3

echo ""
echo "=== DATA SAVED TO $OUTPUT_DIR ==="
```

### 2. Baseline de Performance

**Objectif** : Ã‰tablir les mÃ©triques normales pour comparaison future.

```bash
#!/bin/bash
# performance_baseline.sh

MONGO_URI="mongodb://monitoring:password@prod.example.com:27017/?authSource=admin"
BASELINE_FILE="/opt/monitoring/mongodb_baseline.json"

# Capturer 1 heure de mÃ©triques (Ã©chantillonnage toutes les 30s)
mongostat --uri="$MONGO_URI" --rowcount=120 30 -o=json > /tmp/baseline_raw.json

# Calculer les moyennes
jq '[.[] | {
  insert: .insert,
  query: .query,
  update: .update,
  delete: .delete,
  qr: .qr,
  qw: .qw,
  ar: .ar,
  aw: .aw,
  dirty: .dirty,
  used: .used,
  conn: .conn
}] | {
  insert_avg: ([.[].insert] | add / length),
  query_avg: ([.[].query] | add / length),
  update_avg: ([.[].update] | add / length),
  delete_avg: ([.[].delete] | add / length),
  qr_avg: ([.[].qr] | add / length),
  qw_avg: ([.[].qw] | add / length),
  dirty_avg: ([.[].dirty] | add / length),
  used_avg: ([.[].used] | add / length),
  conn_avg: ([.[].conn] | add / length),
  timestamp: now
}' /tmp/baseline_raw.json > $BASELINE_FILE

echo "Baseline saved to $BASELINE_FILE"
cat $BASELINE_FILE
```

### 3. DÃ©tection d'Anomalies

```python
#!/usr/bin/env python3
# anomaly_detector.py

import json
import subprocess
import statistics
from datetime import datetime

class MongoStatAnomalyDetector:
    def __init__(self, baseline_file):
        with open(baseline_file) as f:
            self.baseline = json.load(f)

    def get_current_stats(self, uri):
        """Capture current mongostat"""
        result = subprocess.run(
            ['mongostat', '--uri', uri, '--rowcount', '10', '-o', 'json'],
            capture_output=True,
            text=True
        )
        return json.loads(result.stdout)

    def detect_anomalies(self, current_stats):
        """Compare current with baseline"""
        anomalies = []

        # Calculer moyennes courantes
        current_avg = {
            'query': statistics.mean([s['query'] for s in current_stats]),
            'qr': statistics.mean([s['qr'] for s in current_stats]),
            'qw': statistics.mean([s['qw'] for s in current_stats]),
            'dirty': statistics.mean([s['dirty'] for s in current_stats]),
        }

        # DÃ©tecter anomalies (> 3x baseline)
        for metric, value in current_avg.items():
            baseline_value = self.baseline.get(f'{metric}_avg', 0)
            if value > baseline_value * 3:
                anomalies.append({
                    'metric': metric,
                    'current': value,
                    'baseline': baseline_value,
                    'ratio': value / baseline_value if baseline_value > 0 else float('inf'),
                    'severity': 'CRITICAL' if value > baseline_value * 5 else 'WARNING'
                })

        return anomalies

    def report(self, anomalies):
        """Generate report"""
        if not anomalies:
            print("âœ“ No anomalies detected")
            return

        print(f"\nâš ï¸  DETECTED {len(anomalies)} ANOMALIES\n")
        for a in sorted(anomalies, key=lambda x: x['ratio'], reverse=True):
            print(f"[{a['severity']}] {a['metric']}")
            print(f"  Current: {a['current']:.2f}")
            print(f"  Baseline: {a['baseline']:.2f}")
            print(f"  Ratio: {a['ratio']:.2f}x")
            print()

if __name__ == '__main__':
    import sys

    if len(sys.argv) < 3:
        print("Usage: anomaly_detector.py <baseline_file> <mongo_uri>")
        sys.exit(1)

    detector = MongoStatAnomalyDetector(sys.argv[1])
    stats = detector.get_current_stats(sys.argv[2])
    anomalies = detector.detect_anomalies(stats)
    detector.report(anomalies)
```

### 4. Monitoring Continu avec Alerting

```bash
#!/bin/bash
# continuous_monitor.sh

MONGO_URI="mongodb://monitoring:password@prod.example.com:27017/?authSource=admin"
ALERT_WEBHOOK="https://hooks.slack.com/services/XXX"

# Seuils d'alerte
THRESHOLD_QR=10
THRESHOLD_QW=10
THRESHOLD_DIRTY=20
THRESHOLD_CONN=500

while true; do
  # Capturer un snapshot
  STATS=$(mongostat --uri="$MONGO_URI" --rowcount=1 -o=json | jq '.[0]')

  # Extraire mÃ©triques
  QR=$(echo $STATS | jq -r '.qr')
  QW=$(echo $STATS | jq -r '.qw')
  DIRTY=$(echo $STATS | jq -r '.dirty')
  CONN=$(echo $STATS | jq -r '.conn')

  # VÃ©rifier seuils
  ALERTS=""

  if (( $(echo "$QR > $THRESHOLD_QR" | bc -l) )); then
    ALERTS="${ALERTS}Read queue: $QR (threshold: $THRESHOLD_QR)\n"
  fi

  if (( $(echo "$QW > $THRESHOLD_QW" | bc -l) )); then
    ALERTS="${ALERTS}Write queue: $QW (threshold: $THRESHOLD_QW)\n"
  fi

  if (( $(echo "$DIRTY > $THRESHOLD_DIRTY" | bc -l) )); then
    ALERTS="${ALERTS}Dirty cache: $DIRTY% (threshold: $THRESHOLD_DIRTY%)\n"
  fi

  if [ "$CONN" -gt "$THRESHOLD_CONN" ]; then
    ALERTS="${ALERTS}Connections: $CONN (threshold: $THRESHOLD_CONN)\n"
  fi

  # Envoyer alertes si nÃ©cessaire
  if [ ! -z "$ALERTS" ]; then
    MESSAGE="MongoDB Alert - $(date)\n$ALERTS"
    curl -X POST $ALERT_WEBHOOK \
      -H 'Content-Type: application/json' \
      -d "{\"text\": \"$MESSAGE\"}"
  fi

  # Attendre 30 secondes
  sleep 30
done
```

### 5. CorrÃ©lation mongostat + mongotop

```bash
#!/bin/bash
# correlate_stats.sh

MONGO_URI="mongodb://monitoring:password@prod.example.com:27017/?authSource=admin"
DURATION=60  # 1 minute

echo "Collecting data for $DURATION seconds..."

# Collecter en parallÃ¨le
mongostat --uri="$MONGO_URI" --rowcount=$DURATION -o=json > /tmp/mongostat.json &
mongotop --uri="$MONGO_URI" --rowcount=$DURATION -o=json > /tmp/mongotop.json &

wait

echo ""
echo "=== CORRELATION ANALYSIS ==="

# PÃ©riode de haute charge ?
HIGH_LOAD=$(jq '[.[] | select(.qr > 10 or .qw > 10)] | length' /tmp/mongostat.json)
echo "Periods of high queue: $HIGH_LOAD / $DURATION"

if [ "$HIGH_LOAD" -gt 10 ]; then
  echo ""
  echo "High load detected. Top collections during that period:"

  # Extraire les timestamps des pÃ©riodes de haute charge
  TIMESTAMPS=$(jq -r '.[] | select(.qr > 10 or .qw > 10) | .time' /tmp/mongostat.json)

  # Trouver les collections correspondantes dans mongotop
  # (Note : nÃ©cessite correspondance temporelle prÃ©cise)
  echo "(Analysis simplified - check /tmp/mongotop.json for details)"
fi
```

---

## IntÃ©gration avec Stack de Monitoring

### 1. Export vers Prometheus

```python
#!/usr/bin/env python3
# mongostat_exporter.py

from prometheus_client import start_http_server, Gauge
import subprocess
import json
import time

# DÃ©finir les mÃ©triques Prometheus
metrics = {
    'insert': Gauge('mongodb_operations_insert_total', 'Insert operations per second'),
    'query': Gauge('mongodb_operations_query_total', 'Query operations per second'),
    'update': Gauge('mongodb_operations_update_total', 'Update operations per second'),
    'delete': Gauge('mongodb_operations_delete_total', 'Delete operations per second'),
    'qr': Gauge('mongodb_queue_read', 'Read queue depth'),
    'qw': Gauge('mongodb_queue_write', 'Write queue depth'),
    'ar': Gauge('mongodb_active_read', 'Active read operations'),
    'aw': Gauge('mongodb_active_write', 'Active write operations'),
    'dirty': Gauge('mongodb_cache_dirty_percent', 'Dirty cache percentage'),
    'used': Gauge('mongodb_cache_used_percent', 'Used cache percentage'),
    'conn': Gauge('mongodb_connections_current', 'Current connections'),
}

def collect_stats(uri):
    """Collect stats from mongostat"""
    result = subprocess.run(
        ['mongostat', '--uri', uri, '--rowcount', '1', '-o', 'json'],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        data = json.loads(result.stdout)
        return data[0] if data else {}
    return {}

def update_metrics(uri):
    """Update Prometheus metrics"""
    stats = collect_stats(uri)

    for metric_name, gauge in metrics.items():
        value = stats.get(metric_name, 0)
        gauge.set(value)

if __name__ == '__main__':
    import sys

    if len(sys.argv) < 2:
        print("Usage: mongostat_exporter.py <mongo_uri>")
        sys.exit(1)

    uri = sys.argv[1]

    # DÃ©marrer le serveur HTTP pour Prometheus
    start_http_server(9216)
    print("Exporter started on port 9216")

    # Boucle de collecte
    while True:
        try:
            update_metrics(uri)
            time.sleep(15)  # Collecter toutes les 15 secondes
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(15)
```

**Configuration Prometheus** :
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'mongodb_mongostat'
    static_configs:
      - targets: ['localhost:9216']
        labels:
          cluster: 'prod'
          environment: 'production'
```

### 2. Dashboard Grafana

```json
{
  "dashboard": {
    "title": "MongoDB mongostat Dashboard",
    "panels": [
      {
        "title": "Operations per Second",
        "targets": [
          {
            "expr": "mongodb_operations_query_total",
            "legendFormat": "Queries"
          },
          {
            "expr": "mongodb_operations_insert_total",
            "legendFormat": "Inserts"
          },
          {
            "expr": "mongodb_operations_update_total",
            "legendFormat": "Updates"
          },
          {
            "expr": "mongodb_operations_delete_total",
            "legendFormat": "Deletes"
          }
        ]
      },
      {
        "title": "Queue Depth",
        "targets": [
          {
            "expr": "mongodb_queue_read",
            "legendFormat": "Read Queue"
          },
          {
            "expr": "mongodb_queue_write",
            "legendFormat": "Write Queue"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [10],
                "type": "gt"
              },
              "query": {
                "params": ["A", "5m", "now"]
              }
            }
          ]
        }
      },
      {
        "title": "Cache Usage",
        "targets": [
          {
            "expr": "mongodb_cache_dirty_percent",
            "legendFormat": "Dirty %"
          },
          {
            "expr": "mongodb_cache_used_percent",
            "legendFormat": "Used %"
          }
        ]
      }
    ]
  }
}
```

### 3. Logging StructurÃ©

```bash
#!/bin/bash
# mongostat_logger.sh

MONGO_URI="mongodb://monitoring:password@prod.example.com:27017/?authSource=admin"
LOG_FILE="/var/log/mongodb/mongostat.log"

while true; do
  STATS=$(mongostat --uri="$MONGO_URI" --rowcount=1 -o=json | jq -c '.[0]')

  # Ajouter timestamp et hostname
  echo "{\"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\", \"host\": \"$(hostname)\", \"stats\": $STATS}" >> $LOG_FILE

  sleep 60
done
```

---

## Comparaison et ComplÃ©mentaritÃ©

### mongostat vs mongotop

| Aspect | mongostat | mongotop |
|--------|-----------|----------|
| **GranularitÃ©** | Serveur global | Par collection |
| **MÃ©triques** | OpÃ©rations, cache, queues | Temps lecture/Ã©criture |
| **Usage principal** | SantÃ© gÃ©nÃ©rale | Hot collections |
| **Performance impact** | TrÃ¨s faible | TrÃ¨s faible |
| **FrÃ©quence** | 1s typique | 1-5s typique |

**Utilisation combinÃ©e** :
```bash
# Terminal 1 : Vue globale
mongostat --uri="$MONGO_URI"

# Terminal 2 : Collections chaudes
mongotop --uri="$MONGO_URI" 5

# Analyse : Si mongostat montre qr/qw Ã©levÃ©s, mongotop identifie quelle(s) collection(s)
```

### mongostat vs Prometheus Exporter

| Aspect | mongostat | Prometheus Exporter |
|--------|-----------|-------------------|
| **Temps rÃ©el** | Oui (1s) | Non (15-60s) |
| **Historique** | Non | Oui |
| **Alerting** | Manuel | Automatique |
| **Visualisation** | Terminal | Dashboard |
| **Export** | Possible | Natif |

**StratÃ©gie recommandÃ©e** :
- **Production normale** : Prometheus + Grafana pour monitoring continu
- **Incident/debug** : mongostat pour diagnostic immÃ©diat
- **Analyse** : Combiner les deux sources

### mongostat vs Profiler

| Aspect | mongostat | Profiler |
|--------|-----------|----------|
| **Niveau** | Serveur | RequÃªte |
| **Detail** | AgrÃ©gÃ© | Individuel |
| **Impact** | NÃ©gligeable | 1-5% |
| **Usage** | Temps rÃ©el | Post-mortem |

**Workflow typique** :
1. **Alerte** â†’ mongostat identifie le problÃ¨me global (ex: qr Ã©levÃ©)
2. **mongotop** â†’ Identifie la collection problÃ©matique
3. **Profiler** â†’ Analyse les requÃªtes spÃ©cifiques sur cette collection

---

## Bonnes Pratiques pour SRE

### 1. Checklist de Monitoring

```yaml
Setup Initial:
  âœ“ CrÃ©er utilisateur dÃ©diÃ© monitoring (clusterMonitor)
  âœ“ Tester connexion mongostat/mongotop
  âœ“ Ã‰tablir baseline de performance
  âœ“ Documenter valeurs normales
  âœ“ Configurer alerting sur seuils

Monitoring Quotidien:
  âœ“ VÃ©rifier dashboard Grafana (vue long terme)
  âœ“ ExÃ©cuter mongostat pendant 1-2 minutes si anomalie
  âœ“ Comparer avec baseline
  âœ“ Documenter tout Ã©cart significatif

Investigation Incident:
  âœ“ Capturer mongostat + mongotop immÃ©diatement
  âœ“ Sauvegarder output pour analyse post-mortem
  âœ“ CorrÃ©ler avec logs application
  âœ“ Documenter timeline

Post-Incident:
  âœ“ Analyser donnÃ©es capturÃ©es
  âœ“ Ajuster seuils d'alerte si nÃ©cessaire
  âœ“ Mettre Ã  jour baseline si changement lÃ©gitime
  âœ“ CrÃ©er/mettre Ã  jour runbook
```

### 2. Seuils d'Alerte RecommandÃ©s

```yaml
Seuils Critiques (P0 - Page immÃ©diat):
  qr OR qw: > 50
  dirty: > 40%
  used: > 95%
  conn: > 90% de maxIncomingConnections

Seuils Warning (P1 - Investigation 30min):
  qr OR qw: > 20
  dirty: > 20%
  used: > 85%
  operations (insert+query+update+delete): > 10000/s
  conn: > 75% de maxIncomingConnections

Seuils Information (P2 - Investigation 4h):
  qr OR qw: > 10
  dirty: > 10%
  operations: > 5000/s
  getmore: > 100/s
```

### 3. Scripts d'Automatisation

```bash
#!/bin/bash
# /opt/monitoring/mongodb_health_check.sh

MONGO_URI="mongodb://monitoring:password@prod.example.com:27017/?authSource=admin"
ALERT_EMAIL="ops@example.com"
SLACK_WEBHOOK="https://hooks.slack.com/services/XXX"

# Fonction d'alerte
send_alert() {
  local severity=$1
  local message=$2

  # Email
  echo "$message" | mail -s "MongoDB Alert [$severity]" $ALERT_EMAIL

  # Slack
  local color
  case $severity in
    CRITICAL) color="danger" ;;
    WARNING) color="warning" ;;
    *) color="good" ;;
  esac

  curl -X POST $SLACK_WEBHOOK \
    -H 'Content-Type: application/json' \
    -d "{\"attachments\":[{\"color\":\"$color\",\"text\":\"$message\"}]}"
}

# Capturer stats
STATS=$(mongostat --uri="$MONGO_URI" --rowcount=10 -o=json)

# Analyser
QR_AVG=$(echo $STATS | jq '[.[].qr] | add / length')
QW_AVG=$(echo $STATS | jq '[.[].qw] | add / length')
DIRTY_AVG=$(echo $STATS | jq '[.[].dirty] | add / length')
USED_AVG=$(echo $STATS | jq '[.[].used] | add / length')

# VÃ©rifier seuils
if (( $(echo "$QR_AVG > 50 || $QW_AVG > 50" | bc -l) )); then
  send_alert "CRITICAL" "Queue depth critical: qr=$QR_AVG, qw=$QW_AVG"
elif (( $(echo "$QR_AVG > 20 || $QW_AVG > 20" | bc -l) )); then
  send_alert "WARNING" "Queue depth high: qr=$QR_AVG, qw=$QW_AVG"
fi

if (( $(echo "$DIRTY_AVG > 40" | bc -l) )); then
  send_alert "CRITICAL" "Dirty cache critical: $DIRTY_AVG%"
elif (( $(echo "$DIRTY_AVG > 20" | bc -l) )); then
  send_alert "WARNING" "Dirty cache high: $DIRTY_AVG%"
fi

# Log rÃ©sultat
echo "[$(date)] Health check completed. qr=$QR_AVG, qw=$QW_AVG, dirty=$DIRTY_AVG%, used=$USED_AVG%" >> /var/log/mongodb_health.log
```

### 4. Runbook Template

```markdown
# Runbook: High Queue Depth (mongostat qr/qw > 20)

## Detection
Alert: High queue depth detected via mongostat monitoring

## Initial Response (< 2 minutes)
1. Confirm issue:
   ```bash
   mongostat --uri="$MONGO_URI" --rowcount=30
   ```

2. Check queue type:
   - High qr (read queue) â†’ Go to Read Queue Section
   - High qw (write queue) â†’ Go to Write Queue Section
   - Both high â†’ Go to General Overload Section

## Read Queue (qr > 20)

### Diagnosis
```bash
# 1. Identify slow queries
mongo "$MONGO_URI" --eval "
  db.currentOp({
    'active': true,
    'op': { '\$in': ['query', 'getmore'] },
    'secs_running': { '\$gt': 3 }
  })
"

# 2. Check for missing indexes
mongotop --uri="$MONGO_URI" 5
# Note the hottest read-heavy collections

# 3. Check if collection scans
mongo "$MONGO_URI/mydb" --eval "
  db.system.profile.find({
    'planSummary': /COLLSCAN/
  }).sort({ts: -1}).limit(5)
"
```

### Actions
1. If missing indexes identified:
   - Create index in background
   - Monitor queue reduction

2. If slow queries identified:
   - Consider killing longest running (if safe)
   - Review application code

3. If legitimate load spike:
   - Scale horizontally (add secondaries)
   - Route reads to secondaries

## Write Queue (qw > 20)

### Diagnosis
```bash
# 1. Check I/O performance
iostat -x 1 10

# 2. Check dirty cache
mongostat --uri="$MONGO_URI" -O='dirty,used,flushes'

# 3. Identify write-heavy collections
mongotop --uri="$MONGO_URI" 5
```

### Actions
1. If I/O bottleneck:
   - Investigate disk saturation
   - Consider faster storage (SSD)

2. If dirty cache high (> 20%):
   - Checkpoints may be slow
   - Review storage configuration

3. If bulk operations:
   - Throttle application writes
   - Use bulk operations

## Escalation
If queue remains > 50 for > 5 minutes:
- Page secondary on-call
- Prepare for potential scale-out
```

### 5. Performance Regression Detection

```python
#!/usr/bin/env python3
# regression_detector.py

import json
import statistics
import subprocess
from datetime import datetime, timedelta

class PerformanceRegressionDetector:
    def __init__(self, baseline_file, mongo_uri):
        self.baseline_file = baseline_file
        self.mongo_uri = mongo_uri

        with open(baseline_file) as f:
            self.baseline = json.load(f)

    def collect_current_stats(self, duration_seconds=300):
        """Collect stats for specified duration"""
        rowcount = duration_seconds

        result = subprocess.run(
            ['mongostat', '--uri', self.mongo_uri, '--rowcount', str(rowcount), '-o', 'json'],
            capture_output=True,
            text=True
        )

        return json.loads(result.stdout)

    def calculate_stats(self, data):
        """Calculate statistical metrics"""
        metrics = {}

        for key in ['insert', 'query', 'update', 'delete', 'qr', 'qw', 'dirty', 'used']:
            values = [d[key] for d in data if key in d]
            if values:
                metrics[key] = {
                    'mean': statistics.mean(values),
                    'median': statistics.median(values),
                    'stdev': statistics.stdev(values) if len(values) > 1 else 0,
                    'p95': sorted(values)[int(len(values) * 0.95)] if len(values) > 0 else 0
                }

        return metrics

    def detect_regressions(self, current_metrics):
        """Detect performance regressions"""
        regressions = []

        for metric, stats in current_metrics.items():
            baseline_key = f'{metric}_avg'
            if baseline_key not in self.baseline:
                continue

            baseline_value = self.baseline[baseline_key]
            current_value = stats['mean']

            # Regression si > 50% plus Ã©levÃ© que baseline
            if current_value > baseline_value * 1.5:
                regressions.append({
                    'metric': metric,
                    'baseline': baseline_value,
                    'current_mean': current_value,
                    'current_p95': stats['p95'],
                    'degradation_percent': ((current_value - baseline_value) / baseline_value) * 100
                })

        return regressions

    def report(self, regressions):
        """Generate regression report"""
        if not regressions:
            print("âœ“ No performance regressions detected")
            return

        print(f"\nâš ï¸  DETECTED {len(regressions)} PERFORMANCE REGRESSIONS\n")

        for r in sorted(regressions, key=lambda x: x['degradation_percent'], reverse=True):
            print(f"Metric: {r['metric']}")
            print(f"  Baseline: {r['baseline']:.2f}")
            print(f"  Current (mean): {r['current_mean']:.2f}")
            print(f"  Current (p95): {r['current_p95']:.2f}")
            print(f"  Degradation: {r['degradation_percent']:.1f}%")
            print()

if __name__ == '__main__':
    import sys

    if len(sys.argv) < 3:
        print("Usage: regression_detector.py <baseline_file> <mongo_uri>")
        sys.exit(1)

    detector = PerformanceRegressionDetector(sys.argv[1], sys.argv[2])

    print("Collecting current stats (5 minutes)...")
    current_data = detector.collect_current_stats(300)

    print("Calculating metrics...")
    current_metrics = detector.calculate_stats(current_data)

    print("Detecting regressions...")
    regressions = detector.detect_regressions(current_metrics)

    detector.report(regressions)
```

---

## Troubleshooting

### ProblÃ¨me : mongostat Ne Se Connecte Pas

**SymptÃ´mes** :
```
Error: could not connect to server
```

**Solutions** :
```bash
# 1. VÃ©rifier la connectivitÃ© rÃ©seau
telnet mongodb-host 27017

# 2. VÃ©rifier les credentials
mongo --uri="$MONGO_URI" --eval "db.runCommand({ping: 1})"

# 3. VÃ©rifier les permissions
mongo --uri="$MONGO_URI" --eval "db.runCommand({serverStatus: 1})"

# 4. VÃ©rifier le firewall
sudo iptables -L -n | grep 27017
```

### ProblÃ¨me : MÃ©triques IncohÃ©rentes

**SymptÃ´mes** :
```
mongostat montre des valeurs aberrantes
```

**Solutions** :
```bash
# 1. Augmenter l'intervalle de polling
mongostat --uri="$MONGO_URI" 5  # Au lieu de 1s

# 2. VÃ©rifier la charge serveur
uptime
top

# 3. VÃ©rifier l'horloge systÃ¨me
date
ntpq -p
```

### ProblÃ¨me : mongotop Montre Temps > Intervalle

**SymptÃ´mes** :
```
total: 15.2s (sur intervalle de 1s)
```

**Explication** :
Ceci est normal ! Le temps est cumulÃ© sur toutes les opÃ©rations concurrentes.

**Exemple** :
```
Si 15 opÃ©rations de 1s chacune s'exÃ©cutent en parallÃ¨le,
mongotop montre: total = 15s (sur 1s d'intervalle)
```

---

## RÃ©sumÃ© pour SRE

### Commandes Essentielles

```bash
# Monitoring de base
mongostat --uri="$MONGO_URI"
mongotop --uri="$MONGO_URI" 5

# Diagnostic incident
mongostat --uri="$MONGO_URI" --rowcount=120 > incident_mongostat.txt
mongotop --uri="$MONGO_URI" --rowcount=120 > incident_mongotop.txt

# Replica set
mongostat --uri="$RS_URI" --discover

# Export JSON
mongostat --uri="$MONGO_URI" -o=json --rowcount=60

# Colonnes personnalisÃ©es
mongostat --uri="$MONGO_URI" -O='host,qrw,arw,dirty,used,conn'
```

### MÃ©triques Critiques Ã  Surveiller

```yaml
Tier 1 (Critique):
  - qr/qw: Seuil > 20 = WARNING, > 50 = CRITICAL
  - dirty: Seuil > 20% = WARNING, > 40% = CRITICAL
  - used: Seuil > 85% = WARNING, > 95% = CRITICAL

Tier 2 (Important):
  - ar/aw: Ratio avec CPU cores
  - conn: ProximitÃ© avec maxIncomingConnections
  - operations totales: Tendance croissante

Tier 3 (Informatif):
  - net_in/net_out: Patterns anormaux
  - getmore: Curseurs non fermÃ©s
  - flushes: Checkpoints multiples
```

### Workflow de Diagnostic Standard

```
1. Alerte reÃ§ue
   â†“
2. mongostat (30s) â†’ Vue globale
   â†“
3. ProblÃ¨me identifiÃ© ?
   â”œâ”€ Cache (dirty/used) â†’ VÃ©rifier I/O, ajuster config
   â”œâ”€ Queue (qr/qw) â†’ mongotop pour identifier collections
   â”‚   â†“
   â”‚   4. mongotop (60s) â†’ Collections chaudes
   â”‚   â†“
   â”‚   5. Profiler/currentOp â†’ RequÃªtes spÃ©cifiques
   â”‚   â†“
   â”‚   6. Actions correctives (index, optimisation, scale)
   â”‚
   â””â”€ Connexions â†’ Analyser sources, vÃ©rifier pooling
```

---

## Conclusion

**mongostat** et **mongotop** sont des outils essentiels et complÃ©mentaires pour tout SRE MongoDB :

- **mongostat** fournit une **vue d'ensemble** instantanÃ©e de la santÃ© du serveur
- **mongotop** permet d'**identifier rapidement** les collections problÃ©matiques
- CombinÃ©s, ils permettent un **diagnostic en < 5 minutes**
- Leur **faible impact** permet un usage continu sans affecter la production

**Points clÃ©s Ã  retenir** :
- Ã‰tablir une baseline des valeurs normales
- Configurer l'alerting sur les mÃ©triques critiques
- Les utiliser comme premiÃ¨re ligne de dÃ©fense
- ComplÃ©ter avec profiler et logs pour analyse approfondie
- IntÃ©grer dans le stack de monitoring global

**Prochaines Ã©tapes** :
- CrÃ©er les scripts d'automatisation pour votre environnement
- Ã‰tablir la baseline de performance
- Configurer l'export vers Prometheus/Grafana
- Former l'Ã©quipe aux patterns de diagnostic
- Documenter les runbooks spÃ©cifiques

---

**RÃ©fÃ©rences** :
- [mongostat Documentation](https://www.mongodb.com/docs/database-tools/mongostat/)
- [mongotop Documentation](https://www.mongodb.com/docs/database-tools/mongotop/)
- [MongoDB Performance Best Practices](https://www.mongodb.com/docs/manual/administration/analyzing-mongodb-performance/)
- [serverStatus Command Reference](https://www.mongodb.com/docs/manual/reference/command/serverStatus/)

â­ï¸ [IntÃ©gration avec Prometheus et Grafana](/13-monitoring-administration/07-prometheus-grafana.md)
