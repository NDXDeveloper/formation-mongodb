ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 13.2.3 collStats

## Introduction

La commande `collStats` (collection statistics) fournit des statistiques dÃ©taillÃ©es au niveau d'une collection spÃ©cifique. C'est l'outil le plus granulaire pour analyser les performances, la structure, la fragmentation et l'utilisation des ressources d'une collection individuelle. Pour les SRE et administrateurs systÃ¨me, `collStats` est indispensable pour le diagnostic ciblÃ©, l'optimisation des collections critiques et l'audit dÃ©taillÃ© des index.

Alors que `serverStatus` donne une vue globale et `dbStats` une vue par base de donnÃ©es, `collStats` permet une analyse chirurgicale au niveau collection.

## Vue d'ensemble

### CaractÃ©ristiques principales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       collStats                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Type:        Commande de collection                        â”‚
â”‚  Scope:       Collection spÃ©cifique                         â”‚
â”‚  Permission:  read sur la collection                        â”‚
â”‚  Impact:      Moyen (50-500ms selon taille)                 â”‚
â”‚  FrÃ©quence:   Ã€ la demande / diagnostic                     â”‚
â”‚  Taille:      ~2-10 KB JSON                                 â”‚
â”‚  CoÃ»t:        Parcours mÃ©tadonnÃ©es + stats WiredTiger       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Syntaxe et options

```javascript
// Syntaxe de base
db.collection.stats()

// Ou via runCommand
db.runCommand({ collStats: "collectionName" })

// Avec facteur d'Ã©chelle
db.collection.stats({ scale: 1024*1024 })  // Megabytes

// Options dÃ©taillÃ©es
db.runCommand({
  collStats: "collectionName",
  scale: 1024*1024,         // Facteur d'Ã©chelle (dÃ©faut: 1)
  indexDetails: false,      // DÃ©tails par index (verbose)
  indexDetailsKey: {},      // Stats pour un index spÃ©cifique
  indexDetailsName: "idx"   // Stats pour un index par nom
})

// Obtenir les dÃ©tails de tous les index (ATTENTION: coÃ»teux)
db.collection.stats({ indexDetails: true })

// DÃ©tails d'un index spÃ©cifique
db.collection.stats({
  indexDetailsKey: { username: 1 }
})
```

### Impact des options

```javascript
// Mesurer l'impact des diffÃ©rentes options

function measureCollStatsImpact(collectionName) {
  var results = []

  // Option 1: Stats basiques
  var start = Date.now()
  db[collectionName].stats()
  results.push({
    option: "basic",
    duration: Date.now() - start + "ms"
  })

  // Option 2: Avec scale
  start = Date.now()
  db[collectionName].stats({ scale: 1024*1024 })
  results.push({
    option: "with scale",
    duration: Date.now() - start + "ms"
  })

  // Option 3: Avec indexDetails (COÃ›TEUX)
  start = Date.now()
  db[collectionName].stats({ indexDetails: true })
  results.push({
    option: "indexDetails: true",
    duration: Date.now() - start + "ms",
    warning: "âš ï¸ Can be very slow on large collections"
  })

  return results
}

// Utilisation (sur une petite collection pour test)
var impact = measureCollStatsImpact("users")
print(JSON.stringify(impact, null, 2))
```

---

## Structure du document retournÃ©

```javascript
{
  "ns": "mydb.users",           // Namespace (database.collection)
  "size": 123456789,            // Taille des donnÃ©es (bytes)
  "count": 1000000,             // Nombre de documents
  "avgObjSize": 123,            // Taille moyenne d'un document (bytes)
  "storageSize": 134217728,     // Espace disque allouÃ©
  "freeStorageSize": 10485760,  // Espace libre (MongoDB 6.0+)
  "capped": false,              // Collection capped?
  "max": null,                  // Max documents (si capped)
  "maxSize": null,              // Max size (si capped)

  // Statistiques WiredTiger
  "wiredTiger": {
    "metadata": { ... },
    "creationString": "...",
    "block-manager": { ... },
    "btree": { ... },
    "cache": { ... },
    "compression": { ... }
  },

  // Index
  "nindexes": 5,                // Nombre d'index
  "indexBuilds": [],            // Index en cours de construction
  "totalIndexSize": 45678901,   // Taille totale des index
  "totalSize": 179896629,       // size + totalIndexSize
  "indexSizes": {               // Taille par index
    "_id_": 8388608,
    "username_1": 10485760,
    "email_1": 12582912,
    "created_at_1": 7340032,
    "status_username_1": 14680064
  },

  // DÃ©tails optionnels
  "indexDetails": { ... },      // Si indexDetails: true

  "scaleFactor": 1,             // Facteur d'Ã©chelle appliquÃ©
  "ok": 1
}
```

### HiÃ©rarchie des tailles

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                TAILLES AU NIVEAU COLLECTION                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  totalSize (Taille totale de la collection)                 â”‚
â”‚  â”œâ”€ storageSize (Stockage des donnÃ©es)                      â”‚
â”‚  â”‚  â”œâ”€ size (DonnÃ©es rÃ©elles)                               â”‚
â”‚  â”‚  â””â”€ freeStorageSize (Fragmentation)                      â”‚
â”‚  â””â”€ totalIndexSize (Tous les index)                         â”‚
â”‚     â”œâ”€ _id_ (Index primaire)                                â”‚
â”‚     â”œâ”€ username_1 (Index secondaire)                        â”‚
â”‚     â”œâ”€ email_1 (Index secondaire)                           â”‚
â”‚     â””â”€ ...                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 1 : MÃ©triques de base

### Informations gÃ©nÃ©rales

```javascript
function analyzeBasicStats(stats) {
  var mb = 1024 * 1024
  var gb = 1024 * 1024 * 1024

  return {
    collection: stats.ns,
    documents: {
      count: stats.count.toLocaleString(),
      avgSize: stats.avgObjSize + " bytes (" +
               (stats.avgObjSize / 1024).toFixed(2) + " KB)"
    },
    data: {
      actualSize: (stats.size / mb).toFixed(2) + " MB",
      storageSize: (stats.storageSize / mb).toFixed(2) + " MB",
      freeSpace: stats.freeStorageSize ?
        (stats.freeStorageSize / mb).toFixed(2) + " MB" : "N/A"
    },
    indexes: {
      count: stats.nindexes,
      totalSize: (stats.totalIndexSize / mb).toFixed(2) + " MB",
      avgSizePerIndex: (stats.totalIndexSize / stats.nindexes / mb).toFixed(2) + " MB"
    },
    total: {
      size: (stats.totalSize / mb).toFixed(2) + " MB"
    },
    specialType: {
      capped: stats.capped,
      maxDocuments: stats.max || "N/A",
      maxSize: stats.maxSize ? (stats.maxSize / mb).toFixed(2) + " MB" : "N/A"
    }
  }
}

var stats = db.users.stats()
var analysis = analyzeBasicStats(stats)
print(JSON.stringify(analysis, null, 2))

// RÃ©sultat exemple :
// {
//   "collection": "mydb.users",
//   "documents": {
//     "count": "1,000,000",
//     "avgSize": "123 bytes (0.12 KB)"
//   },
//   "data": {
//     "actualSize": "117.74 MB",
//     "storageSize": "128.00 MB",
//     "freeSpace": "10.00 MB"
//   },
//   "indexes": {
//     "count": 5,
//     "totalSize": "43.58 MB",
//     "avgSizePerIndex": "8.72 MB"
//   },
//   "total": {
//     "size": "171.58 MB"
//   }
// }
```

### Calcul de mÃ©triques dÃ©rivÃ©es

```javascript
function calculateDerivedMetrics(stats) {
  // Ratio de fragmentation
  var fragmentation = stats.storageSize / stats.size

  // Overhead des index (ratio index/data)
  var indexOverhead = stats.totalIndexSize / stats.size

  // DensitÃ© de documents (docs/MB)
  var docDensity = stats.count / (stats.size / (1024*1024))

  // Espace gaspillÃ© par fragmentation
  var wastedSpace = stats.storageSize - stats.size
  var wastedPercent = (wastedSpace / stats.storageSize) * 100

  // EfficacitÃ© du stockage (ratio data/total)
  var storageEfficiency = stats.size / stats.totalSize

  return {
    fragmentation: {
      ratio: fragmentation.toFixed(2) + "x",
      wastedSpace: (wastedSpace / (1024*1024)).toFixed(2) + " MB",
      wastedPercent: wastedPercent.toFixed(2) + "%",
      level: fragmentation < 1.2 ? "LOW âœ…" :
             fragmentation < 1.5 ? "MODERATE âš ï¸" :
             fragmentation < 2.0 ? "HIGH ğŸ”¥" : "SEVERE ğŸ’€"
    },
    indexOverhead: {
      ratio: indexOverhead.toFixed(2) + "x",
      percent: (indexOverhead * 100).toFixed(2) + "%",
      assessment: indexOverhead < 0.3 ? "GOOD âœ…" :
                  indexOverhead < 0.5 ? "MODERATE âš ï¸" : "HIGH ğŸ”¥"
    },
    density: {
      docsPerMB: Math.round(docDensity).toLocaleString(),
      bytesPerDoc: stats.avgObjSize
    },
    efficiency: {
      ratio: storageEfficiency.toFixed(2),
      percent: (storageEfficiency * 100).toFixed(2) + "%",
      assessment: storageEfficiency > 0.7 ? "GOOD âœ…" :
                  storageEfficiency > 0.5 ? "FAIR âš ï¸" : "POOR ğŸ”¥"
    }
  }
}

var derived = calculateDerivedMetrics(db.users.stats())
print(JSON.stringify(derived, null, 2))
```

---

## Section 2 : Analyse des index

### Taille par index

```javascript
function analyzeIndexSizes(stats) {
  var indexes = []
  var totalSize = stats.totalIndexSize

  for (var indexName in stats.indexSizes) {
    var size = stats.indexSizes[indexName]
    var percent = (size / totalSize * 100).toFixed(2)

    indexes.push({
      name: indexName,
      sizeBytes: size,
      sizeMB: (size / (1024*1024)).toFixed(2),
      percentOfTotal: percent + "%",
      isLarge: size > totalSize * 0.3  // > 30% du total
    })
  }

  // Trier par taille dÃ©croissante
  indexes.sort((a, b) => b.sizeBytes - a.sizeBytes)

  return {
    collection: stats.ns,
    totalIndexSize: (totalSize / (1024*1024)).toFixed(2) + " MB",
    indexCount: stats.nindexes,
    indexes: indexes,
    largestIndex: indexes[0],
    recommendations: indexes.filter(idx => idx.isLarge).map(idx => ({
      index: idx.name,
      message: `âš ï¸ ${idx.name} is ${idx.percentOfTotal} of total index size`,
      action: "Review if this index is necessary and optimally designed"
    }))
  }
}

var indexAnalysis = analyzeIndexSizes(db.users.stats())

print(`\nğŸ“Š Index Size Analysis: ${indexAnalysis.collection}`)
print(`Total index size: ${indexAnalysis.totalIndexSize}`)
print(`Index count: ${indexAnalysis.indexCount}\n`)

print("Index breakdown:")
indexAnalysis.indexes.forEach(idx => {
  var icon = idx.isLarge ? "ğŸ”¥" : "âœ…"
  print(`${icon} ${idx.name.padEnd(30)} ${idx.sizeMB.padStart(10)} MB (${idx.percentOfTotal.padStart(6)})`)
})

if (indexAnalysis.recommendations.length > 0) {
  print("\nâš ï¸ Recommendations:")
  indexAnalysis.recommendations.forEach(rec => {
    print(`  ${rec.message}`)
    print(`  â†’ ${rec.action}`)
  })
}
```

### Comparaison avec $indexStats

```javascript
// Combiner collStats avec $indexStats pour analyse complÃ¨te
function comprehensiveIndexAnalysis(collectionName) {
  var coll = db[collectionName]
  var stats = coll.stats()

  // Obtenir les stats d'utilisation via $indexStats
  var indexUsage = coll.aggregate([
    { $indexStats: {} }
  ]).toArray()

  var analysis = []

  indexUsage.forEach(idx => {
    var size = stats.indexSizes[idx.name] || 0
    var accesses = idx.accesses.ops
    var since = idx.accesses.since

    // Calculer l'Ã¢ge de l'index
    var ageMs = Date.now() - since.getTime()
    var ageDays = Math.floor(ageMs / (1000 * 3600 * 24))

    // AccÃ¨s par jour
    var accessesPerDay = ageDays > 0 ? (accesses / ageDays).toFixed(2) : accesses

    analysis.push({
      name: idx.name,
      size: (size / (1024*1024)).toFixed(2) + " MB",
      accesses: accesses,
      accessesPerDay: accessesPerDay,
      ageDays: ageDays,
      key: idx.key,
      efficiency: accesses > 0 ?
        ((accesses / ageDays) / (size / (1024*1024))).toFixed(2) : 0,
      status: accesses === 0 ? "UNUSED âš ï¸" :
              accessesPerDay < 1 ? "RARELY USED âš ï¸" : "ACTIVE âœ…"
    })
  })

  // Trier par efficacitÃ© (accesses/day per MB)
  analysis.sort((a, b) => parseFloat(b.efficiency) - parseFloat(a.efficiency))

  return {
    collection: collectionName,
    totalIndexes: analysis.length,
    analysis: analysis,
    unused: analysis.filter(idx => idx.status.includes("UNUSED")),
    rarelyUsed: analysis.filter(idx => idx.status.includes("RARELY"))
  }
}

var fullIndexAnalysis = comprehensiveIndexAnalysis("users")

print(`\nğŸ“ˆ Comprehensive Index Analysis: ${fullIndexAnalysis.collection}`)
print(`Total indexes: ${fullIndexAnalysis.totalIndexes}\n`)

print("Index efficiency (accesses/day per MB):")
fullIndexAnalysis.analysis.forEach(idx => {
  print(`${idx.status.padEnd(20)} ${idx.name.padEnd(30)} ` +
        `${idx.size.padStart(10)} | ${idx.accessesPerDay.toString().padStart(8)} acc/day | ` +
        `Efficiency: ${idx.efficiency}`)
})

if (fullIndexAnalysis.unused.length > 0) {
  print(`\nâš ï¸ ${fullIndexAnalysis.unused.length} unused indexes found:`)
  fullIndexAnalysis.unused.forEach(idx => {
    print(`  - ${idx.name} (${idx.size}) - Age: ${idx.ageDays} days`)
    print(`    Key: ${JSON.stringify(idx.key)}`)
    print(`    Consider dropping: db.${fullIndexAnalysis.collection}.dropIndex("${idx.name}")`)
  })
}
```

---

## Section 3 : Statistiques WiredTiger

### Analyse du cache et compression

```javascript
function analyzeWiredTigerStats(stats) {
  if (!stats.wiredTiger) {
    return { error: "WiredTiger stats not available" }
  }

  var wt = stats.wiredTiger

  // MÃ©tadonnÃ©es
  var metadata = {
    formatVersion: wt.metadata?.formatVersion || "N/A",
    internalPageSize: wt.btree?.["btree: internal page maximum key size"] || "N/A",
    leafPageSize: wt.btree?.["btree: leaf page maximum key size"] || "N/A"
  }

  // Compression
  var compression = wt.compression || {}
  var compressionRatio = compression["compression: compressed bytes written"] &&
                         compression["compression: uncompressed bytes read"] ?
    (compression["compression: uncompressed bytes read"] /
     compression["compression: compressed bytes written"]).toFixed(2) : "N/A"

  // Block manager (I/O)
  var blockMgr = wt["block-manager"] || {}
  var bytesRead = blockMgr["block-manager: blocks read"] || 0
  var bytesWritten = blockMgr["block-manager: blocks written"] || 0

  // Cache
  var cache = wt.cache || {}
  var cacheHits = cache["cache: pages read into cache"] || 0
  var cacheSize = cache["cache: bytes currently in the cache"] || 0

  return {
    metadata: metadata,
    compression: {
      enabled: compression["compression: pages compressed"] > 0,
      ratio: compressionRatio + "x",
      bytesCompressed: compression["compression: compressed bytes written"] ?
        (compression["compression: compressed bytes written"] / (1024*1024)).toFixed(2) + " MB" : "N/A",
      bytesUncompressed: compression["compression: uncompressed bytes read"] ?
        (compression["compression: uncompressed bytes read"] / (1024*1024)).toFixed(2) + " MB" : "N/A",
      saved: compressionRatio !== "N/A" ?
        ((1 - 1/parseFloat(compressionRatio)) * 100).toFixed(2) + "% space saved" : "N/A"
    },
    io: {
      blocksRead: bytesRead,
      blocksWritten: bytesWritten,
      readMB: (bytesRead * 4096 / (1024*1024)).toFixed(2),  // Block size = 4KB typically
      writtenMB: (bytesWritten * 4096 / (1024*1024)).toFixed(2)
    },
    cache: {
      pagesInCache: cacheHits,
      sizeInCache: (cacheSize / (1024*1024)).toFixed(2) + " MB"
    }
  }
}

var wtAnalysis = analyzeWiredTigerStats(db.users.stats())
print(JSON.stringify(wtAnalysis, null, 2))
```

### Analyse de la structure B-tree

```javascript
function analyzeBTreeStructure(stats) {
  if (!stats.wiredTiger || !stats.wiredTiger.btree) {
    return { error: "B-tree stats not available" }
  }

  var btree = stats.wiredTiger.btree

  return {
    structure: {
      entries: btree["btree: number of key/value pairs"] || 0,
      overflow: btree["btree: overflow pages"] || 0,
      internalPages: btree["btree: internal pages"] || 0,
      leafPages: btree["btree: leaf pages"] || 0
    },
    size: {
      internalPageMax: btree["btree: internal page maximum key size"] || 0,
      leafPageMax: btree["btree: leaf page maximum key size"] || 0,
      overflowPages: btree["btree: overflow pages"] || 0
    },
    health: {
      overflowRatio: btree["btree: overflow pages"] && btree["btree: leaf pages"] ?
        ((btree["btree: overflow pages"] / btree["btree: leaf pages"]) * 100).toFixed(2) + "%" : "0%",
      assessment: (btree["btree: overflow pages"] || 0) === 0 ?
        "âœ… No overflow pages" :
        ((btree["btree: overflow pages"] / btree["btree: leaf pages"]) * 100) > 5 ?
        "âš ï¸ High overflow page ratio - consider document size" : "âœ… OK"
    }
  }
}

var btreeAnalysis = analyzeBTreeStructure(db.users.stats())
print(JSON.stringify(btreeAnalysis, null, 2))
```

---

## Section 4 : DÃ©tection de problÃ¨mes

### Identification de collections problÃ©matiques

```javascript
function identifyCollectionIssues(collectionName) {
  var stats = db[collectionName].stats()
  var issues = []

  // Issue 1: Fragmentation excessive
  var fragmentation = stats.storageSize / stats.size
  if (fragmentation > 1.5) {
    issues.push({
      type: "FRAGMENTATION",
      severity: fragmentation > 2 ? "HIGH" : "MEDIUM",
      value: fragmentation.toFixed(2) + "x",
      description: `Storage is ${fragmentation.toFixed(2)}x larger than data`,
      impact: `Wasting ${((stats.storageSize - stats.size) / (1024*1024)).toFixed(2)} MB`,
      action: "Run db." + collectionName + ".compact() during maintenance window"
    })
  }

  // Issue 2: Index overhead excessif
  var indexOverhead = stats.totalIndexSize / stats.size
  if (indexOverhead > 0.5) {
    issues.push({
      type: "INDEX_OVERHEAD",
      severity: indexOverhead > 1 ? "HIGH" : "MEDIUM",
      value: (indexOverhead * 100).toFixed(2) + "%",
      description: `Indexes consume ${(indexOverhead * 100).toFixed(2)}% of data size`,
      impact: `${(stats.totalIndexSize / (1024*1024)).toFixed(2)} MB in indexes`,
      action: "Audit indexes with $indexStats and remove unused ones"
    })
  }

  // Issue 3: Documents trÃ¨s volumineux
  if (stats.avgObjSize > 10000) {  // > 10KB
    issues.push({
      type: "LARGE_DOCUMENTS",
      severity: stats.avgObjSize > 100000 ? "HIGH" : "MEDIUM",
      value: (stats.avgObjSize / 1024).toFixed(2) + " KB avg",
      description: `Average document size is ${(stats.avgObjSize / 1024).toFixed(2)} KB`,
      impact: "May impact query performance and memory usage",
      action: "Consider schema optimization or splitting large fields"
    })
  }

  // Issue 4: Documents trÃ¨s petits (overhead Ã©levÃ©)
  if (stats.avgObjSize < 50 && stats.count > 100000) {  // < 50 bytes
    issues.push({
      type: "SMALL_DOCUMENTS",
      severity: "MEDIUM",
      value: stats.avgObjSize + " bytes avg",
      description: "Very small documents with high overhead",
      impact: "Storage efficiency is low",
      action: "Consider embedding related data or batch operations"
    })
  }

  // Issue 5: Trop d'index
  if (stats.nindexes > 10) {
    issues.push({
      type: "TOO_MANY_INDEXES",
      severity: stats.nindexes > 15 ? "HIGH" : "MEDIUM",
      value: stats.nindexes + " indexes",
      description: `Collection has ${stats.nindexes} indexes`,
      impact: "Slows down write operations",
      action: "Review index usage with $indexStats, remove unused"
    })
  }

  // Issue 6: Collection trÃ¨s volumineuse (> 10 GB)
  if (stats.size > 10 * 1024 * 1024 * 1024) {
    issues.push({
      type: "LARGE_COLLECTION",
      severity: stats.size > 100 * 1024 * 1024 * 1024 ? "HIGH" : "MEDIUM",
      value: (stats.size / (1024*1024*1024)).toFixed(2) + " GB",
      description: "Very large collection",
      impact: "May benefit from sharding",
      action: "Consider sharding if growth continues"
    })
  }

  return {
    collection: collectionName,
    namespace: stats.ns,
    issueCount: issues.length,
    issues: issues,
    overallHealth: issues.some(i => i.severity === "HIGH") ? "CRITICAL ğŸ”¥" :
                   issues.length > 0 ? "WARNING âš ï¸" : "HEALTHY âœ…"
  }
}

// Analyser une collection
var healthCheck = identifyCollectionIssues("users")

print(`\nğŸ¥ Health Check: ${healthCheck.collection}`)
print(`Overall: ${healthCheck.overallHealth}`)
print(`Issues found: ${healthCheck.issueCount}\n`)

if (healthCheck.issues.length > 0) {
  healthCheck.issues.forEach((issue, idx) => {
    var icon = issue.severity === "HIGH" ? "ğŸ”¥" : "âš ï¸"
    print(`${icon} Issue ${idx + 1}: ${issue.type} [${issue.severity}]`)
    print(`   Value: ${issue.value}`)
    print(`   ${issue.description}`)
    print(`   Impact: ${issue.impact}`)
    print(`   Action: ${issue.action}`)
    print("")
  })
} else {
  print("âœ… No issues detected")
}
```

### Scanner toutes les collections d'une base

```javascript
function scanAllCollections(dbName) {
  var db = db.getSiblingDB(dbName)
  var collections = db.getCollectionNames()
  var report = {
    database: dbName,
    scannedAt: new Date(),
    totalCollections: collections.length,
    collections: [],
    summary: {
      healthy: 0,
      warning: 0,
      critical: 0
    }
  }

  collections.forEach(collName => {
    try {
      var health = identifyCollectionIssues(collName)

      report.collections.push({
        name: collName,
        health: health.overallHealth,
        issueCount: health.issueCount,
        issues: health.issues.map(i => i.type)
      })

      // Compter par niveau de santÃ©
      if (health.overallHealth.includes("HEALTHY")) {
        report.summary.healthy++
      } else if (health.overallHealth.includes("CRITICAL")) {
        report.summary.critical++
      } else {
        report.summary.warning++
      }

    } catch (e) {
      print(`Error scanning ${collName}: ${e.message}`)
    }
  })

  // Trier par criticitÃ©
  report.collections.sort((a, b) => {
    if (a.health.includes("CRITICAL") && !b.health.includes("CRITICAL")) return -1
    if (!a.health.includes("CRITICAL") && b.health.includes("CRITICAL")) return 1
    return b.issueCount - a.issueCount
  })

  return report
}

// ExÃ©cuter le scan
var scanReport = scanAllCollections("production")

print(`\nğŸ“‹ Database Health Scan: ${scanReport.database}`)
print(`Scanned: ${scanReport.totalCollections} collections`)
print(`Healthy: ${scanReport.summary.healthy} âœ…`)
print(`Warning: ${scanReport.summary.warning} âš ï¸`)
print(`Critical: ${scanReport.summary.critical} ğŸ”¥\n`)

// Afficher les collections avec problÃ¨mes
if (scanReport.summary.critical > 0 || scanReport.summary.warning > 0) {
  print("Collections requiring attention:")
  scanReport.collections
    .filter(c => !c.health.includes("HEALTHY"))
    .forEach(c => {
      print(`${c.health.padEnd(20)} ${c.name.padEnd(30)} Issues: ${c.issues.join(", ")}`)
    })
}
```

---

## Section 5 : Comparaison et tendances

### Comparer plusieurs collections

```javascript
function compareCollections(collectionNames) {
  var comparison = []

  collectionNames.forEach(collName => {
    try {
      var stats = db[collName].stats()

      comparison.push({
        name: collName,
        documents: stats.count,
        dataSize: stats.size,
        storageSize: stats.storageSize,
        indexSize: stats.totalIndexSize,
        totalSize: stats.totalSize,
        avgDocSize: stats.avgObjSize,
        indexes: stats.nindexes,
        fragmentation: (stats.storageSize / stats.size).toFixed(2),
        indexRatio: (stats.totalIndexSize / stats.size).toFixed(2)
      })
    } catch (e) {
      print(`Error getting stats for ${collName}: ${e.message}`)
    }
  })

  return comparison
}

// Comparer les principales collections
var collections = ["users", "orders", "products", "logs"]
var comparison = compareCollections(collections)

// Afficher sous forme de tableau
print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
print("â•‘                       COLLECTION COMPARISON                                â•‘")
print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
print("â•‘ Collection    Docs      Data(MB)  Index(MB) Total(MB)  Frag   IdxRatio  â•‘")
print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")

comparison.forEach(c => {
  var line = `â•‘ ${c.name.padEnd(12)} ` +
             `${c.documents.toString().padStart(8)} ` +
             `${(c.dataSize / (1024*1024)).toFixed(1).padStart(9)} ` +
             `${(c.indexSize / (1024*1024)).toFixed(1).padStart(9)} ` +
             `${(c.totalSize / (1024*1024)).toFixed(1).padStart(9)} ` +
             `${c.fragmentation.padStart(6)} ` +
             `${c.indexRatio.padStart(8)} â•‘`
  print(line)
})

print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

// Identifier les extrÃªmes
var largest = comparison.reduce((max, c) => c.totalSize > max.totalSize ? c : max)
var mostFragmented = comparison.reduce((max, c) =>
  parseFloat(c.fragmentation) > parseFloat(max.fragmentation) ? c : max)
var mostIndexes = comparison.reduce((max, c) => c.indexes > max.indexes ? c : max)

print(`\nğŸ“Š Key findings:`)
print(`  Largest collection: ${largest.name} (${(largest.totalSize / (1024*1024*1024)).toFixed(2)} GB)`)
print(`  Most fragmented: ${mostFragmented.name} (${mostFragmented.fragmentation}x)`)
print(`  Most indexes: ${mostIndexes.name} (${mostIndexes.indexes} indexes)`)
```

### Suivi historique des collections

```javascript
// Enregistrer les stats pÃ©riodiquement
function recordCollectionStats(collectionName, storageDb = "monitoring") {
  var stats = db[collectionName].stats()

  var record = {
    timestamp: new Date(),
    collection: collectionName,
    namespace: stats.ns,
    count: stats.count,
    size: stats.size,
    storageSize: stats.storageSize,
    indexSize: stats.totalIndexSize,
    totalSize: stats.totalSize,
    avgObjSize: stats.avgObjSize,
    nindexes: stats.nindexes,
    fragmentation: (stats.storageSize / stats.size).toFixed(2),
    indexRatio: (stats.totalIndexSize / stats.size).toFixed(2)
  }

  db.getSiblingDB(storageDb).collectionStats.insertOne(record)
  return record
}

// Analyser la croissance
function analyzeCollectionGrowth(collectionName, days = 30) {
  var cutoff = new Date(Date.now() - days * 24 * 3600 * 1000)

  var records = db.getSiblingDB("monitoring").collectionStats.find({
    collection: collectionName,
    timestamp: { $gte: cutoff }
  }).sort({ timestamp: 1 }).toArray()

  if (records.length < 2) {
    return { error: "Not enough data points" }
  }

  var first = records[0]
  var last = records[records.length - 1]
  var timeDiffDays = (last.timestamp - first.timestamp) / (1000 * 3600 * 24)

  return {
    collection: collectionName,
    period: {
      days: timeDiffDays.toFixed(1),
      from: first.timestamp,
      to: last.timestamp
    },
    growth: {
      documents: {
        start: first.count,
        end: last.count,
        added: last.count - first.count,
        percent: ((last.count - first.count) / first.count * 100).toFixed(2) + "%",
        perDay: Math.round((last.count - first.count) / timeDiffDays)
      },
      dataSize: {
        start: (first.size / (1024*1024)).toFixed(2) + " MB",
        end: (last.size / (1024*1024)).toFixed(2) + " MB",
        growth: ((last.size - first.size) / (1024*1024)).toFixed(2) + " MB",
        percent: ((last.size - first.size) / first.size * 100).toFixed(2) + "%",
        perDay: ((last.size - first.size) / timeDiffDays / (1024*1024)).toFixed(2) + " MB/day"
      },
      totalSize: {
        start: (first.totalSize / (1024*1024)).toFixed(2) + " MB",
        end: (last.totalSize / (1024*1024)).toFixed(2) + " MB",
        growth: ((last.totalSize - first.totalSize) / (1024*1024)).toFixed(2) + " MB"
      }
    },
    trends: {
      avgDocSize: {
        start: first.avgObjSize,
        end: last.avgObjSize,
        change: last.avgObjSize - first.avgObjSize,
        trend: last.avgObjSize > first.avgObjSize ? "INCREASING â¬†ï¸" : "DECREASING â¬‡ï¸"
      },
      fragmentation: {
        start: first.fragmentation,
        end: last.fragmentation,
        change: (parseFloat(last.fragmentation) - parseFloat(first.fragmentation)).toFixed(2),
        trend: parseFloat(last.fragmentation) > parseFloat(first.fragmentation) ?
          "WORSENING âš ï¸" : "IMPROVING âœ…"
      }
    },
    projection30Days: {
      documents: last.count + Math.round((last.count - first.count) / timeDiffDays * 30),
      dataSize: ((last.size + (last.size - first.size) / timeDiffDays * 30) / (1024*1024)).toFixed(2) + " MB"
    }
  }
}

// Utilisation
var growth = analyzeCollectionGrowth("users", 30)
print(JSON.stringify(growth, null, 2))
```

---

## Section 6 : Cas d'usage pratiques

### 1. PrÃ©paration au compact

```javascript
// Ã‰valuer si un compact est nÃ©cessaire et bÃ©nÃ©fique
function evaluateCompactNeed(collectionName) {
  var stats = db[collectionName].stats()
  var fragmentation = stats.storageSize / stats.size
  var wastedSpace = stats.storageSize - stats.size
  var wastedMB = wastedSpace / (1024*1024)

  var evaluation = {
    collection: collectionName,
    currentStats: {
      dataSize: (stats.size / (1024*1024)).toFixed(2) + " MB",
      storageSize: (stats.storageSize / (1024*1024)).toFixed(2) + " MB",
      fragmentation: fragmentation.toFixed(2) + "x",
      wastedSpace: wastedMB.toFixed(2) + " MB"
    },
    needsCompact: fragmentation > 1.3,
    priority: fragmentation > 2 ? "HIGH" :
              fragmentation > 1.5 ? "MEDIUM" : "LOW",
    potentialSavings: {
      storage: wastedMB.toFixed(2) + " MB",
      percent: ((wastedSpace / stats.storageSize) * 100).toFixed(2) + "%"
    },
    estimatedDuration: {
      // Estimation grossiÃ¨re : 1-2 min par GB
      minutes: Math.round((stats.size / (1024*1024*1024)) * 1.5),
      note: "Actual duration depends on I/O performance"
    },
    impact: {
      blocking: true,
      writeLocked: true,
      recommendation: "Schedule during maintenance window with low traffic"
    },
    command: `db.${collectionName}.compact()`,
    alternativeCommand: `db.runCommand({ compact: "${collectionName}", force: true })`
  }

  return evaluation
}

var compactEval = evaluateCompactNeed("orders")

if (compactEval.needsCompact) {
  print(`\nğŸ”§ Compact Evaluation: ${compactEval.collection}`)
  print(`Priority: ${compactEval.priority}`)
  print(`Current fragmentation: ${compactEval.currentStats.fragmentation}`)
  print(`Potential savings: ${compactEval.potentialSavings.storage} (${compactEval.potentialSavings.percent})`)
  print(`Estimated duration: ~${compactEval.estimatedDuration.minutes} minutes`)
  print(`\nâš ï¸ WARNING: compact blocks all operations on the collection`)
  print(`Recommendation: ${compactEval.impact.recommendation}`)
  print(`\nCommand to run:`)
  print(`  ${compactEval.command}`)
} else {
  print(`âœ… ${compactEval.collection} does not need compacting`)
}
```

### 2. Audit prÃ©-sharding

```javascript
// Analyser une collection pour dÃ©terminer si elle est candidate au sharding
function evaluateShardingCandidate(collectionName) {
  var stats = db[collectionName].stats()
  var sizeGB = stats.size / (1024*1024*1024)

  // Obtenir quelques documents pour analyser la structure
  var samples = db[collectionName].aggregate([
    { $sample: { size: 100 } }
  ]).toArray()

  // Identifier les champs potentiels pour shard key
  var fieldFrequency = {}
  samples.forEach(doc => {
    Object.keys(doc).forEach(field => {
      if (field !== "_id") {
        fieldFrequency[field] = (fieldFrequency[field] || 0) + 1
      }
    })
  })

  var evaluation = {
    collection: collectionName,
    currentSize: {
      dataGB: sizeGB.toFixed(2),
      documents: stats.count.toLocaleString(),
      avgDocSize: stats.avgObjSize
    },
    shardingRecommendation: {
      recommended: sizeGB > 100 || stats.count > 10000000,
      reason: sizeGB > 100 ? "Size exceeds 100 GB" :
              stats.count > 10000000 ? "Document count exceeds 10M" :
              "Collection is manageable in single shard",
      urgency: sizeGB > 500 ? "HIGH" :
               sizeGB > 200 ? "MEDIUM" : "LOW"
    },
    potentialShardKeys: Object.keys(fieldFrequency)
      .filter(field => fieldFrequency[field] === 100)  // Present in all samples
      .slice(0, 5),
    considerations: {
      growthRate: "Check with analyzeCollectionGrowth()",
      queryPatterns: "Analyze with profiler or slow query logs",
      writePattern: "Consider if writes are distributed or concentrated",
      readPattern: "Consider read preference and targeting"
    },
    nextSteps: [
      "Analyze query patterns to identify best shard key",
      "Check cardinality of potential shard key fields",
      "Verify field is present in all documents",
      "Test sharding in staging environment",
      "Plan migration strategy"
    ]
  }

  return evaluation
}

var shardEval = evaluateShardingCandidate("users")
print(JSON.stringify(shardEval, null, 2))
```

### 3. Monitoring de collections critiques

```javascript
// Dashboard pour collections critiques
function criticalCollectionDashboard(collectionNames) {
  print("\n" + "â•".repeat(80))
  print("                   CRITICAL COLLECTIONS DASHBOARD")
  print("â•".repeat(80))
  print(`Updated: ${new Date().toLocaleString()}\n`)

  collectionNames.forEach(collName => {
    try {
      var stats = db[collName].stats()
      var fragmentation = (stats.storageSize / stats.size).toFixed(2)
      var indexRatio = (stats.totalIndexSize / stats.size).toFixed(2)

      // Health indicators
      var health = "âœ…"
      if (parseFloat(fragmentation) > 1.5 || parseFloat(indexRatio) > 0.5) {
        health = "âš ï¸"
      }
      if (parseFloat(fragmentation) > 2 || parseFloat(indexRatio) > 1) {
        health = "ğŸ”¥"
      }

      print(`${health} ${collName}`)
      print(`   Documents: ${stats.count.toLocaleString()}`)
      print(`   Size: ${(stats.size / (1024*1024)).toFixed(2)} MB (data) + ` +
            `${(stats.totalIndexSize / (1024*1024)).toFixed(2)} MB (index)`)
      print(`   Fragmentation: ${fragmentation}x | Index ratio: ${indexRatio}x`)
      print(`   Indexes: ${stats.nindexes} | Avg doc: ${stats.avgObjSize} bytes`)
      print("")

    } catch (e) {
      print(`âŒ ${collName}: Error - ${e.message}\n`)
    }
  })

  print("â•".repeat(80))
}

// Utilisation
var criticalCollections = ["users", "orders", "products", "sessions"]
criticalCollectionDashboard(criticalCollections)

// Peut Ãªtre exÃ©cutÃ© pÃ©riodiquement via cron
```

---

## Section 7 : Automatisation

### Script de monitoring complet

```bash
#!/bin/bash
# monitor_collections.sh
# Monitoring pÃ©riodique des collections critiques

MONGO_HOST="localhost:27017"
MONGO_USER="monitoring_user"
MONGO_PASS="password"
DATABASE="production"
COLLECTIONS=("users" "orders" "products" "logs")
ALERT_FRAG=1.5
ALERT_INDEX=0.5

echo "=== Collection Monitoring - $(date) ===" | tee -a /var/log/mongodb/collection_monitor.log

for COLL in "${COLLECTIONS[@]}"; do
  STATS=$(mongosh "$MONGO_HOST" -u "$MONGO_USER" -p "$MONGO_PASS" --quiet --eval "
    var stats = db.getSiblingDB('$DATABASE')['$COLL'].stats();
    print(JSON.stringify({
      collection: '$COLL',
      count: stats.count,
      size: stats.size,
      storageSize: stats.storageSize,
      indexSize: stats.totalIndexSize,
      fragmentation: (stats.storageSize / stats.size).toFixed(2),
      indexRatio: (stats.totalIndexSize / stats.size).toFixed(2)
    }));
  ")

  # Parse and check thresholds
  FRAG=$(echo "$STATS" | jq -r '.fragmentation')
  INDEX_RATIO=$(echo "$STATS" | jq -r '.indexRatio')

  # Alert if thresholds exceeded
  if (( $(echo "$FRAG > $ALERT_FRAG" | bc -l) )); then
    echo "âš ï¸  ALERT: $COLL fragmentation $FRAG exceeds threshold $ALERT_FRAG" |
      tee -a /var/log/mongodb/collection_monitor.log
    # Send alert (email, Slack, PagerDuty, etc.)
  fi

  if (( $(echo "$INDEX_RATIO > $ALERT_INDEX" | bc -l) )); then
    echo "âš ï¸  ALERT: $COLL index ratio $INDEX_RATIO exceeds threshold $ALERT_INDEX" |
      tee -a /var/log/mongodb/collection_monitor.log
  fi

  # Log stats
  echo "$STATS" >> /var/log/mongodb/collection_stats_$(date +%Y%m%d).json
done

echo "=== Monitoring complete ===" | tee -a /var/log/mongodb/collection_monitor.log
```

### Export vers Prometheus

```javascript
// Formater collStats pour Prometheus
function exportCollStatsPrometheus(collectionName) {
  var stats = db[collectionName].stats()
  var timestamp = Date.now()
  var labels = `{collection="${collectionName}",database="${stats.ns.split('.')[0]}"}`

  var metrics = []

  // MÃ©triques de base
  metrics.push(`mongodb_collection_size_bytes${labels} ${stats.size} ${timestamp}`)
  metrics.push(`mongodb_collection_storage_size_bytes${labels} ${stats.storageSize} ${timestamp}`)
  metrics.push(`mongodb_collection_count${labels} ${stats.count} ${timestamp}`)
  metrics.push(`mongodb_collection_avg_obj_size_bytes${labels} ${stats.avgObjSize} ${timestamp}`)

  // Index
  metrics.push(`mongodb_collection_index_count${labels} ${stats.nindexes} ${timestamp}`)
  metrics.push(`mongodb_collection_index_size_bytes${labels} ${stats.totalIndexSize} ${timestamp}`)

  // MÃ©triques calculÃ©es
  var fragmentation = stats.storageSize / stats.size
  var indexRatio = stats.totalIndexSize / stats.size

  metrics.push(`mongodb_collection_fragmentation_ratio${labels} ${fragmentation.toFixed(4)} ${timestamp}`)
  metrics.push(`mongodb_collection_index_ratio${labels} ${indexRatio.toFixed(4)} ${timestamp}`)

  return metrics.join("\n")
}

// Export pour toutes les collections d'une base
function exportAllCollectionsPrometheus(dbName) {
  var db = db.getSiblingDB(dbName)
  var collections = db.getCollectionNames()
  var allMetrics = []

  collections.forEach(collName => {
    try {
      var metrics = exportCollStatsPrometheus(collName)
      allMetrics.push(metrics)
    } catch (e) {
      print(`Error exporting ${collName}: ${e.message}`)
    }
  })

  return allMetrics.join("\n")
}

// Utilisation
var promMetrics = exportAllCollectionsPrometheus("production")
print(promMetrics)

// Ã‰crire dans un fichier pour node_exporter textfile collector
// writeFile("/var/lib/node_exporter/textfile_collector/mongodb_collections.prom", promMetrics)
```

---

## RÃ©sumÃ©

La commande `collStats` est l'outil le plus granulaire pour :

âœ… **Analyse dÃ©taillÃ©e** : MÃ©triques prÃ©cises au niveau collection

âœ… **Optimisation ciblÃ©e** : Identification des collections problÃ©matiques

âœ… **Gestion des index** : Audit de taille et utilisation par index

âœ… **DÃ©tection de fragmentation** : Ã‰valuation du besoin de compact

âœ… **Planning de sharding** : Identification des candidates

âœ… **Monitoring WiredTiger** : Stats cache, compression, B-tree

**MÃ©triques critiques** :
- `size` / `storageSize` : Fragmentation
- `totalIndexSize` / `size` : Overhead des index
- `avgObjSize` : Taille moyenne des documents
- `nindexes` : Nombre d'index
- `wiredTiger.*` : DÃ©tails moteur de stockage

**Bonnes pratiques** :
- Utiliser pour diagnostic ciblÃ©, pas monitoring continu
- Ã‰viter `indexDetails: true` en production (trÃ¨s coÃ»teux)
- Combiner avec `$indexStats` pour analyse complÃ¨te
- Historiser pour tracking de croissance
- DÃ©finir des seuils d'alerte par collection
- Automatiser la dÃ©tection de problÃ¨mes

**Alertes recommandÃ©es** :
- Fragmentation > 1.5x â†’ Planifier compact
- Index ratio > 0.5 â†’ Auditer les index
- Avg doc size > 10KB â†’ VÃ©rifier schÃ©ma
- Nombre d'index > 10 â†’ Audit d'utilisation
- Croissance > 50%/mois â†’ RÃ©viser capacitÃ©

**Comparaison des commandes** :
- `serverStatus` : Vue globale serveur, < 10ms, collecte continue
- `dbStats` : Vue par base, 10-100ms, collecte pÃ©riodique (5-15min)
- `collStats` : Vue par collection, 50-500ms, diagnostic Ã  la demande

---


â­ï¸ [currentOp](/13-monitoring-administration/02.4-currentop.md)
