üîù Retour au [Sommaire](/SOMMAIRE.md)

# 20.4 Internet des Objets (IoT)

## Introduction

L'Internet des Objets (IoT) g√©n√®re des volumes massifs de donn√©es temporelles n√©cessitant une infrastructure capable de g√©rer :

- **Volume extr√™me** : Millions d'√©v√©nements par seconde
- **V√©locit√© √©lev√©e** : Ingestion continue sans interruption
- **Vari√©t√© de donn√©es** : Multiples types de capteurs et formats
- **Donn√©es temporelles** : S√©ries chronologiques avec agr√©gations
- **Latence critique** : Alertes en temps r√©el (< 1 seconde)
- **R√©tention intelligente** : Donn√©es r√©centes d√©taill√©es, anciennes agr√©g√©es
- **Edge computing** : Traitement distribu√© pr√®s des sources
- **Scalabilit√© massive** : De dizaines √† millions d'appareils

MongoDB excelle dans ce contexte gr√¢ce √† :
- **Time Series Collections** : Optimis√©es pour donn√©es temporelles
- **Sch√©ma flexible** : Diff√©rents types de capteurs sans migration
- **Agr√©gations puissantes** : Analytics en temps r√©el
- **Sharding automatique** : Scalabilit√© horizontale transparente
- **TTL Index** : R√©tention automatique des donn√©es
- **Change Streams** : R√©activit√© temps r√©el pour alertes

## Architecture de r√©f√©rence

### Stack IoT moderne

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              IoT Devices (Edge)                     ‚îÇ
‚îÇ   Sensors ‚Ä¢ Actuators ‚Ä¢ Gateways ‚Ä¢ Controllers      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ MQTT / CoAP / HTTP
                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    Message Broker       ‚îÇ
        ‚îÇ  (MQTT Broker, Kafka)   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ               ‚îÇ               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Stream   ‚îÇ   ‚îÇ Rules    ‚îÇ   ‚îÇ  Device     ‚îÇ
‚îÇProcessor ‚îÇ   ‚îÇ Engine   ‚îÇ   ‚îÇ  Registry   ‚îÇ
‚îÇ(Flink)   ‚îÇ   ‚îÇ(Drools)  ‚îÇ   ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ              ‚îÇ                ‚îÇ
     ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
     ‚îÇ         ‚îÇ  Redis   ‚îÇ          ‚îÇ
     ‚îÇ         ‚îÇ (Cache)  ‚îÇ          ‚îÇ
     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
     ‚îÇ                               ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  MongoDB Time Series    ‚îÇ
     ‚îÇ    Sharded Cluster      ‚îÇ
     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
     ‚îÇ  ‚îÇ  Shard 1 (Hot)   ‚îÇ   ‚îÇ
     ‚îÇ  ‚îÇ  Recent data     ‚îÇ   ‚îÇ
     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
     ‚îÇ  ‚îÇ  Shard 2 (Warm)  ‚îÇ   ‚îÇ
     ‚îÇ  ‚îÇ  Aggregated data ‚îÇ   ‚îÇ
     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
     ‚îÇ  ‚îÇ  Shard 3 (Cold)  ‚îÇ   ‚îÇ
     ‚îÇ  ‚îÇ  Historical data ‚îÇ   ‚îÇ
     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ            ‚îÇ            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAnalytics‚îÇ  ‚îÇ Real-time‚îÇ  ‚îÇ  Alerts  ‚îÇ
‚îÇDashboard‚îÇ  ‚îÇ   API    ‚îÇ  ‚îÇ  Service ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants architecturaux

#### 1. Edge Layer (Devices & Gateways)

**Types d'appareils IoT :**
- **Capteurs** : Temp√©rature, humidit√©, pression, mouvement
- **Actuateurs** : Moteurs, valves, relais
- **Gateways** : Agr√©gation locale et pr√©-traitement
- **Edge Computing Nodes** : Traitement distribu√©

**Protocoles de communication :**
- **MQTT** : L√©ger, pub/sub, id√©al pour IoT
- **CoAP** : REST-like pour appareils contraints
- **HTTP/HTTPS** : Pour appareils puissants
- **LoRaWAN** : Longue port√©e, faible consommation

#### 2. Message Broker

**Options :**
- **MQTT Broker** (Mosquitto, EMQX) : Protocole natif IoT
- **Apache Kafka** : Haute performance, persistance
- **RabbitMQ** : Flexibilit√©, multi-protocoles
- **AWS IoT Core** : Managed cloud solution

**Justification Kafka pour grande √©chelle :**
- Throughput √©lev√© (millions de messages/s)
- Persistance pour replay
- Partitionnement pour parall√©lisme
- √âcosyst√®me riche (Kafka Streams, Connect)

#### 3. Stream Processing

**Apache Flink / Kafka Streams :**
- Agr√©gations temps r√©el (moyennes, compteurs)
- D√©tection d'anomalies
- Enrichissement de donn√©es
- Fen√™tres temporelles (tumbling, sliding)

#### 4. MongoDB Time Series Collections

**Pourquoi Time Series Collections ?**
- Compression automatique (10x vs collections normales)
- Index optimis√©s pour queries temporelles
- Performance en √©criture (bulk inserts)
- Agr√©gations optimis√©es
- R√©tention automatique avec TTL

## Mod√©lisation des donn√©es

### 1. Time Series Collections (MongoDB 5.0+)

#### Configuration de base

```javascript
// Cr√©ation d'une Time Series Collection
db.createCollection("sensor_readings", {
  timeseries: {
    timeField: "timestamp",
    metaField: "metadata",
    granularity: "seconds"  // seconds, minutes, hours
  },
  expireAfterSeconds: 2592000  // 30 jours
});

// Structure recommand√©e
{
  timestamp: ISODate("2024-12-09T14:30:00.000Z"),

  // M√©tadonn√©es (index√©es automatiquement)
  metadata: {
    deviceId: "sensor_temp_001",
    deviceType: "temperature_sensor",
    location: {
      building: "Building_A",
      floor: 3,
      room: "302",
      coordinates: {
        type: "Point",
        coordinates: [2.3522, 48.8566]
      }
    },
    tags: ["critical", "hvac"],
    firmware: "v2.1.3"
  },

  // Mesures (donn√©es temporelles)
  temperature: 22.5,
  humidity: 45.2,
  pressure: 1013.25,

  // M√©tadonn√©es additionnelles
  unit: "celsius",
  accuracy: 0.1,
  batteryLevel: 87,
  signalStrength: -62,  // dBm

  // Qualit√© de la mesure
  quality: {
    valid: true,
    errorCode: null,
    calibrated: true,
    lastCalibration: ISODate("2024-11-01T00:00:00Z")
  }
}
```

#### Choix de granularit√©

```javascript
// Granularit√© selon fr√©quence de mesures

// "seconds" : < 1000 mesures/heure par device
db.createCollection("high_frequency_sensors", {
  timeseries: {
    timeField: "timestamp",
    metaField: "metadata",
    granularity: "seconds"
  }
});

// "minutes" : 1000-100000 mesures/heure par device
db.createCollection("medium_frequency_sensors", {
  timeseries: {
    timeField: "timestamp",
    metaField: "metadata",
    granularity: "minutes"
  }
});

// "hours" : > 100000 mesures/heure par device
db.createCollection("low_frequency_sensors", {
  timeseries: {
    timeField: "timestamp",
    metaField: "metadata",
    granularity: "hours"
  }
});

// Impact sur performance:
// - Granularit√© appropri√©e = compression optimale
// - Granularit√© incorrecte = perte de performance
```

### 2. Device Registry

```javascript
// Collection: devices
{
  _id: "sensor_temp_001",

  // Informations du device
  deviceInfo: {
    type: "temperature_sensor",
    model: "TempPro-2000",
    manufacturer: "IoTSensors Inc.",
    serialNumber: "SN-2024-001234",
    firmwareVersion: "v2.1.3",
    hardwareVersion: "v1.0"
  },

  // Configuration
  config: {
    samplingInterval: 60,  // secondes
    transmissionInterval: 300,  // 5 minutes

    // Seuils d'alerte
    thresholds: {
      temperature: {
        min: 15,
        max: 30,
        critical_min: 10,
        critical_max: 35
      },
      humidity: {
        min: 30,
        max: 70
      }
    },

    // Mode de fonctionnement
    mode: "continuous",  // continuous, on_demand, scheduled
    powerSaving: true
  },

  // Localisation
  location: {
    building: "Building_A",
    floor: 3,
    room: "302",
    zone: "HVAC_Zone_A",
    coordinates: {
      type: "Point",
      coordinates: [2.3522, 48.8566]
    },
    installationDate: ISODate("2024-01-15T10:00:00Z")
  },

  // √âtat actuel
  status: {
    state: "active",  // active, inactive, maintenance, error
    online: true,
    lastSeen: ISODate("2024-12-09T14:30:00Z"),
    lastMeasurement: ISODate("2024-12-09T14:30:00Z"),

    // Sant√© du device
    health: {
      batteryLevel: 87,
      signalStrength: -62,
      errorCount: 0,
      uptime: 2592000,  // secondes
      dataQuality: 0.99  // 99% de donn√©es valides
    }
  },

  // Statistiques (pr√©-calcul√©es)
  stats: {
    totalReadings: 259200,  // 30 jours √ó 86400s / 10s
    lastHourReadings: 360,
    averageTemperature24h: 22.3,
    alertsTriggered: 5,
    lastMaintenanceDate: ISODate("2024-11-01T00:00:00Z")
  },

  // M√©tadonn√©es
  tags: ["critical", "hvac", "monitoring"],
  owner: {
    department: "Facilities",
    contact: "facilities@example.com"
  },

  // Audit
  createdAt: ISODate("2024-01-15T10:00:00Z"),
  updatedAt: ISODate("2024-12-09T14:30:00Z"),
  createdBy: ObjectId("..."),

  // Maintenance
  maintenance: {
    schedule: "monthly",
    lastMaintenance: ISODate("2024-11-01T00:00:00Z"),
    nextMaintenance: ISODate("2025-01-01T00:00:00Z"),
    maintenanceHistory: [
      {
        date: ISODate("2024-11-01T00:00:00Z"),
        type: "calibration",
        notes: "Sensor recalibrated",
        performedBy: "John Technician"
      }
    ]
  }
}

// Index pour devices
db.devices.createIndex({ "status.state": 1, "status.online": 1 });
db.devices.createIndex({ "location.building": 1, "location.floor": 1 });
db.devices.createIndex({ "location.coordinates": "2dsphere" });
db.devices.createIndex({ "deviceInfo.type": 1 });
db.devices.createIndex({ tags: 1 });
db.devices.createIndex({ "status.lastSeen": 1 });
```

### 3. Alertes et √©v√©nements

```javascript
// Collection: alerts
{
  _id: ObjectId("..."),

  // Identification
  alertId: "alert_20241209_143000_001",
  type: "threshold_exceeded",  // threshold, anomaly, offline, error
  severity: "warning",  // info, warning, critical, emergency

  // Device concern√©
  deviceId: "sensor_temp_001",
  deviceType: "temperature_sensor",
  location: {
    building: "Building_A",
    floor: 3,
    room: "302"
  },

  // D√©tails de l'alerte
  trigger: {
    metric: "temperature",
    value: 32.5,
    threshold: 30,
    condition: "greater_than",
    duration: 300  // Dur√©e de d√©passement en secondes
  },

  // Contexte
  context: {
    previousValue: 29.8,
    averageLast24h: 22.3,
    deviation: 10.2,  // √âcart par rapport √† la moyenne

    // Mesures corr√©l√©es
    relatedMetrics: {
      humidity: 38.5,
      pressure: 1012.8
    }
  },

  // √âtat de l'alerte
  status: "active",  // active, acknowledged, resolved, ignored

  // Actions
  actions: [
    {
      type: "notification",
      channel: "email",
      recipient: "facilities@example.com",
      sentAt: ISODate("2024-12-09T14:30:05Z"),
      status: "sent"
    },
    {
      type: "webhook",
      url: "https://api.example.com/alerts",
      sentAt: ISODate("2024-12-09T14:30:05Z"),
      status: "success"
    }
  ],

  // R√©solution
  resolution: {
    acknowledgedBy: ObjectId("user_id"),
    acknowledgedAt: ISODate("2024-12-09T14:35:00Z"),
    resolvedBy: ObjectId("user_id"),
    resolvedAt: ISODate("2024-12-09T15:00:00Z"),
    notes: "HVAC system adjusted, temperature normalized",
    actions_taken: ["adjusted_hvac_settings"]
  },

  // Timestamps
  triggeredAt: ISODate("2024-12-09T14:30:00Z"),
  lastUpdatedAt: ISODate("2024-12-09T15:00:00Z"),

  // TTL pour cleanup automatique
  expiresAt: ISODate("2025-01-08T14:30:00Z")  // 30 jours
}

// Index pour alerts
db.alerts.createIndex({ deviceId: 1, triggeredAt: -1 });
db.alerts.createIndex({ status: 1, severity: 1, triggeredAt: -1 });
db.alerts.createIndex({ "location.building": 1, severity: 1 });
db.alerts.createIndex({ type: 1, triggeredAt: -1 });
db.alerts.createIndex(
  { expiresAt: 1 },
  { expireAfterSeconds: 0 }
);
```

## Ingestion haute performance

### 1. Batch Insert optimis√©

```javascript
class IoTDataIngestionService {
  constructor(db, options = {}) {
    this.db = db;
    this.batchSize = options.batchSize || 1000;
    this.flushInterval = options.flushInterval || 1000;  // ms
    this.buffer = [];
    this.flushTimer = null;
  }

  async ingest(reading) {
    this.buffer.push(reading);

    if (this.buffer.length >= this.batchSize) {
      await this.flush();
    } else if (!this.flushTimer) {
      this.flushTimer = setTimeout(() => this.flush(), this.flushInterval);
    }
  }

  async flush() {
    if (this.buffer.length === 0) return;

    clearTimeout(this.flushTimer);
    this.flushTimer = null;

    const batch = this.buffer.splice(0);

    try {
      // Bulk insert avec ordered: false pour parall√©lisation
      await this.db.collection('sensor_readings').insertMany(
        batch,
        {
          ordered: false,
          writeConcern: { w: 1, j: false }  // Performance over durability
        }
      );

      console.log(`Flushed ${batch.length} readings`);
    } catch (error) {
      console.error('Batch insert failed:', error);

      // Retry logic ou dead letter queue
      await this.handleFailedBatch(batch, error);
    }
  }

  async handleFailedBatch(batch, error) {
    // Option 1: Retry avec exponential backoff
    // Option 2: Stocker dans dead letter queue
    // Option 3: Logger pour investigation manuelle

    await this.db.collection('failed_readings').insertMany(
      batch.map(reading => ({
        ...reading,
        error: error.message,
        failedAt: new Date()
      })),
      { ordered: false }
    );
  }
}

// Utilisation
const ingestionService = new IoTDataIngestionService(db, {
  batchSize: 1000,
  flushInterval: 1000
});

// Consumer Kafka
kafkaConsumer.on('message', async (message) => {
  const reading = JSON.parse(message.value);
  await ingestionService.ingest(reading);
});
```

### 2. Write Concern optimis√©

```javascript
// Configuration selon criticit√© des donn√©es

// Donn√©es critiques (√©quipements m√©dicaux, s√©curit√©)
const criticalWriteConcern = {
  writeConcern: {
    w: "majority",
    j: true,
    wtimeout: 5000
  }
};

// Donn√©es importantes (monitoring industriel)
const standardWriteConcern = {
  writeConcern: {
    w: 1,
    j: true,
    wtimeout: 1000
  }
};

// Donn√©es volumineuses non-critiques (t√©l√©m√©trie g√©n√©rale)
const bulkWriteConcern = {
  writeConcern: {
    w: 1,
    j: false  // Performance over durability
  }
};

// Utilisation
async function insertReading(reading, criticality = 'standard') {
  const concerns = {
    critical: criticalWriteConcern,
    standard: standardWriteConcern,
    bulk: bulkWriteConcern
  };

  return db.collection('sensor_readings').insertOne(
    reading,
    concerns[criticality]
  );
}
```

### 3. Sharding pour scalabilit√©

```javascript
// Strat√©gie de sharding pour Time Series

// Option 1: Shard key sur metaField + timeField
// Avantage: Distribution par device et temps
// Inconv√©nient: Queries cross-shard pour agr√©gations globales

sh.shardCollection(
  "iot.sensor_readings",
  { "metadata.deviceId": "hashed", timestamp: 1 }
);

// Option 2: Shard key sur location + timeField
// Avantage: Queries localis√©es efficaces
// Inconv√©nient: Hotspots si concentration g√©ographique

sh.shardCollection(
  "iot.sensor_readings",
  { "metadata.location.building": 1, timestamp: 1 }
);

// Option 3: Ranged sharding par temps (avec pre-split)
// Avantage: Queries temporelles optimales
// Inconv√©nient: Hotspot sur shard le plus r√©cent

// Pre-split pour √©viter hotspot initial
for (let i = 0; i < 12; i++) {
  const date = new Date(2024, i, 1);
  sh.splitAt(
    "iot.sensor_readings",
    { timestamp: date }
  );
}

// Zone sharding pour archivage
sh.addShardToZone("shard01", "hot");   // Donn√©es r√©centes (SSD)
sh.addShardToZone("shard02", "warm");  // Donn√©es agr√©g√©es
sh.addShardToZone("shard03", "cold");  // Donn√©es archiv√©es (HDD)

const now = new Date();
const weekAgo = new Date(now - 7 * 24 * 3600000);
const monthAgo = new Date(now - 30 * 24 * 3600000);

// Hot data: derni√®re semaine
sh.updateZoneKeyRange(
  "iot.sensor_readings",
  { timestamp: weekAgo },
  { timestamp: new Date(2099, 12, 31) },
  "hot"
);

// Warm data: derni√®res 4 semaines
sh.updateZoneKeyRange(
  "iot.sensor_readings",
  { timestamp: monthAgo },
  { timestamp: weekAgo },
  "warm"
);

// Cold data: > 1 mois
sh.updateZoneKeyRange(
  "iot.sensor_readings",
  { timestamp: new Date(2020, 1, 1) },
  { timestamp: monthAgo },
  "cold"
);
```

## Agr√©gations et analytics

### 1. Fen√™tres temporelles

```javascript
// Agr√©gation par fen√™tres de 5 minutes
async function getAveragesByTimeWindow(deviceId, startDate, endDate) {
  const pipeline = [
    {
      $match: {
        "metadata.deviceId": deviceId,
        timestamp: {
          $gte: startDate,
          $lte: endDate
        }
      }
    },

    // Grouper par fen√™tres de 5 minutes
    {
      $group: {
        _id: {
          $dateTrunc: {
            date: "$timestamp",
            unit: "minute",
            binSize: 5
          }
        },

        avgTemperature: { $avg: "$temperature" },
        minTemperature: { $min: "$temperature" },
        maxTemperature: { $max: "$temperature" },

        avgHumidity: { $avg: "$humidity" },

        count: { $sum: 1 },

        // Calcul de d√©viation standard
        temperatureStdDev: { $stdDevPop: "$temperature" }
      }
    },

    { $sort: { _id: 1 } },

    // Renommer pour clart√©
    {
      $project: {
        _id: 0,
        window: "$_id",
        temperature: {
          avg: { $round: ["$avgTemperature", 2] },
          min: { $round: ["$minTemperature", 2] },
          max: { $round: ["$maxTemperature", 2] },
          stdDev: { $round: ["$temperatureStdDev", 2] }
        },
        humidity: {
          avg: { $round: ["$avgHumidity", 2] }
        },
        sampleCount: "$count"
      }
    }
  ];

  return db.collection('sensor_readings')
    .aggregate(pipeline)
    .toArray();
}
```

### 2. D√©tection d'anomalies

```javascript
// D√©tection bas√©e sur √©cart √† la moyenne mobile
async function detectAnomalies(deviceId, hours = 24) {
  const cutoffDate = new Date(Date.now() - hours * 3600000);

  const pipeline = [
    {
      $match: {
        "metadata.deviceId": deviceId,
        timestamp: { $gte: cutoffDate }
      }
    },

    { $sort: { timestamp: 1 } },

    // Calcul de statistiques globales
    {
      $group: {
        _id: null,
        avgTemp: { $avg: "$temperature" },
        stdDevTemp: { $stdDevPop: "$temperature" },
        readings: { $push: "$$ROOT" }
      }
    },

    // Identifier anomalies (> 3 √©carts-types)
    {
      $project: {
        avgTemp: 1,
        stdDevTemp: 1,
        anomalies: {
          $filter: {
            input: "$readings",
            as: "reading",
            cond: {
              $gt: [
                {
                  $abs: {
                    $subtract: [
                      "$$reading.temperature",
                      "$avgTemp"
                    ]
                  }
                },
                { $multiply: ["$stdDevTemp", 3] }
              ]
            }
          }
        }
      }
    },

    { $unwind: "$anomalies" },

    {
      $project: {
        _id: "$anomalies._id",
        timestamp: "$anomalies.timestamp",
        temperature: "$anomalies.temperature",
        deviation: {
          $abs: {
            $subtract: ["$anomalies.temperature", "$avgTemp"]
          }
        },
        expectedRange: {
          min: { $subtract: ["$avgTemp", { $multiply: ["$stdDevTemp", 3] }] },
          max: { $add: ["$avgTemp", { $multiply: ["$stdDevTemp", 3] }] }
        }
      }
    }
  ];

  return db.collection('sensor_readings')
    .aggregate(pipeline)
    .toArray();
}
```

### 3. Corr√©lation multi-capteurs

```javascript
// Corr√©ler temp√©rature et humidit√© pour d√©tection de patterns
async function correlateSensors(buildingId, startDate, endDate) {
  const pipeline = [
    {
      $match: {
        "metadata.location.building": buildingId,
        timestamp: { $gte: startDate, $lte: endDate }
      }
    },

    // Grouper par fen√™tres horaires et floor
    {
      $group: {
        _id: {
          hour: {
            $dateTrunc: {
              date: "$timestamp",
              unit: "hour"
            }
          },
          floor: "$metadata.location.floor"
        },

        avgTemp: { $avg: "$temperature" },
        avgHumidity: { $avg: "$humidity" },

        devices: { $addToSet: "$metadata.deviceId" },
        sampleCount: { $sum: 1 }
      }
    },

    // Calcul du comfort index
    {
      $addFields: {
        comfortIndex: {
          $subtract: [
            100,
            {
              $add: [
                {
                  $abs: {
                    $subtract: ["$avgTemp", 22]  // Temp√©rature id√©ale
                  }
                },
                {
                  $divide: [
                    {
                      $abs: {
                        $subtract: ["$avgHumidity", 50]  // Humidit√© id√©ale
                      }
                    },
                    2
                  ]
                }
              ]
            }
          ]
        }
      }
    },

    { $sort: { "_id.hour": 1, "_id.floor": 1 } },

    {
      $project: {
        _id: 0,
        timestamp: "$_id.hour",
        floor: "$_id.floor",
        temperature: { $round: ["$avgTemp", 2] },
        humidity: { $round: ["$avgHumidity", 2] },
        comfortIndex: { $round: ["$comfortIndex", 2] },
        devicesCount: { $size: "$devices" },
        sampleCount: 1
      }
    }
  ];

  return db.collection('sensor_readings')
    .aggregate(pipeline)
    .toArray();
}
```

### 4. Downsampling et agr√©gation hi√©rarchique

```javascript
// Pattern : Donn√©es granulaires ‚Üí Agr√©gations pr√©-calcul√©es
// Permet queries rapides sur historique

// Job p√©riodique pour pr√©-agr√©gation
class DataAggregationJob {
  async runHourlyAggregation() {
    const oneHourAgo = new Date(Date.now() - 3600000);
    const twoHoursAgo = new Date(Date.now() - 7200000);

    const pipeline = [
      {
        $match: {
          timestamp: {
            $gte: twoHoursAgo,
            $lt: oneHourAgo
          }
        }
      },

      {
        $group: {
          _id: {
            deviceId: "$metadata.deviceId",
            hour: {
              $dateTrunc: {
                date: "$timestamp",
                unit: "hour"
              }
            }
          },

          // Statistiques compl√®tes
          temperature: {
            avg: { $avg: "$temperature" },
            min: { $min: "$temperature" },
            max: { $max: "$temperature" },
            stdDev: { $stdDevPop: "$temperature" }
          },

          humidity: {
            avg: { $avg: "$humidity" },
            min: { $min: "$humidity" },
            max: { $max: "$humidity" }
          },

          pressure: {
            avg: { $avg: "$pressure" }
          },

          sampleCount: { $sum: 1 },

          // Premi√®re et derni√®re mesure
          firstReading: { $first: "$$ROOT" },
          lastReading: { $last: "$$ROOT" }
        }
      },

      {
        $project: {
          _id: 0,
          deviceId: "$_id.deviceId",
          timestamp: "$_id.hour",
          granularity: "hourly",

          temperature: 1,
          humidity: 1,
          pressure: 1,
          sampleCount: 1,

          metadata: "$firstReading.metadata",

          createdAt: new Date()
        }
      },

      // √âcrire dans collection d'agr√©gats
      {
        $merge: {
          into: "sensor_readings_hourly",
          whenMatched: "replace",
          whenNotMatched: "insert"
        }
      }
    ];

    await db.collection('sensor_readings')
      .aggregate(pipeline)
      .toArray();

    console.log('Hourly aggregation completed');
  }

  async runDailyAggregation() {
    // Similaire mais agr√®ge depuis sensor_readings_hourly
    const oneDayAgo = new Date(Date.now() - 86400000);
    const twoDaysAgo = new Date(Date.now() - 172800000);

    const pipeline = [
      {
        $match: {
          timestamp: {
            $gte: twoDaysAgo,
            $lt: oneDayAgo
          }
        }
      },

      {
        $group: {
          _id: {
            deviceId: "$deviceId",
            day: {
              $dateTrunc: {
                date: "$timestamp",
                unit: "day"
              }
            }
          },

          temperature: {
            avg: { $avg: "$temperature.avg" },
            min: { $min: "$temperature.min" },
            max: { $max: "$temperature.max" }
          },

          humidity: {
            avg: { $avg: "$humidity.avg" },
            min: { $min: "$humidity.min" },
            max: { $max: "$humidity.max" }
          },

          sampleCount: { $sum: "$sampleCount" }
        }
      },

      {
        $merge: {
          into: "sensor_readings_daily",
          whenMatched: "replace",
          whenNotMatched: "insert"
        }
      }
    ];

    await db.collection('sensor_readings_hourly')
      .aggregate(pipeline)
      .toArray();

    console.log('Daily aggregation completed');
  }
}

// Collections pour diff√©rentes granularit√©s:
// - sensor_readings: Donn√©es brutes (TTL: 7 jours)
// - sensor_readings_hourly: Agr√©gats horaires (TTL: 90 jours)
// - sensor_readings_daily: Agr√©gats quotidiens (TTL: 2 ans)
// - sensor_readings_monthly: Agr√©gats mensuels (permanent)
```

## Alerting en temps r√©el

### 1. Change Streams pour d√©tection

```javascript
class RealtimeAlertingService {
  constructor(db) {
    this.db = db;
    this.alertRules = new Map();
  }

  async start() {
    // Watch sur Time Series Collection
    const changeStream = this.db.collection('sensor_readings').watch([
      {
        $match: {
          operationType: 'insert'
        }
      }
    ]);

    changeStream.on('change', async (change) => {
      const reading = change.fullDocument;
      await this.evaluateAlertRules(reading);
    });

    console.log('Real-time alerting service started');
  }

  async evaluateAlertRules(reading) {
    // R√©cup√©rer device config pour seuils
    const device = await this.db.collection('devices')
      .findOne({ _id: reading.metadata.deviceId });

    if (!device) return;

    const thresholds = device.config.thresholds;

    // V√©rifier seuils de temp√©rature
    if (reading.temperature) {
      if (reading.temperature > thresholds.temperature.critical_max) {
        await this.triggerAlert({
          deviceId: device._id,
          severity: 'critical',
          type: 'threshold_exceeded',
          metric: 'temperature',
          value: reading.temperature,
          threshold: thresholds.temperature.critical_max,
          reading
        });
      } else if (reading.temperature > thresholds.temperature.max) {
        await this.triggerAlert({
          deviceId: device._id,
          severity: 'warning',
          type: 'threshold_exceeded',
          metric: 'temperature',
          value: reading.temperature,
          threshold: thresholds.temperature.max,
          reading
        });
      }
    }

    // V√©rifier anomalies (√©cart soudain)
    await this.checkForAnomalies(reading, device);
  }

  async checkForAnomalies(reading, device) {
    // R√©cup√©rer derni√®res mesures pour comparaison
    const recentReadings = await this.db.collection('sensor_readings')
      .find({
        "metadata.deviceId": device._id,
        timestamp: {
          $gte: new Date(Date.now() - 3600000)  // derni√®re heure
        }
      })
      .sort({ timestamp: -1 })
      .limit(10)
      .toArray();

    if (recentReadings.length < 3) return;

    const avgTemp = recentReadings.reduce((sum, r) => sum + r.temperature, 0)
      / recentReadings.length;

    const deviation = Math.abs(reading.temperature - avgTemp);

    // Alerte si √©cart > 5¬∞C soudainement
    if (deviation > 5) {
      await this.triggerAlert({
        deviceId: device._id,
        severity: 'warning',
        type: 'anomaly',
        metric: 'temperature',
        value: reading.temperature,
        expectedValue: avgTemp,
        deviation,
        reading
      });
    }
  }

  async triggerAlert(alertData) {
    const alert = {
      alertId: `alert_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      type: alertData.type,
      severity: alertData.severity,
      deviceId: alertData.deviceId,

      trigger: {
        metric: alertData.metric,
        value: alertData.value,
        threshold: alertData.threshold,
        condition: 'greater_than'
      },

      context: {
        previousValue: alertData.expectedValue,
        deviation: alertData.deviation
      },

      status: 'active',
      triggeredAt: new Date(),

      actions: []
    };

    // Ins√©rer alerte
    await this.db.collection('alerts').insertOne(alert);

    // D√©clencher notifications
    await this.sendNotifications(alert);

    console.log(`Alert triggered: ${alert.alertId}`);
  }

  async sendNotifications(alert) {
    // Email
    if (alert.severity === 'critical') {
      // Envoyer email urgent
      await this.sendEmail({
        to: 'ops@example.com',
        subject: `CRITICAL ALERT: ${alert.type}`,
        body: `Device ${alert.deviceId}: ${alert.trigger.metric} = ${alert.trigger.value}`
      });
    }

    // Webhook
    await this.sendWebhook({
      url: 'https://api.example.com/alerts',
      payload: alert
    });

    // Push notification
    // SMS pour alertes critiques
    // etc.
  }
}
```

### 2. R√®gles d'alerting complexes

```javascript
// Collection: alert_rules
{
  _id: ObjectId("..."),
  name: "HVAC Temperature Monitoring",
  description: "Alert when temperature exceeds threshold for sustained period",

  // Condition
  condition: {
    metric: "temperature",
    operator: "greater_than",
    threshold: 30,
    duration: 300,  // Doit persister 5 minutes
    aggregation: "avg"  // avg, min, max, count
  },

  // Scope
  scope: {
    deviceTypes: ["temperature_sensor"],
    locations: [
      { building: "Building_A", floors: [3, 4] }
    ],
    tags: ["critical", "hvac"]
  },

  // Severity
  severity: "warning",

  // Actions
  actions: [
    {
      type: "email",
      recipients: ["facilities@example.com"],
      template: "temperature_alert"
    },
    {
      type: "webhook",
      url: "https://api.example.com/alerts",
      method: "POST"
    }
  ],

  // Throttling (√©viter spam)
  throttle: {
    enabled: true,
    cooldownPeriod: 1800  // 30 minutes entre alertes similaires
  },

  // Schedule
  schedule: {
    enabled: true,
    timezone: "Europe/Paris",
    activeHours: {
      start: "08:00",
      end: "18:00"
    },
    activeDays: ["monday", "tuesday", "wednesday", "thursday", "friday"]
  },

  // Status
  enabled: true,
  createdAt: ISODate("..."),
  updatedAt: ISODate("...")
}
```

## Edge Computing et MongoDB

### 1. MongoDB on Edge Devices

```javascript
// Configuration MongoDB Realm pour edge
// Synchronisation bi-directionnelle edge ‚Üî cloud

// Sur le gateway edge
const app = new Realm.App({ id: "iot-app-xxxxx" });

// Authentification
const credentials = Realm.Credentials.apiKey(edgeApiKey);
const user = await app.logIn(credentials);

// Ouvrir Realm local avec sync
const realm = await Realm.open({
  schema: [SensorReadingSchema],
  sync: {
    user: user,
    partitionValue: "building_A_floor_3",

    // Configuration sync
    newRealmFileBehavior: {
      type: 'downloadBeforeOpen',
      timeOut: 30000
    },

    existingRealmFileBehavior: {
      type: 'openImmediately'
    }
  }
});

// √âcriture locale (sync automatique vers cloud)
realm.write(() => {
  realm.create('SensorReading', {
    _id: new ObjectId(),
    timestamp: new Date(),
    metadata: {
      deviceId: 'sensor_temp_001',
      location: { building: 'Building_A', floor: 3 }
    },
    temperature: 22.5,
    humidity: 45.2
  });
});

// Le reading sera automatiquement sync vers MongoDB Atlas
// R√©solution de conflits automatique (last-write-wins)
```

### 2. Edge Analytics (pr√©-traitement)

```javascript
// Traitement sur gateway edge avant envoi cloud
class EdgeAnalytics {
  constructor() {
    this.buffer = new Map();  // deviceId -> readings
    this.anomalyThreshold = 3;  // √©carts-types
  }

  async processReading(reading) {
    const deviceId = reading.metadata.deviceId;

    // Maintenir buffer glissant (100 derni√®res mesures)
    if (!this.buffer.has(deviceId)) {
      this.buffer.set(deviceId, []);
    }

    const deviceBuffer = this.buffer.get(deviceId);
    deviceBuffer.push(reading);

    if (deviceBuffer.length > 100) {
      deviceBuffer.shift();
    }

    // Calculs locaux
    const stats = this.calculateStats(deviceBuffer);

    // D√©cision: envoyer au cloud?
    const shouldSendToCloud = this.decideTransmission(reading, stats);

    if (shouldSendToCloud) {
      await this.sendToCloud(reading, stats);
    } else {
      // Stocker localement seulement
      await this.storeLocally(reading);
    }

    // V√©rifier anomalies localement (alerting rapide)
    await this.checkAnomalies(reading, stats);
  }

  calculateStats(readings) {
    const temps = readings.map(r => r.temperature);
    const avg = temps.reduce((a, b) => a + b) / temps.length;

    const variance = temps.reduce((sum, t) =>
      sum + Math.pow(t - avg, 2), 0) / temps.length;

    const stdDev = Math.sqrt(variance);

    return {
      avg,
      stdDev,
      min: Math.min(...temps),
      max: Math.max(...temps)
    };
  }

  decideTransmission(reading, stats) {
    // Strat√©gies pour r√©duire bande passante:

    // 1. Envoyer si √©cart significatif
    const deviation = Math.abs(reading.temperature - stats.avg);
    if (deviation > stats.stdDev * 2) {
      return true;
    }

    // 2. Envoyer p√©riodiquement (heartbeat)
    const timeSinceLastSent = Date.now() - this.lastSentTime;
    if (timeSinceLastSent > 300000) {  // 5 minutes
      this.lastSentTime = Date.now();
      return true;
    }

    // 3. Sinon, agr√©ger localement
    return false;
  }

  async sendToCloud(reading, stats) {
    // Enrichir avec statistiques edge
    const enrichedReading = {
      ...reading,
      edgeStats: stats,
      processedAt: new Date()
    };

    // Envoyer via MQTT/HTTP
    await mqttClient.publish(
      `iot/readings/${reading.metadata.deviceId}`,
      JSON.stringify(enrichedReading)
    );
  }
}
```

## Performance et optimisation

### 1. M√©triques de performance IoT

```javascript
const iotPerformanceMetrics = {
  // Ingestion
  'ingestion.throughput': {
    description: 'Messages ing√©r√©s par seconde',
    target: 100000,  // 100K messages/s
    alert_threshold: 50000
  },

  'ingestion.latency.p99': {
    description: 'Latence p99 d\'ingestion',
    target: 100,  // ms
    alert_threshold: 500
  },

  'ingestion.batch_size': {
    description: 'Taille moyenne des batches',
    target: 1000,
    alert_threshold: 100
  },

  // Storage
  'storage.compression_ratio': {
    description: 'Ratio de compression Time Series',
    target: 10,  // 10x compression
    alert_threshold: 5
  },

  'storage.growth_rate': {
    description: 'Croissance du stockage (GB/jour)',
    monitor: true
  },

  // Queries
  'query.aggregation_time.p95': {
    description: 'Temps d\'agr√©gation p95',
    target: 200,  // ms
    alert_threshold: 1000
  },

  // Devices
  'devices.online_ratio': {
    description: 'Pourcentage de devices online',
    target: 0.98,  // 98%
    alert_threshold: 0.90
  },

  'devices.data_quality': {
    description: 'Qualit√© moyenne des donn√©es',
    target: 0.99,  // 99%
    alert_threshold: 0.95
  },

  // Alerting
  'alerts.detection_latency.p99': {
    description: 'Latence d√©tection alerte p99',
    target: 1000,  // 1s
    alert_threshold: 5000
  }
};
```

### 2. Optimisations sp√©cifiques IoT

```javascript
// Configuration MongoDB optimis√©e pour IoT
const mongoConfig = {
  // WiredTiger cache
  storage: {
    wiredTiger: {
      engineConfig: {
        cacheSizeGB: 32,  // 50-60% RAM
        journalCompressor: "snappy",
        directoryForIndexes: true
      },

      collectionConfig: {
        blockCompressor: "zstd"  // Meilleure compression
      },

      indexConfig: {
        prefixCompression: true
      }
    }
  },

  // Op√©rations
  operationProfiling: {
    mode: 1,  // Log slow operations
    slowms: 100
  },

  // Network
  net: {
    maxIncomingConnections: 10000,
    compression: {
      compressors: ["snappy", "zstd"]
    }
  },

  // Replication
  replication: {
    oplogSizeMB: 50000  // 50 GB pour IoT haute fr√©quence
  }
};

// Index optimis√©s pour Time Series
db.sensor_readings.createIndex(
  { "metadata.deviceId": 1, timestamp: -1 },
  {
    name: "device_time_idx",
    background: true,
    // Partial index: seulement derni√®res 24h en m√©moire
    partialFilterExpression: {
      timestamp: {
        $gte: new Date(Date.now() - 86400000)
      }
    }
  }
);
```

## Checklist de d√©ploiement IoT

### ‚úÖ Architecture

- [ ] Message broker configur√© (MQTT/Kafka)
- [ ] Stream processing d√©ploy√© (Flink/Kafka Streams)
- [ ] Time Series Collections cr√©√©es
- [ ] Sharding configur√© selon charge
- [ ] Edge gateways provisionn√©s
- [ ] Device registry initialis√©

### ‚úÖ Ingestion

- [ ] Batch inserts impl√©ment√©s (1000+ par batch)
- [ ] Write Concern adapt√© √† la criticit√©
- [ ] Dead letter queue pour √©checs
- [ ] Monitoring de throughput
- [ ] Rate limiting configur√©

### ‚úÖ R√©tention

- [ ] TTL configur√© sur donn√©es brutes (7-30 jours)
- [ ] Agr√©gations horaires automatis√©es
- [ ] Agr√©gations quotidiennes automatis√©es
- [ ] Archive vers cold storage (S3/Glacier)
- [ ] Strat√©gie de downsampling d√©finie

### ‚úÖ Alerting

- [ ] Change Streams pour temps r√©el
- [ ] R√®gles d'alerting configur√©es
- [ ] Throttling anti-spam activ√©
- [ ] Canaux de notification (email, SMS, webhook)
- [ ] Escalation automatique pour alertes critiques

### ‚úÖ Performance

- [ ] Index optimis√©s pour queries temporelles
- [ ] Compression activ√©e (zstd)
- [ ] Oplog surdimensionn√© (50 GB+)
- [ ] Cache WiredTiger optimis√©
- [ ] M√©triques de performance suivies

### ‚úÖ S√©curit√©

- [ ] Authentification MQTT (TLS client certificates)
- [ ] Chiffrement en transit activ√©
- [ ] Device credentials rot√©s r√©guli√®rement
- [ ] Access control par device/location
- [ ] Audit logging activ√©

### ‚úÖ Op√©rations

- [ ] Backup automatique quotidien
- [ ] Monitoring devices online/offline
- [ ] Alertes sur qualit√© de donn√©es
- [ ] Runbooks pour incidents
- [ ] Plan de disaster recovery

## Conclusion

MongoDB est particuli√®rement adapt√© aux workloads IoT gr√¢ce √† :

**‚úÖ Forces d√©montr√©es :**
- Time Series Collections avec compression 10x
- Ingestion haute performance (millions de messages/s)
- Sch√©ma flexible pour types de capteurs h√©t√©rog√®nes
- Agr√©gations puissantes pour analytics temps r√©el
- TTL automatique pour r√©tention intelligente
- Sharding transparent pour scalabilit√© illimit√©e
- Change Streams pour alerting r√©actif

**‚ö†Ô∏è Consid√©rations importantes :**
- Write Concern doit balancer durabilit√© et performance
- Strat√©gie de r√©tention essentielle (co√ªts storage)
- Agr√©gations pr√©-calcul√©es pour historique
- Monitoring de qualit√© de donn√©es critique
- Edge processing r√©duit bande passante et latence

**üéØ Patterns essentiels IoT :**
1. **Time Series Collections** pour donn√©es temporelles
2. **Batch Inserts** pour ingestion haute performance
3. **Downsampling hi√©rarchique** pour historique
4. **Zone Sharding** pour hot/warm/cold data
5. **Change Streams** pour alerting temps r√©el
6. **Edge Analytics** pour pr√©-traitement local

Cette architecture supporte des d√©ploiements IoT de quelques centaines √† plusieurs millions d'appareils, avec ingestion de milliards de points de donn√©es par jour.

---

**R√©f√©rences :**
- MongoDB Time Series Documentation
- "Designing Data-Intensive Applications" - Martin Kleppmann
- AWS IoT Core Best Practices
- "Building the Internet of Things" - Maciej Kranz
- Apache Kafka for IoT Use Cases

‚è≠Ô∏è [Gaming et leaderboards](/20-cas-usage-architectures/05-gaming-leaderboards.md)
