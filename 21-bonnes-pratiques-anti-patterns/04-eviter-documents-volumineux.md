üîù Retour au [Sommaire](/SOMMAIRE.md)

# 21.4 √âviter les Documents Trop Volumineux

## Introduction

MongoDB impose une limite stricte de 16 MB par document, mais cette limite technique n'est que le sommet de l'iceberg. Bien avant d'atteindre cette limite fatale, des documents volumineux cr√©ent des probl√®mes de performance, de m√©moire, de r√©seau et de maintenabilit√© qui peuvent paralyser une application. Un document de "seulement" 1-2 MB peut d√©j√† causer des probl√®mes significatifs en production.

Cette section explore pourquoi la taille des documents est critique, comment identifier et pr√©venir la croissance excessive, et quels patterns architecturaux utiliser pour g√©rer efficacement les grandes quantit√©s de donn√©es tout en maintenant des documents compacts et performants.

---

## Comprendre les Limites et les Impacts

### La Limite Technique : 16 MB

MongoDB utilise le format BSON (Binary JSON) qui impose une limite stricte :

```javascript
// ‚ùå Erreur fatale √† l'insertion
BSONObj size: 16777217 (0x1000001) is invalid.
Size must be between 0 and 16793600(16MB)
```

**Pourquoi cette limite existe** :
- Pr√©vention des allocations m√©moire massives
- Protection contre les attaques par d√©ni de service
- Garantie de performance raisonnable
- Limite de stockage contigu√´

### L'Impact R√©el Commence Bien Avant

Les probl√®mes de performance apparaissent progressivement :

| Taille Document | Impact | Sympt√¥mes |
|-----------------|--------|-----------|
| **< 100 KB** | ‚úÖ Optimal | Performance nominale |
| **100 KB - 500 KB** | ‚ö†Ô∏è Acceptable | L√©ger impact sur transfert r√©seau |
| **500 KB - 1 MB** | ‚ö†Ô∏è Probl√©matique | Latence accrue, cache moins efficace |
| **1 MB - 5 MB** | ‚ùå S√©rieux | Performance d√©grad√©e, fragmentation |
| **5 MB - 16 MB** | ‚ùå Critique | Probl√®mes majeurs, r√©vision n√©cessaire |
| **> 16 MB** | üí• Impossible | Erreur, document rejet√© |

---

## ‚úÖ DO : Maintenir les Documents Sous 1 MB

**Explication** : Un seuil de 1 MB devrait √™tre consid√©r√© comme la limite pratique pour la majorit√© des cas d'usage, et id√©alement viser moins de 100 KB.

**Seuils recommand√©s** :
```javascript
// ‚úÖ Tailles cibles
const DOCUMENT_SIZE_LIMITS = {
  OPTIMAL: 100 * 1024,        // 100 KB - Cible id√©ale
  ACCEPTABLE: 500 * 1024,     // 500 KB - Limite confortable
  WARNING: 1 * 1024 * 1024,   // 1 MB - Seuil d'alerte
  CRITICAL: 5 * 1024 * 1024,  // 5 MB - R√©vision urgente
  MAXIMUM: 16 * 1024 * 1024   // 16 MB - Limite technique
};

// Fonction de v√©rification
function validateDocumentSize(doc) {
  const size = JSON.stringify(doc).length;

  if (size > DOCUMENT_SIZE_LIMITS.CRITICAL) {
    throw new Error(`Document too large: ${(size/1024/1024).toFixed(2)} MB`);
  }

  if (size > DOCUMENT_SIZE_LIMITS.WARNING) {
    console.warn(`Document approaching size limit: ${(size/1024).toFixed(2)} KB`);
  }

  return size;
}
```

**B√©n√©fices de documents compacts** :

### 1. Performance R√©seau
```javascript
// Document 10 KB vs 2 MB
// Temps de transfert sur connexion 100 Mbps

10 KB : ~1 ms
2 MB  : ~160 ms
= 160x plus lent
```

### 2. Utilisation M√©moire
```javascript
// MongoDB charge les documents entiers en m√©moire
// 1000 requ√™tes simultan√©es

Documents 10 KB : 10 MB RAM
Documents 2 MB  : 2 GB RAM
= 200x plus de m√©moire
```

### 3. Efficacit√© du Cache
- Plus de documents tiennent en cache WiredTiger
- Taux de cache hit am√©lior√©
- Moins d'I/O disque

### 4. Rapidit√© des Op√©rations
```javascript
// Temps de lecture compl√®te du document
Document 10 KB : <1 ms
Document 2 MB  : 50-100 ms
```

---

## ‚ùå DON'T : Laisser les Tableaux Cro√Ætre Ind√©finiment

**Explication** : Le pattern le plus dangereux et commun : des tableaux qui s'agrandissent sans limite au fil du temps.

**Anti-pattern critique** :
```javascript
// ‚ùå DANGER : Tableau qui grossit sans limite
{
  _id: ObjectId("..."),
  userId: "user123",
  username: "alice",
  loginHistory: [
    { date: ISODate("2024-01-01T10:00:00Z"), ip: "192.168.1.1" },
    { date: ISODate("2024-01-02T09:30:00Z"), ip: "192.168.1.1" },
    // ... des milliers d'entr√©es s'accumulent
    // Document grossit de 100 bytes √† chaque login
    // Apr√®s 10,000 logins = 1 MB
    // Apr√®s 100,000 logins = 10 MB
    // Apr√®s 160,000 logins = 16 MB = ERREUR!
  ]
}
```

**Sc√©narios courants** :
```javascript
// ‚ùå Historique d'√©v√©nements illimit√©
{
  orderId: "ORD123",
  events: [
    { timestamp: "...", action: "created" },
    { timestamp: "...", action: "updated" },
    // Des centaines/milliers d'√©v√©nements...
  ]
}

// ‚ùå Commentaires illimit√©s
{
  articleId: "ART456",
  comments: [
    { user: "...", text: "...", date: "..." },
    // Des milliers de commentaires...
  ]
}

// ‚ùå Messages de conversation illimit√©s
{
  conversationId: "CONV789",
  messages: [
    { from: "...", to: "...", text: "...", date: "..." },
    // Des dizaines de milliers de messages...
  ]
}

// ‚ùå Logs applicatifs dans le document
{
  processId: "PROC001",
  logs: [
    { level: "info", message: "...", timestamp: "..." },
    // Des millions d'entr√©es de log...
  ]
}
```

**Cons√©quences** :

### 1. Croissance Exponentielle
```javascript
// Progression typique d'un document avec array illimit√©
Jour 1    : 5 KB
Mois 1    : 50 KB
Mois 6    : 500 KB
An 1      : 2 MB
An 2      : 4 MB
An 3      : 8 MB
An 4      : 16 MB = CRASH!
```

### 2. Performance D√©grad√©e
- Lecture compl√®te obligatoire m√™me pour un seul √©l√©ment
- Temps de parsing JSON explosif
- Saturation m√©moire client

### 3. Fragmentation du Stockage
```javascript
// Document initialement : 5 KB, allou√© dans un espace de 5 KB
// Apr√®s 6 mois : 500 KB, ne tient plus dans l'espace original
// MongoDB doit :
// 1. Allouer 500 KB ailleurs
// 2. Copier le document
// 3. Laisser l'ancien espace vide (fragmentation)
// 4. Mettre √† jour tous les index

// Impact :
// - Performance d√©grad√©e lors des updates
// - Espace disque gaspill√©
// - Besoin de compaction r√©guli√®re
```

### 4. Impossibilit√© d'Indexer Efficacement
```javascript
// ‚ùå Impossible d'indexer efficacement dans un grand tableau
db.articles.createIndex({ "comments.userId": 1 });
// Si 10,000 commentaires par article :
// - Index devient √©norme
// - Performance catastrophique
// - Multikey index inefficace
```

---

## ‚úÖ DO : Utiliser le Pattern Bucket pour les S√©ries Temporelles

**Explication** : Le pattern Bucket limite le nombre d'√©l√©ments par document et cr√©e de nouveaux documents au besoin.

**Pattern recommand√©** :
```javascript
// ‚úÖ Pattern Bucket : Limiter √† 100 √©v√©nements par document
{
  _id: ObjectId("..."),
  userId: "user123",
  bucketDate: ISODate("2024-01-01T00:00:00Z"), // D√©but du bucket
  eventCount: 100,  // Compteur
  events: [
    { date: ISODate("2024-01-01T10:00:00Z"), action: "login" },
    { date: ISODate("2024-01-01T10:15:00Z"), action: "view_page" },
    // Maximum 100 √©v√©nements
  ]
}

// Quand le bucket est plein, cr√©er un nouveau document
{
  _id: ObjectId("..."),
  userId: "user123",
  bucketDate: ISODate("2024-01-02T00:00:00Z"),
  eventCount: 45,
  events: [
    // Nouveaux √©v√©nements
  ]
}
```

**Impl√©mentation** :
```javascript
// ‚úÖ Fonction d'insertion avec bucketing
async function addUserEvent(userId, event) {
  const today = new Date();
  today.setHours(0, 0, 0, 0);

  const result = await db.userEvents.updateOne(
    {
      userId: userId,
      bucketDate: today,
      eventCount: { $lt: 100 }  // Bucket pas encore plein
    },
    {
      $push: { events: event },
      $inc: { eventCount: 1 }
    },
    { upsert: true }
  );

  // Si aucun document modifi√©, tous les buckets sont pleins
  if (result.matchedCount === 0) {
    // Cr√©er un nouveau bucket avec un identifiant unique
    await db.userEvents.insertOne({
      userId: userId,
      bucketDate: today,
      bucketSequence: await getNextBucketSequence(userId, today),
      eventCount: 1,
      events: [event]
    });
  }
}
```

**Avantages mesurables** :
```javascript
// Sans bucketing (1 document par user)
// 100,000 √©v√©nements par user = 10 MB par document
// 1,000 users actifs = 10 GB en m√©moire pour requ√™tes courantes

// Avec bucketing (100 √©v√©nements max)
// 100 √©v√©nements par document = ~10 KB par document
// 1,000 buckets (100 √©v√©nements √ó 1,000 users) = 10 MB
= 1000x moins de m√©moire pour donn√©es r√©centes
```

**Variantes du pattern** :
```javascript
// ‚úÖ Bucket par heure (donn√©es haute fr√©quence)
{
  sensorId: "SENSOR123",
  hourBucket: ISODate("2024-01-15T14:00:00Z"),
  readings: [/* max 3600 lectures */]
}

// ‚úÖ Bucket par taille maximale (plut√¥t que count)
{
  userId: "user123",
  bucketId: 42,
  maxSizeBytes: 100000,  // 100 KB max
  currentSize: 87234,
  items: [/* donn√©es variables */]
}

// ‚úÖ Bucket avec m√©tadonn√©es agr√©g√©es
{
  storeId: "STORE001",
  date: ISODate("2024-01-15"),
  transactionCount: 150,
  totalAmount: 15750.00,
  transactions: [/* max 200 transactions */],
  // M√©tadonn√©es pour requ√™tes rapides sans parser le tableau
  summary: {
    minAmount: 5.00,
    maxAmount: 250.00,
    avgAmount: 105.00
  }
}
```

---

## ‚úÖ DO : Utiliser le Pattern Subset pour Donn√©es Partielles

**Explication** : Imbriquer seulement un sous-ensemble des donn√©es les plus pertinentes, avec r√©f√©rence vers les donn√©es compl√®tes.

**Pattern recommand√©** :
```javascript
// ‚úÖ Document article avec subset de commentaires r√©cents
{
  _id: ObjectId("..."),
  title: "Understanding MongoDB Patterns",
  content: "...",
  author: "Alice",

  // Subset : seulement les 10 commentaires les plus r√©cents
  recentComments: [
    {
      _id: ObjectId("..."),
      userId: "user123",
      text: "Great article!",
      createdAt: ISODate("2024-01-15T10:00:00Z")
    },
    // ... 9 autres commentaires r√©cents
  ],

  // M√©tadonn√©es
  totalComments: 5247,  // Nombre total
  commentsCollectionRef: "article_comments"  // R√©f√©rence √† collection compl√®te
}

// Collection s√©par√©e pour tous les commentaires
// Collection: article_comments
{
  _id: ObjectId("..."),
  articleId: ObjectId("..."),  // R√©f√©rence √† l'article
  userId: "user123",
  text: "Great article!",
  createdAt: ISODate("2024-01-15T10:00:00Z")
}
```

**Impl√©mentation** :
```javascript
// ‚úÖ Maintenir le subset √† jour
async function addComment(articleId, comment) {
  // 1. Ajouter √† la collection compl√®te
  const commentDoc = await db.article_comments.insertOne({
    articleId: articleId,
    ...comment,
    createdAt: new Date()
  });

  // 2. Ajouter au subset (limit√© √† 10 plus r√©cents)
  await db.articles.updateOne(
    { _id: articleId },
    {
      $push: {
        recentComments: {
          $each: [{
            _id: commentDoc.insertedId,
            ...comment,
            createdAt: new Date()
          }],
          $sort: { createdAt: -1 },
          $slice: 10  // Garder seulement les 10 plus r√©cents
        }
      },
      $inc: { totalComments: 1 }
    }
  );
}

// Lire tous les commentaires si n√©cessaire
async function getAllComments(articleId) {
  return await db.article_comments
    .find({ articleId: articleId })
    .sort({ createdAt: -1 })
    .toArray();
}
```

**Cas d'usage** :
```javascript
// ‚úÖ E-commerce : Produit avec reviews r√©centes
{
  productId: "PROD123",
  name: "Wireless Headphones",
  price: 99.99,

  // Top 5 reviews les plus utiles
  topReviews: [/* 5 reviews */],
  totalReviews: 1547,
  averageRating: 4.2
}

// ‚úÖ R√©seau social : Profil avec posts r√©cents
{
  userId: "user123",
  username: "alice",
  bio: "...",

  // 20 posts les plus r√©cents
  recentPosts: [/* 20 posts */],
  totalPosts: 3421
}

// ‚úÖ Syst√®me de tickets : Ticket avec derni√®res interactions
{
  ticketId: "TICK789",
  subject: "Login issue",
  status: "open",

  // 10 derni√®res interactions
  recentUpdates: [/* 10 updates */],
  totalUpdates: 156
}
```

**Avantages** :
- Chargement rapide des cas d'usage courants (90%)
- Document reste compact
- Acc√®s aux donn√©es compl√®tes quand n√©cessaire
- Pas de fragmentation

---

## ‚ùå DON'T : Imbriquer des Objets Volumineux Sans Limite

**Explication** : Les objets imbriqu√©s profonds et volumineux sont aussi probl√©matiques que les tableaux sans limite.

**Anti-patterns** :
```javascript
// ‚ùå Historique complet de modifications imbriqu√©
{
  _id: ObjectId("..."),
  documentId: "DOC123",
  currentVersion: { /* contenu actuel */ },

  // Historique complet de toutes les versions
  versionHistory: {
    v1: { content: "...", metadata: {...}, date: "..." },
    v2: { content: "...", metadata: {...}, date: "..." },
    // ... des centaines de versions
    v847: { content: "...", metadata: {...}, date: "..." }
  }
  // Document devient √©norme au fil du temps
}

// ‚ùå Configuration avec tous les profils
{
  appId: "APP001",
  config: {
    production: { /* 1000 param√®tres */ },
    staging: { /* 1000 param√®tres */ },
    development: { /* 1000 param√®tres */ },
    test: { /* 1000 param√®tres */ },
    user1: { /* param√®tres personnalis√©s */ },
    user2: { /* param√®tres personnalis√©s */ },
    // ... des milliers d'utilisateurs
  }
}

// ‚ùå Cache volumineux imbriqu√©
{
  userId: "user123",
  profile: { /* donn√©es profil */ },

  // Cache de toutes les donn√©es utilisateur
  cache: {
    orders: [/* tous les orders */],
    wishlist: [/* tous les items */],
    browsing_history: [/* tout l'historique */],
    recommendations: [/* toutes les recommandations */]
  }
  // Document devient multi-megabytes
}
```

**Solution** :
```javascript
// ‚úÖ S√©parer les versions dans une collection d√©di√©e
// Collection: documents
{
  _id: ObjectId("..."),
  documentId: "DOC123",
  currentVersion: 847,
  content: { /* contenu actuel */ },
  lastModified: ISODate("..."),
  // R√©f√©rence √† collection de versions
  versionsCollection: "document_versions"
}

// Collection: document_versions
{
  _id: ObjectId("..."),
  documentId: "DOC123",
  version: 1,
  content: { /* contenu de v1 */ },
  createdAt: ISODate("...")
}

// ‚úÖ S√©parer les configurations par environnement
// Collection: app_configs
{
  _id: ObjectId("..."),
  appId: "APP001",
  environment: "production",
  config: { /* param√®tres production uniquement */ }
}
```

---

## ‚úÖ DO : Utiliser GridFS pour les Fichiers Volumineux

**Explication** : GridFS est con√ßu sp√©cifiquement pour stocker des fichiers d√©passant 16 MB en les divisant en chunks.

**Quand utiliser GridFS** :
- Fichiers > 16 MB (obligation)
- Fichiers binaires volumineux (images, vid√©os, PDFs)
- Besoin de streaming
- Fichiers qui changent rarement

**Usage** :
```javascript
// ‚úÖ Stocker un fichier avec GridFS
import { GridFSBucket } from 'mongodb';

const bucket = new GridFSBucket(db, {
  bucketName: 'uploads'
});

// Upload d'un fichier
const uploadStream = bucket.openUploadStream('large_video.mp4', {
  metadata: {
    userId: 'user123',
    contentType: 'video/mp4',
    uploadedAt: new Date()
  }
});

fs.createReadStream('./large_video.mp4').pipe(uploadStream);

uploadStream.on('finish', () => {
  console.log('File uploaded:', uploadStream.id);
});

// R√©f√©rencer le fichier dans votre document
await db.posts.insertOne({
  _id: ObjectId("..."),
  userId: 'user123',
  caption: "Check out this video!",
  videoFileId: uploadStream.id,  // R√©f√©rence GridFS
  createdAt: new Date()
});
```

**Architecture GridFS** :
```javascript
// GridFS cr√©e deux collections automatiquement

// Collection: uploads.files (m√©tadonn√©es)
{
  _id: ObjectId("..."),
  length: 52428800,  // 50 MB
  chunkSize: 261120,  // 255 KB par chunk
  uploadDate: ISODate("..."),
  filename: "large_video.mp4",
  metadata: {
    userId: "user123",
    contentType: "video/mp4"
  }
}

// Collection: uploads.chunks (donn√©es)
{
  _id: ObjectId("..."),
  files_id: ObjectId("..."),  // R√©f√©rence au fichier
  n: 0,  // Num√©ro du chunk (0, 1, 2, ...)
  data: BinData(0, "...base64...")  // 255 KB de donn√©es
}
// 200 chunks pour un fichier de 50 MB
```

**Ne PAS utiliser pour** :
```javascript
// ‚ùå Petits fichiers (< 100 KB)
// Overhead de GridFS non justifi√©
// Mieux : stocker directement en BinData

// ‚ùå Donn√©es fr√©quemment modifi√©es
// GridFS pas optimis√© pour les updates partielles

// ‚ùå Besoin d'acc√®s par champ
// GridFS stocke les donn√©es en binaire opaque
```

---

## ‚úÖ DO : Monitorer la Taille des Documents

**Explication** : Mettre en place une surveillance proactive de la taille des documents pour d√©tecter les croissances anormales.

**Monitoring** :
```javascript
// ‚úÖ Analyser la distribution des tailles
db.collection.aggregate([
  {
    $project: {
      sizeInBytes: { $bsonSize: "$$ROOT" },
      _id: 1,
      // Autres champs pertinents pour l'analyse
    }
  },
  {
    $bucket: {
      groupBy: "$sizeInBytes",
      boundaries: [
        0,
        10240,      // 10 KB
        102400,     // 100 KB
        524288,     // 500 KB
        1048576,    // 1 MB
        5242880,    // 5 MB
        16777216    // 16 MB
      ],
      default: "Over 16MB",
      output: {
        count: { $sum: 1 },
        avgSize: { $avg: "$sizeInBytes" },
        maxSize: { $max: "$sizeInBytes" },
        examples: { $push: "$_id" }
      }
    }
  }
]);

// R√©sultat
[
  { _id: 0, count: 850000, avgSize: 5120, maxSize: 10239 },       // < 10 KB
  { _id: 10240, count: 145000, avgSize: 45678, maxSize: 102399 }, // 10-100 KB
  { _id: 102400, count: 4500, avgSize: 256789, maxSize: 524287 }, // 100-500 KB
  { _id: 524288, count: 450, avgSize: 750123, maxSize: 1048575 }, // 500 KB-1 MB
  { _id: 1048576, count: 45, avgSize: 2500000, maxSize: 5242879 },// 1-5 MB
  { _id: 5242880, count: 5, avgSize: 8900000, maxSize: 15000000 } // 5-16 MB ‚ö†Ô∏è
]
```

**Alertes proactives** :
```javascript
// ‚úÖ Script de surveillance quotidien
async function checkDocumentSizes() {
  const largeDocs = await db.collection.find({
    $expr: {
      $gt: [{ $bsonSize: "$$ROOT" }, 1048576]  // > 1 MB
    }
  }).limit(100).toArray();

  if (largeDocs.length > 0) {
    console.warn(`Found ${largeDocs.length} documents over 1 MB`);

    // Analyser les causes
    for (const doc of largeDocs) {
      const size = JSON.stringify(doc).length;
      console.warn(`Document ${doc._id}: ${(size/1024/1024).toFixed(2)} MB`);

      // Identifier les champs volumineux
      for (const [key, value] of Object.entries(doc)) {
        const fieldSize = JSON.stringify(value).length;
        if (fieldSize > 100000) {  // > 100 KB
          console.warn(`  - Field "${key}": ${(fieldSize/1024).toFixed(2)} KB`);
        }
      }
    }

    // Envoyer alerte
    await sendAlert({
      type: 'LARGE_DOCUMENTS',
      count: largeDocs.length,
      collection: 'collection_name'
    });
  }
}
```

**M√©triques √† suivre** :
```javascript
// Dashboard de monitoring
const metrics = {
  totalDocuments: 1000000,
  averageSize: 12345,         // bytes
  medianSize: 8192,           // bytes
  p95Size: 102400,            // 95th percentile
  p99Size: 524288,            // 99th percentile
  maxSize: 2097152,           // Document le plus gros

  // Distribution
  under10KB: 850000,          // 85%
  under100KB: 995000,         // 99.5%
  under1MB: 999550,           // 99.955%
  over1MB: 450,               // 0.045% ‚ö†Ô∏è
  over5MB: 5,                 // 0.0005% üö®

  // Croissance
  growthRate: {
    daily: 1.02,              // +2% par jour
    weekly: 1.15,             // +15% par semaine
    monthly: 1.75             // +75% par mois üö®
  }
};
```

---

## ‚ùå DON'T : Ignorer les Signaux d'Alerte

**Explication** : Des sympt√¥mes sp√©cifiques indiquent que vos documents deviennent trop volumineux.

**Signaux d'alerte** :

### 1. Performance D√©grad√©e
```javascript
// Sympt√¥mes :
// - Requ√™tes qui prenaient 10ms prennent maintenant 500ms
// - Timeout des requ√™tes simples
// - Latence r√©seau en hausse constante
// - CPU spike lors de parsing JSON

// ‚ö†Ô∏è Investiguer la taille des documents
```

### 2. Erreurs de Fragmentation
```javascript
// Logs MongoDB
"Moved document to new location due to growth"
"Document padding factor increased"

// ‚ö†Ô∏è Documents qui grossissent fr√©quemment
// Solution : R√©viser la mod√©lisation
```

### 3. Utilisation M√©moire √âlev√©e
```javascript
// Sympt√¥mes :
// - WiredTiger cache constamment plein
// - Page faults en hausse
// - Swap m√©moire utilis√©
// - OOM (Out of Memory) errors

// ‚ö†Ô∏è Documents trop gros pour le cache
```

### 4. Croissance Lin√©aire de la Base
```javascript
// Base de donn√©es
Jour 1  : 10 GB
Mois 1  : 50 GB
Mois 6  : 300 GB
An 1    : 600 GB

// Si croissance lin√©aire mais nombre de documents stable
// ‚ö†Ô∏è Documents grossissent en moyenne
```

### 5. √âchecs d'Op√©rations
```javascript
// Erreurs fr√©quentes
"BSONObj size: X is invalid"
"Document too large"
"Allocation failed"

// üö® Documents approchent ou d√©passent la limite
```

**Actions correctives** :
```javascript
// ‚úÖ Plan d'action
1. Identifier les documents volumineux (query ci-dessus)
2. Analyser les champs responsables
3. Choisir le pattern appropri√© :
   - Bucket si s√©ries temporelles
   - Subset si donn√©es partielles suffisent
   - R√©f√©rencement si donn√©es ind√©pendantes
   - GridFS si fichiers binaires
4. Migrer progressivement
5. Mettre en place monitoring continu
6. Documenter les limites et alertes
```

---

## ‚úÖ DO : R√©f√©rencer Plut√¥t qu'Imbriquer pour Donn√©es Volumineuses

**Explication** : Les donn√©es volumineuses ou ind√©pendantes doivent √™tre dans des documents s√©par√©s avec r√©f√©rences.

**Crit√®res de d√©cision** :
```javascript
// ‚úÖ Imbriquer (Embedded) si :
- Donn√©es < 10 KB
- Toujours acc√©d√©es ensemble (ratio 1:1)
- Ne change pas ind√©pendamment
- Pas r√©utilis√© ailleurs
- Pas de risque de croissance

// ‚úÖ R√©f√©rencer si :
- Donn√©es > 10 KB
- Acc√®s ind√©pendant possible
- Change fr√©quemment de fa√ßon ind√©pendante
- Partag√© entre plusieurs documents
- Risque de croissance
```

**Exemple** :
```javascript
// ‚ùå Imbriquer des donn√©es volumineuses
{
  _id: ObjectId("..."),
  orderId: "ORD123",
  customerId: "CUST456",

  // Customer data imbriqu√© (peut √™tre volumineux)
  customer: {
    _id: "CUST456",
    name: "Alice Smith",
    email: "alice@example.com",
    // 50 autres champs
    addresses: [/* 10 adresses */],
    paymentMethods: [/* 5 m√©thodes */],
    orderHistory: [/* 100 commandes */],  // üö® PROBL√àME
    preferences: { /* nombreux param√®tres */ }
  },

  items: [/* 20 produits */],
  // Document devient > 1 MB
}

// ‚úÖ R√©f√©rencer pour garder documents compacts
{
  _id: ObjectId("..."),
  orderId: "ORD123",
  customerId: ObjectId("..."),  // Simple r√©f√©rence

  // Denormalisation minimale pour performance
  customerName: "Alice Smith",
  customerEmail: "alice@example.com",

  items: [/* 20 produits */]
  // Document reste < 50 KB
}

// Document customer s√©par√©
{
  _id: ObjectId("..."),
  customerId: "CUST456",
  name: "Alice Smith",
  email: "alice@example.com",
  // Toutes les donn√©es customer
}
```

---

## ‚úÖ DO : Impl√©menter la Pagination Stricte

**Explication** : Limiter syst√©matiquement le nombre d'√©l√©ments retourn√©s par les requ√™tes, surtout pour les tableaux imbriqu√©s.

**Pagination des tableaux imbriqu√©s** :
```javascript
// ‚úÖ Slice pour limiter les √©l√©ments retourn√©s
db.articles.find(
  { _id: articleId },
  {
    title: 1,
    content: 1,
    // Retourner seulement les 10 premiers commentaires
    comments: { $slice: 10 }
  }
);

// ‚úÖ Slice avec offset (pagination)
db.articles.find(
  { _id: articleId },
  {
    title: 1,
    // Sauter les 20 premiers, retourner les 10 suivants
    comments: { $slice: [20, 10] }
  }
);

// ‚úÖ Utiliser $slice dans les agr√©gations
db.articles.aggregate([
  { $match: { _id: articleId } },
  {
    $project: {
      title: 1,
      content: 1,
      // Commentaires r√©cents seulement
      recentComments: { $slice: ["$comments", -10] }
    }
  }
]);
```

**Pagination avec m√©tadonn√©es** :
```javascript
// ‚úÖ R√©ponse API pagin√©e avec m√©tadonn√©es
async function getComments(articleId, page = 1, pageSize = 20) {
  const skip = (page - 1) * pageSize;

  // Compter le total
  const total = await db.article_comments.countDocuments({
    articleId: articleId
  });

  // R√©cup√©rer la page
  const comments = await db.article_comments
    .find({ articleId: articleId })
    .sort({ createdAt: -1 })
    .skip(skip)
    .limit(pageSize)
    .toArray();

  return {
    data: comments,
    pagination: {
      page: page,
      pageSize: pageSize,
      total: total,
      totalPages: Math.ceil(total / pageSize),
      hasNext: skip + pageSize < total,
      hasPrev: page > 1
    }
  };
}
```

---

## ‚ùå DON'T : Charger des Documents Entiers Quand Seule une Partie est N√©cessaire

**Explication** : Toujours utiliser des projections pour ne r√©cup√©rer que les champs n√©cessaires, surtout avec des documents volumineux.

**Anti-pattern** :
```javascript
// ‚ùå Charger le document entier (2 MB)
const user = await db.users.findOne({ _id: userId });
// Puis utiliser seulement
const email = user.email;

// Transfert r√©seau : 2 MB
// Parsing JSON : 100ms
// Pour utiliser 50 bytes (l'email)
```

**Solution** :
```javascript
// ‚úÖ Projection minimale
const user = await db.users.findOne(
  { _id: userId },
  { projection: { email: 1, _id: 0 } }
);

// Transfert r√©seau : 50 bytes
// Parsing JSON : <1ms
// = 40,000x plus efficace
```

**Projections avanc√©es** :
```javascript
// ‚úÖ Exclure les champs volumineux
const article = await db.articles.findOne(
  { _id: articleId },
  {
    projection: {
      // Inclure les champs l√©gers
      title: 1,
      author: 1,
      publishedAt: 1,
      summary: 1,

      // Exclure les champs volumineux
      content: 0,           // Contenu complet (peut √™tre 100 KB+)
      comments: 0,          // Tous les commentaires (peut √™tre 1 MB+)
      versionHistory: 0     // Historique (peut √™tre 500 KB+)
    }
  }
);

// Charger le contenu complet seulement si n√©cessaire
if (needFullContent) {
  const fullArticle = await db.articles.findOne(
    { _id: articleId },
    { projection: { content: 1, _id: 0 } }
  );
  article.content = fullArticle.content;
}
```

---

## Patterns Avanc√©s

### ‚úÖ DO : Pattern Outlier pour Cas Exceptionnels

**Explication** : Traiter diff√©remment les documents exceptionnellement volumineux des documents normaux.

**Pattern** :
```javascript
// ‚úÖ D√©tecter et marquer les outliers
{
  _id: ObjectId("..."),
  userId: "celebrity_user",
  username: "famous_person",
  followerCount: 10000000,  // 10M followers

  // Flag outlier
  isOutlier: true,

  // Subset seulement pour les outliers
  topFollowers: [/* 100 top followers */],

  // R√©f√©rence vers collection s√©par√©e
  followersCollection: "celebrity_followers"
}

// Documents normaux (< 10k followers)
{
  _id: ObjectId("..."),
  userId: "normal_user",
  username: "alice",
  followerCount: 523,

  // Embedded directement (pas un outlier)
  followers: [/* 523 followers */]
}
```

**Impl√©mentation** :
```javascript
// ‚úÖ Logique conditionnelle bas√©e sur outlier
async function getFollowers(userId) {
  const user = await db.users.findOne({ _id: userId });

  if (user.isOutlier) {
    // Cas outlier : r√©cup√©rer depuis collection s√©par√©e
    return await db.celebrity_followers
      .find({ userId: userId })
      .limit(100)
      .toArray();
  } else {
    // Cas normal : embedded directement
    return user.followers || [];
  }
}

// Migration automatique vers outlier
async function checkAndMigrateOutlier(userId) {
  const user = await db.users.findOne({ _id: userId });

  if (!user.isOutlier && user.followers.length > 10000) {
    // Migrer vers outlier
    await db.celebrity_followers.insertMany(
      user.followers.map(f => ({
        userId: userId,
        followerId: f
      }))
    );

    await db.users.updateOne(
      { _id: userId },
      {
        $set: {
          isOutlier: true,
          topFollowers: user.followers.slice(0, 100)
        },
        $unset: { followers: "" }
      }
    );
  }
}
```

---

### ‚úÖ DO : Pattern Extended Reference pour D√©normalisation Contr√¥l√©e

**Explication** : Dupliquer s√©lectivement quelques champs fr√©quemment acc√©d√©s plut√¥t que tout imbriquer.

**Pattern** :
```javascript
// ‚úÖ Extended reference : r√©f√©rence + champs critiques
{
  _id: ObjectId("..."),
  orderId: "ORD123",

  // R√©f√©rence
  customerId: ObjectId("..."),

  // Champs d√©normalis√©s (extended reference)
  customerName: "Alice Smith",
  customerEmail: "alice@example.com",
  customerTier: "premium",

  // Pas toutes les donn√©es customer (50+ champs)
  // Juste ce qui est fr√©quemment n√©cessaire

  items: [/* produits */],
  total: 150.00
}
```

**B√©n√©fices** :
- Performance : Pas besoin de jointure pour 90% des cas
- Taille : Document reste compact
- Maintenance : Mise √† jour synchronis√©e si n√©cessaire

---

## Checklist Documents Volumineux

### Pr√©vention
- [ ] Documents cibles < 100 KB
- [ ] Limite d'alerte √† 1 MB configur√©e
- [ ] Pas de tableaux sans limite de taille
- [ ] Pas d'objets imbriqu√©s profonds volumineux
- [ ] GridFS utilis√© pour fichiers > 16 MB
- [ ] Patterns appropri√©s (Bucket, Subset, R√©f√©rence)

### Monitoring
- [ ] Surveillance taille documents en place
- [ ] Alertes sur documents > 1 MB
- [ ] M√©triques de distribution de taille
- [ ] Tracking de croissance dans le temps
- [ ] Analyse des champs volumineux

### Requ√™tes
- [ ] Projections utilis√©es syst√©matiquement
- [ ] Pagination stricte impl√©ment√©e
- [ ] Slice pour tableaux imbriqu√©s
- [ ] Pas de chargement complet inutile

### Architecture
- [ ] S√©paration donn√©es volumineuses
- [ ] Collections d√©di√©es si n√©cessaire
- [ ] R√©f√©rences plut√¥t qu'embedding
- [ ] Pattern Outlier pour cas exceptionnels

---

## Tableau de D√©cision : Imbriquer vs R√©f√©rencer

| Crit√®re | Imbriquer | R√©f√©rencer |
|---------|-----------|------------|
| **Taille totale** | < 10 KB | > 10 KB |
| **Fr√©quence d'acc√®s ensemble** | Toujours (>95%) | Variable (<50%) |
| **Ind√©pendance** | Totalement d√©pendant | Peut exister seul |
| **R√©utilisation** | Unique √† ce document | Partag√©/r√©utilis√© |
| **Fr√©quence de modification** | Rare ou synchrone | Fr√©quente et ind√©pendante |
| **Croissance** | Fixe ou limit√©e | Potentiellement illimit√©e |
| **Performance critique** | Oui (1 requ√™te) | Non (2+ requ√™tes OK) |

---

## Conclusion

La gestion de la taille des documents est un √©quilibre entre :

- **Performance** : Documents compacts = rapidit√©
- **Flexibilit√©** : Sch√©ma adaptable sans migration lourde
- **Maintenabilit√©** : Code simple et pr√©visible
- **Scalabilit√©** : Croissance supportable √† long terme

**R√®gles d'or** :
1. **Limite pratique** : Viser < 100 KB, alerter √† 1 MB
2. **Pas de croissance illimit√©e** : Utiliser Bucket, Subset ou R√©f√©rences
3. **GridFS** : Pour fichiers > 16 MB
4. **Monitoring proactif** : D√©tecter les probl√®mes t√¥t
5. **Projections** : Ne charger que le n√©cessaire

Un document qui commence petit mais grossit sans contr√¥le devient un probl√®me majeur en production. La pr√©vention et le monitoring sont essentiels.

---


‚è≠Ô∏è [√âviter les collections excessives](/21-bonnes-pratiques-anti-patterns/05-eviter-collections-excessives.md)
