üîù Retour au [Sommaire](/SOMMAIRE.md)

# 10.11 Jumbo Chunks et R√©solution

## Introduction

Les **jumbo chunks** repr√©sentent l'un des probl√®mes les plus courants et les plus frustrants dans la gestion d'un cluster shard√© MongoDB. Un jumbo chunk est un chunk qui d√©passe la taille maximale configur√©e (par d√©faut 64 Mo) mais qui **ne peut pas √™tre divis√©** par le processus normal de splitting. Cette situation paralyse le balancer, emp√™che une distribution √©quilibr√©e des donn√©es, et peut conduire √† des d√©s√©quilibres massifs de charge entre les shards.

Comprendre les causes profondes des jumbo chunks, savoir les d√©tecter rapidement, et ma√Ætriser les diff√©rentes strat√©gies de r√©solution sont des comp√©tences essentielles pour tout administrateur de cluster shard√© MongoDB en production. Cette section explore en profondeur ce ph√©nom√®ne et fournit des solutions √©prouv√©es pour le r√©soudre et, surtout, le pr√©venir.

---

## Qu'est-ce qu'un Jumbo Chunk ?

### D√©finition Technique

```javascript
// Un chunk devient "jumbo" quand :
// 1. Sa taille d√©passe le seuil configur√© (chunkSize)
// 2. Il ne peut pas √™tre divis√© (split) car toutes ses valeurs de shard key sont identiques

// Exemple de jumbo chunk dans les m√©tadonn√©es
db.getSiblingDB("config").chunks.findOne({
  ns: "mydb.orders",
  jumbo: true
})

// R√©sultat :
{
  "_id": ObjectId("..."),
  "ns": "mydb.orders",
  "min": { "status": "active" },
  "max": { "status": "inactive" },
  "shard": "shardA",
  "jumbo": true,          // ‚ö†Ô∏è Marqu√© comme jumbo
  "lastmod": Timestamp(123, 45),
  "history": [...]
}
```

### Anatomie d'un Jumbo Chunk

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CHUNK NORMAL                         ‚îÇ
‚îÇ  min: { user_id: "user_1000" }                          ‚îÇ
‚îÇ  max: { user_id: "user_2000" }                          ‚îÇ
‚îÇ  Taille: 45 MB                                          ‚îÇ
‚îÇ  Documents: 50,000 avec user_id diff√©rents              ‚îÇ
‚îÇ  ‚úÖ Peut √™tre split en 2 morceaux                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    JUMBO CHUNK                          ‚îÇ
‚îÇ  min: { status: "active" }                              ‚îÇ
‚îÇ  max: { status: "inactive" }                            ‚îÇ
‚îÇ  Taille: 2.5 GB (> 64 MB)                               ‚îÇ
‚îÇ  Documents: 3,000,000 TOUS avec status="active"         ‚îÇ
‚îÇ  ‚ùå IMPOSSIBLE √† split (m√™me valeur de shard key)       ‚îÇ
‚îÇ  jumbo: true                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### M√©canisme de Cr√©ation

```javascript
// Sc√©nario typique de cr√©ation d'un jumbo chunk

// 1. Collection avec shard key de faible cardinalit√©
sh.shardCollection("mydb.orders", { status: 1 })

// 2. Distribution initiale (3 valeurs possibles)
// Chunk 1: { status: MinKey } ‚Üí { status: "active" }    ‚Üí shardA
// Chunk 2: { status: "active" } ‚Üí { status: "inactive" } ‚Üí shardB
// Chunk 3: { status: "inactive" } ‚Üí { status: MaxKey }   ‚Üí shardC

// 3. Insertion massive de documents avec status="active"
for (var i = 0; i < 3000000; i++) {
  db.orders.insertOne({
    status: "active",  // M√™me valeur pour tous !
    customer_id: "CUST" + i,
    amount: Math.random() * 1000
  });
}

// 4. Le chunk 2 grossit d√©mesur√©ment
// Taille: 2.5 GB
// MongoDB tente de le splitter mais √âCHOUE
// ‚Üí Toutes les valeurs sont identiques (status="active")
// ‚Üí Chunk marqu√© "jumbo"

// 5. Le balancer ne peut plus migrer ce chunk
// Log: "Cannot move chunk: chunk is jumbo"
```

---

## Causes des Jumbo Chunks

### 1. Shard Key de Faible Cardinalit√©

**Cause principale** : Le champ choisi comme shard key n'a que peu de valeurs distinctes.

```javascript
// ‚ùå Exemples de shard keys probl√©matiques

// Exemple 1 : Statut (3 valeurs)
sh.shardCollection("mydb.orders", { status: 1 })
// status ‚àà {"pending", "completed", "cancelled"}

// Exemple 2 : Bool√©en (2 valeurs)
sh.shardCollection("mydb.users", { is_active: 1 })
// is_active ‚àà {true, false}

// Exemple 3 : Type (5 valeurs)
sh.shardCollection("mydb.products", { category: 1 })
// category ‚àà {"electronics", "clothing", "books", "home", "sports"}

// Exemple 4 : R√©gion (4 valeurs)
sh.shardCollection("mydb.customers", { region: 1 })
// region ‚àà {"north", "south", "east", "west"}
```

**Impact** :

```javascript
// Si 90% des documents ont status="active"
// Le chunk correspondant contiendra 90% des donn√©es

// Exemple avec 10M de documents :
// - Chunk "active" : 9M documents (2.7 GB) ‚Üí JUMBO
// - Chunk "completed" : 800K documents (240 MB)
// - Chunk "cancelled" : 200K documents (60 MB)

// Distribution catastrophique :
// shardA : 2.7 GB (90%)
// shardB : 240 MB (8%)
// shardC : 60 MB (2%)
```

### 2. Distribution Non Uniforme (Skewed Data)

**Cause** : M√™me avec cardinalit√© correcte, certaines valeurs sont beaucoup plus fr√©quentes.

```javascript
// Exemple : E-commerce avec quelques gros clients

// Shard key : customer_id (bonne cardinalit√© : 1M clients)
sh.shardCollection("mydb.orders", { customer_id: 1 })

// Mais distribution r√©elle :
// - customer_AMAZON : 2M commandes (1.5 GB) ‚Üí JUMBO
// - customer_WALMART : 1M commandes (750 MB) ‚Üí JUMBO
// - 999,998 autres clients : 7M commandes au total

// "Hot customers" cr√©ent des jumbo chunks
```

### 3. Shard Key Compos√©e avec Pr√©fixe de Faible Cardinalit√©

```javascript
// ‚ùå Mauvais ordre des champs

// Exemple 1 : Type d'abord (faible cardinalit√©)
sh.shardCollection("mydb.events", { event_type: 1, timestamp: 1 })
// Si event_type a 5 valeurs, seulement 5 chunks initiaux possibles

// Exemple 2 : R√©gion d'abord
sh.shardCollection("mydb.sales", { region: 1, customer_id: 1 })
// Si r√©gion EU a 80% des ventes ‚Üí jumbo chunk r√©gion EU
```

### 4. Absence de Pr√©-splitting lors de l'Import

```javascript
// ‚ùå Sc√©nario probl√©matique

// 1. Activer le sharding sans pr√©-splitting
sh.shardCollection("mydb.products", { category: 1, product_id: 1 })

// Chunk initial unique :
// min: { category: MinKey, product_id: MinKey }
// max: { category: MaxKey, product_id: MaxKey }

// 2. Import massif (10M produits)
// Tous les documents vont dans le chunk initial

// 3. MongoDB tente de splitter pendant l'import
// Mais si beaucoup de produits ont category="electronics"
// ‚Üí Cr√©ation de jumbo chunk
```

### 5. Croissance Organique Non Contr√¥l√©e

```javascript
// Chunk initialement normal qui devient jumbo avec le temps

// T0 : Chunk avec 50K documents, 30 MB
{
  min: { category: "electronics", product_id: "PROD_0000" },
  max: { category: "electronics", product_id: "PROD_5000" }
}

// T+6 mois : Croissance du catalogue √©lectronique
// 500K documents, 350 MB ‚Üí JUMBO

// Cause : Pas de monitoring de la croissance des chunks
```

---

## Impact des Jumbo Chunks

### 1. Paralysie du Balancer

```javascript
// Le balancer ne peut pas migrer les jumbo chunks

// Tentative de migration :
sh.moveChunk("mydb.orders", { status: "active" }, "shardB")

// Erreur :
{
  "ok": 0,
  "errmsg": "chunk too large to move: 2684354560 bytes > 268435456 bytes (configured chunk size)",
  "code": 13439
}

// Log du balancer :
"Skipping chunk migration because chunk is jumbo: { ns: 'mydb.orders', min: { status: 'active' }, max: { status: 'inactive' } }"
```

**Cons√©quence** : Le d√©s√©quilibre s'aggrave avec le temps.

### 2. D√©s√©quilibre de Charge Permanent

```javascript
// √âtat du cluster avec jumbo chunks

sh.status()

// Output :
shards:
  { "_id": "shardA", "host": "...", "state": 1 }
  { "_id": "shardB", "host": "...", "state": 1 }
  { "_id": "shardC", "host": "...", "state": 1 }

databases:
  mydb.orders
    shard key: { "status": 1 }
    unique: false
    balancing: true
    chunks:
      shardA  1       // ‚ö†Ô∏è 1 chunk mais 2.5 GB !
      shardB  45      // 45 chunks mais seulement 500 MB
      shardC  45      // 45 chunks mais seulement 500 MB

    // Distribution :
    // shardA : 2.5 GB (83%)  ‚Üê JUMBO CHUNK
    // shardB : 250 MB (8.3%)
    // shardC : 250 MB (8.3%)
```

### 3. Performances D√©grad√©es

```javascript
// Impact sur les performances

// Requ√™tes ciblant le jumbo chunk (shardA surcharg√©)
db.orders.find({ status: "active", amount: { $gt: 1000 } })

// M√©triques sur shardA :
// - CPU: 85% (vs 20% sur shardB et shardC)
// - RAM: Cache WiredTiger satur√© (90%)
// - IOPS: 5000 read/s (vs 500 sur les autres shards)
// - Latence: 150ms (vs 20ms sur les autres shards)

// Impact utilisateur :
// - Timeouts applicatifs
// - Exp√©rience d√©grad√©e
```

### 4. Scaling Inefficace

```javascript
// Ajout d'un nouveau shard pour r√©soudre le probl√®me

sh.addShard("shardD/shardD1:27018,shardD2:27018,shardD3:27018")

// R√©sultat attendu : Redistribution de la charge
// R√©sultat r√©el : shardD reste presque vide

// Le jumbo chunk reste bloqu√© sur shardA
// Les nouveaux chunks (petits) se redistribuent
// Mais le probl√®me de fond persiste

sh.status()
// shardA : 2.5 GB (jumbo)
// shardB : 200 MB
// shardC : 200 MB
// shardD : 100 MB
// ‚Üí shardA toujours surcharg√© !
```

### 5. Risque lors des Maintenances

```bash
# Sc√©nario : Maintenance sur shardA (qui contient le jumbo chunk)

# 1. Tentative de drain avant maintenance
db.adminCommand({ removeShard: "shardA" })

# 2. Le balancer tente de migrer les chunks
# ‚úÖ Chunks normaux : migr√©s en quelques heures
# ‚ùå Jumbo chunk : BLOQU√â

# R√©sultat :
{
  "msg": "draining ongoing",
  "state": "ongoing",
  "remaining": {
    "chunks": 1,  // Le jumbo chunk !
    "dbs": 0
  },
  "note": "Cannot move jumbo chunk. Manual intervention required."
}

# 3. Maintenance IMPOSSIBLE sans r√©soudre le jumbo chunk d'abord
```

---

## D√©tection des Jumbo Chunks

### M√©thode 1 : Requ√™te Directe sur Config

```javascript
// Identifier tous les jumbo chunks
db.getSiblingDB("config").chunks.find({ jumbo: true })

// Avec d√©tails
db.getSiblingDB("config").chunks.find({ jumbo: true }).forEach(function(chunk) {
  printjson({
    namespace: chunk.ns,
    shard: chunk.shard,
    min: chunk.min,
    max: chunk.max,
    lastModified: chunk.lastmod
  });
});
```

### M√©thode 2 : Agr√©gation par Collection

```javascript
// Compter les jumbo chunks par collection
db.getSiblingDB("config").chunks.aggregate([
  { $match: { jumbo: true } },
  { $group: {
      _id: "$ns",
      count: { $sum: 1 },
      shards: { $addToSet: "$shard" }
    }
  },
  { $sort: { count: -1 } }
])

// R√©sultat :
[
  {
    "_id": "mydb.orders",
    "count": 1,
    "shards": ["shardA"]
  },
  {
    "_id": "mydb.events",
    "count": 3,
    "shards": ["shardB", "shardC"]
  }
]
```

### M√©thode 3 : Via sh.status()

```javascript
// sh.status() indique les jumbo chunks
sh.status()

// Output (extrait) :
mydb.orders
  shard key: { "status": 1 }
  chunks:
    shardA  1
      { "status": { "$minKey": 1 } } -->> { "status": "active" } on: shardA Timestamp(1, 0) jumbo
    shardB  45
      ...
```

### M√©thode 4 : Script de Diagnostic Complet

```javascript
// Script complet pour analyser les jumbo chunks
function diagnoseJumboChunks() {
  print("=== DIAGNOSTIC DES JUMBO CHUNKS ===\n");

  var jumboChunks = db.getSiblingDB("config").chunks.find({
    jumbo: true
  }).toArray();

  if (jumboChunks.length === 0) {
    print("‚úÖ Aucun jumbo chunk d√©tect√©\n");
    return;
  }

  print("‚ö†Ô∏è  " + jumboChunks.length + " jumbo chunk(s) d√©tect√©(s)\n");

  // Grouper par collection
  var byCollection = {};

  jumboChunks.forEach(function(chunk) {
    if (!byCollection[chunk.ns]) {
      byCollection[chunk.ns] = [];
    }
    byCollection[chunk.ns].push(chunk);
  });

  // Analyser chaque collection
  Object.keys(byCollection).forEach(function(ns) {
    var chunks = byCollection[ns];

    print("Collection : " + ns);
    print("  Jumbo chunks : " + chunks.length);

    // R√©cup√©rer la shard key
    var collInfo = db.getSiblingDB("config").collections.findOne({ _id: ns });
    print("  Shard key : " + JSON.stringify(collInfo.key));

    // Analyser chaque jumbo chunk
    chunks.forEach(function(chunk, index) {
      print("\n  Jumbo Chunk #" + (index + 1) + " :");
      print("    Shard : " + chunk.shard);
      print("    Min : " + JSON.stringify(chunk.min));
      print("    Max : " + JSON.stringify(chunk.max));

      // Estimer la taille (approximatif)
      try {
        var dbName = ns.split(".")[0];
        var collName = ns.split(".")[1];

        var count = db.getSiblingDB(dbName)[collName].countDocuments({
          $and: [
            { [Object.keys(chunk.min)[0]]: { $gte: chunk.min[Object.keys(chunk.min)[0]] } },
            { [Object.keys(chunk.max)[0]]: { $lt: chunk.max[Object.keys(chunk.max)[0]] } }
          ]
        });

        print("    Documents (estim√©s) : " + count);

        // Estimer taille (1 KB par document en moyenne)
        var estimatedSizeMB = (count * 1024) / 1024 / 1024;
        print("    Taille estim√©e : " + estimatedSizeMB.toFixed(2) + " MB");

      } catch (e) {
        print("    ‚ö†Ô∏è  Impossible d'estimer la taille");
      }

      // Analyser la cardinalit√© de la shard key
      try {
        var shardKeyField = Object.keys(collInfo.key)[0];
        var distinctCount = db.getSiblingDB(dbName)[collName].distinct(shardKeyField, {
          $and: [
            { [shardKeyField]: { $gte: chunk.min[shardKeyField] } },
            { [shardKeyField]: { $lt: chunk.max[shardKeyField] } }
          ]
        }).length;

        print("    Valeurs distinctes (shard key) : " + distinctCount);

        if (distinctCount === 1) {
          print("    ‚ö†Ô∏è  PROBL√àME : Toutes les valeurs sont identiques !");
          print("    ‚Üí Ce chunk ne peut PAS √™tre splitt√©");
        } else if (distinctCount < 10) {
          print("    ‚ö†Ô∏è  ATTENTION : Tr√®s faible cardinalit√© (" + distinctCount + " valeurs)");
        }

      } catch (e) {
        print("    ‚ö†Ô∏è  Impossible d'analyser la cardinalit√©");
      }
    });

    // Recommandations
    print("\n  Recommandations :");

    var shardKeyFields = Object.keys(collInfo.key);

    if (shardKeyFields.length === 1) {
      print("    1. ‚ö†Ô∏è  Shard key simple ‚Üí Consid√©rer une shard key compos√©e");
      print("       Exemple : " + JSON.stringify({
        [shardKeyFields[0]]: 1,
        "_id": 1
      }));
    }

    print("    2. Utiliser refineCollectionShardKey (MongoDB 4.4+)");
    print("       db.adminCommand({");
    print("         refineCollectionShardKey: \"" + ns + "\",");
    print("         key: { " + shardKeyFields[0] + ": 1, additionalField: 1 }");
    print("       })");

    print("    3. Ou forcer la migration avec attemptToBalanceJumboChunks (MongoDB 4.4+)");

    print("    4. En dernier recours : Resharding complet (MongoDB 5.0+)");
    print("       db.adminCommand({");
    print("         reshardCollection: \"" + ns + "\",");
    print("         key: { newShardKey: 1 }");
    print("       })");

    print("\n" + "=".repeat(60) + "\n");
  });
}

// Ex√©cuter
diagnoseJumboChunks();
```

---

## Strat√©gies de R√©solution

### Strat√©gie 1 : Refine Collection Shard Key (MongoDB 4.4+)

**Principe** : Ajouter un champ √† la shard key existante pour augmenter la granularit√©.

```javascript
// Situation initiale
sh.shardCollection("mydb.orders", { status: 1 })
// ‚Üí Jumbo chunk sur status="active"

// Solution : Ajouter un champ √† la shard key
db.adminCommand({
  refineCollectionShardKey: "mydb.orders",
  key: { status: 1, customer_id: 1 }  // Ajouter customer_id
})

// Processus :
// 1. MongoDB ajoute customer_id √† la shard key
// 2. Les chunks existants peuvent maintenant √™tre splitt√©s
// 3. Chunk { status: "active" } devient splittable en :
//    - { status: "active", customer_id: MinKey } ‚Üí { status: "active", customer_id: "CUST_5000" }
//    - { status: "active", customer_id: "CUST_5000" } ‚Üí { status: "active", customer_id: "CUST_10000" }
//    - etc.

// Validation
sh.status()
// Le jumbo chunk n'est plus marqu√© jumbo
// Il peut maintenant √™tre splitt√© et migr√©
```

**Pr√©requis** :

```javascript
// 1. Cr√©er un index compatible
db.orders.createIndex({ status: 1, customer_id: 1 })

// 2. V√©rifier la cardinalit√© du nouveau champ
db.orders.aggregate([
  { $match: { status: "active" } },
  { $group: { _id: "$customer_id" } },
  { $count: "distinctCustomers" }
])
// Doit retourner un nombre √©lev√© (id√©alement > 1000)

// 3. Ex√©cuter refineCollectionShardKey
// (Voir commande ci-dessus)
```

**Limitations** :

```javascript
// - N√©cessite MongoDB 4.4+
// - Le nouveau champ doit exister dans tous les documents
// - L'index doit √™tre cr√©√© avant
// - Pas de downgrade possible apr√®s
```

### Strat√©gie 2 : Splitter Manuellement avec un Suffix

**Principe** : Si le chunk contient plusieurs valeurs distinctes (mais pas assez pour un split auto), forcer le split.

```javascript
// Situation : Chunk avec 2 valeurs distinctes

// Chunk actuel :
{
  min: { category: "electronics", product_id: MinKey },
  max: { category: "home", product_id: MinKey },
  jumbo: true
}

// Contient :
// - category="electronics" : 1.8 GB
// - category="food" : 700 MB

// Solution : Splitter entre les deux valeurs
sh.splitAt("mydb.products", {
  category: "food",
  product_id: MinKey
})

// R√©sultat : 2 chunks
// Chunk 1 : { category: "electronics" } ‚Üí { category: "food" }     (1.8 GB)
// Chunk 2 : { category: "food" } ‚Üí { category: "home" }            (700 MB)

// Chunk 1 reste gros mais plus petit
// Chunk 2 est de taille acceptable

// Si chunk 1 est encore trop gros et contient uniquement "electronics"
// ‚Üí N√©cessite strat√©gie 1 (refine) ou strat√©gie 4 (reshape)
```

### Strat√©gie 3 : Forcer la Migration (MongoDB 4.4+)

**Principe** : Autoriser MongoDB √† migrer les jumbo chunks malgr√© leur taille.

```javascript
// Activer l'option globale
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  { $set: { attemptToBalanceJumboChunks: true } },
  { upsert: true }
)

// Le balancer tentera maintenant de migrer les jumbo chunks
// ‚ö†Ô∏è ATTENTION : Migration tr√®s lente et co√ªteuse en ressources

// Monitoring de la migration
db.currentOp({
  op: "command",
  "command.moveChunk": { $exists: true }
})

// R√©sultat :
{
  "opid": "shardA:12345",
  "op": "command",
  "ns": "mydb.orders",
  "command": { "moveChunk": "mydb.orders", ... },
  "msg": "Cloning phase: 1500000/3000000 documents",  // Progression
  "microsecs_running": 1800000000  // 30 minutes !
}

// Une fois termin√©, v√©rifier
sh.status()
// Le jumbo chunk devrait √™tre migr√© sur un autre shard

// D√©sactiver apr√®s (recommand√©)
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  { $set: { attemptToBalanceJumboChunks: false } }
)
```

**Consid√©rations** :

```javascript
// Avantages :
// - Pas de modification de la shard key
// - Le balancer g√®re automatiquement

// Inconv√©nients :
// - Tr√®s lent (plusieurs heures pour GB de donn√©es)
// - Impact performance important pendant la migration
// - Risque de timeout si chunk trop volumineux (> 5 GB)
// - Ne r√©sout pas le probl√®me de fond (shard key inad√©quate)

// Recommand√© uniquement pour :
// - Jumbo chunks temporaires (pic de donn√©es)
// - Migration one-shot avant refactoring
// - Cluster avec beaucoup de ressources disponibles
```

### Strat√©gie 4 : Resharding Complet (MongoDB 5.0+)

**Principe** : Changer compl√®tement la shard key de la collection.

```javascript
// Situation : Shard key fondamentalement inad√©quate
// Shard key actuelle : { status: 1 }
// ‚Üí Impossible √† corriger avec refine

// Solution : Resharding complet
db.adminCommand({
  reshardCollection: "mydb.orders",
  key: { customer_id: "hashed" },  // Nouvelle shard key compl√®tement diff√©rente
  numInitialChunks: 10  // Nombre de chunks initiaux
})

// Processus interne (automatis√© par MongoDB) :
// 1. Cr√©ation d'une collection temporaire avec la nouvelle shard key
// 2. Copie de toutes les donn√©es vers la nouvelle collection
// 3. Synchronisation incr√©mentale des modifications
// 4. Basculement atomique
// 5. Suppression de l'ancienne collection

// Monitoring du resharding
db.getSiblingDB("config").reshardingOperations.find().pretty()

// R√©sultat (apr√®s quelques heures) :
{
  "ns": "mydb.orders",
  "key": { "customer_id": "hashed" },
  "state": "done",
  "durationMillis": 14400000  // 4 heures
}

// Validation
sh.status()
// Collection maintenant shard√©e sur customer_id
// Plus de jumbo chunks
```

**Consid√©rations** :

```javascript
// Avantages :
// - R√©solution d√©finitive du probl√®me
// - Nouvelle shard key optimale
// - Distribution √©quilibr√©e garantie

// Inconv√©nients :
// - Tr√®s lourd (copie compl√®te des donn√©es)
// - Plusieurs heures √† jours selon volume
// - Consommation importante de ressources (CPU, disque, r√©seau)
// - N√©cessite MongoDB 5.0+

// Planification recommand√©e :
// 1. Tester en staging avec donn√©es r√©elles
// 2. Planifier une fen√™tre de maintenance longue
// 3. Augmenter temporairement les ressources du cluster
// 4. Monitoring continu pendant l'op√©ration
// 5. Plan de rollback pr√©par√©
```

### Strat√©gie 5 : Recr√©ation de la Collection

**Principe** : Solution radicale quand les autres strat√©gies ne sont pas disponibles (MongoDB < 4.4).

```javascript
// ‚ö†Ô∏è DERNI√àRE OPTION : Downtime important

// √âtape 1 : Exporter les donn√©es
mongodump \
  --host mongos1.example.com \
  --db mydb \
  --collection orders \
  --out /backup/orders-resharding

// √âtape 2 : Cr√©er une nouvelle collection avec bonne shard key
db.orders_new.createIndex({ customer_id: "hashed" })

sh.shardCollection("mydb.orders_new", { customer_id: "hashed" })

// √âtape 3 : Pr√©-splitter pour distribution optimale
for (var i = 0; i < 20; i++) {
  sh.splitAt("mydb.orders_new", { customer_id: "hash_value_" + i });
}

// √âtape 4 : Importer les donn√©es
mongorestore \
  --host mongos1.example.com \
  --db mydb \
  --collection orders_new \
  /backup/orders-resharding/mydb/orders.bson

// √âtape 5 : Basculer les applications
// Modifier la connection string pour pointer vers orders_new

// √âtape 6 : Renommer (n√©cessite downtime)
db.adminCommand({
  renameCollection: "mydb.orders",
  to: "mydb.orders_old",
  dropTarget: false
})

db.adminCommand({
  renameCollection: "mydb.orders_new",
  to: "mydb.orders",
  dropTarget: false
})

// √âtape 7 : Valider et supprimer l'ancienne
db.orders_old.drop()
```

---

## Pr√©vention des Jumbo Chunks

### 1. Choix d'une Shard Key Appropri√©e

```javascript
// ‚úÖ Crit√®res d'une bonne shard key (r√©vision)

// 1. Cardinalit√© √©lev√©e
// ‚úÖ Bon : user_id (millions de valeurs)
// ‚ùå Mauvais : status (3 valeurs)

// 2. Distribution uniforme
// ‚úÖ Bon : UUID v4, hashed _id
// ‚ùå Mauvais : timestamp (monotone), r√©gion (skewed)

// 3. Localit√© des requ√™tes
// ‚úÖ Bon : Requ√™tes fr√©quentes incluent la shard key
// ‚ùå Mauvais : Requ√™tes n'utilisent jamais la shard key

// Exemples de bonnes shard keys :
sh.shardCollection("users", { user_id: "hashed" })
sh.shardCollection("orders", { customer_id: 1, order_date: 1 })
sh.shardCollection("events", { application: 1, timestamp: 1 })
sh.shardCollection("iot_data", { sensor_id: "hashed", timestamp: 1 })
```

### 2. Analyse Pr√©alable des Donn√©es

```javascript
// Avant de sharder, analyser la distribution

function analyzeShardKeyCandidate(dbName, collName, shardKeyField) {
  print("=== Analyse de la Shard Key Candidate ===\n");

  var coll = db.getSiblingDB(dbName)[collName];

  // 1. Cardinalit√©
  var distinctCount = coll.distinct(shardKeyField).length;
  var totalDocs = coll.countDocuments({});

  print("Cardinalit√© :");
  print("  Valeurs distinctes : " + distinctCount);
  print("  Total documents : " + totalDocs);
  print("  Ratio : " + (distinctCount / totalDocs * 100).toFixed(2) + "%");

  if (distinctCount < 100) {
    print("  ‚ö†Ô∏è  ATTENTION : Cardinalit√© tr√®s faible (< 100)");
    print("  ‚Üí Risque √©lev√© de jumbo chunks !");
  } else if (distinctCount < 1000) {
    print("  ‚ö†Ô∏è  ATTENTION : Cardinalit√© faible (< 1000)");
    print("  ‚Üí Consid√©rer une shard key compos√©e");
  } else {
    print("  ‚úÖ Cardinalit√© acceptable");
  }
  print("");

  // 2. Distribution
  var distribution = coll.aggregate([
    { $group: { _id: "$" + shardKeyField, count: { $sum: 1 } } },
    { $sort: { count: -1 } },
    { $limit: 10 }
  ]).toArray();

  print("Distribution (Top 10) :");

  var maxCount = distribution[0].count;
  var minCount = distribution[distribution.length - 1].count;

  distribution.forEach(function(item) {
    var percent = (item.count / totalDocs * 100).toFixed(2);
    print("  " + item._id + " : " + item.count + " docs (" + percent + "%)");
  });

  var skew = (maxCount / (totalDocs / distinctCount)).toFixed(2);
  print("\n  Skew factor : " + skew);

  if (skew > 5) {
    print("  ‚ö†Ô∏è  ALERTE : Distribution tr√®s d√©s√©quilibr√©e (skew > 5)");
    print("  ‚Üí Une valeur repr√©sente " + skew + "x la moyenne");
    print("  ‚Üí Risque √©lev√© de jumbo chunks !");
  } else if (skew > 2) {
    print("  ‚ö†Ô∏è  ATTENTION : Distribution d√©s√©quilibr√©e (skew > 2)");
  } else {
    print("  ‚úÖ Distribution uniforme");
  }
  print("");

  // 3. Recommandation
  print("Recommandation :");

  if (distinctCount < 1000 || skew > 5) {
    print("  ‚ùå Shard key NON RECOMMAND√âE");
    print("  ‚Üí Utiliser une shard key compos√©e ou hashed");
    print("  Exemples :");
    print("    - { " + shardKeyField + ": \"hashed\" }");
    print("    - { " + shardKeyField + ": 1, _id: 1 }");
  } else if (distinctCount < 10000 || skew > 2) {
    print("  ‚ö†Ô∏è  Shard key ACCEPTABLE mais non optimale");
    print("  ‚Üí Envisager une shard key compos√©e");
  } else {
    print("  ‚úÖ Shard key RECOMMAND√âE");
  }
}

// Exemple d'utilisation
analyzeShardKeyCandidate("mydb", "orders", "status");
// ‚Üí R√©v√®le les probl√®mes potentiels AVANT le sharding
```

### 3. Pr√©-splitting Intelligent

```javascript
// Cr√©er les chunks √† l'avance pour distribution optimale

// Approche 1 : Pr√©-split bas√© sur les valeurs existantes
function presplitCollection(dbName, collName, shardKey, numSplits) {
  var ns = dbName + "." + collName;

  print("Pr√©-splitting de " + ns + " en " + numSplits + " chunks...\n");

  // Activer le sharding
  sh.enableSharding(dbName);
  sh.shardCollection(ns, shardKey);

  // Obtenir les valeurs pour splits
  var shardKeyField = Object.keys(shardKey)[0];
  var coll = db.getSiblingDB(dbName)[collName];

  // Calculer les percentiles
  var total = coll.countDocuments({});
  var step = Math.floor(total / numSplits);

  var splitPoints = [];
  for (var i = 1; i < numSplits; i++) {
    var doc = coll.find().sort({ [shardKeyField]: 1 }).skip(i * step).limit(1).toArray()[0];
    if (doc) {
      splitPoints.push(doc[shardKeyField]);
    }
  }

  // Effectuer les splits
  splitPoints.forEach(function(point) {
    try {
      sh.splitAt(ns, { [shardKeyField]: point });
      print("‚úÖ Split √† : " + point);
    } catch (e) {
      print("‚ùå Erreur split √† " + point + " : " + e.message);
    }
  });

  print("\nPr√©-splitting termin√©");
  sh.status();
}

// Exemple
presplitCollection("mydb", "orders", { customer_id: 1 }, 20);
```

### 4. Monitoring Continu

```javascript
// Script de monitoring automatis√© (√† ex√©cuter quotidiennement)

function monitorChunkGrowth() {
  print("=== Monitoring de la Croissance des Chunks ===\n");

  var collections = db.getSiblingDB("config").collections.find({
    dropped: false
  }).toArray();

  var warnings = [];

  collections.forEach(function(coll) {
    if (coll.key) {
      // Analyser la distribution des chunks
      var chunks = db.getSiblingDB("config").chunks.aggregate([
        { $match: { ns: coll._id } },
        { $group: {
            _id: "$shard",
            numChunks: { $sum: 1 }
          }
        }
      ]).toArray();

      if (chunks.length > 0) {
        var max = Math.max(...chunks.map(c => c.numChunks));
        var min = Math.min(...chunks.map(c => c.numChunks));
        var imbalance = ((max - min) / min) * 100;

        if (imbalance > 30) {
          warnings.push({
            collection: coll._id,
            type: "imbalance",
            severity: "warning",
            message: "D√©s√©quilibre de " + imbalance.toFixed(1) + "%"
          });
        }
      }

      // V√©rifier les jumbo chunks
      var jumboCount = db.getSiblingDB("config").chunks.countDocuments({
        ns: coll._id,
        jumbo: true
      });

      if (jumboCount > 0) {
        warnings.push({
          collection: coll._id,
          type: "jumbo",
          severity: "critical",
          message: jumboCount + " jumbo chunk(s) d√©tect√©(s)"
        });
      }

      // V√©rifier les chunks proches de la limite
      // (n√©cessite un custom exporter pour la taille r√©elle)
    }
  });

  // Afficher les alertes
  if (warnings.length === 0) {
    print("‚úÖ Aucune alerte\n");
  } else {
    print("‚ö†Ô∏è  " + warnings.length + " alerte(s) d√©tect√©e(s) :\n");

    warnings.forEach(function(warning) {
      var icon = warning.severity === "critical" ? "üî¥" : "‚ö†Ô∏è ";
      print(icon + " " + warning.collection);
      print("   Type : " + warning.type);
      print("   " + warning.message);
      print("");
    });

    // Envoyer une notification (email, Slack, etc.)
    // sendAlert(warnings);
  }
}

// Ex√©cuter
monitorChunkGrowth();

// √Ä automatiser via cron :
// 0 9 * * * mongosh --host mongos1 --eval "load('/scripts/monitor_chunks.js'); monitorChunkGrowth()"
```

### 5. Limites de Taille Proactives

```javascript
// Configurer une taille de chunk plus petite pour collections critiques

// Par d√©faut : 64 MB
// Pour collections sensibles : 32 MB ou moins

db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 32 } },  // 32 MB
  { upsert: true }
)

// ‚ö†Ô∏è Impact :
// - Plus de chunks ‚Üí Plus de m√©tadonn√©es
// - Migrations plus fr√©quentes mais plus rapides
// - Meilleure granularit√© de distribution

// Recommand√© pour :
// - Collections avec croissance rapide
// - Shard keys avec cardinalit√© moyenne (1000-10000)
// - Clusters avec beaucoup de shards (> 5)
```

---

## Anti-Patterns √† √âviter

### ‚ùå Anti-Pattern 1 : Ignorer les Alertes Jumbo Chunks

**Probl√®me** :

```javascript
// Jumbo chunks d√©tect√©s mais ignor√©s
// "On verra plus tard, √ßa fonctionne pour l'instant"

db.getSiblingDB("config").chunks.find({ jumbo: true }).count()
// 3 jumbo chunks

// 6 mois plus tard :
db.getSiblingDB("config").chunks.find({ jumbo: true }).count()
// 15 jumbo chunks !

// Probl√®me devenu ing√©rable
```

**Cons√©quence** :
- D√©s√©quilibre croissant
- R√©solution de plus en plus complexe
- Co√ªt de correction augmente exponentiellement

**Solution** :
```javascript
// Traiter imm√©diatement au premier jumbo chunk
// Mise en place d'alertes automatiques
// Review hebdomadaire de la distribution
```

### ‚ùå Anti-Pattern 2 : Tenter de Splitter un Jumbo Chunk Monovalu√©

**Probl√®me** :

```javascript
// Jumbo chunk avec une seule valeur de shard key
// Tentative de split manuel

sh.splitAt("mydb.orders", { status: "active", _id: ObjectId("...") })

// Erreur :
{
  "ok": 0,
  "errmsg": "split point must be different from existing boundaries",
  "code": 141
}

// Ou pire : split r√©ussit mais cr√©e 2 chunks dont 1 reste jumbo
```

**Cons√©quence** :
- Perte de temps
- Faux espoir de r√©solution
- Chunk toujours bloqu√©

**Solution** :
```javascript
// Diagnostiquer d'abord la cardinalit√©
// Si cardinalit√© = 1 ‚Üí Refine ou resharding obligatoire
// Pas de raccourci possible
```

### ‚ùå Anti-Pattern 3 : Migration Forc√©e Sans Monitoring

**Probl√®me** :

```bash
# Activer attemptToBalanceJumboChunks
# Puis partir en weekend sans surveillance

db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  { $set: { attemptToBalanceJumboChunks: true } }
)

# Lundi matin : Cluster satur√©
# Migration a pris 48h et n'est pas termin√©e
# Performance d√©grad√©e pendant tout le weekend
```

**Cons√©quence** :
- Impact utilisateurs prolong√©
- Saturation des ressources
- Possibles timeouts et √©checs

**Solution** :
```bash
# Migration forc√©e uniquement pendant fen√™tre de maintenance
# Monitoring continu
# Ressources cluster augment√©es temporairement
# Plan de rollback pr√©par√©
```

### ‚ùå Anti-Pattern 4 : Resharding en Production Sans Test

**Probl√®me** :

```javascript
// Resharding directement en production
// Sans test en staging

db.adminCommand({
  reshardCollection: "mydb.orders",
  key: { customer_id: "hashed" }
})

// D√©couverte apr√®s coup :
// - Dur√©e : 12h (au lieu de 4h estim√©es)
// - Nouvelle shard key inad√©quate (requ√™tes broadcast)
// - Downgrade impossible
```

**Cons√©quence** :
- Op√©ration irr√©versible
- Probl√®mes d√©couverts trop tard
- Downtime prolong√©

**Solution** :
```bash
# 1. Restaurer backup production en staging
# 2. Tester resharding complet
# 3. Valider performances avec nouvelle shard key
# 4. Estimer dur√©e r√©elle
# 5. Planifier fen√™tre de maintenance adapt√©e
# 6. Ex√©cuter en production avec √©quipe compl√®te
```

### ‚ùå Anti-Pattern 5 : Shard Key "Temporaire"

**Probl√®me** :

```javascript
// "On choisit cette shard key pour l'instant"
// "On changera plus tard si n√©cessaire"

sh.shardCollection("mydb.orders", { status: 1 })

// 2 ans plus tard :
// - 500M de documents
// - Jumbo chunks ing√©rables
// - Resharding n√©cessiterait 1 semaine de downtime
// - Impossible √† corriger sans impact majeur
```

**Cons√©quence** :
- Dette technique croissante
- Correction devient prohibitive
- Cluster bloqu√© avec mauvaise architecture

**Solution** :
```javascript
// Choisir la BONNE shard key d√®s le d√©part
// Analyse approfondie avant sharding
// Tests avec donn√©es r√©alistes
// "Mesurer deux fois, couper une fois"
```

---

## Cas Pratiques de R√©solution

### Cas 1 : E-commerce avec Cat√©gories

**Contexte** :

```javascript
// Collection products shard√©e sur category
sh.shardCollection("ecommerce.products", { category: 1 })

// Apr√®s 1 an :
// - category="Electronics" : 2M produits (1.5 GB) ‚Üí JUMBO
// - Autres cat√©gories : 500K produits au total
```

**R√©solution** :

```javascript
// √âtape 1 : Diagnostic
diagnoseJumboChunks()
// ‚Üí 1 jumbo chunk sur category="Electronics"
// ‚Üí Cardinalit√© du chunk : 1 (toutes les valeurs identiques)

// √âtape 2 : Analyse des requ√™tes applicatives
// Requ√™tes principales :
// - find({ category: "Electronics", brand: "Samsung" })
// - find({ category: "Electronics", price: { $lt: 500 } })
// ‚Üí brand et price sont des champs pertinents

// √âtape 3 : Refine avec brand
db.products.createIndex({ category: 1, brand: 1 })

db.adminCommand({
  refineCollectionShardKey: "ecommerce.products",
  key: { category: 1, brand: 1 }
})

// √âtape 4 : Validation
sh.status()
// Chunk "Electronics" maintenant splittable :
// - { category: "Electronics", brand: "Apple" }
// - { category: "Electronics", brand: "Samsung" }
// - { category: "Electronics", brand: "Sony" }
// etc.

// Distribution √©quilibr√©e retrouv√©e
```

### Cas 2 : SaaS Multi-tenant

**Contexte** :

```javascript
// Application SaaS avec 10K tenants
sh.shardCollection("saas.documents", { tenant_id: 1 })

// Probl√®me : 1 gros client (tenant_BIGCORP)
// - 5M documents (3 GB) ‚Üí JUMBO
// - Autres clients : 10M documents au total mais bien distribu√©s
```

**R√©solution** :

```javascript
// Option 1 : Refine (si MongoDB 4.4+)
db.documents.createIndex({ tenant_id: 1, document_id: 1 })

db.adminCommand({
  refineCollectionShardKey: "saas.documents",
  key: { tenant_id: 1, document_id: 1 }
})

// Option 2 : Si MongoDB < 4.4, migration forc√©e
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  { $set: { attemptToBalanceJumboChunks: true } }
)

// Monitoring pendant 24h
// Migration du jumbo chunk vers un autre shard

// Option 3 : Isolation du gros client (solution architecturale)
// Cr√©er un shard d√©di√© pour BIGCORP
sh.addShardTag("shardBIGCORP", "bigcorp")

sh.addTagRange(
  "saas.documents",
  { tenant_id: "tenant_BIGCORP" },
  { tenant_id: "tenant_BIGCORP\xff" },  // \xff = valeur max
  "bigcorp"
)

// Le gros client reste isol√©, ne perturbe plus le cluster
```

### Cas 3 : Logs Applicatifs

**Contexte** :

```javascript
// Logs shard√©s sur service_name
sh.shardCollection("platform.logs", { service_name: 1, timestamp: 1 })

// Service "api-gateway" g√©n√®re 80% des logs
// Chunk { service_name: "api-gateway" } ‚Üí 5 GB ‚Üí JUMBO
```

**R√©solution** :

```javascript
// Solution : Hashed sur service_name
// N√©cessite resharding (MongoDB 5.0+)

db.adminCommand({
  reshardCollection: "platform.logs",
  key: { service_name: "hashed", timestamp: 1 },
  numInitialChunks: 16
})

// R√©sultat :
// - Logs api-gateway distribu√©s uniform√©ment sur tous les shards
// - Plus de jumbo chunks
// - Performance restaur√©e

// Alternative si MongoDB < 5.0 :
// Refine avec un champ suppl√©mentaire
db.logs.createIndex({ service_name: 1, timestamp: 1, request_id: 1 })

db.adminCommand({
  refineCollectionShardKey: "platform.logs",
  key: { service_name: 1, timestamp: 1, request_id: 1 }
})
```

---

## Checklist de Gestion des Jumbo Chunks

```yaml
detection:
  quotidienne:
    - V√©rifier pr√©sence de jumbo chunks
    - Commande: db.getSiblingDB("config").chunks.find({ jumbo: true }).count()
    - Alerte si count > 0

  hebdomadaire:
    - Analyser la distribution par collection
    - Script: checkChunkDistribution()
    - Alerte si d√©s√©quilibre > 20%

prevention:
  avant_sharding:
    - Analyser la cardinalit√© de la shard key candidate
    - Analyser la distribution des valeurs
    - Tester avec donn√©es r√©elles en staging
    - Pr√©-splitter pour distribution optimale

  en_production:
    - Monitoring continu de la croissance
    - Review mensuelle de la distribution
    - Alertes automatiques sur d√©s√©quilibre

resolution:
  immediat:
    - Diagnostiquer la cause (script diagnoseJumboChunks)
    - Analyser la cardinalit√© du chunk
    - √âvaluer l'impact sur le cluster

  court_terme:
    - MongoDB 4.4+ : refineCollectionShardKey
    - MongoDB 5.0+ : reshardCollection si n√©cessaire
    - < 4.4 : Migration forc√©e ou recr√©ation

  long_terme:
    - Documentation de l'incident
    - Post-mortem pour pr√©vention future
    - Am√©lioration du monitoring
```

---

## Conclusion

Les jumbo chunks sont un sympt√¥me, pas une maladie. Ils r√©v√®lent presque toujours un probl√®me de conception : **une shard key inad√©quate**. Les points cl√©s √† retenir :

- ‚úÖ **Pr√©vention > Correction** : Choisir la bonne shard key d√®s le d√©part
- ‚úÖ **D√©tection pr√©coce** : Monitoring automatis√© et alertes
- ‚úÖ **Action rapide** : Traiter au premier jumbo chunk d√©tect√©
- ‚úÖ **Outils modernes** : refineCollectionShardKey (4.4+) et reshardCollection (5.0+)
- ‚úÖ **Tests rigoureux** : Toujours tester en staging avant production
- ‚úÖ **Documentation** : Apprendre de chaque incident

**Philosophie** : Les jumbo chunks ne sont pas une fatalit√©. Avec une conception soigneuse, une analyse pr√©alable approfondie, et un monitoring proactif, ils sont largement √©vitables. Et quand ils surviennent malgr√© tout, les versions modernes de MongoDB offrent des outils puissants pour les r√©soudre sans downtime majeur.

**Investissez dans le choix de la shard key : c'est la d√©cision la plus importante lors du d√©ploiement d'un cluster shard√©.**

---

## Ressources

- [MongoDB Documentation - Jumbo Chunks](https://docs.mongodb.com/manual/core/sharding-data-partitioning/#jumbo-chunks)
- [MongoDB Documentation - refineCollectionShardKey](https://docs.mongodb.com/manual/reference/command/refineCollectionShardKey/)
- [MongoDB Documentation - reshardCollection](https://docs.mongodb.com/manual/reference/command/reshardCollection/)
- [MongoDB Blog - Resharding in MongoDB 5.0](https://www.mongodb.com/blog)

---


‚è≠Ô∏è [Bonnes pratiques de sharding](/10-sharding/12-bonnes-pratiques-sharding.md)
