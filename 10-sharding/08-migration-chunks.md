üîù Retour au [Sommaire](/SOMMAIRE.md)

# 10.8 Migration des Chunks

## Introduction

La migration des chunks est le m√©canisme fondamental qui permet au cluster shard√© MongoDB de maintenir un √©quilibre de charge entre les shards. Contrairement √† d'autres syst√®mes de bases de donn√©es distribu√©es qui n√©cessitent des arr√™ts planifi√©s pour redistribuer les donn√©es, MongoDB effectue ces migrations **en arri√®re-plan**, de mani√®re **transparente** pour les applications, tout en continuant √† servir les requ√™tes de lecture et d'√©criture.

Cette section explore en profondeur le processus de migration, ses phases techniques, les optimisations possibles, et les situations probl√©matiques qui peuvent survenir en production.

---

## Concepts Fondamentaux

### Qu'est-ce qu'une Migration de Chunk ?

Une **migration de chunk** est le processus de transfert d'un chunk (et de tous ses documents) d'un shard source vers un shard destination. Ce processus :

- Est **initi√© par le balancer** (automatique) ou **manuellement** (op√©ration d'administration)
- S'ex√©cute de mani√®re **asynchrone** en arri√®re-plan
- Maintient la **disponibilit√©** des donn√©es pendant la migration
- Garantit la **coh√©rence** des donn√©es apr√®s la migration
- Peut √™tre **annul√©** en cas de probl√®me

### Acteurs de la Migration

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BALANCER                             ‚îÇ
‚îÇ                (Config Server Primary)                  ‚îÇ
‚îÇ  - D√©tecte le d√©s√©quilibre                              ‚îÇ
‚îÇ  - Planifie les migrations                              ‚îÇ
‚îÇ  - Coordonne l'ex√©cution                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚îÇ Commande moveChunk
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    Transfert      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     SHARD SOURCE              ‚îÇ    de donn√©es     ‚îÇ    SHARD DESTINATION         ‚îÇ
‚îÇ   (Poss√®de le chunk)          ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  (Re√ßoit le chunk)           ‚îÇ
‚îÇ                               ‚îÇ                   ‚îÇ                              ‚îÇ
‚îÇ  - Copie les documents        ‚îÇ                   ‚îÇ  - Re√ßoit les documents      ‚îÇ
‚îÇ  - Synchronise modifications  ‚îÇ                   ‚îÇ  - Applique les mises √† jour ‚îÇ
‚îÇ  - Supprime apr√®s commit      ‚îÇ                   ‚îÇ  - Devient propri√©taire      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                                                  ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                                   ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     CONFIG SERVERS           ‚îÇ
                    ‚îÇ  - Mise √† jour m√©tadonn√©es   ‚îÇ
                    ‚îÇ  - Coordination globale      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Triggers de Migration

Les migrations sont d√©clench√©es dans les situations suivantes :

1. **Balancer automatique** : D√©s√©quilibre d√©tect√© entre shards
2. **Migration manuelle** : Commande `moveChunk` explicite
3. **Ajout de shard** : Distribution vers le nouveau shard
4. **Zone sharding** : Respect des contraintes de localisation
5. **Vidage d'un shard** : Avant suppression (`removeShard`)

---

## Processus D√©taill√© de Migration

### Phase 1 : Initiation (Balancer ‚Üí Shard Source)

Le balancer identifie un d√©s√©quilibre et envoie une commande `moveChunk` au shard source.

```javascript
// Commande envoy√©e par le balancer (interne)
{
  moveChunk: "mydb.users",
  find: { user_id: "user_50000" },  // Point dans le chunk
  to: "shardB"                      // Shard destination
}
```

**Actions du shard source** :
1. V√©rifie que le chunk existe et lui appartient
2. Acquiert un **verrou exclusif** sur les m√©tadonn√©es du chunk
3. Notifie le shard destination pour pr√©parer la r√©ception
4. Initialise les structures de tracking des modifications

**Logs observables** :
```
[balancer] Starting chunk migration: mydb.users chunk [MinKey, user_50000) from shardA to shardB
[shardA] Acquired migration lock for chunk mydb.users [MinKey, user_50000)
```

### Phase 2 : Copie Initiale (Bulk Transfer)

Le shard destination **copie en masse** tous les documents du chunk depuis le shard source.

```javascript
// Processus interne de copie
// Le destination lit les documents du source via une connexion directe

// Shard Source : It√©ration sur les documents du chunk
db.users.find({
  $and: [
    { user_id: { $gte: MinKey } },
    { user_id: { $lt: "user_50000" } }
  ]
}).batchSize(10000)

// Shard Destination : Insertion en batch
db.users.insertMany([...documents...], { ordered: false })
```

**Caract√©ristiques** :
- Transfert par **batches** (typiquement 10 000 documents)
- Utilise une **connexion directe** entre les shards (pas via mongos)
- Les **√©critures continuent** sur le shard source pendant la copie
- Dur√©e proportionnelle √† la **taille du chunk**

**M√©triques observables** :
```javascript
// √âtat de la migration en cours
db.currentOp({
  type: "op",
  op: { $in: ["command"] },
  "command.moveChunk": { $exists: true }
})

// R√©sultat exemple :
{
  "opid": "shardA:12345",
  "op": "command",
  "ns": "mydb.users",
  "command": { "moveChunk": "mydb.users", ... },
  "msg": "Cloning phase: copied 500000/800000 documents"
}
```

### Phase 3 : Synchronisation Incr√©mentale (Catch-up)

Pendant la copie initiale, des √©critures ont eu lieu sur le shard source. Cette phase **rattrape les modifications**.

```javascript
// Le shard source maintient un log des op√©rations survenues pendant la copie
// Ces op√©rations sont rejou√©es sur le destination

// Modifications track√©es :
// - Insertions (nouveau documents dans le chunk)
// - Mises √† jour (modifications de documents existants)
// - Suppressions (documents supprim√©s)

// Exemple de rejeu :
// Source : updateOne({ user_id: "user_25000" }, { $set: { status: "active" } })
// Destination : Applique la m√™me mise √† jour
```

**Optimisations** :
- Utilise un **buffer en m√©moire** pour tracker les modifications
- Plusieurs **passes de synchronisation** si n√©cessaire
- Timeout si trop de modifications (charge trop √©lev√©e)

**Cas probl√©matique** :
```javascript
// Si le taux d'√©criture est trop √©lev√© :
// Le buffer de modifications d√©borde
// ‚Üí La migration √©choue et est retent√©e plus tard

// Log d'erreur :
"Migration failed: too much data in transfer mods queue (>500MB)"
```

### Phase 4 : Finalisation (Critical Section)

Phase critique o√π le shard source **bloque bri√®vement les √©critures** pour transf√©rer les derni√®res modifications.

```javascript
// S√©quence critique (quelques centaines de millisecondes) :
// 1. Bloquer les √©critures sur le chunk
// 2. Transf√©rer les derni√®res modifications
// 3. Mettre √† jour les m√©tadonn√©es sur les config servers
// 4. Rediriger les futures √©critures vers le destination
```

**Impact sur les applications** :
```javascript
// Les √©critures pendant cette phase :
db.users.insertOne({ user_id: "user_30000", ... })

// Peuvent recevoir des erreurs transitoires :
// - WriteConcernError si timeout
// - OperationFailed si la migration √©choue

// Les applications avec retry automatique g√®rent cela sans probl√®me
```

**Logs** :
```
[shardA] Entering critical section for chunk migration
[shardA] Critical section duration: 347ms
[configsvr] Updated chunk metadata: mydb.users [MinKey, user_50000) now on shardB
```

### Phase 5 : Commit et Nettoyage

Le chunk est d√©sormais **officiellement** sur le shard destination.

```javascript
// 1. Config servers mettent √† jour les m√©tadonn√©es
db.getSiblingDB("config").chunks.updateOne(
  {
    ns: "mydb.users",
    min: { user_id: MinKey },
    max: { user_id: "user_50000" }
  },
  {
    $set: {
      shard: "shardB",
      lastmod: Timestamp(...)
    }
  }
)

// 2. Les mongos sont notifi√©s du changement
// 3. Ils mettent √† jour leur cache de routing
// 4. Les futures requ√™tes sont dirig√©es vers shardB

// 5. Le shard source supprime les documents du chunk
db.users.deleteMany({
  $and: [
    { user_id: { $gte: MinKey } },
    { user_id: { $lt: "user_50000" } }
  ]
})
```

**D√©lai de suppression** :
```javascript
// Par d√©faut, suppression imm√©diate
// Peut √™tre configur√© pour attendre (prudence) :

db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  { $set: { _waitForDelete: true } }
)

// Avec _waitForDelete: true
// ‚Üí Le balancer attend que la suppression soit termin√©e
// ‚Üí √âvite les surcharges sur le source
```

### Phase 6 : Validation (Post-migration)

```javascript
// V√©rification automatique par MongoDB
// 1. Compter les documents sur le destination
db.users.countDocuments({
  $and: [
    { user_id: { $gte: MinKey } },
    { user_id: { $lt: "user_50000" } }
  ]
})

// 2. V√©rifier les m√©tadonn√©es
db.getSiblingDB("config").chunks.findOne({
  ns: "mydb.users",
  min: { user_id: MinKey }
})

// 3. Logger le succ√®s
// [configsvr] Migration succeeded: mydb.users chunk [MinKey, user_50000)
//             from shardA to shardB in 45.2 seconds
```

---

## Migration Manuelle de Chunks

### Commande moveChunk

```javascript
// Syntaxe de base
sh.moveChunk(
  "mydb.users",                    // Collection
  { user_id: "user_50000" },       // Point dans le chunk √† migrer
  "shardB"                         // Shard destination
)

// Ou via adminCommand
db.adminCommand({
  moveChunk: "mydb.users",
  find: { user_id: "user_50000" },
  to: "shardB",
  _waitForDelete: true             // Attendre suppression sur source
})
```

### Options Avanc√©es

```javascript
// Migration avec options de performance
db.adminCommand({
  moveChunk: "mydb.users",
  find: { user_id: "user_50000" },
  to: "shardB",

  // Options avanc√©es
  _secondaryThrottle: false,       // D√©sactiver throttling (plus rapide)
  writeConcern: { w: 1 },          // Write concern pendant migration
  maxTimeMS: 3600000               // Timeout : 1 heure
})
```

**Param√®tres critiques** :

| Option | Valeur | Impact |
|--------|--------|--------|
| `_secondaryThrottle` | `false` | Migration plus rapide mais charge r√©seau accrue |
| `_waitForDelete` | `true` | √âvite la surcharge du shard source |
| `maxTimeMS` | `3600000` | Timeout pour migrations tr√®s longues |
| `writeConcern` | `{ w: "majority" }` | Garanties de durabilit√© vs performance |

### Cas d'Usage : Migration Manuelle

#### 1. Pr√©paration d'une Maintenance

```javascript
// Avant maintenance sur shardA
// Migrer tous les chunks vers d'autres shards

// Lister les chunks sur shardA
var chunks = db.getSiblingDB("config").chunks.find({
  ns: "mydb.users",
  shard: "shardA"
}).toArray();

// D√©sactiver le balancer
sh.stopBalancer();

// Migrer un par un vers shardB
chunks.forEach(function(chunk) {
  print("Migrating chunk: " + tojson(chunk.min) + " to " + tojson(chunk.max));

  sh.moveChunk(
    "mydb.users",
    chunk.min,
    "shardB"
  );
});

// R√©activer le balancer apr√®s
sh.startBalancer();
```

#### 2. √âquilibrage Forc√©

```javascript
// Distribution initiale d√©s√©quilibr√©e
// shardA : 80 chunks
// shardB : 20 chunks

// Forcer l'√©quilibrage manuel
var targetPerShard = 50;

// Migrer 30 chunks de A vers B
for (var i = 0; i < 30; i++) {
  var chunk = db.getSiblingDB("config").chunks.findOne({
    ns: "mydb.users",
    shard: "shardA"
  });

  if (chunk) {
    sh.moveChunk("mydb.users", chunk.min, "shardB");
    sleep(5000);  // Pause entre migrations
  }
}
```

#### 3. Zone Sharding : Migration vers Zones

```javascript
// D√©finir des zones g√©ographiques
sh.addShardToZone("shardEU", "europe")
sh.addShardToZone("shardUS", "usa")

// D√©finir les plages
sh.updateZoneKeyRange(
  "mydb.users",
  { region: "EU", user_id: MinKey },
  { region: "EU", user_id: MaxKey },
  "europe"
)

sh.updateZoneKeyRange(
  "mydb.users",
  { region: "US", user_id: MinKey },
  { region: "US", user_id: MaxKey },
  "usa"
)

// Le balancer migrera automatiquement les chunks
// Ou forcer manuellement :
sh.moveChunk(
  "mydb.users",
  { region: "EU", user_id: "user_1000" },
  "shardEU"
)
```

---

## Performance et Optimisation

### M√©triques de Performance

```javascript
// Dur√©e typique d'une migration par taille de chunk
// (sur r√©seau 1 Gbps, charge normale)

// Chunk 10 MB   : 2-5 secondes
// Chunk 64 MB   : 10-30 secondes
// Chunk 128 MB  : 30-60 secondes
// Chunk 256 MB  : 1-3 minutes
// Chunk 1 GB    : 5-15 minutes

// Mesurer la dur√©e d'une migration
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.commit",
  ns: "mydb.users"
}).sort({ time: -1 }).limit(10).forEach(function(entry) {
  var start = entry.details.step1of6;
  var end = entry.time;
  var durationMs = end - start;

  print(
    "Chunk: " + tojson(entry.details.min) + " ‚Üí " + tojson(entry.details.max) +
    " | Duration: " + (durationMs / 1000).toFixed(2) + "s" +
    " | From: " + entry.details.from + " | To: " + entry.details.to
  );
});
```

### Facteurs Impactant la Performance

#### 1. Taille du Chunk

```javascript
// Impact direct sur la dur√©e
// Optimiser la taille des chunks :

// Chunks trop petits (< 32 MB)
// - Migrations fr√©quentes ‚Üí overhead
// - Fragmentation m√©tadonn√©es

// Chunks trop gros (> 256 MB)
// - Migrations lentes
// - Impact performance prolong√©
// - Risque de timeout

// Taille optimale : 64-128 MB
use config
db.settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 64 } },
  { upsert: true }
)
```

#### 2. Charge du Syst√®me

```javascript
// Migration pendant pic de charge
// ‚Üí Ralentissement, timeouts possibles

// Strat√©gie : Fen√™tre de migration
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  {
    $set: {
      activeWindow: {
        start: "01:00",  // 1h du matin
        stop: "05:00"    // 5h du matin
      }
    }
  },
  { upsert: true }
)
```

#### 3. Bande Passante R√©seau

```javascript
// Mesurer la bande passante entre shards
// Sur le shard source :

// Test avec iperf (hors MongoDB)
// shardA ‚Üí shardB : iperf3 -c shardB_ip

// R√©sultats typiques :
// - 1 Gbps r√©seau : 100-120 MB/s th√©orique
// - R√©el MongoDB : 60-80 MB/s (overhead protocole)

// Pour 1 chunk de 128 MB :
// Temps transfert = 128 MB / 80 MB/s ‚âà 1.6 secondes
// + overhead copie + sync + commit ‚âà 30-60 secondes total
```

#### 4. Type de Stockage

```javascript
// HDD vs SSD impact significatif

// HDD (7200 RPM)
// - Lecture s√©quentielle : 100-150 MB/s
// - IOPS random : 100-200
// ‚Üí Migrations lentes (bottleneck disque)

// SSD SATA
// - Lecture s√©quentielle : 500-550 MB/s
// - IOPS random : 10 000+
// ‚Üí Migrations 3-5x plus rapides

// NVMe
// - Lecture s√©quentielle : 3000-7000 MB/s
// - IOPS random : 100 000+
// ‚Üí Migrations tr√®s rapides (bottleneck r√©seau)
```

### Optimisations Avanc√©es

#### 1. Parall√©lisation des Migrations

```javascript
// MongoDB 4.2+ : Migrations parall√®les
// Par d√©faut : 1 migration √† la fois par collection

// Augmenter le parall√©lisme (avec pr√©caution)
db.adminCommand({
  setParameter: 1,
  migrationMaxConcurrency: 2  // 2 migrations simultan√©es
})

// Attention : Charge CPU/r√©seau doubl√©e
// Recommand√© uniquement pour :
// - Maintenance planifi√©e
// - Hardware puissant
// - R√©seau haute capacit√© (10 Gbps+)
```

#### 2. D√©sactivation du Throttling

```javascript
// Par d√©faut, MongoDB "throttle" les migrations
// pour limiter l'impact sur les r√©plications secondaires

// D√©sactiver pour migrations rapides (maintenance)
db.adminCommand({
  moveChunk: "mydb.users",
  find: { user_id: "user_50000" },
  to: "shardB",
  _secondaryThrottle: false  // Plus rapide
})

// Impact : R√©plication vers secondaries peut prendre du retard
// Acceptable lors de fen√™tre de maintenance
```

#### 3. Compression du Transfert

```javascript
// MongoDB 4.2+ : Compression automatique du transfert r√©seau
// (si activ√©e dans la configuration)

// mongod.conf
net:
  compression:
    compressors: "snappy,zlib,zstd"

// Gains typiques :
// - Donn√©es texte : 60-80% r√©duction
// - Donn√©es binaires : 20-40% r√©duction
// - Trade-off CPU vs r√©seau
```

---

## Monitoring et Diagnostics

### Commandes de Monitoring

```javascript
// 1. √âtat du balancer
sh.getBalancerState()  // true/false
sh.isBalancerRunning()  // true si migration en cours

// 2. Migrations en cours
db.currentOp({
  $or: [
    { op: "command", "command.moveChunk": { $exists: true } },
    { desc: /^migrateThread/ }
  ]
})

// R√©sultat d√©taill√© :
{
  "opid": "shardA:67890",
  "op": "command",
  "ns": "mydb.users",
  "command": {
    "moveChunk": "mydb.users",
    "find": { "user_id": "user_50000" },
    "to": "shardB"
  },
  "msg": "Cloning phase: 700000/800000 documents",
  "progress": { "done": 700000, "total": 800000 },
  "microsecs_running": 34567890,  // ~34 secondes
  "numYields": 0
}

// 3. Historique des migrations
db.getSiblingDB("config").changelog.find({
  what: { $in: ["moveChunk.start", "moveChunk.commit", "moveChunk.error"] },
  time: { $gte: ISODate("2024-01-15T00:00:00Z") }
}).sort({ time: -1 }).limit(20)

// 4. Distribution actuelle
db.users.getShardDistribution()

// 5. Chunks par shard
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "mydb.users" } },
  { $group: {
      _id: "$shard",
      numChunks: { $sum: 1 },
      jumbo: { $sum: { $cond: ["$jumbo", 1, 0] } }
    }
  }
])
```

### Dashboard de Monitoring

```javascript
// Script de monitoring automatis√©
function monitorMigrations() {
  print("=== √âtat des Migrations ===");
  print("Date : " + new Date());
  print("");

  // 1. Balancer
  var balancerState = sh.getBalancerState();
  var balancerRunning = sh.isBalancerRunning();

  print("Balancer activ√© : " + balancerState);
  print("Migration en cours : " + balancerRunning);
  print("");

  // 2. Migrations actives
  var activeMigrations = db.currentOp({
    op: "command",
    "command.moveChunk": { $exists: true }
  }).inprog;

  if (activeMigrations.length > 0) {
    print("Migrations actives : " + activeMigrations.length);
    activeMigrations.forEach(function(op) {
      var progress = op.progress ?
        ((op.progress.done / op.progress.total) * 100).toFixed(2) + "%" :
        "N/A";

      print("  - Collection : " + op.ns);
      print("    Progression : " + progress);
      print("    Dur√©e : " + (op.microsecs_running / 1000000).toFixed(2) + "s");
      print("    Message : " + op.msg);
      print("");
    });
  } else {
    print("Aucune migration active");
  }
  print("");

  // 3. Derni√®res migrations (24h)
  var recentMigrations = db.getSiblingDB("config").changelog.aggregate([
    {
      $match: {
        what: "moveChunk.commit",
        time: { $gte: new Date(Date.now() - 86400000) }
      }
    },
    {
      $group: {
        _id: "$ns",
        count: { $sum: 1 }
      }
    },
    { $sort: { count: -1 } }
  ]).toArray();

  if (recentMigrations.length > 0) {
    print("Migrations (24h) :");
    recentMigrations.forEach(function(stat) {
      print("  - " + stat._id + " : " + stat.count + " migrations");
    });
  }
}

// Ex√©cuter toutes les 5 minutes
while (true) {
  monitorMigrations();
  sleep(300000);  // 5 minutes
}
```

### Alertes Critiques

```javascript
// Sc√©narios n√©cessitant des alertes

// 1. Migration √©choue de mani√®re r√©p√©t√©e
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.error",
  time: { $gte: new Date(Date.now() - 3600000) }  // Derni√®re heure
}).count()
// Si count > 5 ‚Üí Alerte

// 2. Migration dure trop longtemps
db.currentOp({
  op: "command",
  "command.moveChunk": { $exists: true },
  microsecs_running: { $gt: 3600000000 }  // > 1 heure
})
// Si r√©sultat ‚Üí Alerte

// 3. Jumbo chunks bloquent le balancer
db.getSiblingDB("config").chunks.find({
  ns: { $regex: /^mydb\./ },
  jumbo: true
}).count()
// Si count > 0 ‚Üí Alerte

// 4. D√©s√©quilibre important persiste
var distribution = db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "mydb.users" } },
  { $group: { _id: "$shard", count: { $sum: 1 } } }
]).toArray();

var max = Math.max(...distribution.map(d => d.count));
var min = Math.min(...distribution.map(d => d.count));
var imbalance = ((max - min) / min) * 100;

if (imbalance > 20) {
  print("‚ö†Ô∏è  D√©s√©quilibre : " + imbalance.toFixed(2) + "%");
}
```

---

## Probl√®mes Courants et R√©solutions

### Probl√®me 1 : Migration Timeout

**Sympt√¥mes** :
```javascript
// Log sur le shard source
"Migration failed: chunk migration timed out after 3600 seconds"

// Ou dans db.currentOp() apr√®s longue dur√©e
{
  "opid": "shardA:12345",
  "microsecs_running": 3600000000,  // 1 heure
  "msg": "Waiting for critical section"
}
```

**Causes** :
1. Chunk trop volumineux (>500 MB)
2. Charge d'√©criture trop √©lev√©e sur le chunk
3. R√©seau lent ou instable
4. Disque satur√© (I/O bottleneck)

**Solutions** :

```javascript
// Solution 1 : Augmenter le timeout
db.adminCommand({
  moveChunk: "mydb.users",
  find: { user_id: "user_50000" },
  to: "shardB",
  maxTimeMS: 7200000  // 2 heures au lieu de 1
})

// Solution 2 : Splitter le chunk avant migration
sh.splitAt("mydb.users", { user_id: "user_75000" })
// Puis migrer les deux morceaux s√©par√©ment

// Solution 3 : R√©duire la charge pendant la migration
// - Activer la fen√™tre de balancing (heures creuses)
// - Throttle des √©critures applicatives temporairement

// Solution 4 : D√©sactiver throttling (si acceptable)
db.adminCommand({
  moveChunk: "mydb.users",
  find: { user_id: "user_50000" },
  to: "shardB",
  _secondaryThrottle: false
})
```

### Probl√®me 2 : Jumbo Chunk Immobile

**Sympt√¥mes** :
```javascript
// Chunk marqu√© jumbo
db.getSiblingDB("config").chunks.findOne({
  ns: "mydb.orders",
  min: { status: "active" }
})

// R√©sultat :
{
  "_id": ObjectId("..."),
  "ns": "mydb.orders",
  "min": { "status": "active" },
  "max": { "status": "inactive" },
  "shard": "shardA",
  "jumbo": true  // ‚ö†Ô∏è Jumbo chunk
}

// Le balancer ne peut pas le migrer
// Log : "cannot move chunk: too large to migrate"
```

**Causes** :
- Shard key de faible cardinalit√©
- Tous les documents ont la m√™me valeur de shard key

**Solutions** :

```javascript
// Solution 1 : Forcer la migration (MongoDB 4.4+)
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  { $set: { attemptToBalanceJumboChunks: true } },
  { upsert: true }
)

sh.moveChunk("mydb.orders", { status: "active" }, "shardB")

// Solution 2 : Refine la shard key (MongoDB 5.0+)
db.adminCommand({
  refineCollectionShardKey: "mydb.orders",
  key: { status: 1, customer_id: 1 }  // Ajouter customer_id
})

// Solution 3 : Resharding complet (MongoDB 5.0+)
db.adminCommand({
  reshardCollection: "mydb.orders",
  key: { customer_id: "hashed" }  // Nouvelle shard key
})

// Solution 4 : Recr√©er la collection (derni√®re option)
// - Exporter les donn√©es
// - Cr√©er nouvelle collection avec bonne shard key
// - Importer les donn√©es
// - Renommer
```

### Probl√®me 3 : √âchec R√©p√©t√© de Migration

**Sympt√¥mes** :
```javascript
// Changelog montre des √©checs r√©p√©t√©s
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.error",
  ns: "mydb.users",
  time: { $gte: new Date(Date.now() - 3600000) }
}).count()
// R√©sultat : 10 √©checs dans la derni√®re heure

// Logs d√©taill√©s
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.error",
  ns: "mydb.users"
}).sort({ time: -1 }).limit(1).pretty()

// Erreur typique :
{
  "what": "moveChunk.error",
  "ns": "mydb.users",
  "details": {
    "errmsg": "Migration abort requested",
    "from": "shardA",
    "to": "shardB"
  }
}
```

**Causes** :
1. Charge trop √©lev√©e (buffer overflow)
2. Probl√®me r√©seau entre shards
3. Espace disque insuffisant sur destination
4. Config servers inaccessibles

**Diagnostic et R√©solution** :

```javascript
// Diagnostic 1 : V√©rifier la charge
db.currentOp({ secs_running: { $gte: 10 } }).inprog.length
// Si > 50 ‚Üí surcharge

// Diagnostic 2 : V√©rifier l'espace disque
db.adminCommand({ dbStats: 1, scale: 1024*1024*1024 })  // GB
// V√©rifier dataSize vs diskSpace

// Diagnostic 3 : Tester la connectivit√© r√©seau
// Depuis shardA vers shardB
mongosh --host shardB_ip:27018 --eval "db.adminCommand({ ping: 1 })"

// Diagnostic 4 : V√©rifier les config servers
db.adminCommand({ replSetGetStatus: 1 })  // Sur config server

// R√©solution :
// - R√©duire la charge (throttle applicatif)
// - Nettoyer l'espace disque
// - R√©soudre les probl√®mes r√©seau
// - Red√©marrer les config servers si n√©cessaire
```

### Probl√®me 4 : Migration Bloqu√©e en Critical Section

**Sympt√¥mes** :
```javascript
// Migration bloqu√©e depuis longtemps
db.currentOp({
  op: "command",
  "command.moveChunk": { $exists: true },
  microsecs_running: { $gt: 300000000 }  // > 5 minutes
})

// msg: "Entering critical section"
// ‚Üí Bloqu√©, les √©critures sont bloqu√©es
```

**Causes** :
- Deadlock avec autre op√©ration
- Config servers injoignables
- Bug MongoDB (rare)

**R√©solution d'urgence** :

```javascript
// 1. Identifier l'op√©ration bloqu√©e
var blockedOp = db.currentOp({
  op: "command",
  "command.moveChunk": { $exists: true }
}).inprog[0];

print("OpID : " + blockedOp.opid);

// 2. Tenter d'annuler (attention : risque)
db.killOp(blockedOp.opid)

// 3. Si pas de r√©ponse, red√©marrer le mongod (shard source)
// En dernier recours seulement

// 4. V√©rifier apr√®s red√©marrage
sh.status()
db.getSiblingDB("config").chunks.findOne({
  ns: "mydb.users",
  min: { user_id: "user_50000" }
})

// 5. Re-tenter la migration apr√®s investigation
```

---

## Strat√©gies de Migration en Production

### Strat√©gie 1 : Migration Progressive (√âvolutive)

Pour ajout d'un nouveau shard sans impact :

```javascript
// Contexte : Ajouter shardC au cluster existant (shardA, shardB)

// √âtape 1 : Ajouter le shard
sh.addShard("shardC/shardC1.example.com:27018,shardC2.example.com:27018,shardC3.example.com:27018")

// √âtape 2 : Le balancer migrera automatiquement
// Mais pour contr√¥ler le rythme :

// 2a. D√©sactiver le balancer
sh.stopBalancer()

// 2b. Migrer manuellement par petits lots
var targetChunksPerShard = 30;  // Objectif final

for (var i = 0; i < targetChunksPerShard; i++) {
  // Trouver un chunk sur shardA ou shardB
  var chunk = db.getSiblingDB("config").chunks.findOne({
    ns: "mydb.users",
    shard: { $in: ["shardA", "shardB"] }
  });

  if (chunk) {
    print("Migrating chunk " + (i+1) + "/" + targetChunksPerShard);
    sh.moveChunk("mydb.users", chunk.min, "shardC");

    // Pause entre migrations
    sleep(10000);  // 10 secondes
  }
}

// 2c. R√©activer le balancer pour finaliser
sh.startBalancer()
```

### Strat√©gie 2 : Vidage d'un Shard (D√©commission)

Pour retirer un shard du cluster :

```javascript
// Contexte : Retirer shardA (hardware obsol√®te)

// √âtape 1 : Marquer le shard pour removal
db.adminCommand({ removeShard: "shardA" })

// R√©sultat :
{
  "msg": "draining started successfully",
  "state": "started",
  "shard": "shardA",
  "remaining": {
    "chunks": 45,
    "dbs": 2
  }
}

// √âtape 2 : MongoDB migre automatiquement tous les chunks
// Monitoring de la progression
db.adminCommand({ removeShard: "shardA" })

// R√©sultat interm√©diaire :
{
  "msg": "draining ongoing",
  "state": "ongoing",
  "remaining": {
    "chunks": 12,  // R√©duction progressive
    "dbs": 2
  }
}

// √âtape 3 : G√©rer les bases primaires
// Certaines bases ont shardA comme shard primaire
// Il faut les d√©placer manuellement

db.adminCommand({ movePrimary: "mydb", to: "shardB" })

// √âtape 4 : Finaliser la removal
db.adminCommand({ removeShard: "shardA" })

// R√©sultat final :
{
  "msg": "removeshard completed successfully",
  "state": "completed",
  "shard": "shardA"
}

// √âtape 5 : Arr√™ter le shard physiquement
// Les membres du replica set shardA peuvent √™tre arr√™t√©s
```

### Strat√©gie 3 : Zone Sharding et Migration Cibl√©e

Pour conformit√© g√©ographique (GDPR, etc.) :

```javascript
// Contexte : Donn√©es europ√©ennes doivent rester en Europe

// √âtape 1 : D√©finir les zones
sh.addShardToZone("shardEU1", "europe")
sh.addShardToZone("shardEU2", "europe")
sh.addShardToZone("shardUS1", "usa")
sh.addShardToZone("shardUS2", "usa")

// √âtape 2 : Associer les plages de donn√©es aux zones
sh.updateZoneKeyRange(
  "mydb.users",
  { country: "FR", user_id: MinKey },
  { country: "FR", user_id: MaxKey },
  "europe"
)

sh.updateZoneKeyRange(
  "mydb.users",
  { country: "DE", user_id: MinKey },
  { country: "DE", user_id: MaxKey },
  "europe"
)

sh.updateZoneKeyRange(
  "mydb.users",
  { country: "US", user_id: MinKey },
  { country: "US", user_id: MaxKey },
  "usa"
)

// √âtape 3 : Le balancer migre automatiquement
// V√©rifier que les migrations respectent les zones
db.getSiblingDB("config").tags.find().pretty()

// √âtape 4 : Valider la conformit√©
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "mydb.users" } },
  {
    $lookup: {
      from: "shards",
      localField: "shard",
      foreignField: "_id",
      as: "shardInfo"
    }
  },
  {
    $group: {
      _id: {
        min: "$min",
        max: "$max",
        shard: "$shard"
      }
    }
  }
])
```

---

## Anti-Patterns et Erreurs Critiques

### ‚ùå Anti-Pattern 1 : Migrations Pendant Pic de Charge

**Probl√®me** :
```javascript
// Le balancer est actif 24/7
// Migrations se produisent pendant les heures de pointe
// ‚Üí Impact performance pour les utilisateurs
```

**Cons√©quence** :
- Latence accrue pour les requ√™tes
- Timeouts applicatifs
- D√©gradation de l'exp√©rience utilisateur

**Solution** :
```javascript
// D√©finir une fen√™tre de maintenance
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  {
    $set: {
      activeWindow: {
        start: "02:00",  // 2h du matin
        stop: "06:00"    // 6h du matin
      }
    }
  },
  { upsert: true }
)

// Ou d√©sactiver compl√®tement pendant √©v√©nements critiques
// (Black Friday, lancement produit)
sh.stopBalancer()
```

### ‚ùå Anti-Pattern 2 : Ignorer les √âchecs de Migration

**Probl√®me** :
```javascript
// Les migrations √©chouent r√©guli√®rement
// Mais aucune alerte ni investigation
// ‚Üí D√©s√©quilibre s'aggrave
```

**Cons√©quence** :
- Distribution d√©s√©quilibr√©e persistante
- Hot spots sur certains shards
- Performances d√©grad√©es √† long terme

**Solution** :
```javascript
// Monitoring actif des √©checs
// Script d'alerte (√† int√©grer dans votre monitoring)
function checkMigrationFailures() {
  var failures = db.getSiblingDB("config").changelog.find({
    what: "moveChunk.error",
    time: { $gte: new Date(Date.now() - 86400000) }
  }).count();

  if (failures > 5) {
    // D√©clencher alerte
    print("‚ö†Ô∏è  ALERTE : " + failures + " √©checs de migration dans les 24h");

    // Logger les d√©tails
    db.getSiblingDB("config").changelog.find({
      what: "moveChunk.error",
      time: { $gte: new Date(Date.now() - 86400000) }
    }).forEach(function(failure) {
      printjson(failure.details);
    });
  }
}

// Ex√©cuter quotidiennement
checkMigrationFailures();
```

### ‚ùå Anti-Pattern 3 : Migration Manuelle Sans Arr√™ter le Balancer

**Probl√®me** :
```javascript
// Migration manuelle pendant que le balancer est actif
sh.moveChunk("mydb.users", { user_id: "user_50000" }, "shardB")

// En parall√®le, le balancer tente de migrer d'autres chunks
// ‚Üí Conflit possible, comportement impr√©visible
```

**Cons√©quence** :
- Conflits de verrouillage
- Migrations √©chou√©es
- M√©tadonn√©es incoh√©rentes (rare mais possible)

**Solution** :
```javascript
// TOUJOURS arr√™ter le balancer avant migration manuelle
sh.stopBalancer()

// Attendre que toutes migrations en cours se terminent
while (sh.isBalancerRunning()) {
  print("Attente arr√™t du balancer...");
  sleep(1000);
}

// Puis effectuer les migrations manuelles
sh.moveChunk("mydb.users", { user_id: "user_50000" }, "shardB")
// ...

// R√©activer apr√®s
sh.startBalancer()
```

### ‚ùå Anti-Pattern 4 : Pas de Test de Migration

**Probl√®me** :
```bash
# D√©ployer un nouveau cluster en production
# Sans jamais avoir test√© les migrations
# ‚Üí Comportement inconnu en condition r√©elle
```

**Cons√©quence** :
- Surprises d√©sagr√©ables (timeouts, √©checs)
- Pas de baseline de performance
- Panique lors de probl√®mes

**Solution** :
```javascript
// Phase de test pr√©-production
// 1. Cr√©er un environnement de staging identique
// 2. Charger des donn√©es r√©alistes
// 3. Tester les migrations

// Test de migration
function testMigration() {
  var startTime = new Date();

  sh.moveChunk("mydb.users", { user_id: "user_50000" }, "shardB");

  // Attendre la fin
  while (sh.isBalancerRunning()) {
    sleep(100);
  }

  var endTime = new Date();
  var duration = (endTime - startTime) / 1000;

  print("Migration test√©e : " + duration + " secondes");

  // V√©rifier la distribution
  db.users.getShardDistribution();
}

testMigration();

// Documenter les r√©sultats
// Cr√©er des runbooks bas√©s sur l'exp√©rience
```

### ‚ùå Anti-Pattern 5 : Sur-Parall√©lisation

**Probl√®me** :
```javascript
// Augmenter excessivement le parall√©lisme
db.adminCommand({
  setParameter: 1,
  migrationMaxConcurrency: 10  // 10 migrations simultan√©es !
})
```

**Cons√©quence** :
- Saturation r√©seau
- Contention CPU/disque
- Toutes les migrations sont lentes
- Instabilit√© du cluster

**Solution** :
```javascript
// Parall√©lisme raisonnable
// R√®gle : max 2-3 migrations simultan√©es
db.adminCommand({
  setParameter: 1,
  migrationMaxConcurrency: 2
})

// Uniquement pendant maintenance planifi√©e
// Surveiller les m√©triques :
// - Utilisation r√©seau < 70%
// - Utilisation CPU < 80%
// - Latence r√©seau stable
```

---

## Outils et Scripts Utiles

### Script de Monitoring Complet

```javascript
// migration-monitor.js
// √Ä ex√©cuter via mongosh

function comprehensiveMigrationMonitor() {
  print("=".repeat(80));
  print("MONITORING DES MIGRATIONS - " + new Date().toISOString());
  print("=".repeat(80));
  print("");

  // 1. √âtat du balancer
  print("1. √âtat du Balancer");
  print("-".repeat(40));
  print("  Activ√© : " + sh.getBalancerState());
  print("  En cours : " + sh.isBalancerRunning());

  // Fen√™tre active
  var balancerConfig = db.getSiblingDB("config").settings.findOne({ _id: "balancer" });
  if (balancerConfig && balancerConfig.activeWindow) {
    print("  Fen√™tre active : " + balancerConfig.activeWindow.start + " - " + balancerConfig.activeWindow.stop);
  } else {
    print("  Fen√™tre active : Aucune (actif 24/7)");
  }
  print("");

  // 2. Migrations en cours
  print("2. Migrations Actives");
  print("-".repeat(40));
  var activeMigrations = db.currentOp({
    $or: [
      { op: "command", "command.moveChunk": { $exists: true } },
      { desc: /^migrateThread/ }
    ]
  }).inprog;

  if (activeMigrations.length === 0) {
    print("  Aucune migration active");
  } else {
    activeMigrations.forEach(function(mig) {
      print("  Collection : " + mig.ns);
      print("  Shard source : " + (mig.command ? mig.command.find : "N/A"));
      print("  Shard destination : " + (mig.command ? mig.command.to : "N/A"));
      print("  Dur√©e : " + (mig.microsecs_running / 1000000).toFixed(2) + "s");
      print("  √âtat : " + mig.msg);
      print("");
    });
  }
  print("");

  // 3. Statistiques r√©centes (24h)
  print("3. Statistiques (24 heures)");
  print("-".repeat(40));

  var last24h = new Date(Date.now() - 86400000);

  var successCount = db.getSiblingDB("config").changelog.count({
    what: "moveChunk.commit",
    time: { $gte: last24h }
  });

  var errorCount = db.getSiblingDB("config").changelog.count({
    what: "moveChunk.error",
    time: { $gte: last24h }
  });

  print("  Migrations r√©ussies : " + successCount);
  print("  Migrations √©chou√©es : " + errorCount);

  if (errorCount > 0) {
    print("  ‚ö†Ô∏è  Taux d'√©chec : " + ((errorCount / (successCount + errorCount)) * 100).toFixed(2) + "%");
  }
  print("");

  // 4. Distribution par collection
  print("4. Distribution des Chunks");
  print("-".repeat(40));

  var collections = db.getSiblingDB("config").collections.find({ dropped: false }).toArray();

  collections.forEach(function(coll) {
    if (coll.key) {  // Collection shard√©e
      var distribution = db.getSiblingDB("config").chunks.aggregate([
        { $match: { ns: coll._id } },
        { $group: { _id: "$shard", count: { $sum: 1 } } },
        { $sort: { count: -1 } }
      ]).toArray();

      print("  " + coll._id);
      distribution.forEach(function(dist) {
        print("    - " + dist._id + " : " + dist.count + " chunks");
      });
    }
  });
  print("");

  // 5. Jumbo chunks
  print("5. Jumbo Chunks");
  print("-".repeat(40));

  var jumboChunks = db.getSiblingDB("config").chunks.aggregate([
    { $match: { jumbo: true } },
    { $group: { _id: "$ns", count: { $sum: 1 } } }
  ]).toArray();

  if (jumboChunks.length === 0) {
    print("  Aucun jumbo chunk");
  } else {
    print("  ‚ö†Ô∏è  Collections avec jumbo chunks :");
    jumboChunks.forEach(function(jc) {
      print("    - " + jc._id + " : " + jc.count + " jumbo chunks");
    });
  }
  print("");

  // 6. Recommandations
  print("6. Recommandations");
  print("-".repeat(40));

  var recommendations = [];

  if (errorCount > successCount * 0.1) {
    recommendations.push("Taux d'√©chec √©lev√© (>" + (errorCount / (successCount + errorCount) * 100).toFixed(0) + "%) - Investiguer les causes");
  }

  if (jumboChunks.length > 0) {
    recommendations.push("Jumbo chunks d√©tect√©s - Revoir la shard key ou forcer la migration");
  }

  if (activeMigrations.some(m => m.microsecs_running > 1800000000)) {
    recommendations.push("Migration longue (>30min) - V√©rifier la charge et la bande passante");
  }

  if (recommendations.length === 0) {
    print("  ‚úÖ Tout semble normal");
  } else {
    recommendations.forEach(function(rec) {
      print("  ‚ö†Ô∏è  " + rec);
    });
  }

  print("");
  print("=".repeat(80));
}

// Ex√©cuter
comprehensiveMigrationMonitor();
```

### Script de Migration Contr√¥l√©e

```javascript
// controlled-migration.js
// Pour migrer un lot de chunks de mani√®re contr√¥l√©e

function controlledMigration(namespace, sourceShard, destShard, numChunks, pauseMs) {
  print("D√©but migration contr√¥l√©e");
  print("Collection : " + namespace);
  print("Source : " + sourceShard);
  print("Destination : " + destShard);
  print("Nombre de chunks : " + numChunks);
  print("Pause entre migrations : " + (pauseMs / 1000) + "s");
  print("");

  // Arr√™ter le balancer
  print("Arr√™t du balancer...");
  sh.stopBalancer();

  while (sh.isBalancerRunning()) {
    print("  Attente arr√™t balancer...");
    sleep(1000);
  }
  print("  ‚úÖ Balancer arr√™t√©");
  print("");

  var migratedCount = 0;
  var failedCount = 0;

  for (var i = 0; i < numChunks; i++) {
    // Trouver un chunk sur le source shard
    var chunk = db.getSiblingDB("config").chunks.findOne({
      ns: namespace,
      shard: sourceShard
    });

    if (!chunk) {
      print("Plus de chunks √† migrer sur " + sourceShard);
      break;
    }

    print("[" + (i+1) + "/" + numChunks + "] Migration chunk : " + tojson(chunk.min) + " ‚Üí " + tojson(chunk.max));

    try {
      var startTime = new Date();

      sh.moveChunk(namespace, chunk.min, destShard);

      var endTime = new Date();
      var duration = (endTime - startTime) / 1000;

      print("  ‚úÖ Succ√®s en " + duration.toFixed(2) + "s");
      migratedCount++;

    } catch (e) {
      print("  ‚ùå √âchec : " + e.message);
      failedCount++;
    }

    // Pause avant la prochaine migration
    if (i < numChunks - 1 && pauseMs > 0) {
      print("  Pause de " + (pauseMs / 1000) + "s...");
      sleep(pauseMs);
    }

    print("");
  }

  // R√©sum√©
  print("=".repeat(40));
  print("R√âSUM√â");
  print("Migr√©s avec succ√®s : " + migratedCount);
  print("√âchecs : " + failedCount);
  print("=".repeat(40));
  print("");

  // R√©activer le balancer
  print("R√©activation du balancer...");
  sh.startBalancer();
  print("  ‚úÖ Balancer r√©activ√©");
}

// Exemple d'utilisation :
// controlledMigration("mydb.users", "shardA", "shardB", 10, 5000);
```

---

## Conclusion

La migration de chunks est le c≈ìur de l'√©lasticit√© et de l'√©quilibrage de charge dans MongoDB. Une compr√©hension approfondie de ce processus est essentielle pour :

- ‚úÖ **Maintenir un cluster √©quilibr√©** : Distribution uniforme des donn√©es
- ‚úÖ **Diagnostiquer les probl√®mes** : Timeouts, √©checs, jumbo chunks
- ‚úÖ **Optimiser les performances** : Fen√™tres de migration, parall√©lisme, throttling
- ‚úÖ **G√©rer les op√©rations** : Ajout/retrait de shards, zone sharding
- ‚úÖ **Anticiper les impacts** : Charge r√©seau, latence, disponibilit√©

**Points cl√©s √† retenir** :
- Les migrations sont **automatiques** mais **configurables**
- Elles s'ex√©cutent **en arri√®re-plan** sans interruption de service
- Mais peuvent **impacter les performances** si mal g√©r√©es
- Le **monitoring proactif** est indispensable en production
- Les **fen√™tres de maintenance** prot√®gent les heures critiques

En production, privil√©giez toujours :
- **Monitoring continu** des migrations
- **Alertes** sur √©checs et dur√©es anormales
- **Fen√™tres de balancing** adapt√©es √† votre charge
- **Tests** avant toute op√©ration critique

---

## Ressources

- [MongoDB Documentation - Chunk Migration](https://docs.mongodb.com/manual/core/sharding-balancer-administration/)
- [MongoDB Documentation - moveChunk Command](https://docs.mongodb.com/manual/reference/command/moveChunk/)
- [MongoDB Blog - Chunk Migration Internals](https://www.mongodb.com/blog)
- [MongoDB University - M103: Basic Cluster Administration](https://university.mongodb.com/)

---


‚è≠Ô∏è [Op√©rations sur un cluster shard√©](/10-sharding/09-operations-cluster-sharde.md)
