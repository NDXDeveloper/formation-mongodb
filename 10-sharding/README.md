ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# Chapitre 10 : Sharding (Partitionnement Horizontal)

## Introduction

Le **sharding** reprÃ©sente la solution de mise Ã  l'Ã©chelle horizontale de MongoDB, permettant de distribuer les donnÃ©es sur plusieurs serveurs pour gÃ©rer des volumes massifs et des charges de travail intensives. Ce chapitre s'adresse Ã  un niveau avancÃ© et nÃ©cessite une maÃ®trise solide de la rÃ©plication (Replica Sets), car chaque shard dans une architecture shardÃ©e est typiquement implÃ©mentÃ© comme un Replica Set.

### Pourquoi le Sharding ?

Lorsqu'une base de donnÃ©es atteint ses limites en termes de stockage, de mÃ©moire ou de capacitÃ© de traitement, deux approches s'offrent aux architectes :

1. **Mise Ã  l'Ã©chelle verticale (Scale-up)** : Augmenter les ressources matÃ©rielles d'un serveur unique
   - âœ… Simple Ã  mettre en Å“uvre
   - âŒ CoÃ»ts exponentiels
   - âŒ Limites physiques infranchissables
   - âŒ Point de dÃ©faillance unique

2. **Mise Ã  l'Ã©chelle horizontale (Scale-out)** : Distribuer les donnÃ©es sur plusieurs serveurs
   - âœ… Croissance quasi-linÃ©aire
   - âœ… CoÃ»ts plus prÃ©visibles
   - âœ… Haute disponibilitÃ©
   - âŒ ComplexitÃ© architecturale accrue

Le sharding MongoDB implÃ©mente la deuxiÃ¨me approche en partitionnant automatiquement les donnÃ©es selon une clÃ© de partitionnement (shard key).

### Principes Fondamentaux

Le sharding repose sur trois piliers conceptuels :

**1. Partitionnement des donnÃ©es**
Les collections sont divisÃ©es en **chunks** (morceaux) de donnÃ©es distribuÃ©s sur diffÃ©rents shards. Chaque chunk contient un sous-ensemble des documents basÃ© sur la valeur de la shard key.

**2. Distribution Ã©quilibrÃ©e**
MongoDB maintient automatiquement une distribution Ã©quilibrÃ©e des chunks entre les shards via un processus appelÃ© **balancing**.

**3. Routage transparent**
Les applications interagissent avec le cluster via des **mongos** (routeurs) qui dirigent les requÃªtes vers les shards appropriÃ©s sans que le client n'ait Ã  connaÃ®tre la topologie du cluster.

### Architecture en un Coup d'Å’il

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Application Cliente                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Mongos (Routeurs)      â”‚
            â”‚   Query Router Layer       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”    â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚    â”‚              â”‚
         â–¼              â–¼    â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Shard  â”‚     â”‚ Shard  â”‚   ...   â”‚ Shard  â”‚
    â”‚   A    â”‚     â”‚   B    â”‚         â”‚   N    â”‚
    â”‚(RS*)   â”‚     â”‚(RS*)   â”‚         â”‚(RS*)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²              â–²                  â–²
         â”‚              â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Config Servers  â”‚
              â”‚  (Metadata RS)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

* RS = Replica Set
```

### Quand Sharder ?

Le sharding n'est pas une solution universelle. Ã‰valuez ces indicateurs :

#### âœ… Vous DEVRIEZ sharder si :

- **Volume de donnÃ©es** : Vos donnÃ©es dÃ©passent la capacitÃ© de stockage d'un serveur unique (plusieurs To)
- **DÃ©bit d'Ã©criture** : Le volume d'insertions/updates dÃ©passe la capacitÃ© d'un nÅ“ud primaire
- **Ensemble de travail** : Le working set dÃ©passe la RAM disponible sur un serveur
- **Distribution gÃ©ographique** : Vous devez placer les donnÃ©es prÃ¨s des utilisateurs (latence)
- **Isolation des tenants** : Architecture multi-tenant nÃ©cessitant une isolation par shard

#### âŒ Vous NE DEVRIEZ PAS sharder si :

- Vos donnÃ©es tiennent confortablement sur un serveur (< 500 Go)
- Un Replica Set suffit Ã  gÃ©rer votre charge
- Vous n'avez pas l'expertise pour gÃ©rer la complexitÃ©
- Vos requÃªtes sont majoritairement scatter-gather (inefficaces en sharding)
- Vous pouvez optimiser via indexation ou modÃ©lisation

### StratÃ©gies de Partitionnement : Vue d'Ensemble

Le choix de la **shard key** est la dÃ©cision la plus critique d'une architecture shardÃ©e. Une mauvaise shard key peut :
- CrÃ©er des hotspots (shards surchargÃ©s)
- GÃ©nÃ©rer des jumbo chunks impossibles Ã  dÃ©placer
- Provoquer des scatter-gather queries systÃ©matiques
- Rendre le balancing inefficace

MongoDB propose trois stratÃ©gies principales de sharding :

#### 1. **Range Sharding** (Partitionnement par plage)

```
Shard Key: { "date": 1 }

Shard A:  [MinKey, 2024-01-01)
Shard B:  [2024-01-01, 2024-06-01)
Shard C:  [2024-06-01, MaxKey]
```

**CaractÃ©ristiques :**
- âœ… RequÃªtes par plage trÃ¨s efficaces
- âœ… Bon pour les sÃ©ries temporelles
- âŒ Risque de hotspots sur les insertions monotones
- âŒ Distribution inÃ©gale si les donnÃ©es ne sont pas uniformes

**Exemple d'utilisation :**
```javascript
// Collection de logs avec accÃ¨s principalement par pÃ©riode
sh.shardCollection("logs.events", { "timestamp": 1 })
```

**Anti-pattern :**
```javascript
// âŒ MAUVAIS : timestamp seul crÃ©e un hotspot sur le shard le plus rÃ©cent
sh.shardCollection("events.realtime", { "created_at": 1 })

// Toutes les nouvelles insertions vont au mÃªme shard (le plus rÃ©cent)
```

#### 2. **Hashed Sharding** (Partitionnement par hachage)

```
Shard Key: { "_id": "hashed" }

Shard A:  [hash_min, hash_x)
Shard B:  [hash_x, hash_y)
Shard C:  [hash_y, hash_max]
```

**CaractÃ©ristiques :**
- âœ… Distribution uniforme garantie
- âœ… Excellent pour les insertions Ã  haut dÃ©bit
- âœ… Ã‰limine les hotspots monotones
- âŒ Les requÃªtes par plage deviennent scatter-gather
- âŒ Impossible de cibler un shard spÃ©cifique

**Exemple d'utilisation :**
```javascript
// Collection avec insertions uniformes nÃ©cessaires
sh.shardCollection("users.profiles", { "_id": "hashed" })
```

**Anti-pattern :**
```javascript
// âŒ MAUVAIS : Hashed sharding sur une clÃ© utilisÃ©e pour les requÃªtes par plage
sh.shardCollection("products.catalog", { "price": "hashed" })

// Les requÃªtes "find tous les produits entre 10â‚¬ et 50â‚¬"
// deviennent scatter-gather (tous les shards interrogÃ©s)
```

#### 3. **Zone Sharding** (Partitionnement par zone)

```
Zone "EU":     Shard A, Shard B  â†’  { "country": { $in: ["FR", "DE", "IT"] } }
Zone "US":     Shard C, Shard D  â†’  { "country": { $in: ["US", "CA"] } }
Zone "ASIA":   Shard E, Shard F  â†’  { "country": { $in: ["JP", "CN", "IN"] } }
```

**CaractÃ©ristiques :**
- âœ… ConformitÃ© rÃ©glementaire (RGPD, rÃ©sidence des donnÃ©es)
- âœ… Optimisation de la latence (donnÃ©es prÃ¨s des utilisateurs)
- âœ… Isolation multi-tenant
- âŒ Configuration et maintenance complexes
- âŒ Risque de dÃ©sÃ©quilibre entre zones

**Exemple d'utilisation :**
```javascript
// Sharding gÃ©ographique avec zones
sh.shardCollection("app.users", { "country": 1, "user_id": 1 })

// Configuration des zones
sh.addShardToZone("shard-eu-01", "EU")
sh.addShardToZone("shard-eu-02", "EU")
sh.updateZoneKeyRange("app.users",
  { country: "FR", user_id: MinKey },
  { country: "FR", user_id: MaxKey },
  "EU"
)
```

**Anti-pattern :**
```javascript
// âŒ MAUVAIS : Zones sans granularitÃ© suffisante
sh.shardCollection("orders.transactions", { "region": 1 })

// Si 90% des transactions sont dans la rÃ©gion "US",
// les shards US seront surchargÃ©s
```

### Comparaison des StratÃ©gies

| CritÃ¨re | Range | Hashed | Zone |
|---------|-------|--------|------|
| **Distribution** | InÃ©gale potentielle | Uniforme | ContrÃ´lÃ©e |
| **RequÃªtes ciblÃ©es** | Excellentes (plages) | Mauvaises (scatter) | Bonnes (par zone) |
| **Insertions uniformes** | Risque de hotspot | Excellentes | DÃ©pend de la distribution |
| **ComplexitÃ©** | Faible | Faible | Ã‰levÃ©e |
| **Cas d'usage typique** | Time-series, logs | High-throughput writes | Multi-tenant, RGPD |

### Anti-Patterns Critiques Ã  Ã‰viter

#### ğŸš« **Anti-Pattern 1 : Shard Key Ã  faible cardinalitÃ©**

```javascript
// âŒ MAUVAIS
sh.shardCollection("users.accounts", { "account_type": 1 })
// account_type peut avoir seulement 3 valeurs : "free", "premium", "enterprise"
// Maximum 3 chunks possibles â†’ impossible de distribuer sur plus de 3 shards
```

**Impact :**
- ImpossibilitÃ© de distribuer sur N shards si N > cardinalitÃ©
- Chunks indivisibles (jumbo chunks)
- DÃ©sÃ©quilibre permanent

**Solution :**
```javascript
// âœ… BON : Combiner avec un champ Ã  haute cardinalitÃ©
sh.shardCollection("users.accounts", { "account_type": 1, "user_id": 1 })
```

#### ğŸš« **Anti-Pattern 2 : Shard Key monotone sans hachage**

```javascript
// âŒ MAUVAIS
sh.shardCollection("events.logs", { "timestamp": 1 })
// Tous les inserts vont au chunk le plus rÃ©cent â†’ 1 seul shard actif
```

**Impact :**
- Hotspot permanent sur le shard contenant les valeurs les plus rÃ©centes
- Les autres shards sont sous-utilisÃ©s
- Performance d'Ã©criture limitÃ©e Ã  la capacitÃ© d'un seul shard

**Solution :**
```javascript
// âœ… BON : Hashed ou composÃ©
sh.shardCollection("events.logs", { "timestamp": "hashed" })
// ou
sh.shardCollection("events.logs", { "source_id": 1, "timestamp": 1 })
```

#### ğŸš« **Anti-Pattern 3 : Shard Key mutable**

```javascript
// âŒ MAUVAIS
sh.shardCollection("products.catalog", { "category": 1, "subcategory": 1 })
// Si category ou subcategory change, le document doit migrer de chunk
```

**Impact :**
- Depuis MongoDB 5.0+, possible de changer la shard key mais avec overhead
- ComplexitÃ© opÃ©rationnelle accrue
- Risque de fragmentation

**Solution :**
```javascript
// âœ… BON : Utiliser un champ immuable
sh.shardCollection("products.catalog", { "_id": 1 })
// ou stocker category_hash calculÃ© Ã  l'insertion
```

#### ğŸš« **Anti-Pattern 4 : Shard Key non prÃ©sente dans les requÃªtes**

```javascript
// âŒ MAUVAIS
sh.shardCollection("orders.transactions", { "order_date": 1 })

// Mais 90% des requÃªtes sont :
db.transactions.find({ "customer_id": "123456" })
// â†’ Scatter-gather systÃ©matique sur tous les shards
```

**Impact :**
- Toutes les requÃªtes interrogent tous les shards
- Latence multipliÃ©e par le nombre de shards
- Charge rÃ©seau excessive

**Solution :**
```javascript
// âœ… BON : Aligner la shard key sur le pattern d'accÃ¨s
sh.shardCollection("orders.transactions", { "customer_id": 1, "order_date": 1 })

// Les requÃªtes par customer_id ciblent maintenant un seul shard
```

#### ğŸš« **Anti-Pattern 5 : Sur-sharding prÃ©maturÃ©**

```javascript
// âŒ MAUVAIS : DÃ©ployer 20 shards pour une DB de 100 Go
// Overhead de coordination Ã©norme pour peu de bÃ©nÃ©fices
```

**Impact :**
- ComplexitÃ© opÃ©rationnelle sans gain de performance
- CoÃ»ts d'infrastructure excessifs
- Overhead de balancing et de routage

**RÃ¨gle empirique :**
- Commencer avec 2-3 shards
- Ajouter des shards lorsque chaque shard atteint 2-3 To ou 100k ops/sec

### CritÃ¨res de Choix d'une Shard Key IdÃ©ale

Une shard key optimale doit satisfaire ces trois propriÃ©tÃ©s (paradigme **"CWT"**) :

#### 1. **CardinalitÃ© (Cardinality)**
- âœ… Haute cardinalitÃ© : de nombreuses valeurs distinctes possibles
- âœ… Permet de crÃ©er de nombreux chunks
- âœ… Exemple : `user_id`, `email`, `uuid`

#### 2. **Distribution (Write distribution)**
- âœ… Les Ã©critures sont uniformÃ©ment rÃ©parties
- âœ… Ã‰vite les hotspots
- âœ… Exemple : `hashed(_id)`, `user_id` dans un systÃ¨me avec de nombreux utilisateurs actifs

#### 3. **CiblabilitÃ© (Targetability)**
- âœ… Les requÃªtes frÃ©quentes incluent la shard key
- âœ… Permet des requÃªtes ciblÃ©es (1 seul shard)
- âœ… Ã‰vite les scatter-gather queries

**Exemple de shard key Ã©quilibrÃ©e :**
```javascript
// Collection : e-commerce orders
sh.shardCollection("shop.orders", { "customer_id": 1, "order_date": 1 })

// âœ… CardinalitÃ© : customer_id offre une haute cardinalitÃ©
// âœ… Distribution : Si les clients sont distribuÃ©s uniformÃ©ment
// âœ… CiblabilitÃ© : Les requÃªtes typiques sont :
//    - "Commandes du client X" â†’ ciblÃ©e (1 shard)
//    - "Commandes du client X en 2024" â†’ ciblÃ©e (1 shard)
```

### Cas d'Usage par StratÃ©gie

| StratÃ©gie | Cas d'Usage IdÃ©al | Exemple Concret |
|-----------|-------------------|-----------------|
| **Range** | Time-series, IoT, logs avec accÃ¨s chronologique | MÃ©triques systÃ¨me, Ã©vÃ©nements temporels |
| **Hashed** | Collections avec insertions uniformes nÃ©cessaires | User profiles, inventory items |
| **Zone** | Multi-tenant, conformitÃ© gÃ©ographique | SaaS multi-rÃ©gions, donnÃ©es RGPD |

### MÃ©triques de SantÃ© d'un Sharding

Surveillez ces indicateurs pour Ã©valuer l'efficacitÃ© de votre sharding :

1. **Distribution des chunks :**
   ```javascript
   sh.status()
   // VÃ©rifier que la distribution est Ã©quilibrÃ©e (Â±10%)
   ```

2. **Taux de scatter-gather :**
   ```javascript
   db.collection.explain("executionStats").find({ ... })
   // nShards touched devrait Ãªtre minimal (idÃ©alement 1)
   ```

3. **Taux de migrations :**
   - < 5 migrations/heure en production stable
   - Pics tolÃ©rÃ©s lors d'ajout de shards

4. **Jumbo chunks :**
   ```javascript
   db.chunks.find({ jumbo: true }).count()
   // Devrait Ãªtre 0
   ```

### Structure du Chapitre

Ce chapitre est organisÃ© en 12 sections pour une comprÃ©hension progressive et complÃ¨te du sharding :

- **10.1** : Concepts fondamentaux du sharding
- **10.2** : Architecture dÃ©taillÃ©e (shards, config servers, mongos)
- **10.3** : Shard Key â€“ Choix et stratÃ©gies (approfondissement)
- **10.4** : Types de sharding (Range, Hashed, Zone)
- **10.5** : Chunks et mÃ©canisme de balancing
- **10.6** : DÃ©ploiement pratique d'un cluster shardÃ©
- **10.7** : Activation du sharding sur bases et collections
- **10.8** : Migration des chunks et opÃ©rations de maintenance
- **10.9** : OpÃ©rations spÃ©cifiques aux clusters shardÃ©s
- **10.10** : Monitoring et maintenance continue
- **10.11** : RÃ©solution des jumbo chunks
- **10.12** : Bonnes pratiques et rÃ©capitulatif

### PrÃ©requis

Avant d'aborder ce chapitre, vous devez maÃ®triser :

- âœ… **Replica Sets** (Chapitre 9) : Architecture, Ã©lection, oplog
- âœ… **ModÃ©lisation des donnÃ©es** (Chapitre 4) : Patterns, rÃ©fÃ©rences vs embedded
- âœ… **Index et optimisation** (Chapitre 5) : StratÃ©gies d'indexation
- âœ… **Monitoring** (Chapitre 13) : MÃ©triques, alerting

### Avertissement

> âš ï¸ **Le sharding est irrÃ©versible** : Une fois une collection shardÃ©e, il est extrÃªmement difficile et coÃ»teux de revenir en arriÃ¨re. Prenez le temps de concevoir soigneusement votre shard key et testez en profondeur avant la mise en production.

> ğŸ” **SÃ©curitÃ©** : Un cluster shardÃ© expose une surface d'attaque plus large (mongos, config servers, multiples shards). Assurez-vous d'appliquer toutes les bonnes pratiques de sÃ©curitÃ© (Chapitre 11).

---

**Dans les sections suivantes**, nous allons explorer en dÃ©tail chaque composant de l'architecture shardÃ©e, apprendre Ã  dÃ©ployer et configurer un cluster en production, et maÃ®triser les techniques avancÃ©es de gestion et d'optimisation.

---


â­ï¸ [Concepts du sharding](/10-sharding/01-concepts-sharding.md)
