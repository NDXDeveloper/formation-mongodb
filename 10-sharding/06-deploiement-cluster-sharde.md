üîù Retour au [Sommaire](/SOMMAIRE.md)

# 10.6 D√©ploiement d'un Cluster Shard√©

## Introduction

Le d√©ploiement d'un cluster shard√© MongoDB est une op√©ration complexe qui n√©cessite une planification minutieuse et une ex√©cution rigoureuse. Contrairement √† un d√©ploiement de Replica Set, un cluster shard√© implique plusieurs types de composants qui doivent √™tre configur√©s et orchestr√©s correctement pour fonctionner ensemble.

Cette section fournit un guide complet pour d√©ployer un cluster shard√© en production, depuis la planification initiale jusqu'√† la validation finale, en couvrant les strat√©gies, les anti-patterns et les bonnes pratiques √©prouv√©es.

---

## Architecture Cible

### Topologie Standard de Production

Pour cet exemple, nous d√©ployons un cluster shard√© minimal mais robuste :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   APPLICATION TIER                      ‚îÇ
‚îÇ                  (Load Balancer)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ             ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ   mongos-1   ‚îÇ  ‚îÇ  mongos-2  ‚îÇ   ‚îÇ  mongos-3   ‚îÇ
     ‚îÇ  (Router)    ‚îÇ  ‚îÇ  (Router)  ‚îÇ   ‚îÇ  (Router)   ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                 ‚îÇ                ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                                         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Config  ‚îÇ         ‚îÇ  Config  ‚îÇ      ‚îÇ   Config   ‚îÇ
    ‚îÇ Server 1 ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Server 2 ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  Server 3  ‚îÇ
    ‚îÇ (Replica ‚îÇ         ‚îÇ (Replica ‚îÇ      ‚îÇ  (Replica  ‚îÇ
    ‚îÇ   Set)   ‚îÇ         ‚îÇ   Set)   ‚îÇ      ‚îÇ    Set)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                     ‚îÇ                   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Shard A ‚îÇ                            ‚îÇ Shard B  ‚îÇ
    ‚îÇ Replica ‚îÇ                            ‚îÇ Replica  ‚îÇ
    ‚îÇ   Set   ‚îÇ                            ‚îÇ   Set    ‚îÇ
    ‚îÇ         ‚îÇ                            ‚îÇ          ‚îÇ
    ‚îÇ P S S   ‚îÇ                            ‚îÇ P S S    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Composants** :
- **3 Config Servers** (Replica Set)
- **2 Shards** (chacun un Replica Set de 3 membres : 1 Primary + 2 Secondary)
- **3 Mongos** (Query Routers)

### Dimensionnement Mat√©riel

#### Config Servers

| Ressource | Recommandation Production | Justification |
|-----------|---------------------------|---------------|
| CPU | 2-4 cores | Charge l√©g√®re, m√©tadonn√©es uniquement |
| RAM | 8-16 GB | Cache des m√©tadonn√©es du cluster |
| Stockage | 50-100 GB SSD | Croissance lente des m√©tadonn√©es |
| R√©seau | 1 Gbps | Latence faible critique |

#### Shards (Replica Sets)

| Ressource | Recommandation Production | Justification |
|-----------|---------------------------|---------------|
| CPU | 8-16+ cores | Charge de travail principale |
| RAM | 32-128 GB | Proportionnel au working set |
| Stockage | 1-10+ TB SSD/NVMe | Selon volume de donn√©es |
| R√©seau | 10 Gbps | Migrations de chunks intensives |

#### Mongos (Routers)

| Ressource | Recommandation Production | Justification |
|-----------|---------------------------|---------------|
| CPU | 4-8 cores | Routing et agr√©gation des r√©sultats |
| RAM | 8-16 GB | Pas de stockage permanent |
| Stockage | 20 GB | Logs uniquement |
| R√©seau | 10 Gbps | Point d'entr√©e des applications |

---

## Pr√©requis et Planification

### Checklist Pr√©-d√©ploiement

#### 1. Infrastructure

- ‚úÖ **Machines provisionn√©es** : Toutes les machines sont disponibles et accessibles
- ‚úÖ **R√©seau configur√©** : Les machines peuvent communiquer entre elles
- ‚úÖ **Pare-feu** : Ports ouverts (27017-27019 pour MongoDB)
- ‚úÖ **DNS ou /etc/hosts** : R√©solution de noms configur√©e
- ‚úÖ **NTP synchronis√©** : Horloges synchronis√©es sur tous les serveurs
- ‚úÖ **Stockage** : Volumes mont√©s avec les bonnes permissions

#### 2. Syst√®me d'Exploitation

- ‚úÖ **OS support√©** : Linux (RHEL/CentOS, Ubuntu, Debian) recommand√©
- ‚úÖ **Ulimits configur√©s** : Limites de fichiers et processus augment√©es
- ‚úÖ **Transparent Huge Pages d√©sactiv√©** : Impact n√©gatif sur MongoDB
- ‚úÖ **NUMA d√©sactiv√© ou configur√©** : Si applicable
- ‚úÖ **Filesystem** : XFS ou ext4 recommand√©

#### 3. MongoDB

- ‚úÖ **Version compatible** : M√™me version sur tous les composants
- ‚úÖ **Binaires install√©s** : mongod et mongos disponibles
- ‚úÖ **Utilisateur syst√®me** : Compte d√©di√© (mongodb:mongodb)
- ‚úÖ **R√©pertoires cr√©√©s** : /data/db, /var/log/mongodb

#### 4. S√©curit√©

- ‚úÖ **Certificats TLS/SSL** : Si chiffrement activ√©
- ‚úÖ **Keyfile** : Pour l'authentification inter-cluster
- ‚úÖ **Plan d'authentification** : SCRAM-SHA-256 configur√©
- ‚úÖ **Comptes administrateurs** : Strat√©gie d√©finie

### Plan de D√©ploiement

**Ordre recommand√©** :
1. Config Servers (Replica Set)
2. Shards (Replica Sets)
3. Mongos (Routers)
4. Initialisation du cluster shard√©
5. Configuration de la s√©curit√©
6. Tests de validation
7. Mise en production

**Dur√©e estim√©e** : 4-8 heures pour un premier d√©ploiement
**Personnel** : 2-3 administrateurs recommand√©s

---

## Phase 1 : D√©ploiement des Config Servers

### √âtape 1.1 : Configuration des fichiers mongod.conf

Sur chaque serveur config (3 serveurs : cfg1, cfg2, cfg3) :

```yaml
# /etc/mongod-configsvr.conf

# Stockage
storage:
  dbPath: /data/configdb
  journal:
    enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 2

# R√©plication
replication:
  replSetName: configReplSet

# Sharding
sharding:
  clusterRole: configsvr

# R√©seau
net:
  port: 27019
  bindIp: 0.0.0.0

# Syst√®me
systemLog:
  destination: file
  path: /var/log/mongodb/configsvr.log
  logAppend: true

# Processus
processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/configsvr.pid

# S√©curit√© (optionnel pour l'instant, activ√© apr√®s)
#security:
#  keyFile: /etc/mongodb-keyfile
#  authorization: enabled
```

### √âtape 1.2 : Cr√©ation des r√©pertoires

Sur chaque config server :

```bash
# Cr√©er les r√©pertoires
sudo mkdir -p /data/configdb
sudo mkdir -p /var/log/mongodb
sudo mkdir -p /var/run/mongodb

# Permissions
sudo chown -R mongodb:mongodb /data/configdb
sudo chown -R mongodb:mongodb /var/log/mongodb
sudo chown -R mongodb:mongodb /var/run/mongodb
```

### √âtape 1.3 : D√©marrage des instances

Sur **cfg1, cfg2, cfg3** :

```bash
# D√©marrer mongod en mode config server
sudo mongod --config /etc/mongod-configsvr.conf

# V√©rifier le d√©marrage
sudo tail -f /var/log/mongodb/configsvr.log

# V√©rifier le processus
ps aux | grep mongod
```

### √âtape 1.4 : Initialisation du Replica Set

Se connecter √† **cfg1** :

```javascript
// Connexion au premier config server
mongosh --port 27019

// Initialiser le replica set
rs.initiate({
  _id: "configReplSet",
  configsvr: true,
  members: [
    { _id: 0, host: "cfg1.example.com:27019" },
    { _id: 1, host: "cfg2.example.com:27019" },
    { _id: 2, host: "cfg3.example.com:27019" }
  ]
})

// Attendre l'√©lection du Primary
// V√©rifier le statut
rs.status()

// R√©sultat attendu : 1 PRIMARY, 2 SECONDARY
```

### √âtape 1.5 : Validation

```javascript
// V√©rifier la configuration
rs.conf()

// V√©rifier que tous les membres sont sains
rs.status().members.forEach(function(member) {
  print(member.name + " : " + member.stateStr);
})

// Test d'√©criture (sur le PRIMARY)
use config
db.test.insertOne({ test: "config servers ready" })
db.test.find()

// Nettoyage
db.test.drop()
```

---

## Phase 2 : D√©ploiement des Shards

### √âtape 2.1 : Configuration pour le Shard A

Sur chaque membre du Shard A (shardA1, shardA2, shardA3) :

```yaml
# /etc/mongod-shardA.conf

# Stockage
storage:
  dbPath: /data/shardA
  journal:
    enabled: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 16  # Ajuster selon la RAM disponible

# R√©plication
replication:
  replSetName: shardA

# Sharding
sharding:
  clusterRole: shardsvr

# R√©seau
net:
  port: 27018
  bindIp: 0.0.0.0

# Syst√®me
systemLog:
  destination: file
  path: /var/log/mongodb/shardA.log
  logAppend: true

# Processus
processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/shardA.pid
```

### √âtape 2.2 : D√©marrage et initialisation du Shard A

```bash
# Sur shardA1, shardA2, shardA3
sudo mkdir -p /data/shardA
sudo chown mongodb:mongodb /data/shardA
sudo mongod --config /etc/mongod-shardA.conf
```

Connexion √† **shardA1** :

```javascript
mongosh --port 27018

// Initialiser le replica set
rs.initiate({
  _id: "shardA",
  members: [
    { _id: 0, host: "shardA1.example.com:27018" },
    { _id: 1, host: "shardA2.example.com:27018" },
    { _id: 2, host: "shardA3.example.com:27018" }
  ]
})

// V√©rifier
rs.status()
```

### √âtape 2.3 : Configuration pour le Shard B

R√©p√©ter le processus pour le Shard B (shardB1, shardB2, shardB3) :

```yaml
# /etc/mongod-shardB.conf
# Identique au Shard A mais avec :
# - replSetName: shardB
# - dbPath: /data/shardB
# - logPath: /var/log/mongodb/shardB.log
```

Initialisation du Shard B :

```javascript
mongosh --host shardB1.example.com --port 27018

rs.initiate({
  _id: "shardB",
  members: [
    { _id: 0, host: "shardB1.example.com:27018" },
    { _id: 1, host: "shardB2.example.com:27018" },
    { _id: 2, host: "shardB3.example.com:27018" }
  ]
})
```

### √âtape 2.4 : Validation des Shards

Pour chaque shard :

```javascript
// V√©rifier le statut
rs.status()

// Test d'√©criture
use testdb
db.test.insertOne({ shard: "A", test: "ready" })
db.test.find()

// V√©rifier l'oplog
use local
db.oplog.rs.find().limit(5).sort({ $natural: -1 })
```

---

## Phase 3 : D√©ploiement des Mongos

### √âtape 3.1 : Configuration des Mongos

Sur chaque serveur mongos (mongos1, mongos2, mongos3) :

```yaml
# /etc/mongos.conf

# Sharding
sharding:
  configDB: configReplSet/cfg1.example.com:27019,cfg2.example.com:27019,cfg3.example.com:27019

# R√©seau
net:
  port: 27017
  bindIp: 0.0.0.0

# Syst√®me
systemLog:
  destination: file
  path: /var/log/mongodb/mongos.log
  logAppend: true

# Processus
processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongos.pid
```

### √âtape 3.2 : D√©marrage des Mongos

Sur **mongos1, mongos2, mongos3** :

```bash
# Cr√©er les r√©pertoires de logs
sudo mkdir -p /var/log/mongodb
sudo mkdir -p /var/run/mongodb
sudo chown -R mongodb:mongodb /var/log/mongodb /var/run/mongodb

# D√©marrer mongos
sudo mongos --config /etc/mongos.conf

# V√©rifier
sudo tail -f /var/log/mongodb/mongos.log
ps aux | grep mongos
```

### √âtape 3.3 : Validation

```bash
# Se connecter √† un mongos
mongosh --host mongos1.example.com --port 27017

# V√©rifier la connexion aux config servers
sh.status()  # Devrait montrer le cluster initialis√© mais sans shards
```

---

## Phase 4 : Initialisation du Cluster Shard√©

### √âtape 4.1 : Ajout des Shards au Cluster

Se connecter √† **n'importe quel mongos** :

```javascript
mongosh --host mongos1.example.com --port 27017

// Ajouter le Shard A
sh.addShard("shardA/shardA1.example.com:27018,shardA2.example.com:27018,shardA3.example.com:27018")

// R√©sultat attendu :
// {
//   "shardAdded" : "shardA",
//   "ok" : 1
// }

// Ajouter le Shard B
sh.addShard("shardB/shardB1.example.com:27018,shardB2.example.com:27018,shardB3.example.com:27018")

// R√©sultat attendu :
// {
//   "shardAdded" : "shardB",
//   "ok" : 1
// }
```

### √âtape 4.2 : V√©rification du Cluster

```javascript
// V√©rifier que les shards sont ajout√©s
sh.status()

// Output attendu :
// shards:
//   { "_id" : "shardA", "host" : "shardA/shardA1.example.com:27018,..." }
//   { "_id" : "shardB", "host" : "shardB/shardB1.example.com:27018,..." }

// Lister les shards
db.adminCommand({ listShards: 1 })

// V√©rifier les config servers
db.getSiblingDB("config").shards.find().pretty()
```

---

## Phase 5 : Configuration de la S√©curit√©

### √âtape 5.1 : G√©n√©ration du Keyfile

Sur **un serveur quelconque**, g√©n√©rer le keyfile :

```bash
# G√©n√©rer une cl√© al√©atoire s√©curis√©e
openssl rand -base64 756 > /tmp/mongodb-keyfile

# D√©finir les permissions strictes
chmod 400 /tmp/mongodb-keyfile
```

### √âtape 5.2 : Distribution du Keyfile

Copier le keyfile sur **tous les serveurs** (config servers, shards, mongos) :

```bash
# Exemple avec scp
scp /tmp/mongodb-keyfile mongodb@cfg1.example.com:/etc/mongodb-keyfile
scp /tmp/mongodb-keyfile mongodb@cfg2.example.com:/etc/mongodb-keyfile
# ... r√©p√©ter pour tous les serveurs

# Sur chaque serveur
sudo chown mongodb:mongodb /etc/mongodb-keyfile
sudo chmod 400 /etc/mongodb-keyfile
```

### √âtape 5.3 : Cr√©ation de l'Administrateur

**Avant d'activer l'authentification**, cr√©er un utilisateur admin :

```javascript
// Se connecter √† un mongos
mongosh --host mongos1.example.com --port 27017

// Cr√©er l'administrateur du cluster
use admin
db.createUser({
  user: "clusterAdmin",
  pwd: "SecurePassword123!",  // Utiliser un mot de passe fort
  roles: [
    { role: "clusterAdmin", db: "admin" },
    { role: "userAdminAnyDatabase", db: "admin" },
    { role: "dbAdminAnyDatabase", db: "admin" },
    { role: "readWriteAnyDatabase", db: "admin" }
  ]
})

// V√©rifier
db.getUsers()
```

### √âtape 5.4 : Activation de l'Authentification

**Sur tous les composants**, mettre √† jour les fichiers de configuration :

#### Config Servers

```yaml
# Ajouter dans /etc/mongod-configsvr.conf
security:
  keyFile: /etc/mongodb-keyfile
  authorization: enabled
```

#### Shards

```yaml
# Ajouter dans /etc/mongod-shardA.conf et /etc/mongod-shardB.conf
security:
  keyFile: /etc/mongodb-keyfile
  authorization: enabled
```

#### Mongos

```yaml
# Ajouter dans /etc/mongos.conf
security:
  keyFile: /etc/mongodb-keyfile
```

### √âtape 5.5 : Red√©marrage avec Authentification

**IMPORTANT** : Red√©marrer dans l'ordre pour √©viter les interruptions.

```bash
# 1. Red√©marrer les config servers (un par un)
sudo systemctl restart mongod-configsvr

# 2. Red√©marrer les shards (un replica set √† la fois)
# Secondary d'abord, Primary en dernier
sudo systemctl restart mongod-shardA
sudo systemctl restart mongod-shardB

# 3. Red√©marrer les mongos
sudo systemctl restart mongos
```

### √âtape 5.6 : Test d'Authentification

```javascript
// Connexion avec authentification
mongosh --host mongos1.example.com --port 27017 \
  -u clusterAdmin \
  -p SecurePassword123! \
  --authenticationDatabase admin

// V√©rifier l'acc√®s
sh.status()
```

---

## Phase 6 : Tests de Validation

### Test 1 : Sharding d'une Collection de Test

```javascript
// Connexion authentifi√©e
mongosh --host mongos1.example.com -u clusterAdmin -p SecurePassword123! --authenticationDatabase admin

// Cr√©er une base et activer le sharding
use testdb
sh.enableSharding("testdb")

// Cr√©er une collection et la sharder
db.createCollection("users")
sh.shardCollection("testdb.users", { user_id: "hashed" })

// Ins√©rer des donn√©es de test
for (let i = 0; i < 10000; i++) {
  db.users.insertOne({
    user_id: i,
    name: "User" + i,
    email: "user" + i + "@example.com",
    created_at: new Date()
  });
}

// V√©rifier la distribution
db.users.getShardDistribution()
```

### Test 2 : V√©rification du Balancer

```javascript
// V√©rifier que le balancer fonctionne
sh.getBalancerState()  // Doit retourner true

// V√©rifier les chunks
db.getSiblingDB("config").chunks.find({ ns: "testdb.users" }).count()

// V√©rifier la distribution par shard
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "testdb.users" } },
  { $group: { _id: "$shard", count: { $sum: 1 } } }
])
```

### Test 3 : Haute Disponibilit√©

```javascript
// Test de failover sur un shard
// 1. Se connecter directement au Primary du shardA
mongosh --host shardA1.example.com:27018 -u clusterAdmin -p SecurePassword123! --authenticationDatabase admin

// 2. Forcer une √©lection (step down du Primary)
rs.stepDown(60)

// 3. V√©rifier depuis mongos que les requ√™tes continuent
mongosh --host mongos1.example.com -u clusterAdmin -p SecurePassword123! --authenticationDatabase admin
use testdb
db.users.find().limit(5)  // Doit fonctionner sans erreur

// 4. V√©rifier le nouveau Primary
use admin
sh.status()
```

### Test 4 : V√©rification des Performances

```javascript
// Test de lecture
use testdb
db.users.find({ user_id: 5000 }).explain("executionStats")

// Test d'√©criture
db.users.insertOne({ user_id: 100000, name: "Performance Test" })

// Test d'agr√©gation
db.users.aggregate([
  { $group: { _id: null, total: { $sum: 1 } } }
])
```

---

## Strat√©gies de D√©ploiement Avanc√©es

### Strat√©gie 1 : D√©ploiement Progressif (Rolling Deployment)

Pour minimiser les interruptions en production :

```
1. D√©ployer Config Servers
   ‚îú‚îÄ cfg1 ‚Üí cfg2 ‚Üí cfg3
   ‚îî‚îÄ Valider chaque n≈ìud avant le suivant

2. D√©ployer Shards (un √† un)
   ‚îú‚îÄ shardA : secondary1 ‚Üí secondary2 ‚Üí primary
   ‚îú‚îÄ Valider shardA
   ‚îî‚îÄ shardB : secondary1 ‚Üí secondary2 ‚Üí primary

3. D√©ployer Mongos (un √† un)
   ‚îú‚îÄ mongos1 ‚Üí tester
   ‚îú‚îÄ mongos2 ‚Üí tester
   ‚îî‚îÄ mongos3 ‚Üí tester

4. Basculer le trafic progressivement
   ‚îú‚îÄ 10% ‚Üí mongos1
   ‚îú‚îÄ 50% ‚Üí mongos1 + mongos2
   ‚îî‚îÄ 100% ‚Üí tous les mongos
```

### Strat√©gie 2 : D√©ploiement avec Environnement de Staging

```
1. D√©ployer cluster identique en staging
2. Tester toutes les op√©rations
3. Valider les performances
4. Documenter les proc√©dures
5. R√©pliquer en production avec ajustements
```

### Strat√©gie 3 : D√©ploiement Multi-Datacenter

Pour la haute disponibilit√© g√©ographique :

```yaml
# Exemple : 3 datacenters (DC1, DC2, DC3)

Config Servers:
  - cfg1 : DC1
  - cfg2 : DC2
  - cfg3 : DC3

Shard A:
  - shardA-primary : DC1
  - shardA-secondary1 : DC2
  - shardA-secondary2 : DC3

Shard B:
  - shardB-primary : DC2
  - shardB-secondary1 : DC1
  - shardB-secondary2 : DC3

Mongos:
  - mongos d√©ploy√©s dans chaque DC pr√®s des applications
```

Configuration avec priorit√©s pour l'√©lection :

```javascript
// ShardA : Primary pr√©f√©r√© dans DC1
cfg = rs.conf()
cfg.members[0].priority = 2  // DC1
cfg.members[1].priority = 1  // DC2
cfg.members[2].priority = 1  // DC3
rs.reconfig(cfg)

// ShardB : Primary pr√©f√©r√© dans DC2
cfg = rs.conf()
cfg.members[0].priority = 1  // DC1
cfg.members[1].priority = 2  // DC2
cfg.members[2].priority = 1  // DC3
rs.reconfig(cfg)
```

---

## Anti-Patterns et Erreurs Courantes

### ‚ùå Anti-Pattern 1 : Config Servers non Redondants

**Probl√®me** :
```bash
# D√©ployer seulement 1 ou 2 config servers
mongod --configsvr --replSet configReplSet --port 27019
```

**Cons√©quence** :
- Perte du seul config server ‚Üí **cluster entier inaccessible**
- 2 config servers ‚Üí pas de majorit√© pour √©lection
- **PERTE DE DONN√âES possible**

**Solution** :
```bash
# TOUJOURS d√©ployer 3 config servers minimum
# 5 config servers pour criticit√© extr√™me
```

### ‚ùå Anti-Pattern 2 : Mongos sur les Shards

**Probl√®me** :
```bash
# Installer mongos sur le m√™me serveur que les shards
# serveur1: mongod (shard) + mongos
```

**Cons√©quence** :
- Contention des ressources (CPU, RAM, r√©seau)
- Couplage fort ‚Üí panne d'un shard impacte le routing
- Difficult√© de scaling ind√©pendant

**Solution** :
```bash
# Mongos sur serveurs d√©di√©s ou avec les applications
# Architecture d√©coupl√©e
```

### ‚ùå Anti-Pattern 3 : Shards en Standalone

**Probl√®me** :
```bash
# Shards sans r√©plication
mongod --shardsvr --port 27018  # Standalone !
```

**Cons√©quence** :
- **Aucune tol√©rance aux pannes**
- Panne d'un shard ‚Üí perte de donn√©es
- Maintenance impossible sans downtime

**Solution** :
```bash
# TOUJOURS des Replica Sets pour les shards
# Minimum 3 membres : 1 Primary + 2 Secondary
```

### ‚ùå Anti-Pattern 4 : Keyfile Faible ou R√©utilis√©

**Probl√®me** :
```bash
# Keyfile trop simple
echo "password123" > /etc/mongodb-keyfile

# Ou r√©utiliser un keyfile entre environnements
cp /prod/mongodb-keyfile /staging/mongodb-keyfile
```

**Cons√©quence** :
- S√©curit√© compromise
- Acc√®s non autoris√© possible
- Conformit√© viol√©e

**Solution** :
```bash
# Keyfile al√©atoire et unique par environnement
openssl rand -base64 756 > /etc/mongodb-keyfile
chmod 400 /etc/mongodb-keyfile

# Diff√©rents keyfiles : production, staging, dev
```

### ‚ùå Anti-Pattern 5 : Pas de Test de Failover

**Probl√®me** :
```bash
# D√©ployer et passer en production sans tester
# Ne jamais simuler de panne
```

**Cons√©quence** :
- Comportement inconnu en cas de vraie panne
- Panique et mauvaises d√©cisions en incident
- RTO/RPO non respect√©s

**Solution** :
```bash
# Tests de failover obligatoires
# 1. Arr√™ter un config server
# 2. Arr√™ter le Primary d'un shard
# 3. Red√©marrer un mongos
# 4. Simuler une panne r√©seau
# 5. Documenter les comportements observ√©s
```

### ‚ùå Anti-Pattern 6 : Sharding Imm√©diat d'une Collection

**Probl√®me** :
```javascript
// Sharder sans r√©flexion
sh.enableSharding("mydb")
sh.shardCollection("mydb.users", { _id: 1 })

// Puis ins√©rer des millions de documents
```

**Cons√©quence** :
- Tous les documents sur un seul chunk/shard
- Distribution d√©s√©quilibr√©e durable
- Migrations massives ult√©rieures

**Solution** :
```javascript
// Pr√©-splitter avant insertion massive
sh.shardCollection("mydb.users", { user_id: "hashed" })

// Pr√©-splitter
for (var i = 0; i < 10; i++) {
  sh.splitAt("mydb.users", { user_id: i * 1000 });
}

// Puis ins√©rer les donn√©es
```

### ‚ùå Anti-Pattern 7 : Ignorer les M√©triques R√©seau

**Probl√®me** :
```bash
# D√©ployer dans diff√©rents datacenters
# Sans mesurer la latence r√©seau
```

**Cons√©quence** :
- Latence inter-datacenter √©lev√©e (>50ms)
- R√©plication lente
- √âlections fr√©quentes en cas de fluctuation r√©seau
- Write concern timeout

**Solution** :
```bash
# Mesurer la latence avant d√©ploiement
ping -c 100 cfg2.example.com

# Latence acceptable : < 5ms (m√™me DC), < 50ms (diff√©rents DC)
# Ajuster les timeouts en cons√©quence

# Configuration pour haute latence
replication:
  electionTimeoutMillis: 30000  # 30 secondes au lieu de 10
```

---

## Automatisation du D√©ploiement

### Script Bash de D√©ploiement Complet

```bash
#!/bin/bash
# deploy-sharded-cluster.sh

set -e  # Arr√™t si erreur

# Variables
CONFIG_SERVERS=("cfg1.example.com" "cfg2.example.com" "cfg3.example.com")
SHARD_A_SERVERS=("shardA1.example.com" "shardA2.example.com" "shardA3.example.com")
SHARD_B_SERVERS=("shardB1.example.com" "shardB2.example.com" "shardB3.example.com")
MONGOS_SERVERS=("mongos1.example.com" "mongos2.example.com" "mongos3.example.com")

echo "=== D√©ploiement Cluster Shard√© MongoDB ==="
echo ""

# Phase 1 : Config Servers
echo "Phase 1 : D√©ploiement Config Servers..."
for server in "${CONFIG_SERVERS[@]}"; do
  echo "  - D√©marrage config server sur $server"
  ssh mongodb@$server "sudo mongod --config /etc/mongod-configsvr.conf"
  sleep 5
done

echo "  - Initialisation Replica Set Config Servers"
mongosh --host ${CONFIG_SERVERS[0]} --port 27019 --eval "
  rs.initiate({
    _id: 'configReplSet',
    configsvr: true,
    members: [
      { _id: 0, host: '${CONFIG_SERVERS[0]}:27019' },
      { _id: 1, host: '${CONFIG_SERVERS[1]}:27019' },
      { _id: 2, host: '${CONFIG_SERVERS[2]}:27019' }
    ]
  })
"
sleep 30  # Attendre √©lection

# Phase 2 : Shards
echo ""
echo "Phase 2 : D√©ploiement Shards..."

# Shard A
for server in "${SHARD_A_SERVERS[@]}"; do
  echo "  - D√©marrage shard A sur $server"
  ssh mongodb@$server "sudo mongod --config /etc/mongod-shardA.conf"
  sleep 5
done

mongosh --host ${SHARD_A_SERVERS[0]} --port 27018 --eval "
  rs.initiate({
    _id: 'shardA',
    members: [
      { _id: 0, host: '${SHARD_A_SERVERS[0]}:27018' },
      { _id: 1, host: '${SHARD_A_SERVERS[1]}:27018' },
      { _id: 2, host: '${SHARD_A_SERVERS[2]}:27018' }
    ]
  })
"
sleep 30

# Shard B (similaire)
# ...

# Phase 3 : Mongos
echo ""
echo "Phase 3 : D√©ploiement Mongos..."
for server in "${MONGOS_SERVERS[@]}"; do
  echo "  - D√©marrage mongos sur $server"
  ssh mongodb@$server "sudo mongos --config /etc/mongos.conf"
  sleep 5
done

# Phase 4 : Ajout des shards
echo ""
echo "Phase 4 : Ajout des Shards au cluster..."
mongosh --host ${MONGOS_SERVERS[0]} --port 27017 --eval "
  sh.addShard('shardA/${SHARD_A_SERVERS[0]}:27018,${SHARD_A_SERVERS[1]}:27018,${SHARD_A_SERVERS[2]}:27018')
  sh.addShard('shardB/${SHARD_B_SERVERS[0]}:27018,${SHARD_B_SERVERS[1]}:27018,${SHARD_B_SERVERS[2]}:27018')
"

# Validation
echo ""
echo "=== Validation du d√©ploiement ==="
mongosh --host ${MONGOS_SERVERS[0]} --port 27017 --eval "sh.status()"

echo ""
echo "‚úÖ D√©ploiement termin√© avec succ√®s !"
```

### Ansible Playbook

```yaml
# deploy-sharded-cluster.yml
---
- name: Deploy MongoDB Sharded Cluster
  hosts: all
  become: yes
  vars:
    mongodb_version: "7.0"
    config_servers:
      - cfg1.example.com
      - cfg2.example.com
      - cfg3.example.com

  tasks:
    - name: Install MongoDB
      include_role:
        name: mongodb-install

    - name: Deploy Config Servers
      include_role:
        name: mongodb-config-servers
      when: inventory_hostname in config_servers

    - name: Deploy Shards
      include_role:
        name: mongodb-shards
      when: "'shard' in group_names"

    - name: Deploy Mongos
      include_role:
        name: mongodb-mongos
      when: "'mongos' in group_names"

    - name: Initialize Sharded Cluster
      include_role:
        name: mongodb-init-cluster
      run_once: true
      delegate_to: "{{ groups['mongos'][0] }}"
```

---

## Monitoring Post-D√©ploiement

### M√©triques Critiques √† Surveiller

```javascript
// 1. √âtat du cluster
sh.status()

// 2. Sant√© des replica sets
db.adminCommand({ replSetGetStatus: 1 })

// 3. Distribution des chunks
db.getSiblingDB("config").chunks.aggregate([
  { $group: { _id: "$shard", count: { $sum: 1 } } }
])

// 4. √âtat du balancer
sh.getBalancerState()
sh.isBalancerRunning()

// 5. Op√©rations en cours
db.currentOp({ active: true })

// 6. Logs r√©cents de migration
db.getSiblingDB("config").changelog.find({
  time: { $gte: new Date(Date.now() - 3600000) }
}).sort({ time: -1 })
```

### Dashboard de Monitoring Recommand√©

**Prometheus + Grafana** :

```yaml
# prometheus-mongodb-exporter.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'mongodb-sharded'
    static_configs:
      - targets:
        - 'mongos1.example.com:9216'
        - 'mongos2.example.com:9216'
        - 'mongos3.example.com:9216'
        labels:
          cluster: 'production'
          component: 'mongos'

  - job_name: 'mongodb-config'
    static_configs:
      - targets:
        - 'cfg1.example.com:9216'
        - 'cfg2.example.com:9216'
        - 'cfg3.example.com:9216'
        labels:
          cluster: 'production'
          component: 'config-server'

  - job_name: 'mongodb-shards'
    static_configs:
      - targets:
        - 'shardA1.example.com:9216'
        - 'shardB1.example.com:9216'
        labels:
          cluster: 'production'
          component: 'shard'
```

---

## Checklist de Validation Finale

### ‚úÖ Infrastructure

- [ ] Tous les serveurs sont accessibles
- [ ] R√©solution DNS fonctionne
- [ ] Ports r√©seau ouverts et test√©s
- [ ] NTP synchronis√© sur tous les serveurs
- [ ] Stockage correctement mont√© avec les permissions

### ‚úÖ Config Servers

- [ ] 3 config servers d√©marr√©s
- [ ] Replica Set initialis√© (1 PRIMARY, 2 SECONDARY)
- [ ] Pas d'erreurs dans les logs
- [ ] Test d'√©criture/lecture r√©ussi

### ‚úÖ Shards

- [ ] Tous les shards d√©marr√©s (Replica Sets)
- [ ] Chaque shard : 1 PRIMARY, 2 SECONDARY
- [ ] Pas d'erreurs dans les logs
- [ ] Oplogs sains et r√©pliquant

### ‚úÖ Mongos

- [ ] Tous les mongos d√©marr√©s
- [ ] Connexion aux config servers r√©ussie
- [ ] sh.status() affiche tous les shards
- [ ] Requ√™tes fonctionnent via mongos

### ‚úÖ S√©curit√©

- [ ] Keyfile d√©ploy√© sur tous les composants
- [ ] Authentification activ√©e
- [ ] Utilisateur admin cr√©√©
- [ ] Connexion authentifi√©e fonctionne
- [ ] TLS/SSL activ√© (si applicable)

### ‚úÖ Fonctionnalit√©s

- [ ] Sharding d'une collection test r√©ussi
- [ ] Insertion de donn√©es test r√©ussie
- [ ] Distribution des chunks visible
- [ ] Balancer actif
- [ ] Test de failover r√©ussi

### ‚úÖ Monitoring

- [ ] Logs centralis√©s configur√©s
- [ ] M√©triques collect√©es (Prometheus/Grafana)
- [ ] Alertes configur√©es
- [ ] Dashboards op√©rationnels

### ‚úÖ Documentation

- [ ] Architecture document√©e
- [ ] Proc√©dures de maintenance √©crites
- [ ] Runbooks d'incidents cr√©√©s
- [ ] Contacts d'escalade d√©finis

---

## Troubleshooting : Probl√®mes Courants

### Probl√®me 1 : Config Server ne d√©marre pas

**Sympt√¥mes** :
```bash
# Logs
[error] Failed to start config server: Address already in use
```

**Solutions** :
```bash
# V√©rifier qu'aucun processus n'utilise le port
sudo lsof -i :27019
sudo netstat -tuln | grep 27019

# Tuer le processus si n√©cessaire
sudo kill -9 <PID>

# Ou changer le port dans la configuration
```

### Probl√®me 2 : Impossible d'ajouter un Shard

**Sympt√¥mes** :
```javascript
sh.addShard("shardA/...")
// Error: Connection refused
```

**Solutions** :
```bash
# 1. V√©rifier que le shard est accessible
mongosh --host shardA1.example.com:27018

# 2. V√©rifier le replica set
rs.status()

# 3. S'assurer qu'il y a un PRIMARY
# 4. V√©rifier les r√®gles firewall
sudo iptables -L -n | grep 27018

# 5. V√©rifier la configuration r√©seau
ping shardA1.example.com
```

### Probl√®me 3 : √âlection du Primary √©choue

**Sympt√¥mes** :
```javascript
rs.status()
// Tous les membres en SECONDARY ou STARTUP
```

**Solutions** :
```javascript
// 1. Forcer une reconfiguration
cfg = rs.conf()
cfg.members[0].priority = 2
rs.reconfig(cfg, { force: true })

// 2. V√©rifier les priorit√©s
rs.conf().members.forEach(m => print(m.host + ": " + m.priority))

// 3. V√©rifier la connectivit√© r√©seau entre membres
```

---

## Conclusion

Le d√©ploiement d'un cluster shard√© MongoDB est une op√©ration complexe mais ma√Ætrisable avec :

- ‚úÖ **Une planification rigoureuse** : Dimensionnement, architecture, checklist
- ‚úÖ **Un d√©ploiement m√©thodique** : Phase par phase, validation continue
- ‚úÖ **La s√©curit√© d√®s le d√©part** : Keyfile, authentification, TLS
- ‚úÖ **Des tests exhaustifs** : Failover, performance, distribution
- ‚úÖ **Un monitoring proactif** : M√©triques, logs, alertes

En suivant ce guide et en √©vitant les anti-patterns document√©s, vous disposez d'une base solide pour d√©ployer et maintenir un cluster shard√© MongoDB performant et r√©silient en production.

---

## Ressources

- [MongoDB Documentation - Deploy a Sharded Cluster](https://docs.mongodb.com/manual/tutorial/deploy-shard-cluster/)
- [MongoDB Production Notes](https://docs.mongodb.com/manual/administration/production-notes/)
- [MongoDB Security Checklist](https://docs.mongodb.com/manual/administration/security-checklist/)
- [MongoDB Ops Manager](https://www.mongodb.com/products/ops-manager)

---


‚è≠Ô∏è [Activer le sharding sur une base et une collection](/10-sharding/07-activer-sharding.md)
