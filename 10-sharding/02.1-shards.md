ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 10.2.1 Shards

## Introduction

Un **shard** est l'unitÃ© fondamentale de stockage de donnÃ©es dans une architecture shardÃ©e MongoDB. Contrairement Ã  une idÃ©e reÃ§ue, un shard n'est pas simplement un serveur MongoDB, mais un **Replica Set complet** qui hÃ©berge un sous-ensemble spÃ©cifique des donnÃ©es d'une collection shardÃ©e.

Cette section explore en profondeur l'architecture interne des shards, leur rÃ´le dans le cluster, les stratÃ©gies de gestion et les optimisations spÃ©cifiques Ã  ce composant critique.

## Qu'est-ce qu'un Shard ?

### DÃ©finition PrÃ©cise

```
Shard = Replica Set + Sous-ensemble de Chunks
```

Un shard combine :
1. **Tous les avantages d'un Replica Set** :
   - Haute disponibilitÃ© (failover automatique)
   - Redondance des donnÃ©es
   - Read preference configurable
   - TolÃ©rance aux pannes

2. **ResponsabilitÃ© de partitionnement** :
   - HÃ©berge uniquement certains chunks
   - Participe au balancing (source/destination)
   - IsolÃ© des autres shards

### Architecture Interne d'un Shard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Shard A Replica Set                         â”‚
â”‚                  (shard-a-replica-set)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Primary Node     â”‚  â”‚  Secondary Node    â”‚  â”‚Secondary  â”‚
â”‚  â”‚   mongo-a1:27018   â”‚  â”‚   mongo-a2:27018   â”‚  â”‚mongo-a3â”‚  â”‚
â”‚  â”‚                    â”‚  â”‚                    â”‚  â”‚        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚  â”‚
â”‚  â”‚  â”‚ WiredTiger   â”‚  â”‚  â”‚  â”‚ WiredTiger   â”‚  â”‚  â”‚â”‚WT    â”‚â”‚  â”‚
â”‚  â”‚  â”‚ Storage      â”‚  â”‚  â”‚  â”‚ Storage      â”‚  â”‚  â”‚â”‚Storageâ”‚  â”‚
â”‚  â”‚  â”‚              â”‚  â”‚  â”‚  â”‚              â”‚  â”‚  â”‚â”‚      â”‚â”‚  â”‚
â”‚  â”‚  â”‚ Collections: â”‚  â”‚  â”‚  â”‚ Collections: â”‚  â”‚  â”‚â”‚Coll: â”‚â”‚  â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚â”‚â”Œâ”€â”€â”€â”€â”â”‚â”‚  â”‚
â”‚  â”‚  â”‚ â”‚orders    â”‚ â”‚  â”‚  â”‚  â”‚ â”‚orders    â”‚ â”‚  â”‚  â”‚â”‚â”‚ord.â”‚â”‚â”‚  â”‚
â”‚  â”‚  â”‚ â”‚ chunks:  â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ chunks:  â”‚ â”‚  â”‚  â”‚â”‚â”‚chnkâ”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚ 1,2,3,7  â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ 1,2,3,7  â”‚ â”‚  â”‚  â”‚â”‚â”‚1,2 â”‚â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚â”‚â””â”€â”€â”€â”€â”˜â”‚â”‚  â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”‚â”‚â”Œâ”€â”€â”€â”€â”â”‚â”‚  â”‚
â”‚  â”‚  â”‚ â”‚users     â”‚ â”‚  â”‚  â”‚  â”‚ â”‚users     â”‚ â”‚  â”‚  â”‚â”‚â”‚usr â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚ (non     â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ (non     â”‚ â”‚  â”‚  â”‚â”‚â”‚(ns)â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚ sharded) â”‚ â”‚  â”‚  â”‚  â”‚ â”‚ sharded) â”‚ â”‚  â”‚  â”‚â”‚â””â”€â”€â”€â”€â”˜â”‚â”‚  â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â”‚â”‚      â”‚â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚
â”‚  â”‚                    â”‚  â”‚                    â”‚  â”‚        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚  â”‚
â”‚  â”‚  â”‚ Oplog        â”‚  â”‚  â”‚  â”‚ Oplog        â”‚  â”‚  â”‚â”‚Oplog â”‚â”‚  â”‚
â”‚  â”‚  â”‚ (Repl)       â”‚â—„â”€â”¼â”€â”€â”¼â”€â”€â”¤ (Repl)       â”‚â—„â”€â”¼â”€â”€â”¼â”¤(Repl)â”‚â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚
â”‚  â”‚                    â”‚  â”‚                    â”‚  â”‚        â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”â”‚  â”‚
â”‚  â”‚  â”‚ Metadata     â”‚  â”‚  â”‚  â”‚ Metadata     â”‚  â”‚  â”‚â”‚Metadaâ”‚â”‚
â”‚  â”‚  â”‚ - Shard ID   â”‚  â”‚  â”‚  â”‚ - Shard ID   â”‚  â”‚  â”‚â”‚- Shrdâ”‚â”‚  â”‚
â”‚  â”‚  â”‚ - Chunk IDs  â”‚  â”‚  â”‚  â”‚ - Chunk IDs  â”‚  â”‚  â”‚â”‚- Chnkâ”‚â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  Ã‰tat: RUNNING                                               â”‚
â”‚  Chunks hÃ©bergÃ©s: 1, 2, 3, 7, 8, 15, 22 (7 chunks)           â”‚
â”‚  Taille totale: 1.2 TB                                       â”‚
â”‚  Tags: ["eu", "premium", "ssd"]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Cycle de Vie d'un Shard

### 1. DÃ©ploiement Initial

```bash
# Ã‰tape 1: DÃ©ployer un Replica Set standard
# Sur mongo-a1:
mongod --replSet shard-a-replica-set \
       --port 27018 \
       --dbpath /data/shard-a \
       --logpath /logs/shard-a.log \
       --shardsvr \  # âš ï¸ ParamÃ¨tre critique pour identifier comme shard
       --bind_ip localhost,192.168.1.10

# Sur mongo-a2:
mongod --replSet shard-a-replica-set \
       --port 27018 \
       --dbpath /data/shard-a \
       --logpath /logs/shard-a.log \
       --shardsvr \
       --bind_ip localhost,192.168.1.11

# Sur mongo-a3:
mongod --replSet shard-a-replica-set \
       --port 27018 \
       --dbpath /data/shard-a \
       --logpath /logs/shard-a.log \
       --shardsvr \
       --bind_ip localhost,192.168.1.12

# Ã‰tape 2: Initialiser le Replica Set
mongo --port 27018 << EOF
rs.initiate({
  _id: "shard-a-replica-set",
  members: [
    { _id: 0, host: "192.168.1.10:27018", priority: 2 },
    { _id: 1, host: "192.168.1.11:27018", priority: 1 },
    { _id: 2, host: "192.168.1.12:27018", priority: 1 }
  ]
})
EOF

# VÃ©rification
rs.status()
```

**ParamÃ¨tre `--shardsvr` :**
```javascript
// Impact de --shardsvr:
// 1. Port par dÃ©faut: 27018 (au lieu de 27017)
// 2. RÃ´le: Identifie le membre comme appartenant Ã  un shard
// 3. MÃ©tadonnÃ©es: Autorise stockage chunk metadata
// 4. Balancer: Accepte les opÃ©rations de migration

// âš ï¸ CRITIQUE: Sans --shardsvr, le RS ne peut pas Ãªtre ajoutÃ© au cluster
```

### 2. Ajout au Cluster ShardÃ©

```javascript
// Connexion Ã  un mongos
mongo --host mongos1:27017

// Ajout du shard au cluster
sh.addShard("shard-a-replica-set/192.168.1.10:27018,192.168.1.11:27018,192.168.1.12:27018")

// Output:
{
  "shardAdded": "shard-a-replica-set",
  "ok": 1,
  "$clusterTime": { ... },
  "operationTime": Timestamp(...)
}

// VÃ©rification
sh.status()
// ou
db.getSiblingDB("config").shards.find().pretty()

// Output:
{
  "_id": "shard-a-replica-set",
  "host": "shard-a-replica-set/192.168.1.10:27018,192.168.1.11:27018,192.168.1.12:27018",
  "state": 1,  // 1 = active, 0 = draining
  "tags": [],
  "topologyTime": Timestamp(1701456789, 1)
}
```

### 3. Ã‰tats d'un Shard

```javascript
// Ã‰tat 1: ACTIVE (state: 1)
// - ReÃ§oit de nouveaux chunks via balancing
// - RÃ©pond aux requÃªtes normalement
// - Peut Ãªtre source/destination de migrations

// Ã‰tat 2: DRAINING (state: 0)
// - En cours de vidage (removal du cluster)
// - Ne reÃ§oit PLUS de nouveaux chunks
// - Migrations sortantes uniquement
// - RÃ©pond encore aux requÃªtes pour chunks existants

// Mettre un shard en mode draining:
db.getSiblingDB("admin").runCommand({ removeShard: "shard-a-replica-set" })

// Output pendant le draining:
{
  "msg": "draining started successfully",
  "state": "started",
  "shard": "shard-a-replica-set",
  "note": "you need to drop or movePrimary these databases",
  "dbsToMove": ["testdb", "analytics"],
  "ok": 1,
  "remaining": {
    "chunks": 42,      // Chunks restants Ã  migrer
    "dbs": 2,          // Databases Ã  movePrimary
    "jumboChunks": 0
  }
}

// Suivre la progression:
db.getSiblingDB("admin").runCommand({ removeShard: "shard-a-replica-set" })

// Quand draining terminÃ©:
{
  "msg": "removeshard completed successfully",
  "state": "completed",
  "shard": "shard-a-replica-set",
  "ok": 1
}
```

## Gestion des DonnÃ©es sur un Shard

### Collections ShardÃ©es vs Non-ShardÃ©es

Sur un shard donnÃ©, deux types de donnÃ©es coexistent :

```
Shard A contient:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COLLECTIONS SHARDÃ‰ES                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ecommerce.orders (sharded)               â”‚   â”‚
â”‚ â”‚ - Chunks: 1, 2, 3, 7, 8 (5 chunks)       â”‚   â”‚
â”‚ â”‚ - customer_id: [0, 5000)                 â”‚   â”‚
â”‚ â”‚ - Taille: 640 MB                         â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ analytics.events (sharded)               â”‚   â”‚
â”‚ â”‚ - Chunks: 12, 15, 18 (3 chunks)          â”‚   â”‚
â”‚ â”‚ - timestamp: [2024-01-01, 2024-03-01)    â”‚   â”‚
â”‚ â”‚ - Taille: 384 MB                         â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COLLECTIONS NON-SHARDÃ‰ES                       â”‚
â”‚ (StockÃ©es sur le "Primary Shard" de la DB)     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ecommerce.users (non-sharded)            â”‚   â”‚
â”‚ â”‚ - Tous les documents sur ce shard        â”‚   â”‚
â”‚ â”‚ - Taille: 200 MB                         â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ecommerce.config (non-sharded)           â”‚   â”‚
â”‚ â”‚ - Taille: 10 MB                          â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Taille totale Shard A: 1.23 GB
```

**Primary Shard d'une Database :**
```javascript
// Afficher le primary shard de chaque database
use config
db.databases.find().pretty()

// Output:
{
  "_id": "ecommerce",
  "primary": "shard-a-replica-set",  // â† Primary shard
  "partitioned": true
}

// Toutes les collections NON-shardÃ©es de "ecommerce"
// sont stockÃ©es sur shard-a-replica-set

// Changer le primary shard (opÃ©ration lourde!)
db.adminCommand({
  movePrimary: "ecommerce",
  to: "shard-b-replica-set"
})
```

### Distribution des Chunks

```javascript
// Visualiser la distribution des chunks par shard
use config
db.chunks.aggregate([
  {
    $group: {
      _id: "$shard",
      count: { $sum: 1 },
      collections: { $addToSet: "$ns" }
    }
  },
  { $sort: { count: -1 } }
])

// Output:
[
  {
    "_id": "shard-a-replica-set",
    "count": 127,
    "collections": ["ecommerce.orders", "analytics.events", "logs.system"]
  },
  {
    "_id": "shard-b-replica-set",
    "count": 124,
    "collections": ["ecommerce.orders", "analytics.events", "logs.system"]
  },
  {
    "_id": "shard-c-replica-set",
    "count": 121,
    "collections": ["ecommerce.orders", "analytics.events"]
  }
]

// IdÃ©alement: distribution Ã©quilibrÃ©e (Â±5%)
// Alerte si Ã©cart > 15%
```

### StratÃ©gie de Stockage par Shard

#### Option 1 : HomogÃ¨ne (Tous Shards Identiques)

```
Tous les shards ont le mÃªme matÃ©riel:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard A    â”‚  â”‚  Shard B    â”‚  â”‚  Shard C    â”‚
â”‚  CPU: 16c   â”‚  â”‚  CPU: 16c   â”‚  â”‚  CPU: 16c   â”‚
â”‚  RAM: 64GB  â”‚  â”‚  RAM: 64GB  â”‚  â”‚  RAM: 64GB  â”‚
â”‚  SSD: 2TB   â”‚  â”‚  SSD: 2TB   â”‚  â”‚  SSD: 2TB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Avantages:
âœ… SimplicitÃ© de gestion
âœ… Distribution uniforme possible
âœ… Performance prÃ©visible
âœ… FacilitÃ© de scaling

InconvÃ©nients:
âŒ Pas d'optimisation par workload
âŒ CoÃ»t potentiellement Ã©levÃ©
```

#### Option 2 : HÃ©tÃ©rogÃ¨ne (Shards SpÃ©cialisÃ©s)

```
Shards diffÃ©renciÃ©s par usage:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard HOT      â”‚  â”‚  Shard WARM     â”‚  â”‚  Shard COLD  â”‚
â”‚  (donnÃ©es       â”‚  â”‚  (donnÃ©es       â”‚  â”‚  (archives)  â”‚
â”‚   rÃ©centes)     â”‚  â”‚   6-12 mois)    â”‚  â”‚              â”‚
â”‚  CPU: 32c       â”‚  â”‚  CPU: 16c       â”‚  â”‚  CPU: 8c     â”‚
â”‚  RAM: 128GB     â”‚  â”‚  RAM: 64GB      â”‚  â”‚  RAM: 32GB   â”‚
â”‚  NVMe: 4TB      â”‚  â”‚  SSD: 4TB       â”‚  â”‚  HDD: 20TB   â”‚
â”‚  Tags: ["hot"]  â”‚  â”‚  Tags: ["warm"] â”‚  â”‚  Tags:["cold"]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ImplÃ©mentation via Zone Sharding:
sh.addShardToZone("shard-hot", "hot-data")
sh.addShardToZone("shard-warm", "warm-data")
sh.addShardToZone("shard-cold", "cold-data")

sh.updateZoneKeyRange("analytics.events",
  { timestamp: ISODate("2024-11-01"), event_id: MinKey },
  { timestamp: MaxKey, event_id: MaxKey },
  "hot-data"
)

Avantages:
âœ… Optimisation coÃ»t/performance
âœ… SLA diffÃ©renciÃ©s possibles
âœ… Meilleur ROI matÃ©riel

InconvÃ©nients:
âŒ ComplexitÃ© de gestion
âŒ Balancing plus complexe
âŒ Risque de dÃ©sÃ©quilibre
```

## OpÃ©rations sur les Shards

### Ajout d'un Nouveau Shard (Scale-Out)

```javascript
// ScÃ©nario: Cluster Ã  2 shards saturÃ© â†’ ajout d'un 3Ã¨me shard

// Ã‰tat initial:
sh.status()
// shards:
//   shard-a-replica-set: 150 chunks, 2.5 TB
//   shard-b-replica-set: 148 chunks, 2.4 TB

// Ã‰tape 1: DÃ©ployer le nouveau Replica Set "shard-c-replica-set"
// (voir section DÃ©ploiement Initial)

// Ã‰tape 2: Ajouter au cluster
sh.addShard("shard-c-replica-set/192.168.1.20:27018,192.168.1.21:27018,192.168.1.22:27018")

// Ã‰tape 3: Le balancer s'active automatiquement
// Distribution cible: ~100 chunks par shard

// Suivre la migration:
while (true) {
  sh.status()
  sleep(60000)  // Check toutes les 60 secondes
}

// AprÃ¨s quelques heures/jours:
// shards:
//   shard-a-replica-set: 100 chunks, 1.7 TB
//   shard-b-replica-set: 99 chunks, 1.6 TB
//   shard-c-replica-set: 99 chunks, 1.6 TB  â† Nouveau shard Ã©quilibrÃ©
```

**Calcul du Temps de Migration :**
```javascript
// Formule empirique:
temps_migration_heures = (
  nb_chunks_Ã _migrer Ã— taille_moyenne_chunk_MB / bande_passante_MB_sec / 3600
) Ã— facteur_sÃ©curitÃ©

// Exemple:
// - 100 chunks Ã  migrer vers nouveau shard
// - Taille moyenne: 120 MB
// - Bande passante: 50 MB/sec (balancer throttled)
// - Facteur sÃ©curitÃ©: 2 (overhead, retries)

temps = (100 Ã— 120 / 50 / 3600) Ã— 2
      = (12000 / 180000) Ã— 2
      = 0.067 Ã— 2
      = 0.13 heures = ~8 minutes par chunk

// Total: ~13 heures pour 100 chunks
```

### Suppression d'un Shard (Scale-Down)

```javascript
// ScÃ©nario: Shard dÃ©faillant ou capacitÃ© excÃ©dentaire

// Ã‰tape 1: Initier le draining
db.adminCommand({ removeShard: "shard-c-replica-set" })

// Output:
{
  "msg": "draining started successfully",
  "state": "started",
  "shard": "shard-c-replica-set",
  "note": "you need to drop or movePrimary these databases",
  "dbsToMove": ["analytics"],  // â† Databases dont c'est le primary shard
  "ok": 1,
  "remaining": {
    "chunks": 99,
    "dbs": 1,
    "jumboChunks": 0
  }
}

// Ã‰tape 2: DÃ©placer les databases primary (si nÃ©cessaire)
db.adminCommand({ movePrimary: "analytics", to: "shard-a-replica-set" })

// Ã‰tape 3: Surveiller la progression
db.adminCommand({ removeShard: "shard-c-replica-set" })

// Pendant le draining:
{
  "msg": "draining ongoing",
  "state": "ongoing",
  "remaining": {
    "chunks": 42,  // DÃ©crÃ©mente au fur et Ã  mesure
    "dbs": 0,
    "jumboChunks": 0
  },
  "ok": 1
}

// Ã‰tape 4: Quand terminÃ©
{
  "msg": "removeshard completed successfully",
  "state": "completed",
  "shard": "shard-c-replica-set",
  "ok": 1
}

// Ã‰tape 5: ArrÃªter physiquement le Replica Set
// (Commandes sur chaque membre)
db.shutdownServer()
```

### Maintenance d'un Shard (Rolling Restart)

```javascript
// Objectif: Upgrade, changement config, maintenance OS
// Principe: Respecter la majoritÃ© du Replica Set

// Shard RS: [Primary, Secondary1, Secondary2]

// Ã‰tape 1: Maintenance sur Secondary1
// Sur Secondary1:
db.shutdownServer()
// Maintenance systÃ¨me (upgrade, patch, etc.)
// RedÃ©marrage avec nouvelle config

// Attendre rÃ©intÃ©gration:
rs.status()
// VÃ©rifier que Secondary1 est "SECONDARY" et synced

// Ã‰tape 2: Maintenance sur Secondary2
// RÃ©pÃ©ter le processus

// Ã‰tape 3: Step Down du Primary
rs.stepDown(60)  // 60 secondes

// Un secondary devient Primary
// Ancien Primary devient Secondary

// Ã‰tape 4: Maintenance sur ancien Primary
// Maintenance systÃ¨me
// RedÃ©marrage

// Ã‰tape 5: Optionnel - restaurer Primary initial
// (laisser le RS Ã©lire naturellement, ou forcer avec priority)
```

## Optimisations SpÃ©cifiques aux Shards

### 1. Index Strategy par Shard

```javascript
// StratÃ©gie: Indexes ciblÃ©s selon la distribution

// Exemple: Collection shardÃ©e par region
// Shard EU: principalement requÃªtes sur lang="fr"
// Shard US: principalement requÃªtes sur lang="en"

// âŒ MAUVAIS: Index global identique partout
db.users.createIndex({ lang: 1, created_at: 1 })
// Chaque shard indexe toutes les langues (inefficace)

// âœ… BON: Index partiel par shard
// Sur Shard EU:
db.users.createIndex(
  { lang: 1, created_at: 1 },
  {
    partialFilterExpression: {
      region: { $in: ["EU-FR", "EU-DE", "EU-IT"] }
    }
  }
)

// Sur Shard US:
db.users.createIndex(
  { lang: 1, created_at: 1 },
  {
    partialFilterExpression: {
      region: { $in: ["US-EAST", "US-WEST"] }
    }
  }
)

// BÃ©nÃ©fice: Index plus petits, scans plus rapides
```

### 2. WiredTiger Cache Sizing

```javascript
// Par dÃ©faut: 50% de RAM - 1 GB
// Calcul optimal par shard:

taille_cache_optimal = MIN(
  (RAM_totale - 1GB - OS_reserve) Ã— 0.5,
  working_set_size Ã— 1.2
)

// Exemple Shard A:
// - RAM: 64 GB
// - OS reserve: 2 GB
// - Working set: 30 GB

cache_size = MIN(
  (64 - 1 - 2) Ã— 0.5 = 30.5 GB,
  30 Ã— 1.2 = 36 GB
)
= 30.5 GB

// Configuration:
storage:
  wiredTiger:
    engineConfig:
      cacheSizeGB: 30.5

// âš ï¸ Ajuster selon monitoring:
db.serverStatus().wiredTiger.cache
// - "bytes currently in the cache"
// - "tracked dirty bytes in the cache"
// - "pages evicted by application threads"
```

### 3. Read Preference par Shard

```javascript
// ScÃ©nario: Analytics read-heavy sur donnÃ©es historiques

// Collection: analytics.events (sharded par timestamp)
// Shard A: donnÃ©es rÃ©centes (hot) â†’ beaucoup d'Ã©critures
// Shard B: donnÃ©es anciennes (warm) â†’ principalement lectures

// Application writes (rÃ©cent):
db.events.insertMany([...], {
  writeConcern: { w: "majority" }
})
// â†’ DirigÃ© vers Shard A Primary

// Application reads (analytics sur donnÃ©es anciennes):
db.events.find({
  timestamp: { $lt: ISODate("2024-01-01") }
}).readPref("secondaryPreferred")
// â†’ Peut utiliser les Secondaries de Shard B
// â†’ DÃ©charge le Primary

// Monitoring par shard:
db.serverStatus().opcounters
// VÃ©rifier rÃ©partition reads sur secondaries
```

### 4. Oplog Sizing

```javascript
// RÃ¨gle: Oplog doit contenir 24-72h d'opÃ©rations minimum

// Calcul du taux d'Ã©criture par shard:
taux_Ã©criture_MB_sec = (
  inserts_per_sec Ã— avg_doc_size_bytes +
  updates_per_sec Ã— avg_oplog_entry_bytes
) / 1024 / 1024

// Exemple Shard A (high write):
// - 5000 inserts/sec Ã— 500 bytes = 2.5 MB/sec
// - 3000 updates/sec Ã— 200 bytes = 0.6 MB/sec
// Total: 3.1 MB/sec

// Oplog pour 48h:
oplog_size_MB = 3.1 Ã— 60 Ã— 60 Ã— 48 = 535,680 MB â‰ˆ 523 GB

// Configuration:
replication:
  oplogSizeMB: 550000  # 537 GB (10% marge)

// VÃ©rification:
rs.printReplicationInfo()
// "configured oplog size"
// "log length start to end"
// "oplog first event time"
// "oplog last event time"
```

## StratÃ©gies de Partitionnement AvancÃ©es

### StratÃ©gie 1 : Sharding par Tenant (Multi-Tenancy)

```javascript
// Architecture: Isolation complÃ¨te par client

// Shard key: { tenant_id: 1, entity_id: 1 }
sh.shardCollection("saas.customers", { tenant_id: 1, customer_id: 1 })

// Zone Sharding pour isolation:
// Tenant Enterprise â†’ Shard dÃ©diÃ©
sh.addShardToZone("shard-premium-1", "tenant-enterprise-001")
sh.updateZoneKeyRange(
  "saas.customers",
  { tenant_id: "enterprise-001", customer_id: MinKey },
  { tenant_id: "enterprise-001", customer_id: MaxKey },
  "tenant-enterprise-001"
)

// Tenants Standard â†’ Shards partagÃ©s
sh.addShardToZone("shard-standard-1", "tenants-standard")
sh.addShardToZone("shard-standard-2", "tenants-standard")

sh.updateZoneKeyRange(
  "saas.customers",
  { tenant_id: "standard-*", customer_id: MinKey },
  { tenant_id: "standard-\uffff", customer_id: MaxKey },
  "tenants-standard"
)
```

**Avantages :**
- âœ… Isolation des performances (tenant enterprise n'impacte pas standard)
- âœ… SLA diffÃ©renciÃ©s possibles
- âœ… Facturation granulaire (par shard)
- âœ… ConformitÃ© et audit simplifiÃ©s

**InconvÃ©nients :**
- âŒ ComplexitÃ© de gestion accrue
- âŒ Risque de sous-utilisation de shards dÃ©diÃ©s
- âŒ CoÃ»t infrastructure plus Ã©levÃ©

### StratÃ©gie 2 : Sharding Temporel avec Rotation

```javascript
// Architecture: Shards spÃ©cialisÃ©s par pÃ©riode

// Collection: logs.events (IoT, metrics, etc.)
// Shard key: { timestamp: 1, device_id: 1 }

// AnnÃ©e 2024:
sh.addShardToZone("shard-2024-h1", "2024-h1")
sh.addShardToZone("shard-2024-h2", "2024-h2")

sh.updateZoneKeyRange(
  "logs.events",
  { timestamp: ISODate("2024-01-01"), device_id: MinKey },
  { timestamp: ISODate("2024-07-01"), device_id: MaxKey },
  "2024-h1"
)

sh.updateZoneKeyRange(
  "logs.events",
  { timestamp: ISODate("2024-07-01"), device_id: MinKey },
  { timestamp: ISODate("2025-01-01"), device_id: MaxKey },
  "2024-h2"
)

// Rotation annuelle:
// - Janvier 2025: Provisionner shard-2025-h1
// - Juillet 2025: Provisionner shard-2025-h2
// - DÃ©cembre 2025: Archiver/supprimer shard-2024-h1 (donnÃ©es > 2 ans)
```

**Avantages :**
- âœ… Suppression efficace de donnÃ©es anciennes (drop shard complet)
- âœ… Optimisation du stockage (SSD rÃ©cent, HDD archives)
- âœ… Performance prÃ©visible (shards rÃ©cents = hot, anciens = cold)

**InconvÃ©nients :**
- âŒ Gestion du cycle de vie complexe
- âŒ RequÃªtes cross-pÃ©riode moins efficaces
- âŒ Planification capacitÃ© critique

### StratÃ©gie 3 : Sharding GÃ©ographique

```javascript
// Architecture: DonnÃ©es prÃ¨s des utilisateurs (latence + RGPD)

// Shard key: { country: 1, user_id: 1 }
sh.shardCollection("app.profiles", { country: 1, user_id: 1 })

// Zones gÃ©ographiques:
// EU Shards (RGPD)
sh.addShardToZone("shard-eu-paris-1", "EU")
sh.addShardToZone("shard-eu-frankfurt-1", "EU")

sh.updateZoneKeyRange(
  "app.profiles",
  { country: "FR", user_id: MinKey },
  { country: "FR", user_id: MaxKey },
  "EU"
)
// RÃ©pÃ©ter pour DE, IT, ES, etc.

// US Shards
sh.addShardToZone("shard-us-east-1", "US")
sh.addShardToZone("shard-us-west-1", "US")

// APAC Shards
sh.addShardToZone("shard-apac-singapore-1", "APAC")
sh.addShardToZone("shard-apac-tokyo-1", "APAC")

// Configuration mongos par rÃ©gion:
// mongos EU â†’ se connecte prioritairement aux shards EU
// (via readPreference et nearestNode)
```

**Avantages :**
- âœ… Latence minimale (donnÃ©es locales)
- âœ… ConformitÃ© RGPD automatique
- âœ… RÃ©sidence des donnÃ©es garantie
- âœ… RÃ©silience rÃ©gionale

**InconvÃ©nients :**
- âŒ RequÃªtes cross-rÃ©gion coÃ»teuses
- âŒ ComplexitÃ© opÃ©rationnelle (3+ rÃ©gions)
- âŒ CoÃ»ts de transfert inter-rÃ©gion

## Anti-Patterns SpÃ©cifiques aux Shards

### ğŸš« Anti-Pattern 1 : Shards Sous-DimensionnÃ©s

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ ANTI-PATTERN: Shards trop petits    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cluster: 20 shards Ã— 100 GB = 2 TB     â”‚
â”‚                                        â”‚
â”‚ ProblÃ¨mes:                             â”‚
â”‚ - Overhead config servers Ã©norme       â”‚
â”‚ - Balancer en permanence actif         â”‚
â”‚ - Metadata explosion                   â”‚
â”‚ - RÃ©seau saturÃ© (migrations)           â”‚
â”‚ - CoÃ»t opÃ©rationnel prohibitif         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RÃ¨gle empirique:
âŒ Trop petit: < 500 GB par shard
âœ… Optimal:    2-5 TB par shard
âš ï¸  Limite:    > 10 TB nÃ©cessite attention
```

**Solution :**
```
DÃ©marrer avec moins de shards plus gros:
- 2 shards Ã— 1 TB au lieu de 20 Ã— 100 GB
- Ajouter des shards progressivement
- Cible: 2-3 TB par shard en production
```

### ğŸš« Anti-Pattern 2 : Ignorer les Jumbo Chunks

```javascript
// âŒ ANTI-PATTERN: Laisser les jumbo chunks s'accumuler

// SymptÃ´me:
db.chunks.find({ jumbo: true }).count()
// Output: 25 jumbo chunks

// ConsÃ©quence:
sh.status()
// Shard A: 200 chunks (dont 20 jumbo) â†’ 5 TB
// Shard B: 150 chunks (dont 0 jumbo) â†’ 2 TB
// Shard C: 130 chunks (dont 5 jumbo) â†’ 3 TB
// â†’ DÃ©sÃ©quilibre permanent, balancer impuissant
```

**Solution :**
```javascript
// 1. Identifier la cause
db.chunks.find({ jumbo: true, ns: "app.collection" }).forEach(chunk => {
  print("Chunk: " + chunk.min + " â†’ " + chunk.max)
  print("Shard: " + chunk.shard)

  // Analyser la distribution de la shard key dans ce chunk
  db.getSiblingDB("app").collection.aggregate([
    { $match: { shard_key: { $gte: chunk.min, $lt: chunk.max } } },
    { $group: { _id: "$shard_key", count: { $sum: 1 } } },
    { $sort: { count: -1 } },
    { $limit: 10 }
  ])
})

// 2. Solutions selon la cause:
// - Faible cardinalitÃ© â†’ Raffiner shard key (MongoDB 5.0+)
// - Distribution inÃ©gale â†’ Split manuel avec valeur intermÃ©diaire
// - RÃ©ellement volumineux â†’ RemodÃ©liser (pattern Bucket, Subset)
```

### ğŸš« Anti-Pattern 3 : Shard Key Non PrÃ©fixe des Index

```javascript
// âŒ ANTI-PATTERN
sh.shardCollection("orders.transactions", { customer_id: 1, order_date: 1 })

// Mais requÃªtes frÃ©quentes:
db.transactions.find({ order_id: "ORD-123456" })  // Pas de shard key!
db.transactions.find({ order_date: ISODate("2024-12-08") })  // PrÃ©fixe incomplet!

// RÃ©sultat: Scatter-gather systÃ©matique
```

**Solution :**
```javascript
// âœ… PATTERN 1: Aligner shard key sur requÃªtes
sh.shardCollection("orders.transactions", { order_id: 1 })

// âœ… PATTERN 2: Shard key composÃ©e couvrant les requÃªtes
sh.shardCollection("orders.transactions", { order_date: 1, customer_id: 1 })
// RequÃªtes par order_date seul: CiblÃ©es (prÃ©fixe)
// RequÃªtes par order_date + customer_id: TrÃ¨s ciblÃ©es

// âœ… PATTERN 3: Index secondaires bien planifiÃ©s
sh.shardCollection("orders.transactions", { customer_id: 1, order_date: 1 })
db.transactions.createIndex({ order_id: 1 })  // Index secondaire
// RequÃªte par order_id: Scatter-gather mais index accÃ©lÃ¨re scan par shard
```

### ğŸš« Anti-Pattern 4 : Ignorer la LocalitÃ© des DonnÃ©es

```javascript
// âŒ ANTI-PATTERN: Shards gÃ©ographiquement distribuÃ©s sans zones

// Cluster:
// - Shard A: Europe (Paris)
// - Shard B: US (Virginie)
// - Shard C: APAC (Singapore)

// SANS zone sharding:
// Utilisateur EU: donnÃ©es peuvent Ãªtre sur Shard US â†’ latence 100ms
// Utilisateur US: donnÃ©es peuvent Ãªtre sur Shard APAC â†’ latence 200ms

// Impact sur les Ã©critures:
// Write Concern majority nÃ©cessite 2/3 â†’ peut traverser ocÃ©ans
```

**Solution :**
```javascript
// âœ… PATTERN: Zone Sharding + Read Preference

// 1. Configurer zones
sh.addShardToZone("shard-eu", "EU")
sh.updateZoneKeyRange("app.users",
  { region: "EU", user_id: MinKey },
  { region: "EU", user_id: MaxKey },
  "EU"
)
// RÃ©pÃ©ter pour US et APAC

// 2. Mongos par rÃ©gion avec read preference
// mongos-eu:
db.users.find({ region: "EU", ... }).readPref("nearest")
// Lit depuis Shard EU (local)

// 3. Write Concern rÃ©gional (MongoDB 5.0+)
db.users.insertOne(
  { region: "EU", ... },
  { writeConcern: { w: "majority", wtimeout: 5000 } }
)
// Majority calculÃ© sur membres du Replica Set Shard EU
```

### ğŸš« Anti-Pattern 5 : Replica Set Shard avec PrioritÃ©s DÃ©sÃ©quilibrÃ©es

```javascript
// âŒ ANTI-PATTERN
rs.initiate({
  _id: "shard-a-replica-set",
  members: [
    { _id: 0, host: "mongo-a1:27018", priority: 100 },  // â† Toujours Primary
    { _id: 1, host: "mongo-a2:27018", priority: 1 },
    { _id: 2, host: "mongo-a3:27018", priority: 1 }
  ]
})

// ProblÃ¨me:
// - mongo-a1 TOUJOURS Primary â†’ SPOF pratique
// - Si mongo-a1 en maintenance â†’ downtime garanti
// - Pas de rÃ©elle haute disponibilitÃ©
```

**Solution :**
```javascript
// âœ… PATTERN: PrioritÃ©s Ã©quilibrÃ©es
rs.initiate({
  _id: "shard-a-replica-set",
  members: [
    { _id: 0, host: "mongo-a1:27018", priority: 2 },  // PrÃ©fÃ©rence lÃ©gÃ¨re
    { _id: 1, host: "mongo-a2:27018", priority: 1 },
    { _id: 2, host: "mongo-a3:27018", priority: 1 }
  ]
})

// Ou: PrioritÃ©s Ã©gales (Ã©lection automatique)
rs.initiate({
  _id: "shard-a-replica-set",
  members: [
    { _id: 0, host: "mongo-a1:27018", priority: 1 },
    { _id: 1, host: "mongo-a2:27018", priority: 1 },
    { _id: 2, host: "mongo-a3:27018", priority: 1 }
  ]
})

// Maintenance sans downtime:
// 1. Step down Primary
rs.stepDown(60)
// 2. Maintenance sur ancien Primary
// 3. RÃ©intÃ©gration automatique
```

## Monitoring et Diagnostics par Shard

### MÃ©triques Critiques

```javascript
// 1. Nombre de chunks par shard
use config
db.chunks.aggregate([
  { $group: { _id: "$shard", count: { $sum: 1 } } },
  { $sort: { count: -1 } }
])

// Alerte si Ã©cart > 15% entre shards

// 2. Taille de donnÃ©es par shard
db.adminCommand({ listShards: 1 }).shards.forEach(shard => {
  let conn = new Mongo(shard.host)
  let stats = conn.getDB("admin").runCommand({ serverStatus: 1 })

  print("Shard: " + shard._id)
  print("  Data Size: " + (stats.mem.resident / 1024) + " GB")
  print("  Ops/sec: " + stats.opcounters.command)
  print("  Connections: " + stats.connections.current)
})

// 3. Balance du cluster
sh.status()
// Chercher: "Currently enabled: yes"
// VÃ©rifier: "balancer lock taken"

// 4. Migrations en cours
db.getSiblingDB("config").locks.find({ state: 2 }).pretty()

// 5. Performance par shard
db.currentOp({
  "shard": "shard-a-replica-set",
  "secs_running": { $gte: 5 }
})
// Identifier requÃªtes lentes par shard
```

### Commandes de Diagnostic

```javascript
// 1. Statistiques dÃ©taillÃ©es d'un shard
db.adminCommand({
  connPoolStats: 1,
  shardedDataDistribution: 1
})

// 2. VÃ©rifier la santÃ© du balancer
sh.getBalancerState()  // ActivÃ©?
sh.isBalancerRunning()  // En cours?

db.getSiblingDB("config").settings.find({ _id: "balancer" })
// VÃ©rifier fenÃªtre de balancing

// 3. Historique des migrations
db.getSiblingDB("config").changelog.find({
  what: /moveChunk/,
  time: { $gte: ISODate("2024-12-08T00:00:00Z") }
}).sort({ time: -1 }).limit(10).pretty()

// 4. Jumbo chunks
db.getSiblingDB("config").chunks.find({ jumbo: true }).count()

// 5. Oplog lag par shard
// Se connecter Ã  chaque shard Primary:
rs.printReplicationInfo()
```

## RÃ©sumÃ©

Les **shards** sont les piliers du stockage distribuÃ© MongoDB :

**Architecture :**
- Chaque shard = Replica Set complet (HA native)
- HÃ©berge un sous-ensemble de chunks
- IsolÃ© et indÃ©pendant des autres shards

**Cycle de vie :**
- DÃ©ploiement avec `--shardsvr`
- Ajout via `sh.addShard()`
- Ã‰tats: ACTIVE, DRAINING
- Suppression via `removeShard`

**Gestion des donnÃ©es :**
- Collections shardÃ©es (chunks distribuÃ©s)
- Collections non-shardÃ©es (primary shard)
- Distribution Ã©quilibrÃ©e via balancer

**StratÃ©gies avancÃ©es :**
- âœ… Sharding par tenant (multi-tenancy)
- âœ… Sharding temporel (rotation)
- âœ… Sharding gÃ©ographique (latence + RGPD)

**Anti-patterns critiques :**
- âŒ Shards sous-dimensionnÃ©s (< 500 GB)
- âŒ Ignorer jumbo chunks
- âŒ Shard key non alignÃ©e sur requÃªtes
- âŒ Ignorer localitÃ© gÃ©ographique
- âŒ PrioritÃ©s RS dÃ©sÃ©quilibrÃ©es

**Optimisations :**
- Index partiels par shard
- WiredTiger cache sizing adaptÃ©
- Read preference par workload
- Oplog sizing selon taux d'Ã©criture

**Monitoring essentiel :**
- Distribution des chunks (Ã©quilibre Â±15%)
- Taille par shard
- Jumbo chunks (objectif = 0)
- Migrations actives
- Performance par shard

Une gestion rigoureuse des shards est essentielle pour maintenir un cluster performant, Ã©quilibrÃ© et Ã©volutif en production.

---


â­ï¸ [Config Servers](/10-sharding/02.2-config-servers.md)
