üîù Retour au [Sommaire](/SOMMAIRE.md)

# 10.2.2 Config Servers

## Introduction

Les **Config Servers** constituent le syst√®me nerveux central d'un cluster shard√© MongoDB. Ils stockent l'int√©gralit√© des m√©tadonn√©es du cluster et sont absolument critiques pour son fonctionnement : si les config servers deviennent indisponibles, le cluster passe en **lecture seule** et aucune op√©ration d'√©criture, de migration ou de modification de topologie n'est possible.

Cette section explore en profondeur l'architecture, le r√¥le, le d√©ploiement et la gestion des Config Servers, un composant dont la fiabilit√© conditionne la disponibilit√© de tout le cluster.

## √âvolution Historique

### Architecture Historique (MongoDB < 3.2)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ANCIENNE ARCHITECTURE (< 3.2) - DEPRECATED      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Config 1   ‚îÇ  ‚îÇ Config 2   ‚îÇ  ‚îÇ Config 3   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Standalone‚îÇ  ‚îÇ (Standalone‚îÇ  ‚îÇ (Standalone‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  mongod)   ‚îÇ  ‚îÇ  mongod)   ‚îÇ  ‚îÇ  mongod)   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  Protocole: SCCC (Sync Cluster Connection Conf)  ‚îÇ
‚îÇ  - Pas de r√©plication native                     ‚îÇ
‚îÇ  - Consensus via protocole propri√©taire          ‚îÇ
‚îÇ  - Limitation: Exactly 3 config servers          ‚îÇ
‚îÇ  - Risque: Split-brain possible                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ö†Ô∏è  OBSOL√àTE depuis MongoDB 3.4
```

### Architecture Moderne (MongoDB ‚â• 3.4)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ARCHITECTURE MODERNE (‚â• 3.4) - CSRS             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Config Server Replica Set (CSRS)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Primary  ‚îÇ  ‚îÇSecondary ‚îÇ  ‚îÇSecondary ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (CSR-1) ‚îÇ  ‚îÇ  (CSR-2) ‚îÇ  ‚îÇ  (CSR-3) ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇPort:27019‚îÇ  ‚îÇPort:27019‚îÇ  ‚îÇPort:27019‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ             ‚îÇ             ‚îÇ        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ          Replication Protocol              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ          (Standard Replica Set)            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  Protocole: Standard MongoDB Replication         ‚îÇ
‚îÇ  - R√©plication native (oplog)                    ‚îÇ
‚îÇ  - √âlection automatique (Raft-like)              ‚îÇ
‚îÇ  - Scalabilit√©: 3, 5, 7+ membres possibles       ‚îÇ
‚îÇ  - HA: Tol√©rance aux pannes standard RS          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚úÖ  ARCHITECTURE ACTUELLE
```

**Avantages CSRS :**
- ‚úÖ Haute disponibilit√© native (Replica Set)
- ‚úÖ Failover automatique du Primary
- ‚úÖ Scalabilit√© flexible (3, 5, 7 membres)
- ‚úÖ Outils standard de monitoring RS
- ‚úÖ Backup/restore simplifi√©

## R√¥le et Responsabilit√©s

### M√©tadonn√©es Stock√©es

Les Config Servers stockent **exclusivement** des m√©tadonn√©es dans une base de donn√©es sp√©ciale nomm√©e `config` :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Database: config                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  M√âTADONN√âES DE TOPOLOGIE:                          ‚îÇ
‚îÇ  ‚îú‚îÄ config.shards         (Liste des shards)        ‚îÇ
‚îÇ  ‚îú‚îÄ config.databases      (Databases et primary)    ‚îÇ
‚îÇ  ‚îú‚îÄ config.collections    (Collections shard√©es)    ‚îÇ
‚îÇ  ‚îú‚îÄ config.chunks         (Mapping chunks‚Üíshards)   ‚îÇ
‚îÇ  ‚îî‚îÄ config.tags           (Zones et tags)           ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  M√âTADONN√âES OP√âRATIONNELLES:                       ‚îÇ
‚îÇ  ‚îú‚îÄ config.locks          (Verrous distribu√©s)      ‚îÇ
‚îÇ  ‚îú‚îÄ config.lockpings      (Keepalive des locks)     ‚îÇ
‚îÇ  ‚îú‚îÄ config.changelog      (Historique op√©rations)   ‚îÇ
‚îÇ  ‚îú‚îÄ config.mongos         (Mongos enregistr√©s)      ‚îÇ
‚îÇ  ‚îî‚îÄ config.settings       (Configuration cluster)   ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  M√âTADONN√âES DE VERSIONING:                         ‚îÇ
‚îÇ  ‚îú‚îÄ config.version        (Version du cluster)      ‚îÇ
‚îÇ  ‚îî‚îÄ config.migrations     (Historique migrations)   ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  Taille typique: 100 MB - 5 GB                      ‚îÇ
‚îÇ  (selon nombre de chunks et historique)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Interactions avec les Composants

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         FLUX D'INTERACTIONS CONFIG SERVERS             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. Mongos ‚Üí Config Servers
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Mongos  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇConfig Primary‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   Op√©rations:
   - Lecture metadata (cache refresh)
   - Enregistrement mongos (heartbeat)
   - Queries sur chunks/shards

   Fr√©quence: Continue (cache TTL 30s)

2. Balancer ‚Üí Config Servers
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇBalancer ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇConfig Primary‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   Op√©rations:
   - Acquisition locks
   - Update chunks apr√®s migration
   - Lecture √©tat du cluster

   Fr√©quence: P√©riodique (rounds de balancing)

3. Shards ‚Üí Config Servers
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Shard   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇConfig Primary‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   Op√©rations:
   - Split notification
   - Merge notification
   - Range deletion confirmation

   Fr√©quence: √âv√©nementiel

4. Admin Commands ‚Üí Config Servers
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ DBA CLI  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇConfig Primary‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   Op√©rations:
   - addShard / removeShard
   - shardCollection
   - Zone management

   Fr√©quence: Ad-hoc (op√©rations admin)
```

## Architecture Interne D√©taill√©e

### Structure d'un Config Server

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Config Server Node (Membre du CSRS)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ        Process: mongod --configsvr         ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ     WiredTiger Storage Engine              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Database: config                    ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                                      ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Collections (Total: ~15):           ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ chunks     (~500k docs)        ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Size: 175 MB (gros cluster)    ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ collections (~100 docs)        ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Size: 50 KB                    ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ changelog  (~100k docs rolling)‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Size: 40 MB                    ‚îÇ  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ ... (autres collections)        ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ                                            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Indexes:                            ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - chunks: (ns, min)                 ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - chunks: (ns, max)                 ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - chunks: (ns, shard)               ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - chunks: (uuid, min)               ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ ... (16+ indexes critiques)     ‚îÇ  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ     Oplog (Replication Log)                ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Taille: 5-20 GB (configurable)          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Retention: 48-72h minimum               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Activit√©: Faible (metadata only)        ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ     Configuration:                         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Port: 27019 (default pour configsvr)    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - dbPath: /data/configdb                  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - Logs: /var/log/mongodb/configsvr.log    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  - clusterRole: "configsvr"                ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  Ressources typiques:                               ‚îÇ
‚îÇ  - RAM: 4-8 GB (cache WiredTiger 2-4 GB)            ‚îÇ
‚îÇ  - CPU: 2-4 cores                                   ‚îÇ
‚îÇ  - Disk: 25-100 GB (SSD recommand√©)                 ‚îÇ
‚îÇ  - IOPS: 1000+ (m√©tadonn√©es I/O intensives)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Collections Critiques de la Database Config

#### 1. config.shards

```javascript
// Structure d'un document shard
{
  "_id": "shard-a-replica-set",
  "host": "shard-a-replica-set/mongo-a1:27018,mongo-a2:27018,mongo-a3:27018",
  "state": 1,  // 1 = active, 0 = draining
  "topologyTime": Timestamp(1701456789, 1),
  "tags": ["eu", "premium", "ssd"],  // Pour zone sharding

  // Champs additionnels (MongoDB 5.0+)
  "replSetConfigVersion": NumberLong(5),
  "lastCommittedOpTime": Timestamp(1701456790, 3)
}

// Index critiques:
// - _id (unique)
// - host (unique)

// Op√©rations typiques:
// - sh.addShard() ‚Üí INSERT
// - removeShard ‚Üí UPDATE (state: 0)
// - sh.status() ‚Üí FIND
```

#### 2. config.databases

```javascript
// Structure d'un document database
{
  "_id": "ecommerce",
  "primary": "shard-a-replica-set",  // Primary shard (collections non-shard√©es)
  "partitioned": true,               // Database est shard√©e
  "version": {
    "uuid": UUID("a1b2c3d4-e5f6-7890-abcd-ef1234567890"),
    "timestamp": Timestamp(1701456789, 1),
    "lastMod": 1
  }
}

// Importance du primary shard:
// - Collections NON-shard√©es stock√©es sur primary shard
// - movePrimary pour changer (op√©ration lourde!)

// Exemple de d√©s√©quilibre:
// Database "logs" (primary: shard-a)
//   - logs.app_events (sharded) ‚Üí distribu√© sur tous shards
//   - logs.errors (NON sharded) ‚Üí UNIQUEMENT sur shard-a
//   - logs.debug (NON sharded) ‚Üí UNIQUEMENT sur shard-a
// ‚Üí Shard-a peut √™tre surcharg√© par collections non-shard√©es
```

#### 3. config.collections

```javascript
// Structure d'un document collection shard√©e
{
  "_id": "ecommerce.orders",
  "lastmodEpoch": ObjectId("674f8e9a1234567890abcdef"),
  "lastmod": ISODate("2024-12-08T10:30:00Z"),
  "dropped": false,
  "timestamp": Timestamp(1701456789, 1),

  // Shard key (CRITIQUE)
  "key": {
    "customer_id": 1,
    "order_date": 1
  },

  // Options
  "unique": false,  // Si true, shard key doit √™tre unique globalement
  "uuid": UUID("a1b2c3d4-e5f6-7890-abcd-ef1234567890"),
  "noBalance": false,  // Si true, balancer d√©sactiv√© pour cette collection

  // Collation (MongoDB 4.0+)
  "defaultCollation": {
    "locale": "en_US",
    "caseLevel": false,
    "strength": 3
  },

  // Time-series (MongoDB 5.0+)
  "timeseriesFields": {
    "timeField": "timestamp",
    "metaField": "metadata",
    "granularity": "hours"
  }
}

// ‚ö†Ô∏è  ATTENTION: La shard key est IMMUABLE
// (Avant MongoDB 5.0, impossible de changer)
// (MongoDB 5.0+: refineCollectionShardKey possible mais co√ªteux)
```

#### 4. config.chunks

```javascript
// Structure d'un document chunk (LE PLUS CRITIQUE)
{
  "_id": ObjectId("674f8e9a1234567890abcdef"),

  // Namespace de la collection
  "ns": "ecommerce.orders",  // DEPRECATED en 5.0, remplac√© par uuid
  "uuid": UUID("a1b2c3d4-..."),  // UUID de la collection (5.0+)

  // Bornes du chunk (bas√©es sur la shard key)
  "min": {
    "customer_id": 1000,
    "order_date": ISODate("2024-01-01T00:00:00Z")
  },
  "max": {
    "customer_id": 2000,
    "order_date": ISODate("2024-02-01T00:00:00Z")
  },

  // Shard h√©bergeant ce chunk
  "shard": "shard-a-replica-set",

  // Versioning (pour d√©tecter migrations concurrentes)
  "lastmod": Timestamp(1701456789, 5),
  "lastmodEpoch": ObjectId("674f8e9a1234567890abcdef"),

  // Historique des migrations
  "history": [
    {
      "validAfter": Timestamp(1701456789, 5),
      "shard": "shard-a-replica-set"
    },
    {
      "validAfter": Timestamp(1701356789, 2),
      "shard": "shard-b-replica-set"  // Migration pr√©c√©dente
    }
  ],

  // Flag jumbo (chunk non-splittable)
  "jumbo": false,  // Si true ‚Üí probl√®me!

  // Taille estim√©e (approximative)
  "estimatedDataSize": NumberLong(134217728)  // ~128 MB
}

// Index critiques (performance routage):
// - { ns: 1, min: 1 }  (trouve chunk par range)
// - { ns: 1, shard: 1, min: 1 }  (liste chunks d'un shard)
// - { uuid: 1, min: 1 }  (MongoDB 5.0+)

// Nombre de chunks:
// - Petit cluster: 100-1,000 chunks
// - Moyen cluster: 1,000-100,000 chunks
// - Gros cluster: 100,000-1,000,000+ chunks
```

#### 5. config.locks

```javascript
// Structure d'un verrou distribu√©
{
  "_id": "ecommerce.orders-moveChunk",  // Lock name
  "state": 2,  // 0 = unlocked, 1 = contended, 2 = locked
  "process": "ConfigServer",
  "ts": ObjectId("674f8e9a1234567890abcdef"),
  "when": ISODate("2024-12-08T10:30:00Z"),
  "who": "shard-a-replica-set:Balancer:12345:1701456789:0",
  "why": "Migrating chunk [1000, 2000) from shard-a to shard-b"
}

// Types de locks:
// - balancer: Lock global du balancer
// - <db>.<collection>-moveChunk: Lock par migration
// - <db>-movePrimary: Lock pour movePrimary
// - createDatabase: Lock cr√©ation de DB

// Probl√®me courant: Lock orphelin
// Sympt√¥me: Balancer bloqu√©, migrations impossibles
// Diagnostic:
db.getSiblingDB("config").locks.find({ state: 2 })

// Solution: Supprimer lock orphelin (AVEC PR√âCAUTION!)
db.getSiblingDB("config").locks.remove({
  _id: "balancer",
  state: 2,
  when: { $lt: ISODate("2024-12-08T09:00:00Z") }  // > 1h ancien
})
```

#### 6. config.changelog

```javascript
// Journal d'audit des op√©rations cluster
{
  "_id": "shard-a-2024-12-08T10:30:00.123-674f8e9a1234567890abcdef",
  "server": "shard-a-replica-set",
  "clientAddr": "192.168.1.10:54321",
  "time": ISODate("2024-12-08T10:30:00.123Z"),

  // Type d'√©v√©nement
  "what": "moveChunk.commit",

  // Namespace
  "ns": "ecommerce.orders",

  // D√©tails de l'op√©ration
  "details": {
    "min": { "customer_id": 1000, "order_date": ISODate("2024-01-01") },
    "max": { "customer_id": 2000, "order_date": ISODate("2024-02-01") },
    "from": "shard-a-replica-set",
    "to": "shard-b-replica-set",

    // M√©triques de performance
    "cloneTime": 1500,    // ms pour cloner les donn√©es
    "catchUpTime": 200,   // ms pour sync incrementale
    "deleteTime": 100,    // ms pour delete sur source

    "note": "Success"
  }
}

// Types d'√©v√©nements:
// - moveChunk.start / .commit / .error
// - split / merge
// - addShard / removeShard
// - shardCollection / dropCollection
// - createDatabase / dropDatabase

// Taille: Rolling collection (derniers 7-30 jours)
// Utilit√©: Audit, troubleshooting, analyse performance

// Exemple: Identifier migrations lentes
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.commit",
  "details.cloneTime": { $gt: 5000 }  // > 5 secondes
}).sort({ time: -1 }).limit(10)
```

#### 7. config.settings

```javascript
// Configuration globale du cluster
{
  "_id": "chunksize",
  "value": 128  // Taille chunk en MB (64, 128, 256, 512, 1024)
}

{
  "_id": "balancer",
  "mode": "full",  // "full", "off", "window"
  "stopped": false,

  // Fen√™tre de balancing (optionnelle)
  "activeWindow": {
    "start": "01:00",  // HH:MM
    "stop": "05:00"
  }
}

{
  "_id": "autosplit",
  "enabled": true  // Auto-split des chunks > chunksize
}

// Commandes pour modifier:
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 256 } },
  { upsert: true }
)
```

## D√©ploiement d'un Config Server Replica Set

### Configuration Initiale

```bash
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# D√âPLOIEMENT CONFIG SERVER REPLICA SET (3 MEMBRES)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Serveur 1: configsvr-1 (192.168.1.30)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# 1. Cr√©er r√©pertoires
sudo mkdir -p /data/configdb
sudo mkdir -p /var/log/mongodb
sudo chown -R mongodb:mongodb /data/configdb /var/log/mongodb

# 2. Fichier de configuration: /etc/mongod-config.conf
cat > /etc/mongod-config.conf << 'EOF'
# mongod-config.conf - Config Server

# Network
net:
  port: 27019
  bindIp: 0.0.0.0  # Production: IP sp√©cifique
  maxIncomingConnections: 1000

# Storage
storage:
  dbPath: /data/configdb
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 2  # 2 GB cache
    collectionConfig:
      blockCompressor: snappy
    indexConfig:
      prefixCompression: true

# Replication
replication:
  replSetName: configReplSet
  oplogSizeMB: 10240  # 10 GB oplog

# Sharding
sharding:
  clusterRole: configsvr  # ‚ö†Ô∏è  CRITIQUE

# Security (Production)
security:
  authorization: enabled
  keyFile: /etc/mongodb-keyfile

# Logging
systemLog:
  destination: file
  path: /var/log/mongodb/configsvr.log
  logAppend: true
  logRotate: reopen

# Process Management
processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/configsvr.pid
  timeZoneInfo: /usr/share/zoneinfo

# Profiling
operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 100
EOF

# 3. D√©marrer le processus
sudo mongod --config /etc/mongod-config.conf

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Serveur 2 et 3: R√©p√©ter les √©tapes ci-dessus
# configsvr-2 (192.168.1.31)
# configsvr-3 (192.168.1.32)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Initialisation du Replica Set
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Se connecter au premier config server
mongosh --host 192.168.1.30 --port 27019

# Initialiser le Replica Set
rs.initiate({
  _id: "configReplSet",
  configsvr: true,  // ‚ö†Ô∏è  OBLIGATOIRE pour config servers
  members: [
    {
      _id: 0,
      host: "192.168.1.30:27019",
      priority: 2  // Pr√©f√©rence l√©g√®re pour Primary
    },
    {
      _id: 1,
      host: "192.168.1.31:27019",
      priority: 1
    },
    {
      _id: 2,
      host: "192.168.1.32:27019",
      priority: 1
    }
  ],
  settings: {
    heartbeatIntervalMillis: 2000,  // Heartbeat toutes les 2s
    electionTimeoutMillis: 10000,   // Timeout √©lection 10s
    catchUpTimeoutMillis: -1        // Pas de timeout catch-up
  }
})

# V√©rification
rs.status()
rs.conf()

# Output attendu:
# {
#   "set": "configReplSet",
#   "members": [
#     { ..., "stateStr": "PRIMARY", "health": 1 },
#     { ..., "stateStr": "SECONDARY", "health": 1 },
#     { ..., "stateStr": "SECONDARY", "health": 1 }
#   ]
# }
```

### Configuration Haute Disponibilit√© (5 Membres)

```javascript
// Pour clusters critiques: 5 membres CSRS

rs.initiate({
  _id: "configReplSet",
  configsvr: true,
  members: [
    { _id: 0, host: "configsvr-1:27019", priority: 2 },
    { _id: 1, host: "configsvr-2:27019", priority: 1 },
    { _id: 2, host: "configsvr-3:27019", priority: 1 },
    { _id: 3, host: "configsvr-4:27019", priority: 0 },  // Non-electable
    { _id: 4, host: "configsvr-5:27019", priority: 0 }   // Non-electable
  ]
})

// Avantages 5 membres:
// - Tol√©rance: 2 pannes simultan√©es (majorit√© = 3/5)
// - Maintenance: Arr√™ter 2 membres sans impact
// - √âlections: 3 membres votants + 2 backups

// Topologie recommand√©e:
// - 3 membres votants (priority > 0) dans DC principal
// - 2 membres non-votants (priority = 0) dans DC secondaire
```

### Distribution G√©ographique

```javascript
// Config Servers multi-r√©gions (DR)

rs.initiate({
  _id: "configReplSet",
  configsvr: true,
  members: [
    // R√©gion Primaire (EU)
    {
      _id: 0,
      host: "configsvr-eu-1:27019",
      priority: 2,
      tags: { region: "EU", dc: "paris" }
    },
    {
      _id: 1,
      host: "configsvr-eu-2:27019",
      priority: 1,
      tags: { region: "EU", dc: "paris" }
    },

    // R√©gion Secondaire (US) - Disaster Recovery
    {
      _id: 2,
      host: "configsvr-us-1:27019",
      priority: 0,  // Jamais Primary (latence)
      tags: { region: "US", dc: "virginia" }
    },

    // R√©gion Tertiaire (APAC) - Witness
    {
      _id: 3,
      host: "configsvr-apac-1:27019",
      priority: 0,
      votes: 1,  // Vote mais jamais Primary
      tags: { region: "APAC", dc: "singapore" }
    }
  ],
  settings: {
    // Write Concern pour latence cross-r√©gion
    getLastErrorDefaults: {
      w: "majority",
      wtimeout: 5000
    }
  }
})

// Avantages:
// ‚úÖ Survie √† panne r√©gionale compl√®te
// ‚úÖ Latence optimis√©e (Primary en EU pour apps EU)

// Inconv√©nients:
// ‚ùå Latence replication cross-r√©gion (100-200ms)
// ‚ùå Co√ªts r√©seau inter-r√©gion
// ‚ùå Complexit√© op√©rationnelle
```

## Op√©rations et Maintenance

### Backup des Config Servers

```bash
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# STRAT√âGIES DE BACKUP CONFIG SERVERS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# M√©thode 1: mongodump (Recommand√©e)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Backup de la database config uniquement
mongodump \
  --host configReplSet/configsvr-1:27019,configsvr-2:27019,configsvr-3:27019 \
  --readPreference secondary \
  --db config \
  --out /backup/config-$(date +%Y%m%d-%H%M%S) \
  --gzip

# Taille typique: 100 MB - 2 GB compress√©
# Dur√©e: 1-5 minutes

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# M√©thode 2: Snapshot Filesystem (Production)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Sur un Secondary (pas de lock de la DB)

# 1. Se connecter au Secondary
mongosh --host configsvr-2:27019

# 2. Obtenir un point de coh√©rence
db.fsyncLock()

# 3. Snapshot du filesystem
# (Commande d√©pend du syst√®me: LVM, ZFS, cloud snapshot)
# Exemple LVM:
sudo lvcreate --size 50G --snapshot --name config-snap /dev/vg0/config-lv

# 4. Unlock
db.fsyncUnlock()

# 5. Backup du snapshot
sudo dd if=/dev/vg0/config-snap | gzip > /backup/config-snapshot.gz

# 6. Supprimer snapshot
sudo lvremove -f /dev/vg0/config-snap

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# M√©thode 3: Continuous Backup (Ops Manager / Atlas)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# MongoDB Ops Manager:
# - Backup continu via oplog tailing
# - Point-in-Time Recovery (PITR)
# - Snapshots automatiques toutes les 6h

# MongoDB Atlas:
# - Backup continu natif
# - Retention configurable (1-365 jours)
# - Restore en quelques clics
```

### Restore des Config Servers

```bash
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DISASTER RECOVERY: RESTORE CONFIG SERVERS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# ‚ö†Ô∏è  CRITIQUE: Config Servers perdus = CLUSTER DOWN
# Proc√©dure √† tester r√©guli√®rement!

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Sc√©nario: Perte compl√®te des 3 Config Servers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# 1. Arr√™ter TOUS les mongos et le balancer
# (Connexion √† chaque mongos)
use admin
db.shutdownServer()

# 2. Provisionner 3 nouveaux serveurs Config

# 3. Restore sur le premier membre
mongorestore \
  --host configsvr-1:27019 \
  --db config \
  --gzip \
  /backup/config-20241208-100000/config

# 4. Initialiser le nouveau CSRS
mongosh --host configsvr-1:27019
rs.initiate({
  _id: "configReplSet",
  configsvr: true,
  members: [
    { _id: 0, host: "configsvr-1:27019" },
    { _id: 1, host: "configsvr-2:27019" },
    { _id: 2, host: "configsvr-3:27019" }
  ]
})

# 5. Attendre synchronisation compl√®te
rs.status()
# V√©rifier: tous membres "SECONDARY" sauf Primary

# 6. Red√©marrer mongos
mongos --configdb configReplSet/configsvr-1:27019,configsvr-2:27019,configsvr-3:27019 ...

# 7. V√©rifier int√©grit√© du cluster
sh.status()
db.getSiblingDB("config").chunks.count()
db.getSiblingDB("config").collections.count()

# 8. R√©activer le balancer (si d√©sactiv√©)
sh.startBalancer()

# ‚ö†Ô∏è  VALIDATION CRITIQUE:
# - V√©rifier que tous les shards sont visibles
# - Tester une requ√™te sur chaque collection shard√©e
# - V√©rifier l'int√©grit√© des metadata (pas de chunks orphelins)
```

### Upgrade des Config Servers

```bash
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# UPGRADE CONFIG SERVERS (Rolling Upgrade)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Exemple: MongoDB 6.0 ‚Üí 7.0

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phase 1: Upgrade Secondaries
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Sur configsvr-2 (Secondary):

# 1. Arr√™ter le processus
mongosh --host configsvr-2:27019
use admin
db.shutdownServer()

# 2. Upgrade binaires
sudo apt-get update
sudo apt-get install -y mongodb-org=7.0.0

# 3. Red√©marrer
sudo mongod --config /etc/mongod-config.conf

# 4. V√©rifier r√©int√©gration
mongosh --host configsvr-1:27019
rs.status()
# Attendre: configsvr-2 state "SECONDARY"

# 5. R√©p√©ter pour configsvr-3

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phase 2: Upgrade Primary
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Sur configsvr-1 (Primary):

# 1. Step down
mongosh --host configsvr-1:27019
rs.stepDown(120)  // 2 minutes

# Attendre nouvelle √©lection
rs.status()

# 2. Upgrade (maintenant Secondary)
use admin
db.shutdownServer()

sudo apt-get install -y mongodb-org=7.0.0
sudo mongod --config /etc/mongod-config.conf

# 3. V√©rifier
rs.status()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Phase 3: Upgrade Feature Compatibility Version
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# Se connecter au Primary
mongosh --host configReplSet/configsvr-1:27019,configsvr-2:27019,configsvr-3:27019

# V√©rifier version actuelle
db.adminCommand({ getParameter: 1, featureCompatibilityVersion: 1 })
// { featureCompatibilityVersion: { version: "6.0" } }

# Upgrade FCV (IRR√âVERSIBLE!)
db.adminCommand({ setFeatureCompatibilityVersion: "7.0" })

# V√©rifier
db.adminCommand({ getParameter: 1, featureCompatibilityVersion: 1 })
// { featureCompatibilityVersion: { version: "7.0" } }

# ‚ö†Ô∏è  ATTENTION: Apr√®s upgrade FCV, downgrade impossible sans restore
```

## Strat√©gies Anti-Patterns

### üö´ Anti-Pattern 1 : Config Servers Non-R√©pliqu√©s

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ùå ANTI-PATTERN                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Config Server: 1 seul membre          ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îÇ
‚îÇ  ‚îÇ Config (solo)‚îÇ                      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ
‚îÇ        ‚Üì                               ‚îÇ
‚îÇ   Si down ‚Üí CLUSTER INUTILISABLE       ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  Probl√®mes:                            ‚îÇ
‚îÇ  - SPOF critique                       ‚îÇ
‚îÇ  - Impossibilit√© de maintenance        ‚îÇ
‚îÇ  - Aucune tol√©rance aux pannes         ‚îÇ
‚îÇ  - Pas de backup hot                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Impact :**
```
Config Server down:
‚Üí Mongos: Mode READ-ONLY
‚Üí Balancer: ARR√äT√â
‚Üí Migrations: IMPOSSIBLES
‚Üí Admin ops: IMPOSSIBLES (addShard, shardCollection, etc.)
‚Üí Applications: Lecture seule, pas de nouvelles √©critures
```

**Solution :**
```
‚úÖ MINIMUM ABSOLU: 3 membres CSRS
‚úÖ Production critique: 5 membres CSRS
‚úÖ Multi-r√©gion: 3-5 membres distribu√©s g√©ographiquement
```

### üö´ Anti-Pattern 2 : Sous-Dimensionnement

```javascript
// ‚ùå ANTI-PATTERN: Config Servers avec ressources insuffisantes

// Configuration inad√©quate:
// - RAM: 1 GB  (trop peu pour cache)
// - CPU: 1 core (bottleneck)
// - Disk: HDD  (I/O lent)

// Sympt√¥mes:
// - Mongos cache refresh lent (> 5s)
// - Balancer rounds trop longs
// - Timeouts sur operations metadata
// - √âlections fr√©quentes (heartbeat timeout)

// Monitoring:
db.serverStatus().wiredTiger.cache
// "bytes currently in the cache": proche de la limite
// "pages evicted by application threads": √©lev√©
```

**Solution :**
```yaml
‚úÖ Configuration Minimale (Production):
  RAM: 4 GB (cache WT: 2 GB)
  CPU: 2 cores
  Disk: SSD, 1000+ IOPS
  Network: 1 Gbps

‚úÖ Configuration Recommand√©e (Gros Cluster):
  RAM: 8 GB (cache WT: 4 GB)
  CPU: 4 cores
  Disk: NVMe, 5000+ IOPS
  Network: 10 Gbps
```

### üö´ Anti-Pattern 3 : Ignorer les Backups

```javascript
// ‚ùå ANTI-PATTERN: Pas de backup des Config Servers

// "Les metadata sont petites, pas besoin de backup"
// "En cas de probl√®me, on recr√©√© le cluster"

// FAUX! En cas de perte:
// 1. Perte du mapping chunks ‚Üí shards
//    ‚Üí Impossible de savoir quelles donn√©es sont o√π
// 2. Perte de la shard key des collections
//    ‚Üí Impossible de recr√©er le sharding identique
// 3. Perte des zones et tags
//    ‚Üí Perte de la topologie de distribution
// 4. Perte de l'historique (changelog)
//    ‚Üí Impossible d'audit ou troubleshooting

// Temps de reconstruction sans backup:
// - Petit cluster (10 collections, 100 chunks): ~2-4 heures
// - Gros cluster (100 collections, 100k chunks): PLUSIEURS JOURS
//   (n√©cessite reshard complet de toutes les collections)
```

**Solution :**
```bash
‚úÖ PATTERN: Backup automatique quotidien minimum

# Cron job (tous les jours √† 2h du matin)
0 2 * * * /usr/local/bin/backup-config-servers.sh

# Script backup-config-servers.sh:
#!/bin/bash
BACKUP_DIR=/backup/config-$(date +\%Y\%m\%d)
mongodump \
  --host configReplSet/configsvr-1:27019,configsvr-2:27019,configsvr-3:27019 \
  --readPreference secondary \
  --db config \
  --out $BACKUP_DIR \
  --gzip

# R√©tention: 30 jours minimum
# Test de restore: mensuel
```

### üö´ Anti-Pattern 4 : Co-Localisation avec d'Autres Composants

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ùå ANTI-PATTERN                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Serveur 1:                            ‚îÇ
‚îÇ  ‚îú‚îÄ Config Server (27019)              ‚îÇ
‚îÇ  ‚îú‚îÄ Shard Primary (27018)              ‚îÇ
‚îÇ  ‚îî‚îÄ Mongos (27017)                     ‚îÇ
‚îÇ                                        ‚îÇ
‚îÇ  Probl√®mes:                            ‚îÇ
‚îÇ  - Contention ressources (RAM, CPU)    ‚îÇ
‚îÇ  - I/O competition                     ‚îÇ
‚îÇ  - Perte multiple si serveur down      ‚îÇ
‚îÇ  - Difficult√© troubleshooting          ‚îÇ
‚îÇ  - Impossibilit√© scaling ind√©pendant   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Solution :**
```
‚úÖ PATTERN: Isolation des composants

Serveurs Config (d√©di√©s):
‚îú‚îÄ configsvr-1: Config Server uniquement
‚îú‚îÄ configsvr-2: Config Server uniquement
‚îî‚îÄ configsvr-3: Config Server uniquement

Serveurs Shards (d√©di√©s):
‚îú‚îÄ shard-a-1, shard-a-2, shard-a-3
‚îú‚îÄ shard-b-1, shard-b-2, shard-b-3
‚îî‚îÄ ...

Serveurs Mongos (d√©di√©s ou co-localis√©s avec apps):
‚îú‚îÄ mongos-1: Mongos uniquement (ou + app)
‚îî‚îÄ mongos-2: Mongos uniquement (ou + app)
```

### üö´ Anti-Pattern 5 : N√©gliger le Monitoring

```javascript
// ‚ùå ANTI-PATTERN: Pas de monitoring sp√©cifique Config Servers

// "Les Config Servers sont stables, pas besoin de monitoring"

// FAUX! Probl√®mes silencieux possibles:
// 1. Oplog trop petit ‚Üí Secondary lag
// 2. Locks orphelins ‚Üí Balancer bloqu√©
// 3. Cache thrashing ‚Üí Performance d√©grad√©e
// 4. Disque plein ‚Üí Cluster freeze
// 5. Elections fr√©quentes ‚Üí Instabilit√©
```

**Solution :**
```javascript
// ‚úÖ PATTERN: Monitoring complet Config Servers

// M√©triques critiques √† monitorer:

// 1. √âtat du Replica Set
rs.status()
// Alerter si:
// - Membre DOWN (health: 0)
// - Lag replication > 10 secondes
// - Elections fr√©quentes (> 1/jour)

// 2. Taille database config
db.getSiblingDB("config").stats().dataSize
// Alerter si croissance anormale (> 10% par semaine)

// 3. Nombre de chunks
db.getSiblingDB("config").chunks.count()
// Suivre tendance (indicateur de croissance)

// 4. Locks actifs
db.getSiblingDB("config").locks.find({ state: 2 }).count()
// Alerter si locks > 5 minutes

// 5. Oplog usage
rs.printReplicationInfo()
// "oplog first event time"
// Alerter si < 24h de r√©tention

// 6. WiredTiger cache
db.serverStatus().wiredTiger.cache
// "pages evicted by application threads"
// Alerter si > 1000/sec

// 7. Connexions
db.serverStatus().connections
// "current" / "available"
// Alerter si > 80% utilis√©

// 8. Disk space
db.serverStatus().storageSize
// Alerter si > 80% utilis√©
```

## Monitoring et Troubleshooting

### Diagnostics Communs

```javascript
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// TROUBLESHOOTING CONFIG SERVERS
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Probl√®me 1: Balancer bloqu√©
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Sympt√¥me:
sh.isBalancerRunning()  // false (mais devrait √™tre true)
sh.status()  // "balancer locked" ou "waiting for lock"

// Diagnostic:
db.getSiblingDB("config").locks.find({ _id: "balancer" }).pretty()

// Si lock orphelin (when > 1h):
{
  "_id": "balancer",
  "state": 2,
  "when": ISODate("2024-12-08T02:00:00Z"),  // 10h dans le pass√©!
  "who": "ConfigServer:12345:1701456789:0"
}

// Solution:
db.getSiblingDB("config").locks.deleteOne({
  _id: "balancer",
  state: 2
})

sh.startBalancer()

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Probl√®me 2: Config Server Primary injoignable
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Sympt√¥me:
// mongos logs: "Unable to reach primary of config server"
// Applications: WriteErrors

// Diagnostic:
rs.status()
// V√©rifier: Y a-t-il un Primary?

// Si pas de Primary (elections continues):
// - V√©rifier r√©seau entre membres (ping, telnet)
// - V√©rifier logs: /var/log/mongodb/configsvr.log
// - V√©rifier majorit√© disponible (2/3 members up)

// Solution si membre bloqu√©:
// Sur le membre probl√©matique:
rs.stepDown(0)  // Force step down
// ou
db.adminCommand({ replSetStepDown: 0, force: true })

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Probl√®me 3: Metadata corruption
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Sympt√¥me:
// mongos logs: "chunk metadata inconsistent"
// Requ√™tes: erreurs al√©atoires "chunk not found"

// Diagnostic:
db.getSiblingDB("config").chunks.find({ ns: "db.collection" }).count()
// Comparer avec:
db.getSiblingDB("config").collections.findOne({ _id: "db.collection" })

// V√©rifier chunks orphelins (shard n'existe plus):
db.getSiblingDB("config").chunks.aggregate([
  {
    $lookup: {
      from: "shards",
      localField: "shard",
      foreignField: "_id",
      as: "shard_info"
    }
  },
  {
    $match: { shard_info: { $size: 0 } }
  }
])

// Solution: Restore depuis backup r√©cent (dernier recours)

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Probl√®me 4: Config Server disque plein
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Sympt√¥me:
// Logs: "No space left on device"
// Cluster: Freeze complet

// Diagnostic:
df -h /data/configdb
// Usage > 90%

// Causes fr√©quentes:
// 1. Changelog trop gros
db.getSiblingDB("config").changelog.stats().size

// 2. Oplog trop gros
db.getSiblingDB("local").oplog.rs.stats().size

// Solution imm√©diate:
// 1. Nettoyer changelog ancien
db.getSiblingDB("config").changelog.deleteMany({
  time: { $lt: ISODate("2024-11-08") }  // > 30 jours
})

// 2. Compact (si n√©cessaire, sur Secondary)
use config
db.runCommand({ compact: "changelog", force: true })

// Solution long terme: Augmenter taille disque
```

## R√©sum√©

Les **Config Servers** sont le cerveau du cluster shard√© MongoDB :

**Architecture :**
- Replica Set d√©di√© (CSRS) avec 3+ membres
- Stockage exclusif des m√©tadonn√©es (database `config`)
- Haute disponibilit√© critique (cluster down si CSRS down)

**R√¥le :**
- Mapping chunks ‚Üí shards (config.chunks)
- Shard keys des collections (config.collections)
- Verrous distribu√©s (config.locks)
- Historique des op√©rations (config.changelog)

**D√©ploiement :**
- Minimum absolu : 3 membres
- Production critique : 5 membres
- Multi-r√©gion : Distribution g√©ographique

**Op√©rations critiques :**
- ‚úÖ Backup quotidien automatique (OBLIGATOIRE)
- ‚úÖ Test de restore mensuel
- ‚úÖ Monitoring continu (√©tat RS, locks, taille)
- ‚úÖ Rolling upgrade sans downtime

**Anti-patterns √† √©viter :**
- ‚ùå Config Server non-r√©pliqu√© (SPOF)
- ‚ùå Sous-dimensionnement ressources
- ‚ùå Pas de backup (risque data loss)
- ‚ùå Co-localisation avec autres composants
- ‚ùå N√©gliger le monitoring

**R√®gle d'or :**
> Si les Config Servers sont indisponibles, le cluster entier est en lecture seule. Leur fiabilit√© conditionne la disponibilit√© du cluster.

La gestion rigoureuse des Config Servers, avec backups r√©guliers, monitoring proactif et dimensionnement appropri√©, est essentielle pour un cluster shard√© stable et fiable en production.

---


‚è≠Ô∏è [Mongos (Query Routers)](/10-sharding/02.3-mongos-query-routers.md)
