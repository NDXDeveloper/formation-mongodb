ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 10.4.1 Range Sharding

## Introduction

Le **Range Sharding** (partitionnement par plage) est le type de sharding par dÃ©faut de MongoDB. Il divise les donnÃ©es en chunks basÃ©s sur des **plages contiguÃ«s de valeurs** de la shard key. Ce mÃ©canisme offre une excellente localitÃ© des donnÃ©es et des performances optimales pour les requÃªtes par plage, mais nÃ©cessite une conception soigneuse pour Ã©viter les hotspots.

Cette section explore en profondeur le fonctionnement interne du Range Sharding, ses stratÃ©gies d'optimisation, et les patterns Ã©prouvÃ©s pour une utilisation en production.

## Principe de Fonctionnement DÃ©taillÃ©

### CrÃ©ation des Chunks

```javascript
// Activation du Range Sharding
sh.shardCollection("ecommerce.orders", { customer_id: 1 })

// MongoDB crÃ©e initialement UN SEUL chunk:
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ Chunk Initial                          â”‚
// â”‚ min: { customer_id: MinKey }           â”‚
// â”‚ max: { customer_id: MaxKey }           â”‚
// â”‚ shard: shard-a (primary shard de la DB)â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

// Au fur et Ã  mesure des insertions, le chunk grandit
// Quand il atteint 128 MB (chunkSize par dÃ©faut):
// â†’ Chunk Split dÃ©clenchÃ© automatiquement
```

### MÃ©canisme de Split

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PROCESSUS DE CHUNK SPLIT                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AVANT SPLIT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk A                                              â”‚
â”‚ Range: [0, 10000)                                    â”‚
â”‚ Shard: shard-a                                       â”‚
â”‚ Size: 140 MB (> 128 MB threshold)                    â”‚
â”‚ Documents: ~1,000,000                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰TAPE 1: DÃ©tection du Besoin de Split
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ MongoDB surveille la taille des chunks
â€¢ DÃ©tecte: Chunk A > chunkSize (128 MB)
â€¢ DÃ©cision: Split requis

Ã‰TAPE 2: Calcul du Split Point (MÃ©diane)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Analyse la distribution des valeurs de shard key
â€¢ Calcule la mÃ©diane: splitPoint = 5000
â€¢ Objectif: Diviser en deux chunks ~Ã©gaux

Ã‰TAPE 3: CrÃ©ation de Deux Nouveaux Chunks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk A-1                  â”‚ Chunk A-2                 â”‚
â”‚ Range: [0, 5000)           â”‚ Range: [5000, 10000)      â”‚
â”‚ Shard: shard-a             â”‚ Shard: shard-a            â”‚
â”‚ Size: ~70 MB               â”‚ Size: ~70 MB              â”‚
â”‚ Documents: ~500,000        â”‚ Documents: ~500,000       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰TAPE 4: Update Metadata (config.chunks)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Supprime ancien chunk de config.chunks
â€¢ Insert deux nouveaux chunks
â€¢ Update version numbers
â€¢ DurÃ©e: ~10-50 ms

APRÃˆS SPLIT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Les deux chunks restent sur shard-a (pas de migration)
â€¢ Balancer dÃ©tectera Ã©ventuellement le dÃ©sÃ©quilibre
â€¢ Migration d'un chunk vers autre shard si nÃ©cessaire
```

### Visualisation de la Distribution

```
Range Sharding: customer_id (0 Ã  100,000)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ã‰tat Initial (T0):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 100,000
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Chunk Unique (140 MB)                    â”‚
â”‚              Shard A                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AprÃ¨s Premier Split (T1):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 50,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 100,000
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 1 (70 MB)            â”‚ Chunk 2 (70 MB)          â”‚
â”‚ Shard A                    â”‚ Shard A                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AprÃ¨s Balancing (T2):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 50,000 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 100,000
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 1 (70 MB)            â”‚ Chunk 2 (70 MB)          â”‚
â”‚ Shard A                    â”‚ Shard B â† MigrÃ©          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AprÃ¨s Croissance Continue (T3):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0 â”€â”€â”€ 25k â”€â”€â”€ 50k â”€â”€â”€ 75k â”€â”€â”€ 100k â”€â”€â”€ 125k â”€â”€â”€ 150k
â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¤
â”‚ C1   â”‚ C2   â”‚ C3   â”‚ C4   â”‚ C5   â”‚ C6   â”‚ C7   â”‚
â”‚ ShdA â”‚ ShdB â”‚ ShdC â”‚ ShdA â”‚ ShdB â”‚ ShdC â”‚ ShdA â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

Distribution Ã©quilibrÃ©e si:
â€¢ customer_id uniformÃ©ment rÃ©parti
â€¢ Pas de monotonie dans les insertions
```

## Routage des RequÃªtes

### RequÃªtes CiblÃ©es (Optimal)

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// REQUÃŠTE AVEC VALEUR EXACTE DE SHARD KEY
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

db.orders.find({ customer_id: 35000 })

// Mongos Logic:
// 1. Parse requÃªte: customer_id = 35000
// 2. Consulte routing table (cache):
//    Chunk contenant 35000 ?
//    â†’ Chunk 3: [25000, 50000) sur Shard C
// 3. Route UNIQUEMENT vers Shard C
// 4. Retourne rÃ©sultats

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mongos  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ Query: customer_id = 35000
     â”‚
     â”‚ â”Œâ”€ Routing Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ â”‚ [0, 25k)      â†’ Shard A      â”‚
     â”‚ â”‚ [25k, 50k)    â†’ Shard C  âœ“   â”‚
     â”‚ â”‚ [50k, 75k)    â†’ Shard A      â”‚
     â”‚ â”‚ ...                          â”‚
     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard A  â”‚  â”‚ Shard B  â”‚  â”‚ Shard C  â”‚
â”‚          â”‚  â”‚          â”‚  â”‚  âœ“âœ“âœ“âœ“    â”‚
â”‚          â”‚  â”‚          â”‚  â”‚ QUERIED  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Latence: ~5ms (1 shard uniquement)
```

### RequÃªtes par Plage (Efficace)

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// REQUÃŠTE RANGE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

db.orders.find({
  customer_id: { $gte: 30000, $lte: 60000 }
})

// Mongos Logic:
// 1. Parse: Range [30000, 60000]
// 2. Identifie chunks chevauchant cette plage:
//    Chunk 2: [25k, 50k)   â†’ Shard C âœ“
//    Chunk 3: [50k, 75k)   â†’ Shard A âœ“
// 3. Route vers Shards A et C (2 shards seulement)
// 4. Merge rÃ©sultats

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mongos  â”‚
â””â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
  â”‚      â”‚ Range: [30k, 60k]
  â”‚      â”‚
  â–¼      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard A  â”‚  â”‚ Shard B  â”‚  â”‚ Shard C  â”‚
â”‚  âœ“âœ“âœ“âœ“    â”‚  â”‚          â”‚  â”‚  âœ“âœ“âœ“âœ“    â”‚
â”‚ [50-60k] â”‚  â”‚          â”‚  â”‚ [30-50k] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Latence: ~10-15ms (2 shards + merge)

Vs Hashed Sharding (mÃªme requÃªte):
â†’ TOUS les shards interrogÃ©s (scatter-gather)
â†’ Latence: ~50-100ms (N shards + merge complexe)

Range Sharding = 3-10Ã— plus rapide pour range queries
```

### RequÃªtes $in (Partiellement CiblÃ©e)

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// REQUÃŠTE $in
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

db.orders.find({
  customer_id: { $in: [1000, 35000, 55000, 90000] }
})

// Mongos Logic:
// 1. Pour chaque valeur, identifie chunk:
//    1000   â†’ Chunk 1: [0, 25k)     â†’ Shard A
//    35000  â†’ Chunk 2: [25k, 50k)   â†’ Shard C
//    55000  â†’ Chunk 3: [50k, 75k)   â†’ Shard A
//    90000  â†’ Chunk 4: [75k, 100k)  â†’ Shard B
//
// 2. Route vers Shards A, B, C (3 shards)
// 3. Merge rÃ©sultats

Performance:
â€¢ Nombre de shards = min(valeurs distinctes, nb total shards)
â€¢ Meilleur que scatter-gather complet
â€¢ Moins bon que range query continue
```

## StratÃ©gies d'Optimisation

### StratÃ©gie 1 : Shard Key ComposÃ©e pour Ã‰viter Hotspots

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PROBLÃˆME: MONOTONE CROISSANT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âŒ MAUVAIS: Timestamp seul
sh.shardCollection("logs.events", { timestamp: 1 })

// Ã‰tat Ã  T0:
// Chunk actuel: [2024-12-08 10:00, MaxKey] â†’ Shard A
// TOUTES les insertions â†’ Shard A

// Ã‰tat Ã  T1 (1 heure plus tard):
// Chunk split: [2024-12-08 10:00, 2024-12-08 11:00] â†’ Shard A
//              [2024-12-08 11:00, MaxKey]            â†’ Shard A
// Nouvelles insertions â†’ ENCORE Shard A
// Hotspot persiste!

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SOLUTION: PRÃ‰FIXE NON-MONOTONE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// âœ… BON: PrÃ©fixe distribuÃ© + timestamp
sh.shardCollection("logs.events",
  { server_id: 1, timestamp: 1 }
)

// Avec 100 servers actifs:
// server-001 â†’ Chunks [..., timestamp] â†’ DistribuÃ©s
// server-002 â†’ Chunks [..., timestamp] â†’ DistribuÃ©s
// ...
// server-100 â†’ Chunks [..., timestamp] â†’ DistribuÃ©s

// Insertions simultanÃ©es de 100 servers
// â†’ 100 points d'insertion actifs
// â†’ Distribution sur plusieurs shards
// â†’ PAS de hotspot!

// Exemple de distribution:
// [srv-001, 2024-12-08 10:00] to [srv-001, 2024-12-08 11:00] â†’ Shard A
// [srv-002, 2024-12-08 10:00] to [srv-002, 2024-12-08 11:00] â†’ Shard B
// [srv-003, 2024-12-08 10:00] to [srv-003, 2024-12-08 11:00] â†’ Shard C
// ...

// RequÃªte efficace:
db.events.find({
  server_id: "srv-001",
  timestamp: { $gte: ISODate("2024-12-07") }
})
// â†’ CiblÃ© vers chunks de srv-001 uniquement
```

### StratÃ©gie 2 : Pre-Splitting pour Distribution Initiale

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PROBLÃˆME: CROISSANCE DEPUIS UN SEUL CHUNK
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Comportement par dÃ©faut:
sh.shardCollection("data.collection", { field: 1 })
// â†’ 1 chunk initial: [MinKey, MaxKey]
// â†’ Tous inserts dans 1 chunk
// â†’ Split progressif (lent si gros volume)

// Impact sur bulk insert:
// - 10M documents Ã  insÃ©rer
// - Tous vont initialement au mÃªme shard
// - Splits se produisent pendant insert (ralentissement)
// - Balancing se produit pendant insert (contentions)

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SOLUTION: PRE-SPLITTING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Avant bulk insert, crÃ©er chunks manuellement
sh.shardCollection("data.collection", { user_id: 1 })

// Pre-split en 10 chunks
for (let i = 0; i < 10; i++) {
  sh.splitAt("data.collection", { user_id: i * 10000 })
}

// RÃ©sultat:
// Chunk 1:  [MinKey, 10000)        â†’ Shard A
// Chunk 2:  [10000, 20000)         â†’ Shard A
// Chunk 3:  [20000, 30000)         â†’ Shard A
// ...
// Chunk 10: [90000, MaxKey]        â†’ Shard A

// Distribuer manuellement (optionnel):
sh.moveChunk("data.collection",
  { user_id: 10000 },
  "shard-b"
)
sh.moveChunk("data.collection",
  { user_id: 20000 },
  "shard-c"
)
// ...

// Avantages:
// âœ… Bulk insert immÃ©diatement distribuÃ©
// âœ… Pas de splits pendant insert (plus rapide)
// âœ… Pas de migrations pendant insert
// âœ… Performance optimale

// Script complet de pre-splitting:
function preSplitCollection(ns, shardKey, boundaries) {
  sh.shardCollection(ns, shardKey)

  boundaries.forEach(boundary => {
    sh.splitAt(ns, boundary)
  })

  print(`Pre-split complete: ${boundaries.length} chunks created`)
}

// Utilisation:
preSplitCollection(
  "analytics.events",
  { region: 1, timestamp: 1 },
  [
    { region: "EU", timestamp: MinKey },
    { region: "US", timestamp: MinKey },
    { region: "ASIA", timestamp: MinKey }
  ]
)
```

### StratÃ©gie 3 : Chunk Size Adaptatif

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// AJUSTER CHUNK SIZE SELON CAS D'USAGE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Configuration par dÃ©faut:
db.getSiblingDB("config").settings.findOne({ _id: "chunksize" })
// { _id: "chunksize", value: 128 }  // 128 MB

// ScÃ©narios et ajustements:

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SCÃ‰NARIO 1: Petits Documents, Haut Volume
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Exemple: Ã‰vÃ©nements IoT (1 KB par doc)
// Chunks de 128 MB = ~131,000 documents par chunk

// ProblÃ¨me:
// - Balancer migre 131k docs Ã  la fois
// - Migration longue (~30 secondes)
// - Impact sur performance pendant migration

// Solution: RÃ‰DUIRE chunk size
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 64 } },  // 64 MB au lieu de 128
  { upsert: true }
)

// RÃ©sultat:
// - Chunks plus petits = migrations plus rapides
// - Plus de chunks = meilleure granularitÃ© distribution
// - Trade-off: Plus de metadata (plus de chunks)

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SCÃ‰NARIO 2: Gros Documents, Faible Volume
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Exemple: Archives PDF (5 MB par doc)
// Chunks de 128 MB = ~25 documents par chunk

// ProblÃ¨me:
// - Beaucoup de chunks pour peu de donnÃ©es
// - Overhead metadata Ã©levÃ©
// - Balancer trop actif

// Solution: AUGMENTER chunk size
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 256 } },  // 256 MB ou plus
  { upsert: true }
)

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// RÃˆGLES EMPIRIQUES
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Document size < 10 KB:
//   â†’ chunkSize = 64 MB (plus de granularitÃ©)

// Document size 10-100 KB:
//   â†’ chunkSize = 128 MB (dÃ©faut, optimal)

// Document size > 100 KB:
//   â†’ chunkSize = 256-512 MB (moins de chunks)

// âš ï¸ NOTE: Le changement s'applique aux NOUVEAUX chunks
// Les chunks existants gardent leur taille
```

### StratÃ©gie 4 : Archivage par Drop de Chunks

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PATTERN: TIME-SERIES AVEC ARCHIVAGE AUTOMATIQUE
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Collection: logs avec rÃ©tention 90 jours
sh.shardCollection("archive.logs",
  { date: 1, server_id: 1 }
)

// Distribution typique aprÃ¨s 90 jours:
// [2024-09-01, 2024-09-02] â†’ Shard A (ancien)
// [2024-09-02, 2024-09-03] â†’ Shard B (ancien)
// ...
// [2024-11-27, 2024-11-28] â†’ Shard C (rÃ©cent)
// [2024-11-28, MaxKey]     â†’ Shard A (actif)

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// ARCHIVAGE PAR DROP DE CHUNKS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Script d'archivage quotidien:
function archiveOldLogs(retentionDays) {
  const cutoffDate = new Date()
  cutoffDate.setDate(cutoffDate.getDate() - retentionDays)

  // Identifier chunks Ã  supprimer
  const chunksToRemove = db.getSiblingDB("config").chunks.find({
    ns: "archive.logs",
    "max.date": { $lt: cutoffDate }
  }).toArray()

  print(`Found ${chunksToRemove.length} chunks to archive`)

  chunksToRemove.forEach(chunk => {
    print(`Removing chunk: ${chunk.min.date} to ${chunk.max.date}`)

    // Option 1: Delete documents (si backup dÃ©jÃ  fait)
    db.getSiblingDB("archive").logs.deleteMany({
      date: { $gte: chunk.min.date, $lt: chunk.max.date }
    })

    // Option 2: Archive vers storage S3/GCS puis delete
    // (NÃ©cessite script custom ou MongoDB Atlas Data Lake)
  })
}

// ExÃ©cution quotidienne (cron):
// 0 2 * * * mongosh --eval "archiveOldLogs(90)"

// Avantages Range Sharding pour archivage:
// âœ… Chunks ordonnÃ©s chronologiquement
// âœ… Suppression par chunk = rapide et propre
// âœ… Pas d'impact sur chunks rÃ©cents
// âœ… LibÃ©ration d'espace prÃ©visible

// Avec Hashed Sharding:
// âŒ Dates mÃ©langÃ©es dans tous chunks
// âŒ Impossible de drop chunks par date
// âŒ Delete scatter-gather sur tous shards
```

## Cas d'Usage DÃ©taillÃ©s

### Cas 1 : IoT Time-Series

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COLLECTION: MÃ©triques de capteurs IoT
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Document structure:
{
  _id: ObjectId("..."),
  device_id: "sensor-12345",
  timestamp: ISODate("2024-12-08T10:30:15.123Z"),
  metrics: {
    temperature: 22.5,
    humidity: 65,
    pressure: 1013
  },
  location: { lat: 48.8566, lon: 2.3522 }
}

// Volume: 100,000 devices Ã— 1 metric/sec = 100k inserts/sec
// RequÃªtes typiques:
// - 80%: Par device + time range (derniÃ¨res 24h)
// - 15%: Tous devices dans time range
// - 5%: Device spÃ©cifique, derniÃ¨re valeur

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SHARD KEY DESIGN
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

sh.shardCollection("iot.metrics",
  { device_id: 1, timestamp: 1 }
)

// Justification:
// âœ… device_id en prÃ©fixe â†’ Ã‰vite hotspot temporal
// âœ… timestamp en suffixe â†’ Range queries efficaces
// âœ… 100k devices â†’ Excellente distribution

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// DISTRIBUTION RÃ‰SULTANTE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// AprÃ¨s 1 mois d'opÃ©ration:
// device-00001: [timestamp_min, timestamp_max] â†’ ~10 chunks
// device-00002: [timestamp_min, timestamp_max] â†’ ~10 chunks
// ...
// device-100000: [timestamp_min, timestamp_max] â†’ ~10 chunks

// Total: ~1,000,000 chunks rÃ©partis sur shards

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// PERFORMANCE DES REQUÃŠTES
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// RequÃªte 1: DerniÃ¨res 24h d'un device (80% des requÃªtes)
db.metrics.find({
  device_id: "sensor-12345",
  timestamp: { $gte: ISODate("2024-12-07T10:00:00Z") }
}).sort({ timestamp: -1 })

// Routage: CIBLÃ‰
// - Chunks du device-12345 uniquement
// - Probablement 1-2 chunks (24h de donnÃ©es)
// - 1 seul shard interrogÃ©
// Latence: ~5-10ms âœ…

// RequÃªte 2: Tous devices, derniÃ¨re heure (15%)
db.metrics.find({
  timestamp: { $gte: ISODate("2024-12-08T09:00:00Z") }
})

// Routage: SCATTER-GATHER
// - Tous shards interrogÃ©s
// - Mais... chaque shard scan efficace (range sur timestamp)
// Latence: ~30-50ms âš ï¸ (acceptable)

// RequÃªte 3: DerniÃ¨re mÃ©trique d'un device (5%)
db.metrics.find({
  device_id: "sensor-12345"
}).sort({ timestamp: -1 }).limit(1)

// Routage: CIBLÃ‰
// - Chunks du device uniquement
// - Index (device_id, timestamp) utilisÃ©
// - TrÃ¨s efficace
// Latence: ~3-5ms âœ…âœ…

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// ARCHIVAGE AUTOMATIQUE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// RÃ©tention: 90 jours
// Script quotidien supprime chunks > 90 jours:
db.metrics.deleteMany({
  timestamp: { $lt: ISODate("2024-09-08") }
})

// GrÃ¢ce au Range Sharding:
// - Suppression ordonnÃ©e par timestamp
// - LibÃ©ration propre de l'espace
// - Pas d'impact sur donnÃ©es rÃ©centes
```

### Cas 2 : Logs d'Application avec Recherche

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COLLECTION: Logs applicatifs
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Document structure:
{
  _id: ObjectId("..."),
  timestamp: ISODate("2024-12-08T10:30:15.123Z"),
  level: "ERROR",
  service: "payment-api",
  message: "Connection timeout to database",
  request_id: "req-abc-123",
  user_id: "user-456",
  metadata: { ... }
}

// Volume: 1 TB/jour, rÃ©tention 30 jours
// RequÃªtes typiques:
// - 60%: Par service + time range
// - 20%: Par level (ERROR) + time range
// - 15%: Par request_id (debug)
// - 5%: Full-text search sur message

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SHARD KEY DESIGN
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

sh.shardCollection("logs.application",
  { service: 1, timestamp: 1 }
)

// Justification:
// âœ… service comme prÃ©fixe (20-30 services typiques)
// âœ… timestamp pour range queries temporelles
// âœ… Ã‰vite hotspot (services distribuÃ©s)

// Alternative considÃ©rÃ©e et rejetÃ©e:
// { timestamp: 1, service: 1 } âŒ
// â†’ Hotspot sur timestamp monotone

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// PRE-SPLITTING PAR SERVICE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const services = [
  "auth-api", "payment-api", "catalog-api",
  "user-api", "notification-api", "analytics-api"
]

services.forEach(service => {
  sh.splitAt("logs.application", {
    service: service,
    timestamp: MinKey
  })
})

// RÃ©sultat initial:
// [auth-api, MinKey] to [auth-api, MaxKey]       â†’ Shard A
// [payment-api, MinKey] to [payment-api, MaxKey] â†’ Shard B
// [catalog-api, MinKey] to [catalog-api, MaxKey] â†’ Shard C
// ...

// Au fil du temps, chunks se splitent sur timestamp:
// [auth-api, 2024-12-01] to [auth-api, 2024-12-02]
// [auth-api, 2024-12-02] to [auth-api, 2024-12-03]
// ...

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// PERFORMANCE DES REQUÃŠTES
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// RequÃªte 1: Logs d'un service (60%)
db.application.find({
  service: "payment-api",
  timestamp: { $gte: ISODate("2024-12-07") }
}).sort({ timestamp: -1 })

// Routage: PARTIELLEMENT CIBLÃ‰
// - Chunks du payment-api uniquement
// - Probablement 2-3 shards (selon distribution)
// Latence: ~15-20ms âœ…

// RequÃªte 2: Errors globales (20%)
db.application.find({
  level: "ERROR",
  timestamp: { $gte: ISODate("2024-12-08") }
})

// Routage: SCATTER-GATHER
// - Tous shards (level pas dans shard key)
// - Range efficace sur timestamp par shard
// Latence: ~40-60ms âš ï¸

// Optimisation possible: Index secondaire
db.application.createIndex({
  level: 1,
  timestamp: -1
})
// AmÃ©liore scan mais reste scatter-gather

// RequÃªte 3: Par request_id (15% - debug)
db.application.find({ request_id: "req-abc-123" })

// Routage: SCATTER-GATHER
// Index secondaire nÃ©cessaire:
db.application.createIndex({ request_id: 1 })
// Latence: ~30-50ms (acceptable pour debug)
```

### Cas 3 : E-Commerce Orders

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COLLECTION: Commandes e-commerce
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Document structure:
{
  _id: ObjectId("..."),
  order_id: "ORD-2024-123456",
  customer_id: 789012,
  order_date: ISODate("2024-12-08T10:30:00Z"),
  status: "completed",
  items: [
    { sku: "PROD-001", quantity: 2, price: 29.99 },
    { sku: "PROD-002", quantity: 1, price: 59.99 }
  ],
  shipping_address: { ... },
  total: 119.97
}

// Volume: 100M orders/an, croissance continue
// RequÃªtes typiques:
// - 70%: Par customer_id (mes commandes)
// - 15%: Par order_id (suivi commande)
// - 10%: Par date range (rapports)
// - 5%: Par status + date (admin)

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SHARD KEY DESIGN (Option A)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

sh.shardCollection("ecommerce.orders",
  { customer_id: 1, order_date: 1 }
)

// Justification:
// âœ… customer_id en prÃ©fixe (70% requÃªtes)
// âœ… order_date pour range queries
// âœ… Distribution dÃ©pend de la base client

// ConsidÃ©rations:
// âš ï¸ Si quelques gros clients (B2B) â†’ Skew possible
// âš ï¸ NÃ©cessite analyse distribution customer_id

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SHARD KEY DESIGN (Option B - Alternative)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

sh.shardCollection("ecommerce.orders",
  { customer_id: "hashed" }
)

// Justification:
// âœ… Distribution parfaitement uniforme
// âœ… Pas de skew mÃªme avec gros clients
// âŒ Range queries sur order_date â†’ scatter-gather

// Trade-off:
// - Meilleure distribution
// - Moins bonne performance range queries
// â†’ Choisir selon prioritÃ©s

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// AVEC RANGE SHARDING (Option A)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// RequÃªte 1: Commandes d'un client (70%)
db.orders.find({
  customer_id: 789012
}).sort({ order_date: -1 })

// Routage: CIBLÃ‰
// - Tous chunks du customer_id
// - Probablement 1 shard (client typique)
// Latence: ~5ms âœ…âœ…

// RequÃªte 2: DÃ©tail d'une commande (15%)
db.orders.findOne({ order_id: "ORD-2024-123456" })

// Routage: SCATTER-GATHER âŒ
// - order_id pas dans shard key
// Solution: Index secondaire
db.orders.createIndex({ order_id: 1 })
// Latence: ~20-30ms (acceptable)

// RequÃªte 3: Commandes rÃ©centes d'un client
db.orders.find({
  customer_id: 789012,
  order_date: { $gte: ISODate("2024-11-01") }
})

// Routage: TRÃˆS CIBLÃ‰ âœ…âœ…
// - customer_id + date range
// - Probablement 1-2 chunks
// - 1 shard
// Latence: ~3-5ms (optimal)

// RequÃªte 4: Rapports (toutes commandes du mois)
db.orders.find({
  order_date: {
    $gte: ISODate("2024-11-01"),
    $lt: ISODate("2024-12-01")
  }
})

// Routage: SCATTER-GATHER
// - Tous shards (customer_id manquant)
// - Mais range scan efficace par shard
// Latence: ~50-100ms (acceptable pour rapports)
```

## Anti-Patterns SpÃ©cifiques

### ğŸš« Anti-Pattern 1 : Monotone Sans PrÃ©fixe

```javascript
// âŒ ANTI-PATTERN CRITIQUE

sh.shardCollection("events.logs", { timestamp: 1 })

// Visualisation du problÃ¨me:
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// T0: 10:00 AM
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ Chunk Actif: [10:00, MaxKey]                     â”‚
// â”‚ Shard: shard-a                                   â”‚
// â”‚ Insertions: 100% sur shard-a  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
// Shards B, C, D: IDLE â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘

// T1: 11:00 AM (aprÃ¨s split)
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚[10:00, 11:00]         â”‚[11:00, MaxKey]           â”‚
// â”‚ shard-a (fermÃ©)       â”‚ shard-a (ACTIF)          â”‚
// â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
// Insertions: ENCORE 100% sur shard-a
// Shards B, C, D: ENCORE IDLE

// T2: 11:30 AM (aprÃ¨s migration)
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚[10:00, 11:00]         â”‚[11:00, MaxKey]           â”‚
// â”‚ shard-b (migrÃ©)       â”‚ shard-a (ACTIF)          â”‚
// â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘       â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
// Insertions: TOUJOURS 100% sur shard-a
// Hotspot se dÃ©place mais PERSISTE

// Impact mesurable sur 4 shards:
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ Shard A: 50,000 inserts/sec (CPU 100%, saturÃ©) â”‚
// â”‚ Shard B: 0 inserts/sec      (CPU 0%, idle)     â”‚
// â”‚ Shard C: 0 inserts/sec      (CPU 0%, idle)     â”‚
// â”‚ Shard D: 0 inserts/sec      (CPU 0%, idle)     â”‚
// â”‚                                                â”‚
// â”‚ Throughput cluster = 50k ops/sec               â”‚
// â”‚ (limitÃ© par 1 seul shard)                      â”‚
// â”‚                                                â”‚
// â”‚ Avec distribution uniforme thÃ©orique:          â”‚
// â”‚ Throughput cluster = 200k ops/sec (4 Ã— 50k)    â”‚
// â”‚                                                â”‚
// â”‚ â†’ Perte de 75% de capacitÃ©                     â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš« Anti-Pattern 2 : Shard Key Trop Large

```javascript
// âŒ ANTI-PATTERN

sh.shardCollection("documents.files",
  { region: 1, country: 1, city: 1, user_id: 1, doc_type: 1 }
)

// ProblÃ¨mes:

// 1. INDEX TRÃˆS LARGE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Index size: 5 champs Ã— ~20 bytes = ~100 bytes par document
// Pour 1B documents: 100 GB d'index
// vs shard key simple (2 champs): 40 GB

// 2. REQUÃŠTES DIFFICILES Ã€ CIBLER
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
db.files.find({ user_id: 12345 })
// â†’ SCATTER-GATHER (user_id n'est pas prÃ©fixe valide)
// â†’ Tous shards interrogÃ©s

db.files.find({ region: "EU" })
// â†’ SCATTER-GATHER (prÃ©fixe incomplet)

db.files.find({
  region: "EU",
  country: "FR",
  city: "Paris",
  user_id: 12345,
  doc_type: "invoice"
})
// â†’ CiblÃ©... mais requÃªte ultra-spÃ©cifique (rare!)

// 3. SPLIT COMPLEXE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Beaucoup de combinaisons possibles
// Chunks peuvent Ãªtre vides ou trÃ¨s petits
// Balancer confusion

// 4. MAINTENANCE DIFFICILE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Difficile Ã  comprendre
// Difficile Ã  debugger
// Risque d'erreurs

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SOLUTION: SIMPLIFIER
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

sh.shardCollection("documents.files",
  { region: 1, user_id: 1 }  // âœ… 2 champs maximum
)

// Ou mÃªme:
sh.shardCollection("documents.files",
  { user_id: 1 }  // âœ… Simple
)

// Indexes secondaires pour autres patterns:
db.files.createIndex({ doc_type: 1, created_at: -1 })
db.files.createIndex({ region: 1, country: 1 })
```

### ğŸš« Anti-Pattern 3 : Ignorer la Distribution RÃ©elle

```javascript
// âŒ ANTI-PATTERN: Assumer distribution uniforme

sh.shardCollection("social.posts", { country: 1, user_id: 1 })

// Assumption: Utilisateurs rÃ©partis uniformÃ©ment par pays
// RÃ©alitÃ© aprÃ¨s 6 mois:

db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "social.posts" } },
  {
    $group: {
      _id: "$shard",
      count: { $sum: 1 },
      minKey: { $first: "$min.country" }
    }
  }
])

// RÃ©sultat:
[
  { _id: "shard-a", count: 5,   minKey: "FR" },    // 5 chunks
  { _id: "shard-b", count: 8,   minKey: "DE" },    // 8 chunks
  { _id: "shard-c", count: 850, minKey: "US" }     // 850 chunks âŒ
]

// 98% des chunks sur shard-c (US users dominent)
// Shards a et b sous-utilisÃ©s

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SOLUTION: ANALYSER AVANT, AJUSTER APRÃˆS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Phase 1: Analyse prÃ©-sharding
db.posts.aggregate([
  { $group: { _id: "$country", count: { $sum: 1 } } },
  { $sort: { count: -1 } },
  { $limit: 10 }
])

// Output:
// US: 10M posts (90%)
// FR: 500k (5%)
// DE: 300k (3%)
// UK: 200k (2%)

// DÃ©cision: Hashed plus appropriÃ©
sh.shardCollection("social.posts", { user_id: "hashed" })

// OU: Zone Sharding avec isolation US
sh.shardCollection("social.posts", { country: 1, user_id: 1 })
sh.addShardToZone("shard-us-1", "US")
sh.addShardToZone("shard-us-2", "US")
sh.addShardToZone("shard-us-3", "US")  // 3 shards pour US
sh.addShardToZone("shard-intl", "INTL")

// Phase 2: Monitoring post-dÃ©ploiement
// Alert si dÃ©sÃ©quilibre > 20% entre shards
```

### ğŸš« Anti-Pattern 4 : Chunk Size InappropriÃ©

```javascript
// âŒ ANTI-PATTERN: Garder chunk size par dÃ©faut sans rÃ©flÃ©chir

// ScÃ©nario 1: Petits Ã©vÃ©nements IoT
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Document size: 200 bytes
// Chunk size: 128 MB
// Documents par chunk: 655,000

// ProblÃ¨me:
// - Migration d'un chunk = 655k docs Ã  copier
// - DurÃ©e migration: 30-60 secondes
// - Lock sur chunk pendant migration
// - Impact utilisateurs

// Solution:
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 32 } },  // 32 MB â†’ ~164k docs
  { upsert: true }
)
// Migrations 4Ã— plus rapides

// ScÃ©nario 2: Gros fichiers multimÃ©dia
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Document size: 10 MB (avec GridFS metadata)
// Chunk size: 128 MB
// Documents par chunk: ~12

// ProblÃ¨me:
// - Trop de chunks pour peu de donnÃ©es
// - Overhead metadata: 1M chunks pour 12M documents
// - Balancer constamment actif

// Solution:
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 512 } },  // 512 MB â†’ ~50 docs
  { upsert: true }
)
// Moins de chunks, moins d'overhead
```

### ğŸš« Anti-Pattern 5 : NÃ©gliger Pre-Splitting pour Bulk Load

```javascript
// âŒ ANTI-PATTERN

// Import massif sans prÃ©paration:
sh.shardCollection("import.data", { user_id: 1 })

// Bulk insert de 100M documents:
mongoimport --collection=data --file=huge_file.json

// Ce qui se passe:
// T0: 1 chunk [MinKey, MaxKey] â†’ shard-a
//     â†’ Tous inserts vont Ã  shard-a
// T1: Chunk atteint 128 MB â†’ Split
//     â†’ 2 chunks sur shard-a
//     â†’ Inserts continuent sur shard-a
// T2: Balancer dÃ©tecte dÃ©sÃ©quilibre
//     â†’ Migration chunk â†’ shard-b
//     â†’ Pendant ce temps, inserts continuent...
// T3-T100: Cycle split â†’ migrate en permanence

// RÃ©sultat:
// - Import lent (contentions splits/migrations)
// - Balancer overloaded
// - Cluster instable pendant import

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// SOLUTION: PRE-SPLIT + DÃ‰SACTIVER BALANCER
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

// Ã‰tape 1: Analyser les donnÃ©es Ã  importer
// Identifier min/max de user_id
// Supposons: user_id de 1 Ã  10,000,000

// Ã‰tape 2: Calculer points de split
// Pour 100M docs, cible: ~1000 chunks (100k docs par chunk)
// Split points: every 10,000 users

// Ã‰tape 3: Pre-split
sh.shardCollection("import.data", { user_id: 1 })

for (let i = 10000; i < 10000000; i += 10000) {
  sh.splitAt("import.data", { user_id: i })
}
// CrÃ©e ~1000 chunks instantanÃ©ment

// Ã‰tape 4: DÃ©sactiver balancer temporairement
sh.stopBalancer()

// Ã‰tape 5: Import
mongoimport --collection=data --file=huge_file.json
// Insertions immÃ©diatement distribuÃ©es sur chunks prÃ©-crÃ©Ã©s

// Ã‰tape 6: RÃ©activer balancer
sh.startBalancer()
// Balance une seule fois aprÃ¨s import

// Gain de temps: 50-70% plus rapide
```

## Monitoring et Maintenance

### MÃ©triques ClÃ©s

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// MÃ‰TRIQUES ESSENTIELLES RANGE SHARDING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// 1. DISTRIBUTION DES CHUNKS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "db.collection" } },
  { $group: { _id: "$shard", count: { $sum: 1 } } },
  { $sort: { count: -1 } }
])

// IdÃ©al: Ã‰cart < 10% entre min et max
// Alerte si: Ã‰cart > 20%

// 2. TAILLE DES CHUNKS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "db.collection" } },
  {
    $lookup: {
      from: "collections",
      localField: "ns",
      foreignField: "_id",
      as: "coll"
    }
  },
  {
    $project: {
      shard: 1,
      estimatedSize: "$estimatedDataSize"
    }
  },
  {
    $group: {
      _id: "$shard",
      avgSize: { $avg: "$estimatedSize" },
      maxSize: { $max: "$estimatedSize" },
      minSize: { $min: "$estimatedSize" }
    }
  }
])

// IdÃ©al: avgSize proche de chunkSize (128 MB)
// Alerte si: maxSize > chunkSize Ã— 1.5 (jumbo chunk potentiel)

// 3. SPLITS PAR JOUR
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
db.getSiblingDB("config").changelog.aggregate([
  {
    $match: {
      what: "split",
      time: { $gte: ISODate("2024-12-07") }
    }
  },
  { $count: "splits_last_24h" }
])

// Normal: 10-100 splits/jour selon croissance
// Alerte si: > 1000 splits/jour (croissance trop rapide ou chunk size trop petit)

// 4. JUMBO CHUNKS
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
db.getSiblingDB("config").chunks.find({
  ns: "db.collection",
  jumbo: true
}).count()

// IdÃ©al: 0 jumbo chunks
// Alerte si: > 0 (problÃ¨me de shard key)

// 5. EFFICACITÃ‰ DES REQUÃŠTES
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Utiliser explain() pour mesurer:
db.collection.find({ shard_key_field: value }).explain("executionStats")

// MÃ©triques:
// - nReturned: documents retournÃ©s
// - totalDocsExamined: documents scannÃ©s
// - totalKeysExamined: clÃ©s index scannÃ©es
// - executionTimeMillis: temps total
// - nShards: nombre de shards interrogÃ©s

// IdÃ©al pour Range Sharding:
// - nShards: 1-2 (ciblÃ©)
// - totalDocsExamined / nReturned â‰ˆ 1 (pas de scan inutile)
```

### Scripts de Maintenance

```javascript
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// SCRIPTS UTILITAIRES
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

// Script 1: Rapport de Distribution
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function generateDistributionReport(ns) {
  const chunks = db.getSiblingDB("config").chunks.aggregate([
    { $match: { ns: ns } },
    { $group: { _id: "$shard", count: { $sum: 1 } } },
    { $sort: { count: -1 } }
  ]).toArray()

  const total = chunks.reduce((sum, s) => sum + s.count, 0)
  const avg = total / chunks.length

  print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
  print(`Distribution Report: ${ns}`)
  print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
  print(`Total chunks: ${total}`)
  print(`Average per shard: ${avg.toFixed(2)}`)
  print("")
  print("Per Shard:")
  chunks.forEach(s => {
    const pct = (s.count / total * 100).toFixed(2)
    const diff = ((s.count - avg) / avg * 100).toFixed(2)
    print(`  ${s._id}: ${s.count} chunks (${pct}%, ${diff}% vs avg)`)
  })
  print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
}

// Utilisation:
generateDistributionReport("ecommerce.orders")

// Script 2: Identifier Chunks ProblÃ©matiques
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function findProblematicChunks(ns) {
  const jumbo = db.getSiblingDB("config").chunks.find({
    ns: ns,
    jumbo: true
  }).toArray()

  if (jumbo.length > 0) {
    print("âš ï¸  JUMBO CHUNKS DETECTED:")
    jumbo.forEach(c => {
      print(`  Range: ${tojson(c.min)} to ${tojson(c.max)}`)
      print(`  Shard: ${c.shard}`)
      print("")
    })
  } else {
    print("âœ… No jumbo chunks")
  }
}

// Script 3: SuggÃ©rer Pre-Split Points
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function suggestPreSplitPoints(ns, field, numSplits) {
  const coll = db.getSiblingDB(ns.split('.')[0])
                 .getCollection(ns.split('.')[1])

  const min = coll.find().sort({ [field]: 1 }).limit(1).toArray()[0][field]
  const max = coll.find().sort({ [field]: -1 }).limit(1).toArray()[0][field]

  const range = max - min
  const step = range / (numSplits + 1)

  print(`Suggested split points for ${ns} on ${field}:`)
  for (let i = 1; i <= numSplits; i++) {
    const point = min + (step * i)
    print(`  sh.splitAt("${ns}", { ${field}: ${point} })`)
  }
}
```

## RÃ©sumÃ©

Le **Range Sharding** est le type de sharding par dÃ©faut de MongoDB offrant :

**Avantages :**
- âœ… Range queries trÃ¨s efficaces (routage ciblÃ©)
- âœ… LocalitÃ© des donnÃ©es prÃ©visible
- âœ… Excellent pour time-series et logs
- âœ… Facilite archivage (drop chunks par plage)

**DÃ©fis :**
- âš ï¸ Risque de hotspots si shard key monotone
- âš ï¸ Distribution peut Ãªtre inÃ©gale
- âš ï¸ NÃ©cessite analyse de la distribution

**StratÃ©gies d'Optimisation :**
1. Shard key composÃ©e avec prÃ©fixe non-monotone
2. Pre-splitting pour bulk loads
3. Chunk size adaptatif selon document size
4. Archivage par drop de chunks

**Anti-patterns critiques :**
- âŒ Timestamp seul (hotspot permanent)
- âŒ Shard key trop complexe (> 3 champs)
- âŒ Ignorer distribution rÃ©elle des donnÃ©es
- âŒ Chunk size inadaptÃ©
- âŒ Pas de pre-split pour imports massifs

**Monitoring essentiel :**
- Distribution des chunks (Ã©cart < 20%)
- Jumbo chunks (objectif = 0)
- EfficacitÃ© requÃªtes (nShards = 1-2 idÃ©al)

Le Range Sharding excelle pour les workloads avec requÃªtes par plage frÃ©quentes et distribution non-monotone des insertions. Une conception soigneuse de la shard key et un monitoring proactif sont essentiels pour des performances optimales.

---


â­ï¸ [Hashed Sharding](/10-sharding/04.2-hashed-sharding.md)
