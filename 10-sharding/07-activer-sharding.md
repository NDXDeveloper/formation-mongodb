üîù Retour au [Sommaire](/SOMMAIRE.md)

# 10.7 Activer le Sharding sur une Base et une Collection

## Introduction

L'activation du sharding sur une base de donn√©es et ses collections est une √©tape critique qui d√©termine la r√©ussite ou l'√©chec de votre strat√©gie de distribution horizontale. Une fois le sharding activ√© avec une shard key donn√©e, **modifier cette cl√© n√©cessite des op√©rations complexes** (bien que MongoDB 5.0+ facilite ce processus avec la fonctionnalit√© de resharding).

Cette section d√©taille les m√©canismes d'activation du sharding, les strat√©gies de choix de shard key, les techniques de pr√©-splitting, et les anti-patterns √† √©viter absolument pour garantir des performances optimales √† long terme.

---

## Pr√©requis et Consid√©rations

### Avant d'Activer le Sharding

#### 1. Cluster Shard√© Op√©rationnel

```javascript
// V√©rifier l'√©tat du cluster
mongosh --host mongos1.example.com --port 27017
sh.status()

// R√©sultat attendu :
// - Config servers en Replica Set (3 membres)
// - Au moins 2 shards actifs
// - Balancer activ√©
```

#### 2. Connexion via Mongos

**IMPORTANT** : Le sharding s'active **uniquement via un mongos**, jamais directement sur un shard.

```javascript
// ‚úÖ Correct : Via mongos
mongosh --host mongos1.example.com --port 27017

// ‚ùå Incorrect : Directement sur un shard
mongosh --host shardA1.example.com --port 27018
```

#### 3. Analyse de la Charge de Travail

Avant d'activer le sharding, analysez :

| Crit√®re | Questions √† se poser |
|---------|---------------------|
| **Volume de donn√©es** | Combien de donn√©es actuelles ? Croissance pr√©vue ? |
| **Patterns de requ√™tes** | Quelles sont les requ√™tes les plus fr√©quentes ? |
| **Distribution** | Les donn√©es sont-elles uniform√©ment distribu√©es ? |
| **Hot spots** | Y a-t-il des valeurs tr√®s populaires (hot keys) ? |
| **Cardinalit√©** | La shard key candidate a-t-elle une cardinalit√© √©lev√©e ? |
| **Monotonie** | Les insertions sont-elles s√©quentielles (timestamps, _id) ? |

#### 4. Sauvegarde Pr√©ventive

```bash
# Toujours sauvegarder avant d'activer le sharding
mongodump --host mongos1.example.com --port 27017 \
  --db mydb \
  --out /backup/pre-sharding-$(date +%Y%m%d)
```

---

## Phase 1 : Activation du Sharding sur une Base de Donn√©es

### Commande de Base

```javascript
// Se connecter au mongos
mongosh --host mongos1.example.com --port 27017

// Activer le sharding sur la base de donn√©es
sh.enableSharding("mydb")

// Ou syntaxe alternative
db.adminCommand({ enableSharding: "mydb" })
```

### V√©rification

```javascript
// V√©rifier que le sharding est activ√©
use config
db.databases.find({ _id: "mydb" })

// R√©sultat attendu :
{
  "_id" : "mydb",
  "primary" : "shardA",
  "partitioned" : true,
  "version" : {
    "uuid" : UUID("..."),
    "timestamp" : Timestamp(1640000000, 1),
    "lastMod" : 1
  }
}
```

### Shard Primaire

Lors de l'activation du sharding :
- MongoDB assigne un **shard primaire** √† la base de donn√©es
- Ce shard stocke les collections **non-shard√©es** de cette base
- Les collections shard√©es seront distribu√©es sur tous les shards

```javascript
// Voir le shard primaire
db.getSiblingDB("config").databases.findOne({ _id: "mydb" }).primary

// Changer le shard primaire (si n√©cessaire)
db.adminCommand({ movePrimary: "mydb", to: "shardB" })
```

---

## Phase 2 : Choix de la Shard Key

### Crit√®res d'une Bonne Shard Key

Une shard key efficace doit respecter trois propri√©t√©s fondamentales :

#### 1. Cardinalit√© √âlev√©e (High Cardinality)

**D√©finition** : Nombre de valeurs distinctes possibles pour la shard key.

```javascript
// ‚úÖ Bonne cardinalit√© : user_id (UUID)
// Millions de valeurs possibles
{ user_id: UUID("550e8400-e29b-41d4-a716-446655440000") }

// ‚ùå Mauvaise cardinalit√© : status (3 valeurs seulement)
{ status: "active" }  // 90% des documents
{ status: "inactive" }  // 8% des documents
{ status: "deleted" }  // 2% des documents
```

**Impact** :
- Faible cardinalit√© ‚Üí Peu de chunks ‚Üí Distribution d√©s√©quilibr√©e
- Haute cardinalit√© ‚Üí Nombreux chunks ‚Üí Distribution uniforme

#### 2. Distribution Uniforme (Write Scaling)

**D√©finition** : Les √©critures doivent √™tre r√©parties uniform√©ment entre les shards.

```javascript
// ‚ùå Distribution non uniforme : timestamp (monotone)
{ timestamp: ISODate("2024-01-15T10:30:00Z") }
// Toutes les insertions r√©centes vont sur le m√™me chunk

// ‚úÖ Distribution uniforme : hashed _id
{ _id: "hashed" }
// Les insertions sont distribu√©es al√©atoirement
```

#### 3. Localit√© des Requ√™tes (Query Isolation)

**D√©finition** : Les requ√™tes fr√©quentes doivent cibler un seul shard.

```javascript
// ‚úÖ Bonne localit√© : Requ√™tes par customer_id
db.orders.find({ customer_id: "CUST12345" })
// Si shard key = { customer_id: 1 } ‚Üí 1 seul shard interrog√©

// ‚ùå Mauvaise localit√© : Requ√™tes multi-shards
db.orders.find({ order_date: { $gte: ISODate("2024-01-01") } })
// Si shard key = { customer_id: 1 } ‚Üí TOUS les shards interrog√©s
```

### Tableau de D√©cision : Choix de Shard Key

| Pattern d'Acc√®s | Shard Key Recommand√©e | Justification |
|-----------------|----------------------|---------------|
| Recherche par utilisateur | `{ user_id: 1 }` ou `{ user_id: "hashed" }` | Localit√© des requ√™tes |
| S√©ries temporelles | `{ application: 1, timestamp: 1 }` | √âvite monotonie + localit√© |
| Donn√©es g√©ographiques | `{ region: 1, user_id: 1 }` | Zone sharding possible |
| Catalogues produits | `{ category: 1, product_id: 1 }` | Distribution + requ√™tes |
| Logs applicatifs | `{ service: 1, timestamp: 1 }` | Isolation par service |
| Compteurs/m√©triques | `{ metric_name: "hashed" }` | Distribution uniforme |

---

## Phase 3 : Activation du Sharding sur une Collection

### Syntaxe de Base

```javascript
// Format g√©n√©ral
sh.shardCollection("<database>.<collection>", { <shard_key> })

// Exemples
sh.shardCollection("mydb.users", { user_id: 1 })
sh.shardCollection("mydb.events", { timestamp: "hashed" })
sh.shardCollection("mydb.orders", { customer_id: 1, order_id: 1 })
```

### Avec Options Avanc√©es

```javascript
// Syntaxe compl√®te avec options
db.adminCommand({
  shardCollection: "mydb.orders",
  key: { customer_id: 1 },
  unique: false,
  numInitialChunks: 4,
  collation: { locale: "simple" }
})
```

**Options disponibles** :

| Option | Description | Valeur par d√©faut |
|--------|-------------|-------------------|
| `unique` | Contrainte d'unicit√© sur la shard key | `false` |
| `numInitialChunks` | Nombre de chunks initiaux (hashed uniquement) | 2 |
| `collation` | Collation pour comparaisons de cha√Ænes | H√©rit√©e de la collection |
| `timeseries` | Options pour collections time series | - |

---

## Strat√©gies de Sharding : Range vs Hashed vs Compound

### 1. Range Sharding (Partitionnement par Plage)

#### Principe

Les donn√©es sont partitionn√©es en **plages contigu√´s** de valeurs de la shard key.

```javascript
// Exemple : Range sharding sur user_id
sh.shardCollection("mydb.users", { user_id: 1 })

// Distribution des chunks :
// Chunk 1 : { user_id: MinKey } ‚Üí { user_id: "user_5000" }  ‚Üí Shard A
// Chunk 2 : { user_id: "user_5000" } ‚Üí { user_id: "user_10000" }  ‚Üí Shard B
// Chunk 3 : { user_id: "user_10000" } ‚Üí { user_id: MaxKey }  ‚Üí Shard A
```

#### Avantages

‚úÖ **Requ√™tes par plage efficaces** :
```javascript
// Requ√™te ciblant un seul shard ou peu de shards
db.users.find({ user_id: { $gte: "user_5000", $lt: "user_7000" } })
// ‚Üí Interroge uniquement le Shard B
```

‚úÖ **Tri naturel** : Les donn√©es sont ordonn√©es sur chaque shard

‚úÖ **G√©olocalisation** : Parfait pour le zone sharding

#### Inconv√©nients

‚ùå **Hot spots possibles** : Si les √©critures se concentrent sur une plage

‚ùå **Distribution d√©pendante des donn√©es** : Peut devenir d√©s√©quilibr√©e

#### Cas d'Usage Id√©aux

- Requ√™tes par plage fr√©quentes
- Zone sharding (donn√©es g√©ographiques)
- Donn√©es avec distribution naturellement uniforme

### 2. Hashed Sharding (Partitionnement par Hachage)

#### Principe

La shard key est **hach√©e** avant distribution, garantissant une distribution uniforme.

```javascript
// Hashed sharding sur _id
sh.shardCollection("mydb.events", { _id: "hashed" })

// Distribution :
// hash("event_001") = 123456 ‚Üí Shard A
// hash("event_002") = 789012 ‚Üí Shard B
// hash("event_003") = 345678 ‚Üí Shard A
```

#### Avantages

‚úÖ **Distribution uniforme garantie** : Pas de hot spots

‚úÖ **Parfait pour les cl√©s monotones** : _id, timestamp

‚úÖ **Scaling pr√©visible** : √âquilibrage automatique

#### Inconv√©nients

‚ùå **Pas de requ√™tes par plage efficaces** :
```javascript
// ‚ùå Requ√™te broadcast (tous les shards interrog√©s)
db.events.find({ _id: { $gte: "event_1000", $lt: "event_2000" } })
```

‚ùå **Perte de localit√©** : Donn√©es connexes dispers√©es

#### Cas d'Usage Id√©aux

- Insertions √† haut d√©bit avec cl√© monotone
- Distribution uniforme prioritaire
- Pas de requ√™tes par plage n√©cessaires

### 3. Compound Sharding (Shard Key Compos√©e)

#### Principe

Combine **plusieurs champs** pour obtenir le meilleur des deux mondes.

```javascript
// Shard key compos√©e : application + timestamp
sh.shardCollection("mydb.logs", { application: 1, timestamp: 1 })

// Distribution :
// { application: "webserver", timestamp: ISODate("2024-01-15T10:00:00Z") } ‚Üí Shard A
// { application: "database", timestamp: ISODate("2024-01-15T10:00:00Z") } ‚Üí Shard B
// { application: "webserver", timestamp: ISODate("2024-01-15T11:00:00Z") } ‚Üí Shard A
```

#### Avantages

‚úÖ **Localit√© + Distribution** :
```javascript
// Requ√™te cibl√©e : 1 seul shard
db.logs.find({ application: "webserver", timestamp: { $gte: ISODate("2024-01-15T10:00:00Z") } })

// √âvite les hot spots sur timestamp
```

‚úÖ **Flexibilit√©** : √âquilibre entre les propri√©t√©s

‚úÖ **Multi-tenancy** : Parfait pour les applications SaaS

#### Inconv√©nients

‚ùå **Requ√™tes sans pr√©fixe** : Moins efficaces
```javascript
// ‚ùå Requ√™te sans 'application' ‚Üí broadcast
db.logs.find({ timestamp: ISODate("2024-01-15T10:00:00Z") })
```

‚ùå **Cardinalit√© du pr√©fixe critique** : Si peu de valeurs pour `application`

#### Cas d'Usage Id√©aux

- Applications multi-tenant (tenant_id + autre champ)
- S√©ries temporelles par cat√©gorie
- Besoin de localit√© ET distribution

---

## Exemples Concrets par Type de Donn√©es

### Exemple 1 : E-commerce - Collection Orders

**Contexte** :
- 10M de commandes
- Requ√™tes principalement par `customer_id`
- Certains clients tr√®s actifs (hot customers)

**Strat√©gie** :

```javascript
// Option 1 : Range sharding simple (risque de hot spots)
sh.shardCollection("ecommerce.orders", { customer_id: 1 })

// ‚úÖ Option 2 : Hashed (distribution uniforme)
sh.shardCollection("ecommerce.orders", { customer_id: "hashed" })

// ‚úÖ‚úÖ Option 3 : Compound (meilleur compromis)
sh.shardCollection("ecommerce.orders", { customer_id: 1, order_id: 1 })
```

**Justification Option 3** :
- `customer_id` assure la localit√© des requ√™tes
- `order_id` ajoute de la granularit√© pour √©viter les hot spots
- Requ√™tes par customer_id restent efficaces

### Exemple 2 : IoT - Collection Sensor Data

**Contexte** :
- Millions de capteurs
- Insertions continues avec timestamp
- Requ√™tes par sensor_id et plage de temps

**Strat√©gie** :

```javascript
// ‚ùå Mauvais : Timestamp seul (hot spot sur derni√®res donn√©es)
sh.shardCollection("iot.sensor_data", { timestamp: 1 })

// ‚úÖ Bon : Compound avec sensor_id
sh.shardCollection("iot.sensor_data", { sensor_id: 1, timestamp: 1 })

// ‚úÖ‚úÖ Meilleur : Hashed sensor_id + timestamp
sh.shardCollection("iot.sensor_data", { sensor_id: "hashed", timestamp: 1 })
```

**Justification** :
- `sensor_id: "hashed"` distribue uniform√©ment les capteurs
- `timestamp` permet les requ√™tes temporelles par capteur
- Pas de hot spot sur les insertions r√©centes

### Exemple 3 : R√©seaux Sociaux - Collection Posts

**Contexte** :
- Posts de millions d'utilisateurs
- Requ√™tes par author_id (timeline de l'auteur)
- Requ√™tes par hashtags (moins fr√©quentes)

**Strat√©gie** :

```javascript
// ‚úÖ Strat√©gie choisie
sh.shardCollection("social.posts", { author_id: "hashed" })

// Cr√©er un index secondaire pour les hashtags
db.posts.createIndex({ hashtags: 1 })
```

**Justification** :
- `author_id: "hashed"` distribue uniform√©ment les auteurs populaires
- Les requ√™tes par auteur restent efficaces (1 shard)
- Les requ√™tes par hashtags seront broadcast mais sont moins fr√©quentes

### Exemple 4 : Logs Applicatifs - Collection Logs

**Contexte** :
- Logs de plusieurs microservices
- Volume massif (100 Go/jour)
- Requ√™tes par service + plage temporelle
- R√©tention de 30 jours (TTL)

**Strat√©gie** :

```javascript
// ‚úÖ Shard key compos√©e
sh.shardCollection("platform.logs", { service_name: 1, timestamp: 1 })

// Cr√©er un index TTL pour suppression automatique
db.logs.createIndex(
  { timestamp: 1 },
  { expireAfterSeconds: 2592000 }  // 30 jours
)
```

**Justification** :
- `service_name` isole les logs par service (localit√©)
- `timestamp` √©vite les hot spots et permet les requ√™tes temporelles
- TTL index g√®re automatiquement la r√©tention

### Exemple 5 : Analytics - Collection Page Views

**Contexte** :
- Tracking des pages vues
- Requ√™tes analytiques par date
- Insertions massives en temps r√©el

**Strat√©gie** :

```javascript
// ‚úÖ Hashed sur un identifiant unique
sh.shardCollection("analytics.page_views", { session_id: "hashed" })

// Alternative avec pre-aggregation
sh.shardCollection("analytics.daily_stats", { date: 1, page_url: 1 })
```

**Justification** :
- `session_id: "hashed"` distribue uniform√©ment les insertions
- Pour l'analytics, pr√©-agr√©ger dans `daily_stats` shard√© par date

---

## Pr√©-splitting et Distribution Initiale

### Pourquoi Pr√©-splitter ?

Lors du sharding d'une collection existante ou avant une insertion massive :

1. **Sans pr√©-splitting** :
   - Tous les documents vont initialement dans un seul chunk
   - Le chunk se divise progressivement (overhead)
   - Migrations nombreuses et co√ªteuses

2. **Avec pr√©-splitting** :
   - Chunks cr√©√©s √† l'avance
   - Distribution imm√©diate des insertions
   - Pas de surcharge de splitting/migration

### Pr√©-splitting pour Range Sharding

```javascript
// Activer le sharding
sh.shardCollection("mydb.users", { user_id: 1 })

// Pr√©-splitter en 10 chunks
for (var i = 1; i <= 9; i++) {
  sh.splitAt("mydb.users", { user_id: "user_" + (i * 10000) });
}

// R√©sultat : 10 chunks
// Chunk 1 : MinKey ‚Üí user_10000
// Chunk 2 : user_10000 ‚Üí user_20000
// ...
// Chunk 10 : user_90000 ‚Üí MaxKey
```

### Pr√©-splitting pour Hashed Sharding

```javascript
// Avec numInitialChunks (MongoDB 4.4+)
sh.shardCollection(
  "mydb.events",
  { _id: "hashed" },
  false,  // unique
  {
    numInitialChunks: 16  // 16 chunks initiaux
  }
)

// Ou manuellement pour versions ant√©rieures
sh.shardCollection("mydb.events", { _id: "hashed" })
for (var i = 0; i < 16; i++) {
  sh.splitFind("mydb.events", { _id: ObjectId() });
}
```

### Distribution Initiale des Chunks

Apr√®s le pr√©-splitting, distribuer manuellement :

```javascript
// Lister les shards disponibles
db.getSiblingDB("config").shards.find()

// D√©placer des chunks vers diff√©rents shards
sh.moveChunk("mydb.users", { user_id: "user_10000" }, "shardB")
sh.moveChunk("mydb.users", { user_id: "user_30000" }, "shardC")
sh.moveChunk("mydb.users", { user_id: "user_50000" }, "shardB")

// V√©rifier la distribution
db.users.getShardDistribution()
```

### Calcul du Nombre de Chunks Optimal

```javascript
// Formule recommand√©e :
// numChunks = numShards * 2 * N
// O√π N = facteur d'expansion (2-4)

// Exemple : 3 shards, facteur 3
var numShards = 3;
var expansionFactor = 3;
var numChunks = numShards * 2 * expansionFactor;  // 18 chunks

// Pr√©-splitter
for (var i = 1; i < numChunks; i++) {
  var splitPoint = Math.floor((i / numChunks) * 100000);
  sh.splitAt("mydb.users", { user_id: "user_" + splitPoint });
}
```

---

## Validation et Monitoring

### V√©rification Post-Sharding

```javascript
// 1. Confirmer que la collection est shard√©e
db.collection.stats()
// Rechercher : "sharded" : true

// 2. Voir la distribution des chunks
db.users.getShardDistribution()

// Output :
// Shard shardA at shardA/...
//  data : 1.5GiB docs : 1000000 chunks : 5
//  estimated data per chunk : 300MiB
//  estimated docs per chunk : 200000
//
// Shard shardB at shardB/...
//  data : 1.4GiB docs : 980000 chunks : 5
//  ...
//
// Totals
//  data : 2.9GiB docs : 1980000 chunks : 10

// 3. D√©tails des chunks
db.getSiblingDB("config").chunks.find({ ns: "mydb.users" }).pretty()

// 4. Index utilis√©s
db.users.getIndexes()
```

### M√©triques √† Surveiller

```javascript
// Distribution des documents par shard
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "mydb.users" } },
  { $group: {
      _id: "$shard",
      numChunks: { $sum: 1 }
    }
  }
])

// Taille des donn√©es par shard
db.users.stats().shards

// Identifier les jumbo chunks
db.getSiblingDB("config").chunks.find({
  ns: "mydb.users",
  jumbo: true
})
```

### Tests de Performance

```javascript
// Test 1 : Requ√™te cibl√©e (targeted query)
db.users.find({ user_id: "user_12345" }).explain("executionStats")
// V√©rifier : "totalDocsExamined", "executionTimeMillis", "nReturned"

// Test 2 : Requ√™te broadcast (scatter-gather)
db.users.find({ email: /.*@example.com/ }).explain("executionStats")
// Comparer avec la requ√™te cibl√©e

// Test 3 : Insertion
var startTime = new Date();
for (var i = 0; i < 10000; i++) {
  db.users.insertOne({
    user_id: "user_" + (100000 + i),
    name: "Test User " + i,
    email: "test" + i + "@example.com"
  });
}
var endTime = new Date();
print("Temps d'insertion : " + (endTime - startTime) + " ms");
```

---

## Anti-Patterns et Erreurs Fatales

### ‚ùå Anti-Pattern 1 : Shard Key de Faible Cardinalit√©

**Probl√®me** :

```javascript
// Mauvais : Seulement 3 valeurs possibles
sh.shardCollection("mydb.orders", { status: 1 })

// Distribution :
// status: "pending" ‚Üí 60% des documents ‚Üí Shard A (surcharge)
// status: "completed" ‚Üí 35% des documents ‚Üí Shard B
// status: "cancelled" ‚Üí 5% des documents ‚Üí Shard C
```

**Cons√©quence** :
- Jumbo chunks impossibles √† diviser
- D√©s√©quilibre permanent
- Hot spot sur Shard A

**Solution** :

```javascript
// Utiliser une shard key compos√©e avec haute cardinalit√©
sh.shardCollection("mydb.orders", { customer_id: 1, order_id: 1 })

// Ou hashed
sh.shardCollection("mydb.orders", { customer_id: "hashed" })
```

### ‚ùå Anti-Pattern 2 : Shard Key Monotone sans Hashing

**Probl√®me** :

```javascript
// Mauvais : _id auto-incr√©ment√© ou timestamp
sh.shardCollection("mydb.events", { _id: 1 })
sh.shardCollection("mydb.logs", { timestamp: 1 })
```

**Cons√©quence** :
- Toutes les insertions sur le dernier chunk (hot spot)
- Un seul shard re√ßoit toute la charge d'√©criture
- Migrations constantes

**Solution** :

```javascript
// ‚úÖ Hashed sur la cl√© monotone
sh.shardCollection("mydb.events", { _id: "hashed" })

// ‚úÖ Ou compound avec un pr√©fixe non-monotone
sh.shardCollection("mydb.logs", { application: 1, timestamp: 1 })
```

### ‚ùå Anti-Pattern 3 : Shard Key Non Incluse dans les Requ√™tes

**Probl√®me** :

```javascript
// Shard key : { user_id: 1 }
sh.shardCollection("mydb.posts", { user_id: 1 })

// Mais les requ√™tes principales sont :
db.posts.find({ hashtag: "#mongodb" })  // ‚ùå Broadcast query
db.posts.find({ created_at: { $gte: ISODate("...") } })  // ‚ùå Broadcast query
```

**Cons√©quence** :
- Toutes les requ√™tes interrogent tous les shards
- Aucun b√©n√©fice du sharding pour les lectures
- Latence multipli√©e par le nombre de shards

**Solution** :

```javascript
// Choisir une shard key align√©e avec les patterns de requ√™tes
sh.shardCollection("mydb.posts", { hashtag: 1, created_at: 1 })

// Ou cr√©er des index secondaires (mais requ√™tes restent broadcast)
```

### ‚ùå Anti-Pattern 4 : Shard Key avec Valeur Null

**Probl√®me** :

```javascript
// Shard key : { category: 1 }
sh.shardCollection("mydb.products", { category: 1 })

// Mais certains documents n'ont pas de category
db.products.insertOne({
  name: "Product X",
  price: 99.99
  // category manquant ‚Üí null
})
```

**Cons√©quence** :
- Tous les documents sans `category` vont dans le m√™me chunk
- Jumbo chunk potentiel si beaucoup de documents sans category

**Solution** :

```javascript
// Option 1 : Valeur par d√©faut
db.products.insertOne({
  name: "Product X",
  category: "uncategorized",  // Valeur par d√©faut
  price: 99.99
})

// Option 2 : Validation de sch√©ma
db.createCollection("products", {
  validator: {
    $jsonSchema: {
      required: ["category"],
      properties: {
        category: { bsonType: "string" }
      }
    }
  }
})

// Option 3 : Shard key compos√©e avec _id
sh.shardCollection("mydb.products", { category: 1, _id: 1 })
```

### ‚ùå Anti-Pattern 5 : Sharding Pr√©matur√©

**Probl√®me** :

```javascript
// Sharder une collection avec seulement 1 Go de donn√©es
sh.shardCollection("mydb.small_collection", { _id: 1 })
```

**Cons√©quence** :
- Overhead de gestion (chunks, balancer, m√©tadonn√©es)
- Complexit√© op√©rationnelle
- Pas de b√©n√©fice r√©el

**Solution** :

```javascript
// R√®gles empiriques pour sharder :
// - Volume > 100 Go
// - Taux d'√©criture > 10 000 ops/sec
// - Croissance pr√©vue importante
//
// Sinon : rester sur un Replica Set simple
```

### ‚ùå Anti-Pattern 6 : Modifier les Donn√©es de Shard Key

**Probl√®me** :

```javascript
// Shard key : { user_id: 1 }
sh.shardCollection("mydb.sessions", { user_id: 1 })

// Puis tenter de modifier le user_id
db.sessions.updateOne(
  { _id: ObjectId("...") },
  { $set: { user_id: "new_user_id" } }  // ‚ùå Erreur dans MongoDB < 4.2
)
```

**Cons√©quence** :
- MongoDB < 4.2 : Erreur `ImmutableField`
- MongoDB ‚â• 4.2 : Migration co√ªteuse du document entre shards

**Solution** :

```javascript
// Option 1 : Supprimer et recr√©er (MongoDB < 4.2)
var doc = db.sessions.findOne({ _id: ObjectId("...") });
doc.user_id = "new_user_id";
db.sessions.deleteOne({ _id: ObjectId("...") });
db.sessions.insertOne(doc);

// Option 2 : √âviter de modifier la shard key
// Choisir une shard key immuable d√®s le d√©part
```

### ‚ùå Anti-Pattern 7 : Ignorer le Balancer lors de Maintenances

**Probl√®me** :

```bash
# Effectuer une maintenance sans arr√™ter le balancer
# Exemple : Ajout de RAM, mise √† jour OS

# Le balancer migre des chunks pendant la maintenance
# ‚Üí Impact performance, risque de timeout
```

**Solution** :

```javascript
// Toujours arr√™ter le balancer pour les maintenances
sh.stopBalancer()

// Attendre que les migrations en cours se terminent
while (sh.isBalancerRunning()) {
  print("Attente arr√™t balancer...");
  sleep(1000);
}

// Effectuer la maintenance...

// R√©activer apr√®s
sh.startBalancer()
```

---

## Migration d'une Collection Non-Shard√©e

### Sc√©nario : Collection Existante √† Sharder

**Contexte** :
- Collection `products` avec 50M documents (200 Go)
- Actuellement sur un Replica Set standalone
- Besoin de sharder pour scaling

### √âtape 1 : Analyse Pr√©alable

```javascript
// Connexion au Replica Set existant
mongosh --host primary.example.com --port 27017

// Analyser la collection
use mydb
db.products.stats()

// Analyser les index existants
db.products.getIndexes()

// Identifier la meilleure shard key candidate
// Exemple : Requ√™tes principales par category
db.products.find({ category: "Electronics" }).explain("executionStats")

// V√©rifier la cardinalit√©
db.products.distinct("category").length
```

### √âtape 2 : Pr√©paration

```bash
# 1. Sauvegarde compl√®te
mongodump --host primary.example.com --db mydb --collection products \
  --out /backup/pre-sharding-products

# 2. Cr√©er l'index pour la shard key (si inexistant)
mongosh --host primary.example.com
use mydb
db.products.createIndex({ category: 1, product_id: 1 })
```

### √âtape 3 : Int√©gration au Cluster Shard√©

```javascript
// Connexion au cluster shard√©
mongosh --host mongos1.example.com --port 27017

// Ajouter le Replica Set existant comme shard
sh.addShard("existingRS/primary.example.com:27017,secondary1.example.com:27017,secondary2.example.com:27017")

// V√©rifier
sh.status()
```

### √âtape 4 : Activation du Sharding

```javascript
// Activer le sharding sur la base
sh.enableSharding("mydb")

// Arr√™ter le balancer temporairement
sh.stopBalancer()

// Pr√©-splitter (critique pour une grosse collection)
sh.shardCollection("mydb.products", { category: 1, product_id: 1 })

// Pr√©-splitter manuellement par cat√©gorie
var categories = ["Electronics", "Clothing", "Books", "Home", "Sports", "Toys"];
categories.forEach(function(cat) {
  sh.splitAt("mydb.products", { category: cat, product_id: MinKey });
});

// R√©activer le balancer
sh.startBalancer()
```

### √âtape 5 : Monitoring de la Migration

```javascript
// Surveiller la distribution
db.products.getShardDistribution()

// Surveiller les migrations
db.getSiblingDB("config").changelog.find({
  ns: "mydb.products",
  what: "moveChunk.commit"
}).sort({ time: -1 }).limit(10)

// Surveiller le balancer
sh.isBalancerRunning()

// M√©triques d√©taill√©es
db.getSiblingDB("config").chunks.aggregate([
  { $match: { ns: "mydb.products" } },
  { $group: {
      _id: "$shard",
      chunks: { $sum: 1 }
    }
  }
])
```

### √âtape 6 : Validation

```javascript
// Test de lecture cibl√©e
db.products.find({ category: "Electronics", product_id: "PROD12345" }).explain("executionStats")
// V√©rifier : 1 seul shard interrog√©

// Test de lecture broadcast
db.products.find({ price: { $lt: 100 } }).explain("executionStats")
// Comparer performance avant/apr√®s

// Test d'√©criture
db.products.insertOne({
  category: "Electronics",
  product_id: "PROD99999",
  name: "Test Product",
  price: 99.99
})
```

---

## Bonnes Pratiques de Production

### 1. Planification de la Shard Key

```javascript
// Checklist avant activation :
// ‚úÖ Cardinalit√© analys√©e (>1000 valeurs distinctes id√©al)
// ‚úÖ Patterns de requ√™tes document√©s
// ‚úÖ Distribution des valeurs v√©rifi√©e (pas de skew)
// ‚úÖ Tests effectu√©s en staging avec donn√©es r√©elles
// ‚úÖ Plan de rollback d√©fini
```

### 2. Pr√©-splitting Syst√©matique

```javascript
// Pour toute collection > 1 Go
// R√®gle : numChunks = numShards * 4 (minimum)

var numShards = db.getSiblingDB("config").shards.count();
var numChunks = numShards * 4;

// Pr√©-splitter avant insertion massive
```

### 3. Monitoring Continu

```javascript
// Script de monitoring quotidien
function checkShardingHealth(dbName, collName) {
  var ns = dbName + "." + collName;

  // 1. Distribution des chunks
  var chunkDist = db.getSiblingDB("config").chunks.aggregate([
    { $match: { ns: ns } },
    { $group: { _id: "$shard", count: { $sum: 1 } } }
  ]).toArray();

  print("=== Distribution des chunks ===");
  printjson(chunkDist);

  // 2. Jumbo chunks
  var jumboCount = db.getSiblingDB("config").chunks.count({
    ns: ns,
    jumbo: true
  });

  if (jumboCount > 0) {
    print("‚ö†Ô∏è  ATTENTION : " + jumboCount + " jumbo chunks d√©tect√©s !");
  }

  // 3. Migrations r√©centes (derni√®res 24h)
  var migrations = db.getSiblingDB("config").changelog.count({
    ns: ns,
    what: "moveChunk.commit",
    time: { $gte: new Date(Date.now() - 86400000) }
  });

  print("Migrations (24h) : " + migrations);
}

// Ex√©cuter
checkShardingHealth("mydb", "users");
```

### 4. Documentation

Maintenir une documentation √† jour pour chaque collection shard√©e :

```yaml
# collection-sharding-metadata.yml

collections:
  - name: mydb.users
    shard_key:
      fields: { user_id: "hashed" }
      reason: "Distribution uniforme des utilisateurs"
      decided_by: "team-backend"
      date: "2024-01-15"

    statistics:
      num_chunks: 48
      size: "500 GB"
      num_docs: 50000000

    query_patterns:
      - pattern: "find({ user_id: ... })"
        frequency: "90%"
        targeted: true

      - pattern: "find({ email: ... })"
        frequency: "8%"
        targeted: false

    indexes:
      - { user_id: "hashed" }  # Shard key index
      - { email: 1 }           # Secondary index
      - { created_at: -1 }     # Secondary index
```

### 5. Tests de Charge

```javascript
// Avant de passer en production
// Test avec 10x le volume pr√©vu

// Test d'√©criture
var startTime = new Date();
var numInserts = 1000000;

for (var i = 0; i < numInserts; i++) {
  db.users.insertOne({
    user_id: UUID(),
    name: "User " + i,
    email: "user" + i + "@example.com",
    created_at: new Date()
  });

  if (i % 10000 == 0) {
    print("Inserted " + i + " documents");
  }
}

var endTime = new Date();
var duration = (endTime - startTime) / 1000;
var throughput = numInserts / duration;

print("Dur√©e : " + duration + " secondes");
print("Throughput : " + throughput.toFixed(2) + " insertions/sec");
```

---

## R√©sum√© : Processus D√©cisionnel

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Analyse des Besoins                    ‚îÇ
‚îÇ  - Volume actuel et pr√©vu               ‚îÇ
‚îÇ  - Patterns de requ√™tes                 ‚îÇ
‚îÇ  - Taux d'√©criture/lecture              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Choix de la Shard Key                  ‚îÇ
‚îÇ  - Cardinalit√© √©lev√©e ?                 ‚îÇ
‚îÇ  - Distribution uniforme ?              ‚îÇ
‚îÇ  - Localit√© des requ√™tes ?              ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Range / Hashed / Compound ?            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tests en Staging                       ‚îÇ
‚îÇ  - Pr√©-splitting                        ‚îÇ
‚îÇ  - Tests de charge                      ‚îÇ
‚îÇ  - Validation des performances          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Activation en Production               ‚îÇ
‚îÇ  - Sauvegarde                           ‚îÇ
‚îÇ  - sh.enableSharding()                  ‚îÇ
‚îÇ  - sh.shardCollection()                 ‚îÇ
‚îÇ  - Pr√©-splitting                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Monitoring et Ajustements              ‚îÇ
‚îÇ  - Distribution des chunks              ‚îÇ
‚îÇ  - Jumbo chunks                         ‚îÇ
‚îÇ  - Performances des requ√™tes            ‚îÇ
‚îÇ  - Balancer activity                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Conclusion

L'activation du sharding sur une base et une collection est une d√©cision strat√©gique qui impacte durablement les performances et l'√©volutivit√© de votre syst√®me. Les points cl√©s √† retenir :

- ‚úÖ **Choisir une shard key optimale** : Cardinalit√©, distribution, localit√©
- ‚úÖ **Pr√©-splitter syst√©matiquement** : Pour collections volumineuses
- ‚úÖ **Tester exhaustivement** : En staging avec donn√©es r√©alistes
- ‚úÖ **Monitorer activement** : Distribution, jumbo chunks, migrations
- ‚úÖ **√âviter les anti-patterns** : Shard keys monotones, faible cardinalit√©, non-align√©es

Une shard key bien choisie = performances optimales √† long terme.
Une shard key mal choisie = probl√®mes croissants et migration co√ªteuse.

**Investissez le temps n√©cessaire dans la phase d'analyse et de tests.**

---

## Ressources

- [MongoDB Documentation - Shard Keys](https://docs.mongodb.com/manual/core/sharding-shard-key/)
- [MongoDB Documentation - Choose a Shard Key](https://docs.mongodb.com/manual/core/sharding-choose-a-shard-key/)
- [MongoDB Best Practices - Sharding](https://docs.mongodb.com/manual/core/sharding-data-partitioning/)
- [MongoDB Blog - Shard Key Selection](https://www.mongodb.com/blog)

---


‚è≠Ô∏è [Migration des chunks](/10-sharding/08-migration-chunks.md)
