ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 10.9 OpÃ©rations sur un Cluster ShardÃ©

## Introduction

Les opÃ©rations quotidiennes sur un cluster shardÃ© MongoDB diffÃ¨rent significativement de celles effectuÃ©es sur un simple Replica Set. La distribution des donnÃ©es Ã  travers plusieurs shards introduit de la complexitÃ© mais aussi de nouvelles opportunitÃ©s d'optimisation et de gestion. Cette section couvre l'ensemble des opÃ©rations courantes et avancÃ©es que vous effectuerez en production, depuis les opÃ©rations CRUD jusqu'aux maintenances planifiÃ©es, en passant par la gestion des index et le troubleshooting.

Comprendre le comportement de chaque opÃ©ration dans un contexte distribuÃ© est essentiel pour maintenir un cluster performant, fiable et sÃ©curisÃ©.

---

## Architecture OpÃ©rationnelle : Vue d'Ensemble

### Flux des OpÃ©rations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      APPLICATION                             â”‚
â”‚              (Drivers MongoDB)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ Connection String
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                      â”‚                â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
    â”‚ mongos1 â”‚           â”‚ mongos2 â”‚      â”‚ mongos3 â”‚
    â”‚(Router) â”‚           â”‚(Router) â”‚      â”‚(Router) â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                     â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚        Lecture MÃ©tadonnÃ©es               â”‚
         â”‚        (Config Servers)                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                          â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                               â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Shard A  â”‚                               â”‚ Shard B  â”‚
    â”‚  (RS)    â”‚                               â”‚  (RS)    â”‚
    â”‚  P S S   â”‚                               â”‚  P S S   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                           â”‚
         â”‚                                           â”‚
    DonnÃ©es                                     DonnÃ©es
    Chunk 1-50                                  Chunk 51-100
```

### Points ClÃ©s OpÃ©rationnels

1. **Toutes les opÃ©rations passent par mongos** : Les clients ne se connectent jamais directement aux shards
2. **Mongos route intelligemment** : Utilise les mÃ©tadonnÃ©es pour diriger vers les bons shards
3. **Les config servers sont critiques** : Sans eux, le cluster est en lecture seule
4. **OpÃ©rations targeted vs broadcast** : Impact majeur sur les performances

---

## OpÃ©rations CRUD dans un Cluster ShardÃ©

### Insertions (Create)

#### Comportement Standard

```javascript
// Connexion via mongos
mongosh --host mongos1.example.com --port 27017

use mydb

// Insertion simple
db.users.insertOne({
  user_id: "user_12345",
  name: "Alice Dupont",
  email: "alice@example.com",
  created_at: new Date()
})

// Le mongos :
// 1. DÃ©termine le chunk destination basÃ© sur la shard key (user_id)
// 2. Route l'insertion vers le shard appropriÃ©
// 3. Retourne le rÃ©sultat
```

**Processus interne** :

```javascript
// 1. Mongos consulte les mÃ©tadonnÃ©es
// Config: Chunk [user_10000, user_20000) â†’ shardA

// 2. Mongos envoie l'insertion au Primary de shardA
// write: { insert: "users", documents: [{ user_id: "user_12345", ... }] }

// 3. ShardA Primary rÃ©plique vers ses Secondaries
// 4. Mongos retourne le rÃ©sultat avec write concern
```

#### Insertions en Masse (Bulk)

```javascript
// insertMany avec ordered: true (par dÃ©faut)
db.users.insertMany([
  { user_id: "user_10001", name: "User 1" },
  { user_id: "user_50001", name: "User 2" },  // Peut-Ãªtre sur un autre shard
  { user_id: "user_10002", name: "User 3" }
], { ordered: true })

// Comportement :
// - Mongos route chaque document vers son shard
// - Si un shard diffÃ©rent : nouvelle connexion
// - Si ordered: true, arrÃªt Ã  la premiÃ¨re erreur
// - Si ordered: false, continue malgrÃ© les erreurs

// RÃ©sultat :
{
  "acknowledged": true,
  "insertedIds": {
    "0": ObjectId("..."),
    "1": ObjectId("..."),
    "2": ObjectId("...")
  }
}
```

**Optimisation pour insertions massives** :

```javascript
// Grouper par shard implicitement
// MongoDB 5.0+ optimise automatiquement

// Mais pour contrÃ´le explicite :
var bulkA = db.users.initializeUnorderedBulkOp();
var bulkB = db.users.initializeUnorderedBulkOp();

// Documents pour shardA (user_id < 50000)
for (var i = 10000; i < 20000; i++) {
  bulkA.insert({ user_id: "user_" + i, name: "User " + i });
}

// Documents pour shardB (user_id >= 50000)
for (var i = 50000; i < 60000; i++) {
  bulkB.insert({ user_id: "user_" + i, name: "User " + i });
}

// ExÃ©cuter en parallÃ¨le (via promises dans un driver)
bulkA.execute();
bulkB.execute();
```

#### Write Concern dans un Cluster ShardÃ©

```javascript
// Write concern s'applique au shard cible, pas au cluster entier
db.users.insertOne(
  { user_id: "user_12345", name: "Alice" },
  {
    writeConcern: {
      w: "majority",  // MajoritÃ© des membres du shard
      j: true,        // Journal committÃ©
      wtimeout: 5000  // Timeout 5 secondes
    }
  }
)

// Si le document va sur shardA (replica set de 3 membres) :
// w: "majority" = 2 membres (Primary + 1 Secondary)

// âš ï¸ Attention : w: "majority" ne garantit PAS
// la rÃ©plication sur tous les shards, uniquement sur le shard cible
```

### Lectures (Read)

#### RequÃªtes CiblÃ©es (Targeted Queries)

```javascript
// RequÃªte incluant la shard key
db.users.find({ user_id: "user_12345" })

// Mongos :
// 1. Identifie le chunk contenant user_12345
// 2. Route vers un seul shard (shardA)
// 3. Retourne les rÃ©sultats

// explain() montre que c'est targeted
db.users.find({ user_id: "user_12345" }).explain("executionStats")

// RÃ©sultat :
{
  "queryPlanner": {
    "winningPlan": {
      "stage": "SINGLE_SHARD",  // âœ… RequÃªte ciblÃ©e !
      "shards": [
        {
          "shardName": "shardA",
          // ...
        }
      ]
    }
  }
}
```

#### RequÃªtes Broadcast (Scatter-Gather)

```javascript
// RequÃªte sans shard key
db.users.find({ email: "alice@example.com" })

// Mongos :
// 1. Ne peut pas dÃ©terminer le shard â†’ broadcast
// 2. Envoie la requÃªte Ã  TOUS les shards
// 3. Collecte les rÃ©sultats (gather)
// 4. Merge et retourne

// explain() montre le broadcast
db.users.find({ email: "alice@example.com" }).explain("executionStats")

// RÃ©sultat :
{
  "queryPlanner": {
    "winningPlan": {
      "stage": "SHARD_MERGE",  // âš ï¸ RequÃªte broadcast
      "shards": [
        { "shardName": "shardA", /* ... */ },
        { "shardName": "shardB", /* ... */ }
      ]
    }
  },
  "executionStats": {
    "totalDocsExamined": 10000000,  // Sur tous les shards !
    "executionTimeMillis": 2500
  }
}
```

**Impact performance** :

```javascript
// Comparaison : Targeted vs Broadcast (cluster 5 shards)

// Targeted (avec shard key)
db.orders.find({ customer_id: "CUST12345" })
// - 1 shard interrogÃ©
// - Latence : ~10ms
// - Documents examinÃ©s : 1000

// Broadcast (sans shard key)
db.orders.find({ order_status: "pending" })
// - 5 shards interrogÃ©s
// - Latence : ~50ms (5x)
// - Documents examinÃ©s : 5000000 (5000x)
```

#### Read Preference dans un Cluster ShardÃ©

```javascript
// Read preference s'applique aux replica sets des shards

// Lecture sur Primary uniquement (dÃ©faut)
db.users.find({ user_id: "user_12345" })
  .readPref("primary")

// Lecture sur Secondary prÃ©fÃ©rÃ©e (pour reporting)
db.users.find({ user_id: "user_12345" })
  .readPref("secondaryPreferred")

// Lecture sur Secondary le plus proche
db.users.find({ user_id: "user_12345" })
  .readPref("nearest")

// âš ï¸ Avec broadcast, chaque shard utilise la read preference
// Peut charger les secondaries si configured
```

**Cas d'usage read preferences** :

```javascript
// 1. Analytics/Reporting : DÃ©charger les primaries
db.analytics.aggregate([
  { $match: { date: { $gte: ISODate("2024-01-01") } } },
  { $group: { _id: "$product", totalSales: { $sum: "$amount" } } }
])
.readPref("secondary")
.allowDiskUse(true)

// 2. GÃ©olocalisation : Minimiser latence
// Application en Europe, secondary en Europe
db.users.find({ user_id: "user_EU_12345" })
  .readPref("nearest", [{ "datacenter": "europe" }])
```

### Mises Ã  Jour (Update)

#### Update CiblÃ© (avec shard key)

```javascript
// Update incluant la shard key
db.users.updateOne(
  { user_id: "user_12345" },  // Shard key prÃ©sente
  { $set: { last_login: new Date() } }
)

// Mongos :
// 1. Route vers le shard contenant user_12345
// 2. Update exÃ©cutÃ© localement sur ce shard
// 3. RÃ©sultat retournÃ©

// RÃ©sultat :
{
  "acknowledged": true,
  "matchedCount": 1,
  "modifiedCount": 1
}
```

#### Update Broadcast (sans shard key)

```javascript
// Update sans shard key (ou updateMany)
db.users.updateMany(
  { status: "inactive" },  // Pas de shard key
  { $set: { notification_sent: true } }
)

// Mongos :
// 1. Envoie l'update Ã  TOUS les shards
// 2. Chaque shard exÃ©cute localement
// 3. AgrÃ¨ge les rÃ©sultats

// RÃ©sultat :
{
  "acknowledged": true,
  "matchedCount": 1500,  // Somme de tous les shards
  "modifiedCount": 1500
}
```

#### âš ï¸ Restriction : Modification de la Shard Key

```javascript
// MongoDB < 4.2 : Interdit
db.users.updateOne(
  { user_id: "user_12345" },
  { $set: { user_id: "user_99999" } }  // âŒ Erreur ImmutableField
)

// MongoDB >= 4.2 : AutorisÃ© mais coÃ»teux
db.users.updateOne(
  { user_id: "user_12345" },
  { $set: { user_id: "user_99999" } }  // âœ… OK mais migration interne
)

// Processus interne (MongoDB >= 4.2) :
// 1. VÃ©rifier que user_99999 est sur un autre chunk
// 2. Supprimer le document sur shardA
// 3. InsÃ©rer le document sur shardB
// 4. OpÃ©ration transactionnelle (atomique)
```

### Suppressions (Delete)

#### Delete CiblÃ©

```javascript
// Delete avec shard key
db.users.deleteOne({ user_id: "user_12345" })

// Mongos route vers le bon shard
// Suppression locale
```

#### Delete Broadcast

```javascript
// Delete sans shard key
db.users.deleteMany({ last_login: { $lt: ISODate("2022-01-01") } })

// Broadcast Ã  tous les shards
// Chaque shard supprime localement
// RÃ©sultats agrÃ©gÃ©s

// RÃ©sultat :
{
  "acknowledged": true,
  "deletedCount": 5000  // Somme de tous les shards
}
```

#### Truncate d'une Collection ShardÃ©e

```javascript
// drop() sur une collection shardÃ©e
db.users.drop()

// Processus :
// 1. Mongos envoie dropCollection Ã  tous les shards
// 2. Chaque shard supprime sa partie de la collection
// 3. MÃ©tadonnÃ©es nettoyÃ©es sur config servers
// 4. Confirmation retournÃ©e

// Plus rapide que deleteMany({})
```

---

## AgrÃ©gations dans un Cluster ShardÃ©

### Pipeline d'AgrÃ©gation : ExÃ©cution DistribuÃ©e

```javascript
// AgrÃ©gation complexe
db.orders.aggregate([
  { $match: { order_date: { $gte: ISODate("2024-01-01") } } },
  { $group: {
      _id: "$customer_id",
      totalAmount: { $sum: "$amount" },
      orderCount: { $sum: 1 }
    }
  },
  { $sort: { totalAmount: -1 } },
  { $limit: 10 }
])

// ExÃ©cution optimisÃ©e par MongoDB :
//
// Phase 1 : SHARD (parallÃ¨le sur chaque shard)
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ ShardA : $match â†’ $group (partial)  â”‚
// â”‚ ShardB : $match â†’ $group (partial)  â”‚
// â”‚ ShardC : $match â†’ $group (partial)  â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//                  â”‚
//                  â–¼
// Phase 2 : MERGE (sur mongos ou shard primaire)
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ Mongos : $group (final merge)       â”‚
// â”‚          $sort                       â”‚
// â”‚          $limit                      â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ã‰tapes qui Forcent le Merge

Certaines Ã©tapes nÃ©cessitent un merge centralisÃ© :

```javascript
// 1. $lookup (jointures)
db.orders.aggregate([
  { $match: { customer_id: "CUST12345" } },  // Sur shard
  { $lookup: {  // âš ï¸ Force merge sur mongos
      from: "customers",
      localField: "customer_id",
      foreignField: "_id",
      as: "customer_info"
    }
  }
])

// 2. $sort global sans limite
db.orders.aggregate([
  { $match: { order_date: { $gte: ISODate("2024-01-01") } } },  // Sur shard
  { $sort: { amount: -1 } }  // âš ï¸ Merge nÃ©cessaire (sauf si $limit suit)
])

// 3. $facet
db.orders.aggregate([
  { $match: { status: "completed" } },  // Sur shard
  { $facet: {  // âš ï¸ Force merge
      byCategory: [{ $group: { _id: "$category", count: { $sum: 1 } } }],
      byMonth: [{ $group: { _id: { $month: "$date" }, total: { $sum: "$amount" } } }]
    }
  }
])
```

### Optimisation : allowDiskUse

```javascript
// Pour agrÃ©gations volumineuses
db.orders.aggregate([
  { $match: { order_date: { $gte: ISODate("2023-01-01") } } },
  { $group: {
      _id: "$product_id",
      stats: { $push: "$$ROOT" }  // Peut dÃ©passer 100 MB
    }
  }
], { allowDiskUse: true })

// allowDiskUse : true
// - Permet l'usage du disque temporaire si RAM insuffisante
// - S'applique sur chaque shard individuellement
// - Performance rÃ©duite mais Ã©vite les erreurs de mÃ©moire
```

### AgrÃ©gations avec Shard Key : $merge et $out

```javascript
// $out : CrÃ©er une nouvelle collection shardÃ©e
db.daily_sales.aggregate([
  { $match: { date: ISODate("2024-01-15") } },
  { $group: { _id: "$product_id", totalSales: { $sum: "$amount" } } },
  { $out: "sales_summary" }  // Nouvelle collection
])

// Par dÃ©faut, la nouvelle collection est NON shardÃ©e
// Pour crÃ©er une collection shardÃ©e en sortie :

// 1. PrÃ©-crÃ©er et sharder la collection de sortie
sh.shardCollection("mydb.sales_summary", { product_id: 1 })

// 2. Utiliser $merge au lieu de $out
db.daily_sales.aggregate([
  { $match: { date: ISODate("2024-01-15") } },
  { $group: { _id: "$product_id", totalSales: { $sum: "$amount" } } },
  { $merge: {
      into: "sales_summary",
      on: "_id",  // product_id (doit Ãªtre la shard key ou partie de)
      whenMatched: "replace",
      whenNotMatched: "insert"
    }
  }
])
```

---

## Gestion des Index

### CrÃ©ation d'Index sur Collection ShardÃ©e

```javascript
// CrÃ©er un index via mongos
db.users.createIndex({ email: 1 })

// Processus :
// 1. Mongos envoie createIndex Ã  TOUS les shards
// 2. Chaque shard construit l'index localement
// 3. Construction en parallÃ¨le sur tous les shards
// 4. Mongos attend que tous aient terminÃ©
// 5. Retourne le succÃ¨s global

// DurÃ©e = durÃ©e du shard le plus lent
```

#### CrÃ©ation d'Index en ArriÃ¨re-Plan

```javascript
// Index background (moins de blocage)
db.users.createIndex(
  { last_login: 1 },
  { background: true }  // DÃ©preciÃ© depuis MongoDB 4.2
)

// MongoDB 4.2+ : Toujours en arriÃ¨re-plan par dÃ©faut
// Mais pour foreground (plus rapide) :
db.users.createIndex(
  { last_login: 1 },
  { background: false }
)
```

#### Index avec Options SpÃ©cifiques

```javascript
// Index unique sur collection shardÃ©e
// âš ï¸ L'unicitÃ© est garantie PAR SHARD, pas globalement
// SAUF si l'index inclut la shard key

// âŒ Pas d'unicitÃ© globale (sans shard key)
db.users.createIndex(
  { email: 1 },
  { unique: true }  // âš ï¸ Unique par shard uniquement
)

// âœ… UnicitÃ© globale (inclut shard key)
db.users.createIndex(
  { user_id: 1, email: 1 },  // user_id est la shard key
  { unique: true }  // âœ… Unique globalement
)

// Ou shard key elle-mÃªme
db.users.createIndex(
  { user_id: 1 },
  { unique: true }  // âœ… Garantie par design du sharding
)
```

### Listing et Monitoring des Index

```javascript
// Lister les index via mongos
db.users.getIndexes()

// Retourne les index du premier shard trouvÃ©
// âš ï¸ Assume que tous les shards ont les mÃªmes index

// VÃ©rifier les index sur chaque shard individuellement
var shards = db.getSiblingDB("config").shards.find().toArray();

shards.forEach(function(shard) {
  print("Indexes sur " + shard._id + " :");

  // Connexion directe au shard (pour inspection)
  var shardConn = new Mongo(shard.host.split("/")[1].split(",")[0]);
  var indexes = shardConn.getDB("mydb").users.getIndexes();

  printjson(indexes);
});
```

### Suppression d'Index

```javascript
// Supprimer un index via mongos
db.users.dropIndex("email_1")

// Processus :
// 1. Mongos envoie dropIndex Ã  tous les shards
// 2. Chaque shard supprime l'index localement
// 3. Suppression en parallÃ¨le
// 4. Confirmation globale

// âš ï¸ Attention : Suppression immÃ©diate
// VÃ©rifier d'abord que l'index n'est pas utilisÃ©
db.users.aggregate([
  { $indexStats: {} },
  { $match: { name: "email_1" } }
])
```

---

## OpÃ©rations d'Administration

### Ajout d'un Shard

```javascript
// Contexte : Ajouter un nouveau shard au cluster pour scaling

// Ã‰tape 1 : PrÃ©parer le nouveau replica set (shardD)
// - DÃ©ployer les membres du replica set
// - Initialiser le replica set
// - VÃ©rifier qu'il fonctionne correctement

// Ã‰tape 2 : Ajouter le shard via mongos
sh.addShard("shardD/shardD1.example.com:27018,shardD2.example.com:27018,shardD3.example.com:27018")

// RÃ©sultat :
{
  "shardAdded": "shardD",
  "ok": 1
}

// Ã‰tape 3 : VÃ©rifier l'ajout
sh.status()

// Ã‰tape 4 : Le balancer redistribue automatiquement
// Monitoring de la redistribution
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.commit",
  "details.to": "shardD"
}).sort({ time: -1 })
```

### Retrait d'un Shard (Drain)

```javascript
// Contexte : Retirer shardA du cluster (dÃ©commission hardware)

// Ã‰tape 1 : Initier le drain
db.adminCommand({ removeShard: "shardA" })

// RÃ©sultat initial :
{
  "msg": "draining started successfully",
  "state": "started",
  "shard": "shardA",
  "remaining": {
    "chunks": 45,  // Chunks Ã  migrer
    "dbs": 2       // Bases avec shardA comme primary shard
  }
}

// Ã‰tape 2 : Monitoring du drain
// Les chunks sont migrÃ©s automatiquement vers d'autres shards
db.adminCommand({ removeShard: "shardA" })

// RÃ©sultat intermÃ©diaire :
{
  "msg": "draining ongoing",
  "state": "ongoing",
  "remaining": {
    "chunks": 12,  // Progression
    "dbs": 2
  },
  "note": "you need to drop or movePrimary these databases"
}

// Ã‰tape 3 : GÃ©rer les bases dont shardA est primary
use config
db.databases.find({ primary: "shardA" })

// DÃ©placer le primary de chaque base
db.adminCommand({ movePrimary: "mydb", to: "shardB" })
db.adminCommand({ movePrimary: "analytics", to: "shardC" })

// Ã‰tape 4 : Finaliser le drain
db.adminCommand({ removeShard: "shardA" })

// RÃ©sultat final :
{
  "msg": "removeshard completed successfully",
  "state": "completed",
  "shard": "shardA"
}

// Ã‰tape 5 : ArrÃªter physiquement les serveurs de shardA
// Le replica set shardA peut maintenant Ãªtre dÃ©commissionnÃ©
```

### Changement de Shard Primaire d'une Base

```javascript
// Voir le shard primaire actuel
use config
db.databases.findOne({ _id: "mydb" })

// RÃ©sultat :
{
  "_id": "mydb",
  "primary": "shardA",  // Shard primaire actuel
  "partitioned": true
}

// Changer le shard primaire
db.adminCommand({ movePrimary: "mydb", to: "shardB" })

// âš ï¸ ATTENTION : OpÃ©ration coÃ»teuse
// - DÃ©place toutes les collections NON-shardÃ©es
// - Peut prendre des heures pour grandes bases
// - Impact performance pendant la migration

// Processus :
// 1. Copie des collections non-shardÃ©es de shardA â†’ shardB
// 2. Mise Ã  jour des mÃ©tadonnÃ©es
// 3. Suppression des donnÃ©es sur shardA

// Validation
use config
db.databases.findOne({ _id: "mydb" })

// RÃ©sultat aprÃ¨s :
{
  "_id": "mydb",
  "primary": "shardB",  // âœ… ChangÃ©
  "partitioned": true
}
```

---

## Maintenance et Mises Ã  Jour

### Rolling Upgrade d'un Cluster ShardÃ©

Ordre recommandÃ© pour minimiser l'impact :

```bash
# Ordre de mise Ã  jour (du moins critique au plus critique)
# 1. Mongos (routers)
# 2. Config Servers
# 3. Shards (un Ã  un)

# ========================================
# Phase 1 : Mise Ã  jour des Mongos
# ========================================

# Pour chaque mongos (mongos1, mongos2, mongos3) :

# 1a. Retirer du load balancer (si applicable)
# â†’ Diriger le trafic vers les autres mongos

# 1b. ArrÃªter le mongos
sudo systemctl stop mongos

# 1c. Mettre Ã  jour les binaires
sudo apt-get update && sudo apt-get install mongodb-org=7.0.0

# 1d. RedÃ©marrer
sudo systemctl start mongos

# 1e. VÃ©rifier
mongosh --host mongos1.example.com --eval "db.version()"

# 1f. RÃ©intÃ©grer au load balancer

# RÃ©pÃ©ter pour mongos2, mongos3...

# ========================================
# Phase 2 : Mise Ã  jour des Config Servers
# ========================================

# Pour chaque membre du replica set config (cfg1, cfg2, cfg3) :

# 2a. Sur les SECONDARIES d'abord
# Connexion au secondary
mongosh --host cfg2.example.com:27019

# VÃ©rifier le statut
rs.status()

# ArrÃªter le mongod
use admin
db.shutdownServer()

# Mettre Ã  jour et redÃ©marrer
sudo apt-get install mongodb-org=7.0.0
sudo systemctl start mongod-configsvr

# VÃ©rifier la rÃ©intÃ©gration au replica set
rs.status()

# RÃ©pÃ©ter pour cfg3

# 2b. PRIMARY en dernier
# Connexion au primary
mongosh --host cfg1.example.com:27019

# Forcer une Ã©lection (step down)
rs.stepDown(60)

# Attendre qu'un secondary devienne primary
sleep 10

# Puis mÃªme procÃ©dure : shutdown, upgrade, restart

# ========================================
# Phase 3 : Mise Ã  jour des Shards
# ========================================

# Pour chaque shard (shardA, shardB, etc.) :
# MÃªme procÃ©dure que pour config servers

# âš ï¸ Mettre Ã  jour UN SHARD Ã€ LA FOIS
# Ne pas mettre Ã  jour plusieurs shards simultanÃ©ment

# Pour shardA :
# 3a. Secondaries en premier (shardA2, shardA3)
# 3b. Primary en dernier (shardA1)

# Validation globale aprÃ¨s chaque shard
sh.status()
```

### Sauvegarde d'un Cluster ShardÃ©

#### MÃ©thode 1 : mongodump via Mongos

```bash
# Sauvegarde complÃ¨te du cluster via mongos
mongodump \
  --host mongos1.example.com \
  --port 27017 \
  --username admin \
  --password SecurePass123 \
  --authenticationDatabase admin \
  --out /backup/cluster-$(date +%Y%m%d) \
  --oplog

# Avantages :
# - Simple : une seule commande
# - Vue cohÃ©rente du cluster

# InconvÃ©nients :
# - Lent pour gros volumes (tout passe par mongos)
# - Impact performance pendant la sauvegarde
```

#### MÃ©thode 2 : Sauvegarde par Shard + Config Servers

```bash
# Sauvegarde optimisÃ©e : chaque shard en parallÃ¨le

# 1. ArrÃªter le balancer
mongosh --host mongos1.example.com --eval "sh.stopBalancer()"

# 2. Sauvegarder les config servers
mongodump \
  --host cfg1.example.com:27019 \
  --out /backup/configdb-$(date +%Y%m%d) \
  --oplog

# 3. Sauvegarder chaque shard en parallÃ¨le
# ShardA
mongodump \
  --host shardA1.example.com:27018 \
  --out /backup/shardA-$(date +%Y%m%d) \
  --oplog &

# ShardB
mongodump \
  --host shardB1.example.com:27018 \
  --out /backup/shardB-$(date +%Y%m%d) \
  --oplog &

# Attendre la fin de toutes les sauvegardes
wait

# 4. RÃ©activer le balancer
mongosh --host mongos1.example.com --eval "sh.startBalancer()"

# Avantages :
# - Plus rapide (parallÃ¨le)
# - Moins d'impact sur mongos

# InconvÃ©nients :
# - Plus complexe Ã  orchestrer
# - Restauration plus complexe
```

#### MÃ©thode 3 : Snapshots SystÃ¨me

```bash
# Utiliser les snapshots du systÃ¨me de stockage (LVM, EBS, etc.)

# 1. ArrÃªter le balancer
sh.stopBalancer()

# 2. Pour chaque shard :
#    a. Flush des Ã©critures
mongosh --host shardA1.example.com:27018
use admin
db.fsyncLock()  # Verrouiller les Ã©critures

#    b. CrÃ©er snapshot (exemple AWS EBS)
aws ec2 create-snapshot \
  --volume-id vol-shardA-data \
  --description "ShardA backup $(date +%Y%m%d)"

#    c. DÃ©verrouiller
db.fsyncUnlock()

# 3. RÃ©pÃ©ter pour tous les shards et config servers

# 4. RÃ©activer le balancer
sh.startBalancer()
```

### Restauration d'un Cluster ShardÃ©

```bash
# ScÃ©nario : Restauration complÃ¨te aprÃ¨s disaster

# Ã‰tape 1 : Restaurer les config servers
mongorestore \
  --host cfg1.example.com:27019 \
  --drop \
  /backup/configdb-20240115 \
  --oplogReplay

# RÃ©pÃ©ter pour cfg2, cfg3

# Ã‰tape 2 : Restaurer chaque shard
mongorestore \
  --host shardA1.example.com:27018 \
  --drop \
  /backup/shardA-20240115 \
  --oplogReplay

mongorestore \
  --host shardB1.example.com:27018 \
  --drop \
  /backup/shardB-20240115 \
  --oplogReplay

# Ã‰tape 3 : RedÃ©marrer les mongos
sudo systemctl restart mongos

# Ã‰tape 4 : VÃ©rifier l'intÃ©gritÃ©
mongosh --host mongos1.example.com
sh.status()

# VÃ©rifier les collections
db.users.countDocuments({})
```

---

## Monitoring OpÃ©rationnel

### MÃ©triques Critiques Ã  Surveiller

```javascript
// 1. Distribution des chunks
db.getSiblingDB("config").chunks.aggregate([
  { $group: {
      _id: { ns: "$ns", shard: "$shard" },
      count: { $sum: 1 }
    }
  },
  { $sort: { "_id.ns": 1, "_id.shard": 1 } }
])

// 2. Ã‰tat du balancer
sh.getBalancerState()
sh.isBalancerRunning()

// 3. Migrations rÃ©centes
db.getSiblingDB("config").changelog.find({
  what: { $in: ["moveChunk.start", "moveChunk.commit", "moveChunk.error"] },
  time: { $gte: new Date(Date.now() - 3600000) }
}).count()

// 4. Jumbo chunks
db.getSiblingDB("config").chunks.find({ jumbo: true }).count()

// 5. OpÃ©rations lentes
db.currentOp({
  "secs_running": { $gte: 5 },
  "op": { $in: ["query", "update", "remove"] }
})

// 6. Connexions actives
db.serverStatus().connections
```

### Dashboard de Monitoring Temps RÃ©el

```javascript
// monitoring-dashboard.js
// Script Ã  exÃ©cuter en continu

function displayDashboard() {
  // Clear screen (pour terminal)
  print("\x1Bc");

  print("=".repeat(80));
  print("CLUSTER SHARDÃ‰ - DASHBOARD OPÃ‰RATIONNEL");
  print("Heure : " + new Date().toLocaleString());
  print("=".repeat(80));
  print("");

  // Section 1 : SantÃ© du Cluster
  print("1. SANTÃ‰ DU CLUSTER");
  print("-".repeat(40));

  var shards = db.getSiblingDB("config").shards.find().toArray();
  print("  Nombre de shards : " + shards.length);

  shards.forEach(function(shard) {
    // Ping chaque shard
    try {
      var shardConn = new Mongo(shard.host.split("/")[1].split(",")[0]);
      shardConn.getDB("admin").ping();
      print("  âœ… " + shard._id + " : Online");
    } catch (e) {
      print("  âŒ " + shard._id + " : OFFLINE !");
    }
  });
  print("");

  // Section 2 : Balancer
  print("2. BALANCER");
  print("-".repeat(40));
  print("  Ã‰tat : " + (sh.getBalancerState() ? "ActivÃ©" : "DÃ©sactivÃ©"));
  print("  En cours : " + (sh.isBalancerRunning() ? "Oui" : "Non"));

  var activeMigrations = db.currentOp({
    op: "command",
    "command.moveChunk": { $exists: true }
  }).inprog.length;

  print("  Migrations actives : " + activeMigrations);
  print("");

  // Section 3 : Distribution
  print("3. DISTRIBUTION DES DONNÃ‰ES");
  print("-".repeat(40));

  var distribution = db.getSiblingDB("config").chunks.aggregate([
    { $group: { _id: "$shard", numChunks: { $sum: 1 } } },
    { $sort: { numChunks: -1 } }
  ]).toArray();

  var total = distribution.reduce((sum, d) => sum + d.numChunks, 0);

  distribution.forEach(function(dist) {
    var percent = ((dist.numChunks / total) * 100).toFixed(1);
    print("  " + dist._id + " : " + dist.numChunks + " chunks (" + percent + "%)");
  });
  print("");

  // Section 4 : Performance
  print("4. PERFORMANCE");
  print("-".repeat(40));

  var slowOps = db.currentOp({
    secs_running: { $gte: 5 }
  }).inprog.length;

  print("  OpÃ©rations lentes (>5s) : " + slowOps);

  var serverStatus = db.serverStatus();
  print("  Connexions actives : " + serverStatus.connections.current);
  print("  Opcounters (insert) : " + serverStatus.opcounters.insert);
  print("  Opcounters (query) : " + serverStatus.opcounters.query);
  print("");

  // Section 5 : Alertes
  print("5. ALERTES");
  print("-".repeat(40));

  var alerts = [];

  // Alerte : Jumbo chunks
  var jumboCount = db.getSiblingDB("config").chunks.find({ jumbo: true }).count();
  if (jumboCount > 0) {
    alerts.push("âš ï¸  " + jumboCount + " jumbo chunks dÃ©tectÃ©s");
  }

  // Alerte : DÃ©sÃ©quilibre
  if (distribution.length > 1) {
    var max = Math.max(...distribution.map(d => d.numChunks));
    var min = Math.min(...distribution.map(d => d.numChunks));
    var imbalance = ((max - min) / min) * 100;

    if (imbalance > 20) {
      alerts.push("âš ï¸  DÃ©sÃ©quilibre significatif : " + imbalance.toFixed(1) + "%");
    }
  }

  // Alerte : OpÃ©rations lentes
  if (slowOps > 10) {
    alerts.push("âš ï¸  Trop d'opÃ©rations lentes : " + slowOps);
  }

  if (alerts.length === 0) {
    print("  âœ… Aucune alerte");
  } else {
    alerts.forEach(function(alert) {
      print("  " + alert);
    });
  }

  print("");
  print("=".repeat(80));
}

// Boucle de rafraÃ®chissement
while (true) {
  displayDashboard();
  sleep(5000);  // RafraÃ®chir toutes les 5 secondes
}
```

---

## Troubleshooting OpÃ©rationnel

### ProblÃ¨me 1 : Performances DÃ©gradÃ©es

**Diagnostic** :

```javascript
// 1. Identifier les requÃªtes lentes
db.currentOp({
  "secs_running": { $gte: 3 },
  "op": { $in: ["query", "update"] }
})

// 2. Analyser une requÃªte lente
var slowOp = db.currentOp({ secs_running: { $gte: 3 } }).inprog[0];

print("Namespace : " + slowOp.ns);
print("OpÃ©ration : " + slowOp.op);
print("DurÃ©e : " + slowOp.secs_running + "s");
printjson(slowOp.command);

// 3. VÃ©rifier si c'est une requÃªte broadcast
if (slowOp.command && !slowOp.command.$db) {
  // Analyser avec explain
  var explainResult = db[slowOp.ns.split(".")[1]].explain("executionStats").find(slowOp.command.filter || {});

  if (explainResult.queryPlanner.winningPlan.stage === "SHARD_MERGE") {
    print("âš ï¸  RequÃªte BROADCAST dÃ©tectÃ©e - considÃ©rer ajout shard key au filtre");
  }
}

// 4. VÃ©rifier les index manquants
db[slowOp.ns.split(".")[1]].find(slowOp.command.filter || {}).explain("executionStats")
```

**Solutions** :

```javascript
// Solution 1 : Ajouter un index
db.users.createIndex({ email: 1, last_login: 1 })

// Solution 2 : Optimiser la requÃªte pour inclure shard key
// Avant (broadcast) :
db.orders.find({ order_status: "pending" })

// AprÃ¨s (targeted si possible) :
db.orders.find({
  customer_id: "CUST12345",  // Shard key
  order_status: "pending"
})

// Solution 3 : Utiliser read preference pour dÃ©charger primaries
db.analytics.find({ date: ISODate("2024-01-15") })
  .readPref("secondaryPreferred")
```

### ProblÃ¨me 2 : Cluster en Lecture Seule

**SymptÃ´mes** :

```javascript
// Tentative d'Ã©criture Ã©choue
db.users.insertOne({ user_id: "user_new", name: "Test" })

// Erreur :
MongoServerError: not master and slaveOk=false
// Ou
MongoServerError: can't connect to config server
```

**Diagnostic** :

```javascript
// 1. VÃ©rifier les config servers
sh.status()

// Si erreur : "can't connect to config server"
// Les config servers sont inaccessibles

// 2. VÃ©rifier chaque config server
var configServers = ["cfg1.example.com:27019", "cfg2.example.com:27019", "cfg3.example.com:27019"];

configServers.forEach(function(cs) {
  try {
    var conn = new Mongo(cs);
    var status = conn.getDB("admin").adminCommand({ replSetGetStatus: 1 });
    print(cs + " : " + status.myState + " (" + status.stateStr + ")");
  } catch (e) {
    print(cs + " : âŒ INJOIGNABLE - " + e.message);
  }
});
```

**Solutions** :

```bash
# Si majoritÃ© des config servers down :
# - Le cluster passe en lecture seule (sÃ©curitÃ©)

# Solution : Restaurer les config servers
# 1. RedÃ©marrer les config servers arrÃªtÃ©s
sudo systemctl start mongod-configsvr

# 2. VÃ©rifier le replica set
mongosh --host cfg1.example.com:27019
rs.status()

# 3. Si problÃ¨me d'Ã©lection, forcer une reconfig
cfg = rs.conf()
rs.reconfig(cfg, { force: true })

# 4. VÃ©rifier depuis mongos
mongosh --host mongos1.example.com
sh.status()  # Devrait fonctionner Ã  nouveau
```

### ProblÃ¨me 3 : Migration BloquÃ©e

**Diagnostic** :

```javascript
// Migration bloquÃ©e depuis longtemps
db.currentOp({
  op: "command",
  "command.moveChunk": { $exists: true },
  microsecs_running: { $gt: 1800000000 }  // > 30 minutes
})

// VÃ©rifier les logs de migration
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.error",
  time: { $gte: new Date(Date.now() - 3600000) }
}).sort({ time: -1 })
```

**Solutions** :

```javascript
// Solution 1 : Annuler la migration
var blockedMigration = db.currentOp({
  "command.moveChunk": { $exists: true }
}).inprog[0];

if (blockedMigration) {
  db.killOp(blockedMigration.opid);
  print("Migration annulÃ©e : " + blockedMigration.opid);
}

// Solution 2 : Identifier la cause
// - RÃ©seau saturÃ© ?
// - Chunk trop volumineux ?
// - Charge trop Ã©levÃ©e ?

// VÃ©rifier la taille du chunk
var chunk = db.getSiblingDB("config").chunks.findOne({
  ns: blockedMigration.ns,
  min: blockedMigration.command.find
});

print("Taille estimÃ©e du chunk : " + chunk.estimatedSize);

// Si jumbo chunk, le splitter avant re-tentative
if (chunk.jumbo) {
  sh.splitAt(blockedMigration.ns, chunk.max);
}

// Solution 3 : RÃ©essayer pendant une fenÃªtre de faible charge
sh.stopBalancer();
// Attendre une heure creuse
sh.startBalancer();
```

---

## Anti-Patterns OpÃ©rationnels

### âŒ Anti-Pattern 1 : Connexion Directe aux Shards

**ProblÃ¨me** :

```javascript
// âŒ Se connecter directement Ã  un shard
mongosh --host shardA1.example.com:27018

// Effectuer des opÃ©rations CRUD
db.users.find({ user_id: "user_12345" })
```

**ConsÃ©quence** :
- Vue partielle des donnÃ©es (un seul shard)
- MÃ©tadonnÃ©es non mises Ã  jour
- Risque de corruption du cluster

**Solution** :

```javascript
// âœ… TOUJOURS passer par mongos
mongosh --host mongos1.example.com:27017

// Pour inspection/debugging uniquement, connexion directe acceptable
// Mais JAMAIS pour opÃ©rations CRUD en production
```

### âŒ Anti-Pattern 2 : Write Concern Insuffisant

**ProblÃ¨me** :

```javascript
// âŒ Write concern par dÃ©faut (w: 1)
db.critical_data.insertOne({
  transaction_id: "TXN123",
  amount: 50000
})

// Si le primary du shard crash juste aprÃ¨s :
// - DonnÃ©e pas encore rÃ©pliquÃ©e
// - Perte de donnÃ©es possible
```

**ConsÃ©quence** :
- Risque de perte de donnÃ©es critiques
- Non-conformitÃ© aux exigences mÃ©tier

**Solution** :

```javascript
// âœ… Write concern "majority" pour donnÃ©es critiques
db.critical_data.insertOne(
  {
    transaction_id: "TXN123",
    amount: 50000
  },
  {
    writeConcern: {
      w: "majority",
      j: true,
      wtimeout: 5000
    }
  }
)

// Ou configurer au niveau de la collection
db.runCommand({
  collMod: "critical_data",
  writeConcern: { w: "majority", j: true }
})
```

### âŒ Anti-Pattern 3 : Ignorer les RequÃªtes Broadcast

**ProblÃ¨me** :

```javascript
// âŒ RequÃªtes systÃ©matiques sans shard key
db.orders.find({ status: "pending" })  // Broadcast !
db.users.find({ email: "alice@example.com" })  // Broadcast !
```

**ConsÃ©quence** :
- Latence multipliÃ©e par le nombre de shards
- Charge CPU/rÃ©seau Ã©levÃ©e
- ScalabilitÃ© limitÃ©e

**Solution** :

```javascript
// âœ… Option 1 : Remodeler pour inclure shard key
db.orders.find({
  customer_id: "CUST12345",  // Shard key
  status: "pending"
})

// âœ… Option 2 : Index secondaire + accepter broadcast
db.users.createIndex({ email: 1 })
db.users.find({ email: "alice@example.com" })  // Broadcast mais indexÃ©

// âœ… Option 3 : Refactorer la shard key si nÃ©cessaire
sh.refineCollectionShardKey("mydb.users", { email: 1, _id: 1 })
```

### âŒ Anti-Pattern 4 : Pas de Monitoring du Balancer

**ProblÃ¨me** :

```bash
# Balancer actif 24/7 sans surveillance
# Migrations pendant les pics de charge
# Personne ne monitore les Ã©checs
```

**ConsÃ©quence** :
- Impact performance imprÃ©visible
- DÃ©sÃ©quilibre s'aggrave sans dÃ©tection
- Jumbo chunks non traitÃ©s

**Solution** :

```javascript
// âœ… FenÃªtre de balancing
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  {
    $set: {
      activeWindow: {
        start: "02:00",
        stop: "06:00"
      }
    }
  },
  { upsert: true }
)

// âœ… Alerting automatisÃ©
// Script cron quotidien
function checkBalancerHealth() {
  var errors = db.getSiblingDB("config").changelog.count({
    what: "moveChunk.error",
    time: { $gte: new Date(Date.now() - 86400000) }
  });

  if (errors > 5) {
    // DÃ©clencher alerte PagerDuty/Slack
    print("ALERT: " + errors + " migration errors in last 24h");
  }
}
```

### âŒ Anti-Pattern 5 : Modifications Non TestÃ©es

**ProblÃ¨me** :

```bash
# Modifier la configuration du cluster en production
# Sans test prÃ©alable en staging
# Exemple : Changer la taille des chunks, ajouter un index unique, etc.
```

**ConsÃ©quence** :
- Comportements inattendus
- Downtime imprÃ©vu
- Rollback complexe

**Solution** :

```bash
# âœ… Pipeline de test rigoureux
# 1. Tester en environnement de dev
# 2. Tester en staging avec donnÃ©es rÃ©alistes
# 3. Documenter les rÃ©sultats
# 4. Planifier le dÃ©ploiement en production
# 5. Avoir un plan de rollback

# Exemple : Ajout d'index unique
# Dev â†’ OK
# Staging â†’ OK, durÃ©e: 15 min
# Prod â†’ Planifier fenÃªtre maintenance, informer Ã©quipes
```

---

## Checklist OpÃ©rationnelle Quotidienne

```yaml
daily_operations_checklist:

  morning_checks:
    - title: "VÃ©rifier la santÃ© du cluster"
      command: "sh.status()"
      expected: "Tous les shards et config servers online"

    - title: "Ã‰tat du balancer"
      command: "sh.getBalancerState() && sh.isBalancerRunning()"
      expected: "ActivÃ© mais pas de migration en cours (hors fenÃªtre)"

    - title: "Jumbo chunks"
      command: "db.getSiblingDB('config').chunks.find({ jumbo: true }).count()"
      expected: "0 jumbo chunks"

    - title: "Distribution des chunks"
      action: "VÃ©rifier Ã©cart entre shards < 10%"

    - title: "Ã‰checs de migration (24h)"
      command: "db.getSiblingDB('config').changelog.count({ what: 'moveChunk.error', time: { $gte: ... } })"
      expected: "< 5 Ã©checs"

  afternoon_checks:
    - title: "Performance des requÃªtes"
      command: "db.currentOp({ secs_running: { $gte: 5 } })"
      expected: "< 10 opÃ©rations lentes"

    - title: "Utilisation disque"
      action: "VÃ©rifier < 70% sur tous les shards"

    - title: "Connexions actives"
      command: "db.serverStatus().connections"
      expected: "< 80% du max"

  evening_checks:
    - title: "Sauvegarde quotidienne"
      action: "VÃ©rifier que la sauvegarde s'est bien exÃ©cutÃ©e"

    - title: "Logs d'erreurs"
      action: "Analyser les logs pour erreurs critiques"

  weekly_checks:
    - title: "Index inutilisÃ©s"
      command: "db.collection.aggregate([{ $indexStats: {} }])"
      action: "Supprimer les index non utilisÃ©s"

    - title: "Croissance des donnÃ©es"
      action: "Projeter la croissance et planifier scaling si nÃ©cessaire"

    - title: "Test de restauration"
      action: "Tester la restauration d'une sauvegarde en staging"
```

---

## Conclusion

Les opÃ©rations sur un cluster shardÃ© MongoDB nÃ©cessitent une comprÃ©hension approfondie de l'architecture distribuÃ©e et de ses implications. Les points clÃ©s Ã  retenir :

- âœ… **Toujours passer par mongos** : Jamais de connexion directe aux shards
- âœ… **Comprendre targeted vs broadcast** : Impact majeur sur les performances
- âœ… **Monitorer activement** : Balancer, migrations, distribution, jumbo chunks
- âœ… **Write concern adaptÃ©** : Selon la criticitÃ© des donnÃ©es
- âœ… **Planifier les maintenances** : Rolling upgrades, fenÃªtres de balancing
- âœ… **Tester avant production** : Toute modification doit Ãªtre testÃ©e
- âœ… **Automatiser le monitoring** : Alertes proactives sur anomalies

Les opÃ©rations quotidiennes doivent Ãªtre **routiniÃ¨res** et **automatisÃ©es** autant que possible, permettant aux Ã©quipes de se concentrer sur l'optimisation et l'Ã©volution du cluster plutÃ´t que sur la maintenance rÃ©active.

---

## Ressources

- [MongoDB Documentation - Sharded Cluster Administration](https://docs.mongodb.com/manual/administration/sharded-cluster-administration/)
- [MongoDB Documentation - Backup and Restore Sharded Clusters](https://docs.mongodb.com/manual/tutorial/backup-sharded-cluster-with-database-dumps/)
- [MongoDB Operations Checklist](https://docs.mongodb.com/manual/administration/production-checklist-operations/)
- [MongoDB University - M103: Basic Cluster Administration](https://university.mongodb.com/)

---


â­ï¸ [Monitoring et maintenance](/10-sharding/10-monitoring-maintenance.md)
