üîù Retour au [Sommaire](/SOMMAIRE.md)

# 10.12 Bonnes Pratiques de Sharding

## Introduction

Le sharding MongoDB est une technologie puissante qui permet de faire √©voluer horizontalement votre base de donn√©es pour g√©rer des volumes massifs de donn√©es et de trafic. Cependant, sa complexit√© inh√©rente peut conduire √† des erreurs co√ªteuses si les bonnes pratiques ne sont pas respect√©es d√®s la conception et tout au long du cycle de vie du cluster.

Cette section synth√©tise l'ensemble des meilleures pratiques accumul√©es au fil des chapitres pr√©c√©dents et ajoute des recommandations √©prouv√©es en production. Elle constitue un guide de r√©f√©rence pour architectes, d√©veloppeurs et administrateurs travaillant avec des clusters shard√©s MongoDB.

---

## Bonnes Pratiques de Conception

### 1. Choix de la Shard Key : Le Fondement Critique

La shard key est **la d√©cision la plus importante** lors du d√©ploiement d'un cluster shard√©. Une mauvaise shard key peut rendre le cluster inefficace et difficile √† corriger.

#### Crit√®res d'√âvaluation

| Crit√®re | Description | M√©thode de Validation |
|---------|-------------|----------------------|
| **Cardinalit√© √©lev√©e** | Nombre de valeurs distinctes > 10 000 | `db.collection.distinct("field").length` |
| **Distribution uniforme** | Pas de valeurs ultra-dominantes | Analyse avec agr√©gation $group |
| **Localit√© des requ√™tes** | Requ√™tes fr√©quentes incluent la shard key | Analyse des logs applicatifs |
| **Immutabilit√©** | Valeur ne change pas apr√®s insertion | Revue du mod√®le de donn√©es |
| **Monotonie √©vit√©e** | Pas de timestamps ou _id s√©quentiels seuls | Utiliser hashed ou compound |

#### Processus de D√©cision

```javascript
// √âtape 1 : Identifier les candidats
// Analyser les patterns d'acc√®s de l'application

// √âtape 2 : Pour chaque candidat, ex√©cuter
function evaluateShardKey(dbName, collName, candidateField) {
  print("=== √âvaluation : " + candidateField + " ===\n");

  var coll = db.getSiblingDB(dbName)[collName];
  var totalDocs = coll.countDocuments({});

  // 1. Cardinalit√©
  var distinct = coll.distinct(candidateField).length;
  var cardinalityScore = distinct > 10000 ? 10 : distinct > 1000 ? 7 : distinct > 100 ? 4 : 1;
  print("Cardinalit√© : " + distinct + " valeurs distinctes (Score: " + cardinalityScore + "/10)");

  // 2. Distribution
  var topValues = coll.aggregate([
    { $group: { _id: "$" + candidateField, count: { $sum: 1 } } },
    { $sort: { count: -1 } },
    { $limit: 5 }
  ]).toArray();

  var maxPct = (topValues[0].count / totalDocs * 100).toFixed(2);
  var distributionScore = maxPct < 10 ? 10 : maxPct < 25 ? 7 : maxPct < 50 ? 4 : 1;
  print("Distribution : Top valeur = " + maxPct + "% (Score: " + distributionScore + "/10)");

  // 3. Nullit√©
  var nullCount = coll.countDocuments({ [candidateField]: null });
  var nullPct = (nullCount / totalDocs * 100).toFixed(2);
  var nullScore = nullPct < 1 ? 10 : nullPct < 5 ? 7 : nullPct < 20 ? 4 : 1;
  print("Nullit√© : " + nullPct + "% nulls (Score: " + nullScore + "/10)");

  // Score final
  var totalScore = (cardinalityScore + distributionScore + nullScore) / 3;
  print("\nüìä Score Total : " + totalScore.toFixed(1) + "/10");

  if (totalScore >= 8) {
    print("‚úÖ Excellente shard key candidate");
  } else if (totalScore >= 6) {
    print("‚ö†Ô∏è  Acceptable, mais consid√©rer une compound key");
  } else {
    print("‚ùå Non recommand√©e, chercher alternative");
  }

  print("\n" + "=".repeat(50) + "\n");

  return totalScore;
}

// Exemple d'utilisation
evaluateShardKey("mydb", "orders", "customer_id");
evaluateShardKey("mydb", "orders", "status");
evaluateShardKey("mydb", "orders", "order_date");
```

#### D√©cisions par Type d'Application

```yaml
e_commerce:
  collections:
    orders:
      recommended: { customer_id: 1, order_date: 1 }
      rationale: "Localit√© par client, √©vite monotonie"

    products:
      recommended: { category: 1, product_id: 1 }
      rationale: "Distribution par cat√©gorie, granularit√© par produit"

    sessions:
      recommended: { session_id: "hashed" }
      rationale: "Distribution uniforme garantie"

saas_multi_tenant:
  collections:
    documents:
      recommended: { tenant_id: 1, document_id: 1 }
      rationale: "Isolation par tenant, √©vite jumbo chunks gros clients"

    events:
      recommended: { tenant_id: "hashed", timestamp: 1 }
      rationale: "Distribution uniforme m√™me pour gros tenants"

iot_time_series:
  collections:
    sensor_data:
      recommended: { sensor_id: "hashed", timestamp: 1 }
      rationale: "√âvite hot spots sur timestamps r√©cents"

    metrics:
      recommended: { metric_name: 1, timestamp: 1 }
      rationale: "Regroupement par m√©trique, ordre temporel"

social_media:
  collections:
    posts:
      recommended: { author_id: "hashed" }
      rationale: "Distribution uniforme des utilisateurs populaires"

    messages:
      recommended: { conversation_id: 1, timestamp: 1 }
      rationale: "Localit√© des conversations"

logs_analytics:
  collections:
    logs:
      recommended: { application: 1, timestamp: 1 }
      rationale: "Isolation par application, ordre temporel"

    events:
      recommended: { event_type: "hashed", timestamp: 1 }
      rationale: "Distribution uniforme par type"
```

### 2. Mod√©lisation des Donn√©es pour le Sharding

#### Principes de Mod√©lisation

```javascript
// ‚úÖ BON : Documents auto-suffisants
{
  "_id": ObjectId("..."),
  "order_id": "ORD12345",
  "customer": {
    "id": "CUST789",
    "name": "Alice Dupont",
    "email": "alice@example.com"
  },
  "items": [
    {
      "product_id": "PROD001",
      "name": "Laptop",
      "quantity": 1,
      "price": 999.99
    }
  ],
  "total_amount": 999.99,
  "created_at": ISODate("2024-01-15T10:30:00Z")
}
// Requ√™te cibl√©e : find({ order_id: "ORD12345" })
// ‚Üí 1 seul document, 1 seul shard

// ‚ùå MAUVAIS : Documents n√©cessitant $lookup
{
  "_id": ObjectId("..."),
  "order_id": "ORD12345",
  "customer_id": "CUST789",  // R√©f√©rence
  "items": ["PROD001"]       // R√©f√©rences
}
// Requ√™te : find({ order_id: "ORD12345" }) + $lookup customers + $lookup products
// ‚Üí Plusieurs shards, $lookup co√ªteux
```

#### Strat√©gies de D√©normalisation

```javascript
// Pattern Extended Reference : Embarquer les donn√©es fr√©quemment acc√©d√©es

// Au lieu de :
// Collection orders : { customer_id: "CUST123" }
// Collection customers : { _id: "CUST123", name: "...", email: "...", ... }

// Pr√©f√©rer :
{
  "order_id": "ORD12345",
  "customer": {
    "id": "CUST123",
    "name": "Alice Dupont",      // Donn√©es fr√©quentes
    "email": "alice@example.com", // Donn√©es fr√©quentes
    // "full_address": "..."      // Donn√©es rares ‚Üí pas incluses
  }
}

// Mise √† jour : Si customer.name change
// Option 1 : Accepter l'incoh√©rence temporaire (eventually consistent)
// Option 2 : Mettre √† jour tous les orders (co√ªteux mais coh√©rent)
// Option 3 : Hybrid : n'embarquer que les donn√©es immuables

// D√©cision d√©pend du cas d'usage
```

### 3. Taille des Documents et des Chunks

#### Limites et Recommandations

| √âl√©ment | Limite MongoDB | Recommandation | Rationale |
|---------|---------------|---------------|-----------|
| **Taille document** | 16 MB (hard limit) | < 1 MB | Performance et flexibilit√© |
| **Taille chunk** | Configurable | 64-128 MB | √âquilibre migration/m√©tadonn√©es |
| **Documents par chunk** | Aucune limite | 50K-500K | D√©pend taille moyenne documents |
| **Chunks par shard** | Aucune limite | 100-1000 | √âquilibre distribution/overhead |

#### Configuration de la Taille des Chunks

```javascript
// D√©terminer la taille optimale selon le cas d'usage

// Petits documents (< 10 KB), haute fr√©quence d'insertion
// ‚Üí Chunks plus gros (128 MB)
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 128 } },
  { upsert: true }
)

// Gros documents (> 100 KB), faible cardinalit√© shard key
// ‚Üí Chunks plus petits (32 MB)
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 32 } },
  { upsert: true }
)

// Standard (documents moyens 1-50 KB)
// ‚Üí D√©faut (64 MB) convient
```

---

## Bonnes Pratiques de D√©ploiement

### 1. Topologie Recommand√©e

#### Environnement de Production Minimal

```
Composants Minimums (Haute Disponibilit√©) :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3 Config Servers (Replica Set 3 membres)         ‚îÇ
‚îÇ 2+ Shards (chacun Replica Set 3 membres)         ‚îÇ
‚îÇ 2+ Mongos (load balanced)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Total Minimum : 12 serveurs
- 3 config servers
- 6 shard members (2 shards √ó 3)
- 3 mongos

Recommandation Production : 15-20 serveurs
- 3 config servers
- 9-12 shard members (3-4 shards √ó 3)
- 3-5 mongos
```

#### Distribution Multi-Datacenter

```javascript
// Configuration avec 3 datacenters pour haute disponibilit√©

// Strat√©gie 1 : Priorit√©s d'√©lection
// DC1 (primary), DC2 (secondary), DC3 (disaster recovery)

// Config Servers
configReplSet = {
  _id: "configReplSet",
  configsvr: true,
  members: [
    { _id: 0, host: "cfg1-dc1:27019", priority: 3 },  // DC1
    { _id: 1, host: "cfg2-dc2:27019", priority: 2 },  // DC2
    { _id: 2, host: "cfg3-dc3:27019", priority: 1 }   // DC3
  ]
}

// Shard A : Primary pr√©f√©r√© DC1
shardA = {
  _id: "shardA",
  members: [
    { _id: 0, host: "shardA1-dc1:27018", priority: 3 },
    { _id: 1, host: "shardA2-dc2:27018", priority: 2 },
    { _id: 2, host: "shardA3-dc3:27018", priority: 1 }
  ]
}

// Shard B : Primary pr√©f√©r√© DC2 (distribution de charge)
shardB = {
  _id: "shardB",
  members: [
    { _id: 0, host: "shardB1-dc1:27018", priority: 2 },
    { _id: 1, host: "shardB2-dc2:27018", priority: 3 },
    { _id: 2, host: "shardB3-dc3:27018", priority: 1 }
  ]
}

// Mongos : Un ou plusieurs par datacenter
// Application DC1 ‚Üí mongos DC1
// Application DC2 ‚Üí mongos DC2
```

### 2. Dimensionnement Mat√©riel

#### Calcul des Ressources par Composant

```javascript
// Formules de dimensionnement

// === CONFIG SERVERS ===
// Besoins minimaux (m√©tadonn√©es uniquement)
configServerResources = {
  cpu: "2-4 cores",
  ram: "8-16 GB",
  storage: {
    size: "50-100 GB SSD",
    iops: "1000+",
    calculation: function(numCollections, numChunks) {
      // ~1 KB par chunk en m√©tadonn√©es
      var metadataGB = (numChunks * 1024) / (1024 * 1024 * 1024);
      var withOverhead = metadataGB * 2;  // 2x pour overhead
      return Math.max(50, withOverhead);
    }
  }
};

// === SHARDS ===
// D√©pend du working set
shardResources = {
  cpu: "8-32 cores",
  ram: function(workingSetGB) {
    // Formule : RAM >= 1.5 √ó working set (pour cache WiredTiger)
    return Math.max(32, workingSetGB * 1.5);
  },
  storage: {
    size: function(dataGB, compressionRatio) {
      // Formule : Storage = (Data / compression ratio) √ó 1.5 (overhead)
      // WiredTiger compression ~3-5x pour documents texte
      var compressed = dataGB / compressionRatio;
      return compressed * 1.5;
    },
    iops: "5000-10000+ (SSD/NVMe recommand√©)",
    type: "SSD minimum, NVMe pour haute performance"
  }
};

// === MONGOS ===
// L√©ger (pas de stockage)
mongosResources = {
  cpu: "4-8 cores",
  ram: "8-16 GB",
  storage: "20 GB (logs uniquement)",
  network: "10 Gbps (goulot potentiel)"
};

// Exemple de calcul pour un cluster
function calculateClusterResources(config) {
  print("=== Dimensionnement du Cluster ===\n");

  var totalDataGB = config.totalDataGB;
  var compressionRatio = config.compressionRatio || 3;
  var workingSetPct = config.workingSetPct || 0.3;  // 30% du dataset actif
  var numShards = config.numShards;

  // Working set par shard
  var workingSetPerShardGB = (totalDataGB * workingSetPct) / numShards;

  // RAM par shard
  var ramPerShardGB = Math.max(32, workingSetPerShardGB * 1.5);

  // Stockage par shard
  var compressedDataGB = totalDataGB / compressionRatio;
  var storagePerShardGB = (compressedDataGB / numShards) * 1.5;

  print("Configuration :");
  print("  Donn√©es totales : " + totalDataGB + " GB");
  print("  Nombre de shards : " + numShards);
  print("  Working set : " + (workingSetPct * 100) + "%");
  print("");

  print("Ressources par Shard :");
  print("  CPU : 16+ cores");
  print("  RAM : " + Math.ceil(ramPerShardGB) + " GB");
  print("  Stockage : " + Math.ceil(storagePerShardGB) + " GB SSD/NVMe");
  print("");

  print("Ressources Config Servers (3 membres) :");
  print("  CPU : 4 cores");
  print("  RAM : 16 GB");
  print("  Stockage : 100 GB SSD");
  print("");

  print("Ressources Mongos (3 instances) :");
  print("  CPU : 8 cores");
  print("  RAM : 16 GB");
  print("  Stockage : 20 GB");
}

// Exemple : Cluster avec 10 TB de donn√©es
calculateClusterResources({
  totalDataGB: 10000,
  compressionRatio: 3,
  workingSetPct: 0.3,
  numShards: 5
});
```

### 3. Pr√©-splitting et Distribution Initiale

#### Strat√©gie de Pr√©-splitting

```javascript
// TOUJOURS pr√©-splitter avant import massif

function presplitForImport(namespace, shardKey, config) {
  print("=== Pr√©-splitting pour Import ===\n");

  var numShards = db.getSiblingDB("config").shards.count();
  var targetChunksPerShard = config.targetChunksPerShard || 4;
  var totalChunks = numShards * targetChunksPerShard;

  print("Shards : " + numShards);
  print("Chunks par shard : " + targetChunksPerShard);
  print("Total chunks : " + totalChunks);
  print("");

  // Activer sharding
  var dbName = namespace.split(".")[0];
  var collName = namespace.split(".")[1];

  sh.enableSharding(dbName);

  // Pour hashed shard key
  if (Object.values(shardKey)[0] === "hashed") {
    sh.shardCollection(namespace, shardKey);

    // MongoDB cr√©e automatiquement 2 chunks par shard avec hashed
    // Pour plus de granularit√© :
    for (var i = 0; i < totalChunks; i++) {
      try {
        sh.splitAt(namespace, { [Object.keys(shardKey)[0]]: "split_" + i });
      } catch (e) {
        // Ignore les erreurs de split (normal avec hashed)
      }
    }
  }
  // Pour range shard key
  else {
    sh.shardCollection(namespace, shardKey);

    // Cr√©er les split points bas√©s sur les donn√©es existantes ou estim√©es
    var shardKeyField = Object.keys(shardKey)[0];

    // Si donn√©es existantes, utiliser percentiles
    if (config.existingData) {
      var coll = db.getSiblingDB(dbName)[collName];
      var total = coll.countDocuments({});
      var step = Math.floor(total / totalChunks);

      for (var i = 1; i < totalChunks; i++) {
        var doc = coll.find().sort({ [shardKeyField]: 1 }).skip(i * step).limit(1).toArray()[0];
        if (doc) {
          sh.splitAt(namespace, { [shardKeyField]: doc[shardKeyField] });
        }
      }
    }
    // Sinon, splits uniforms sur la plage attendue
    else if (config.range) {
      var min = config.range.min;
      var max = config.range.max;
      var step = (max - min) / totalChunks;

      for (var i = 1; i < totalChunks; i++) {
        sh.splitAt(namespace, { [shardKeyField]: min + (i * step) });
      }
    }
  }

  print("‚úÖ Pr√©-splitting termin√©");

  // Distribuer les chunks initialement
  var shards = db.getSiblingDB("config").shards.find().toArray();
  var chunks = db.getSiblingDB("config").chunks.find({ ns: namespace }).sort({ min: 1 }).toArray();

  print("\nDistribution des " + chunks.length + " chunks sur " + shards.length + " shards...");

  for (var i = 0; i < chunks.length; i++) {
    var targetShard = shards[i % shards.length]._id;

    try {
      sh.moveChunk(namespace, chunks[i].min, targetShard);
      print("  Chunk " + (i+1) + " ‚Üí " + targetShard);
    } catch (e) {
      // Ignore si d√©j√† sur le bon shard
    }
  }

  print("\n‚úÖ Distribution initiale termin√©e");
}

// Exemple : Import de 100M de commandes
presplitForImport("ecommerce.orders", { customer_id: 1 }, {
  targetChunksPerShard: 5,
  existingData: false,
  range: { min: 0, max: 100000000 }
});
```

---

## Bonnes Pratiques Op√©rationnelles

### 1. Monitoring et Alerting

#### Dashboard KPI Essentiels

```yaml
metriques_critiques:
  cluster_health:
    - metric: "mongodb_up"
      alert_threshold: "< 1 for 1m"
      severity: "critical"

    - metric: "config_server_majority_available"
      alert_threshold: "< 2 for 30s"
      severity: "critical"

  distribution:
    - metric: "chunk_imbalance_percentage"
      alert_threshold: "> 20% for 4h"
      severity: "warning"

    - metric: "jumbo_chunks_count"
      alert_threshold: "> 0 for 1h"
      severity: "warning"

  balancer:
    - metric: "migration_failure_rate_24h"
      alert_threshold: "> 10% for 30m"
      severity: "warning"

    - metric: "migration_duration_p95"
      alert_threshold: "> 600s for 1h"
      severity: "warning"

  performance:
    - metric: "query_latency_p99"
      alert_threshold: "> 500ms for 5m"
      severity: "warning"

    - metric: "wiredtiger_cache_usage"
      alert_threshold: "> 90% for 10m"
      severity: "warning"

    - metric: "replication_lag_max"
      alert_threshold: "> 30s for 5m"
      severity: "critical"

  capacity:
    - metric: "disk_usage_percentage"
      alert_threshold: "> 80% for 1h"
      severity: "warning"

    - metric: "connection_usage_percentage"
      alert_threshold: "> 80% for 10m"
      severity: "warning"
```

#### Script de Health Check Quotidien

```javascript
// health-check-daily.js
// √Ä ex√©cuter chaque matin pour validation du cluster

function dailyHealthCheck() {
  var report = {
    date: new Date(),
    cluster: "production",
    status: "healthy",
    warnings: [],
    errors: []
  };

  print("=== Health Check Quotidien ===");
  print("Date : " + report.date);
  print("");

  // 1. Sant√© du cluster
  try {
    var shards = db.getSiblingDB("config").shards.find().toArray();
    print("‚úÖ Shards : " + shards.length + " actifs");

    shards.forEach(function(shard) {
      try {
        var shardConn = new Mongo(shard.host.split("/")[1].split(",")[0]);
        shardConn.getDB("admin").ping();
      } catch (e) {
        report.errors.push("Shard " + shard._id + " inaccessible");
        report.status = "critical";
      }
    });
  } catch (e) {
    report.errors.push("Impossible de v√©rifier les shards : " + e.message);
    report.status = "critical";
  }

  // 2. Distribution des chunks
  var jumboCount = db.getSiblingDB("config").chunks.countDocuments({ jumbo: true });

  if (jumboCount > 0) {
    report.warnings.push(jumboCount + " jumbo chunks d√©tect√©s");
    if (report.status === "healthy") report.status = "warning";
  }

  print(jumboCount === 0 ? "‚úÖ" : "‚ö†Ô∏è  " + " Jumbo chunks : " + jumboCount);

  // 3. √âtat du balancer
  var balancerEnabled = sh.getBalancerState();
  var balancerRunning = sh.isBalancerRunning();

  print("‚úÖ Balancer : " + (balancerEnabled ? "Activ√©" : "D√©sactiv√©"));

  // 4. Migrations r√©centes
  var last24h = new Date(Date.now() - 86400000);
  var migrations = db.getSiblingDB("config").changelog.aggregate([
    { $match: {
        time: { $gte: last24h },
        what: { $in: ["moveChunk.commit", "moveChunk.error"] }
      }
    },
    { $group: { _id: "$what", count: { $sum: 1 } } }
  ]).toArray();

  var committed = 0, errors = 0;
  migrations.forEach(function(m) {
    if (m._id === "moveChunk.commit") committed = m.count;
    if (m._id === "moveChunk.error") errors = m.count;
  });

  if (errors > committed * 0.1) {
    report.warnings.push("Taux d'√©chec des migrations √©lev√© : " + (errors / (committed + errors) * 100).toFixed(1) + "%");
    if (report.status === "healthy") report.status = "warning";
  }

  print((errors === 0 ? "‚úÖ" : "‚ö†Ô∏è  ") + " Migrations (24h) : " + committed + " succ√®s, " + errors + " √©checs");

  // 5. Espace disque
  shards.forEach(function(shard) {
    try {
      var shardConn = new Mongo(shard.host.split("/")[1].split(",")[0]);
      var dbStats = shardConn.getDB("admin").adminCommand({ listDatabases: 1 });

      var totalSizeGB = dbStats.databases.reduce((sum, db) => sum + (db.sizeOnDisk || 0), 0) / (1024 * 1024 * 1024);

      if (totalSizeGB > 500) {  // Seuil configurable
        report.warnings.push("Shard " + shard._id + " utilise " + totalSizeGB.toFixed(0) + " GB");
      }

    } catch (e) {
      // Ignorer si inaccessible (d√©j√† signal√©)
    }
  });

  // R√©sum√©
  print("");
  print("=== R√©sum√© ===");
  print("Statut : " + report.status.toUpperCase());

  if (report.errors.length > 0) {
    print("\nüî¥ ERREURS :");
    report.errors.forEach(e => print("  - " + e));
  }

  if (report.warnings.length > 0) {
    print("\n‚ö†Ô∏è  AVERTISSEMENTS :");
    report.warnings.forEach(w => print("  - " + w));
  }

  if (report.status === "healthy") {
    print("\n‚úÖ Cluster en bonne sant√©");
  }

  // Enregistrer le rapport
  db.getSiblingDB("admin").health_reports.insertOne(report);

  return report;
}

// Ex√©cuter
dailyHealthCheck();
```

### 2. Backup et Disaster Recovery

#### Strat√©gie de Backup 3-2-1

```yaml
strategy_3_2_1:
  # 3 copies des donn√©es
  copies:
    - location: "Production cluster"
      type: "Live data"
      retention: "N/A"

    - location: "Backup storage (local)"
      type: "Daily snapshots"
      retention: "30 days"

    - location: "Backup storage (off-site)"
      type: "Weekly snapshots"
      retention: "1 year"

  # 2 types de m√©dia
  media:
    - type: "Disk (SAN/NAS)"
      usage: "Daily backups, fast restore"

    - type: "Cloud Object Storage (S3/GCS)"
      usage: "Long-term retention, DR"

  # 1 copie off-site
  offsite:
    location: "Different region/datacenter"
    sync: "Automated"
    test_frequency: "Monthly"

implementation:
  daily_backup:
    time: "02:00 UTC"
    method: "Snapshot or mongodump"
    script: "/scripts/backup-cluster.sh"

  weekly_backup:
    time: "Sunday 01:00 UTC"
    method: "Full snapshot"
    upload: "S3 bucket (encrypted)"

  monthly_restore_test:
    environment: "Staging cluster"
    validation: "Data integrity + query tests"
    documentation: "Test report required"
```

### 3. Maintenance Windows

#### Planification des Maintenances

```javascript
// D√©finir des fen√™tres de maintenance standard

// 1. Fen√™tre de balancing (quotidienne)
db.getSiblingDB("config").settings.updateOne(
  { _id: "balancer" },
  {
    $set: {
      activeWindow: {
        start: "02:00",  // Heures creuses
        stop: "06:00"
      }
    }
  },
  { upsert: true }
)

// 2. Fen√™tre de maintenance planifi√©e (mensuelle)
var maintenanceSchedule = {
  frequency: "monthly",
  dayOfWeek: "Sunday",  // Dimanche
  weekOfMonth: "first",  // Premier dimanche du mois
  startTime: "01:00 UTC",
  duration: "4 hours",
  activities: [
    "Rolling restart for patches",
    "Preventive maintenance script",
    "Backup verification",
    "Performance review"
  ],
  notification: {
    advance: "7 days",
    channels: ["email", "slack", "status-page"]
  }
};

// 3. Communication template
var maintenanceNotification = `
üìÖ MAINTENANCE PLANIFI√âE

Date : ${maintenanceSchedule.dayOfWeek} ${maintenanceSchedule.weekOfMonth} [DATE]
Heure : ${maintenanceSchedule.startTime} (dur√©e estim√©e: ${maintenanceSchedule.duration})

Impact attendu : Aucun (rolling maintenance)

Activit√©s :
${maintenanceSchedule.activities.map(a => "- " + a).join("\n")}

En cas de probl√®me : Contact √©quipe DBA
`;
```

---

## Bonnes Pratiques de Performance

### 1. Optimisation des Requ√™tes

#### Patterns de Requ√™tes Efficaces

```javascript
// ‚úÖ BON : Requ√™te cibl√©e incluant shard key
db.orders.find({
  customer_id: "CUST12345",  // Shard key
  status: "pending"
})
.explain("executionStats")

// R√©sultat :
// - winningPlan.stage: "SINGLE_SHARD"
// - nReturned: 5
// - executionTimeMillis: 10ms

// ‚ùå MAUVAIS : Requ√™te sans shard key
db.orders.find({
  status: "pending"  // Pas de shard key
})
.explain("executionStats")

// R√©sultat :
// - winningPlan.stage: "SHARD_MERGE"
// - nReturned: 5000
// - executionTimeMillis: 250ms (25x plus lent)

// üîß SOLUTION : Remodeler la requ√™te ou cr√©er index secondaire
db.orders.createIndex({ status: 1, created_at: -1 })

// Accepter le broadcast mais optimiser avec index
db.orders.find({ status: "pending" })
  .sort({ created_at: -1 })
  .limit(100)
// Index r√©duit les documents examin√©s sur chaque shard
```

#### Agr√©gations Optimis√©es

```javascript
// Pattern d'optimisation : Push down aggregations

// ‚ùå Inefficace : Sort global sans limite
db.orders.aggregate([
  { $match: { status: "completed" } },
  { $sort: { total_amount: -1 } },  // Sort global co√ªteux
  { $project: { customer_id: 1, total_amount: 1 } }
])

// ‚úÖ Efficace : Sort + Limit (permet push-down)
db.orders.aggregate([
  { $match: { status: "completed" } },
  { $sort: { total_amount: -1 } },
  { $limit: 100 },  // Limite appliqu√©e sur chaque shard
  { $project: { customer_id: 1, total_amount: 1 } }
])

// ‚úÖ Encore mieux : Match avec shard key
db.orders.aggregate([
  { $match: {
      customer_id: { $in: listOfCustomers },  // Shard key
      status: "completed"
    }
  },
  { $sort: { total_amount: -1 } },
  { $limit: 10 }
])
```

### 2. Strat√©gies d'Indexation

#### Index Strategy Matrix

| Sc√©nario | Type d'Index | Justification |
|----------|-------------|---------------|
| Shard key seule | Automatique | Cr√©√© par MongoDB lors du sharding |
| Champs fr√©quents dans filtres | Single field | Pour requ√™tes cibl√©es |
| Requ√™tes multi-champs | Compound | Ordre selon s√©lectivit√© |
| Arrays (tags, categories) | Multikey | Automatique sur arrays |
| Recherche textuelle | Text | Full-text search |
| G√©olocalisation | 2dsphere | Queries g√©ospatiales |
| Unicit√© globale | Unique + shard key | Include shard key pour unicit√© |
| TTL (logs, sessions) | TTL | Suppression automatique |

#### Checklist d'Index par Collection

```javascript
// Template de strat√©gie d'indexation

function defineIndexStrategy(collName, config) {
  print("=== Strat√©gie d'Index : " + collName + " ===\n");

  var strategy = {
    collection: collName,
    shardKey: config.shardKey,
    indexes: []
  };

  // 1. Shard key index (automatique)
  strategy.indexes.push({
    name: "Shard Key (auto)",
    fields: config.shardKey,
    type: "automatic",
    purpose: "Sharding routing"
  });

  // 2. Index pour requ√™tes fr√©quentes
  config.commonQueries.forEach(function(query) {
    strategy.indexes.push({
      name: "Query: " + JSON.stringify(query.filter),
      fields: query.filter,
      type: query.unique ? "unique" : "standard",
      purpose: "Query performance",
      estimatedUsage: query.frequencyPerDay + " queries/day"
    });
  });

  // 3. Index pour agr√©gations
  if (config.aggregations) {
    config.aggregations.forEach(function(agg) {
      strategy.indexes.push({
        name: "Aggregation: " + agg.name,
        fields: agg.fields,
        type: "compound",
        purpose: "Aggregation pipeline",
        estimatedUsage: agg.frequencyPerDay + " pipelines/day"
      });
    });
  }

  // 4. TTL index si applicable
  if (config.ttl) {
    strategy.indexes.push({
      name: "TTL: " + config.ttl.field,
      fields: { [config.ttl.field]: 1 },
      type: "TTL",
      purpose: "Automatic cleanup",
      expireAfterSeconds: config.ttl.seconds
    });
  }

  // Afficher la strat√©gie
  print("Shard Key : " + JSON.stringify(strategy.shardKey));
  print("\nIndex propos√©s :");

  strategy.indexes.forEach(function(idx, i) {
    print("\n" + (i+1) + ". " + idx.name);
    print("   Fields : " + JSON.stringify(idx.fields));
    print("   Type : " + idx.type);
    print("   Purpose : " + idx.purpose);
    if (idx.estimatedUsage) print("   Usage : " + idx.estimatedUsage);
    if (idx.expireAfterSeconds) print("   TTL : " + idx.expireAfterSeconds + "s");
  });

  print("\n" + "=".repeat(60));

  return strategy;
}

// Exemple : Collection orders
defineIndexStrategy("orders", {
  shardKey: { customer_id: 1, order_date: 1 },
  commonQueries: [
    {
      filter: { customer_id: 1, status: 1 },
      frequencyPerDay: 50000,
      unique: false
    },
    {
      filter: { order_id: 1 },
      frequencyPerDay: 100000,
      unique: true
    }
  ],
  aggregations: [
    {
      name: "Daily sales by customer",
      fields: { customer_id: 1, order_date: -1 },
      frequencyPerDay: 1000
    }
  ],
  ttl: null
});
```

---

## Bonnes Pratiques de S√©curit√©

### 1. Defense in Depth

```yaml
security_layers:

  layer_1_network:
    - Firewalls: "Restrict to application IPs only"
    - VPC/Private Network: "No public internet access"
    - TLS/SSL: "Encrypted connections (certificates)"
    - IP Whitelisting: "Mongos and internal IPs only"

  layer_2_authentication:
    - Method: "SCRAM-SHA-256 (minimum)"
    - x.509: "For inter-cluster communication"
    - Keyfile: "For replica set and config servers"
    - Password Policy: "Strong passwords, rotation every 90 days"

  layer_3_authorization:
    - RBAC: "Role-Based Access Control"
    - Principle of Least Privilege: "Minimum permissions"
    - Application Accounts: "Read-only when possible"
    - Admin Accounts: "MFA required"

  layer_4_encryption:
    - At Rest: "Encrypted storage volumes (LUKS, BitLocker)"
    - In Transit: "TLS 1.2+ only"
    - Client-Side: "Field Level Encryption (FLE) for PII"
    - Key Management: "External KMS (AWS KMS, Azure Key Vault)"

  layer_5_auditing:
    - Audit Log: "Enabled for all DDL and authentication"
    - Retention: "90 days minimum"
    - SIEM Integration: "Real-time alerts on anomalies"
    - Regular Review: "Weekly audit log analysis"
```

### 2. Configuration S√©curis√©e

```javascript
// Template de configuration s√©curis√©e

// 1. Cr√©ation des utilisateurs avec r√¥les appropri√©s
use admin

// Admin cluster (√©quipe DBA uniquement)
db.createUser({
  user: "clusterAdmin",
  pwd: passwordPrompt(),  // Ne jamais mettre en clair
  roles: [
    { role: "clusterAdmin", db: "admin" },
    { role: "userAdminAnyDatabase", db: "admin" }
  ]
})

// Backup user (pour mongodump)
db.createUser({
  user: "backupUser",
  pwd: passwordPrompt(),
  roles: [
    { role: "backup", db: "admin" },
    { role: "restore", db: "admin" }
  ]
})

// Application user (lecture/√©criture sur DB sp√©cifique)
use mydb
db.createUser({
  user: "appUser",
  pwd: passwordPrompt(),
  roles: [
    { role: "readWrite", db: "mydb" }
  ]
})

// Monitoring user (lecture seule, m√©triques)
use admin
db.createUser({
  user: "monitoringUser",
  pwd: passwordPrompt(),
  roles: [
    { role: "clusterMonitor", db: "admin" },
    { role: "read", db: "local" }
  ]
})

// 2. Activer l'audit
// Dans mongod.conf :
```

```yaml
auditLog:
  destination: file
  format: JSON
  path: /var/log/mongodb/audit.json
  filter: '{
    atype: { $in: ["authenticate", "createUser", "dropUser", "dropDatabase", "dropCollection", "createIndex", "dropIndex"] }
  }'
```

---

## Anti-Patterns R√©capitulatifs

### Top 10 des Erreurs √† √âviter

| # | Anti-Pattern | Impact | Solution |
|---|--------------|--------|----------|
| 1 | Shard key monotone (timestamp, _id) | Hot spots, jumbo chunks | Hashed ou compound avec pr√©fixe non-monotone |
| 2 | Shard key de faible cardinalit√© | Jumbo chunks, d√©s√©quilibre | Cardinalit√© > 10K, ou compound |
| 3 | Pas de pr√©-splitting avant import | Distribution d√©s√©quilibr√©e initiale | Pr√©-splitter en 4-10 chunks par shard |
| 4 | Requ√™tes sans shard key | Broadcast co√ªteux | Inclure shard key ou accepter + optimiser |
| 5 | Ignorer les jumbo chunks | D√©s√©quilibre croissant | Traiter imm√©diatement (refine/reshape) |
| 6 | Config servers non-redondants | Point de d√©faillance unique | Toujours 3 config servers minimum |
| 7 | Pas de monitoring du balancer | Migrations en heures de pointe | Fen√™tres de balancing + alerting |
| 8 | $lookup intensif | Performance d√©grad√©e | D√©normaliser les donn√©es fr√©quentes |
| 9 | Sharder trop t√¥t | Complexit√© inutile | Attendre 100+ GB ou 10K+ ops/sec |
| 10 | Pas de tests en staging | Surprises en production | Environnement staging identique |

---

## Checklist de Production

### Avant le Go-Live

```yaml
architecture:
  - [ ] Topologie d√©finie et document√©e
  - [ ] Shard key valid√©e (cardinalit√©, distribution, localit√©)
  - [ ] Pr√©-splitting planifi√©
  - [ ] Sizing mat√©riel v√©rifi√© (CPU, RAM, Storage)
  - [ ] Tests de charge effectu√©s en staging

s√©curit√©:
  - [ ] Authentification activ√©e (SCRAM-SHA-256+)
  - [ ] Keyfile d√©ploy√© sur tous les composants
  - [ ] Utilisateurs cr√©√©s avec principe du moindre privil√®ge
  - [ ] TLS/SSL activ√© pour toutes les connexions
  - [ ] Audit log configur√© et test√©
  - [ ] Firewall rules appliqu√©es

monitoring:
  - [ ] Prometheus + Grafana d√©ploy√©s
  - [ ] Alertes configur√©es (critical + warning)
  - [ ] Dashboard cluster op√©rationnel
  - [ ] PagerDuty / on-call int√©gr√©
  - [ ] Runbooks document√©s

backup:
  - [ ] Strat√©gie de backup d√©finie (3-2-1)
  - [ ] Scripts de backup automatis√©s
  - [ ] Test de restauration effectu√©
  - [ ] Backup off-site configur√©
  - [ ] Documentation de DR √† jour

op√©rations:
  - [ ] Fen√™tre de balancing d√©finie
  - [ ] Proc√©dure de rolling restart document√©e
  - [ ] Plan de scaling (vertical et horizontal)
  - [ ] Health check script d√©ploy√©
  - [ ] √âquipe form√©e et on-call d√©fini

application:
  - [ ] Connection strings avec failover
  - [ ] Read/Write concerns appropri√©s
  - [ ] Retry logic impl√©ment√©
  - [ ] Timeout configur√©s
  - [ ] Monitoring APM activ√©
```

### Post Go-Live (premi√®re semaine)

```yaml
jour_1:
  - Monitoring continu 24/7
  - V√©rifier distribution des chunks
  - Analyser latence des requ√™tes
  - Valider comportement du balancer

jour_3:
  - Review des alertes d√©clench√©es
  - Analyse des slow queries
  - V√©rification de la croissance des donn√©es
  - Ajustements de configuration si n√©cessaire

jour_7:
  - Post-mortem de la mise en production
  - Documentation des le√ßons apprises
  - Optimisations identifi√©es
  - Plan d'am√©lioration continue
```

---

## Recommandations par Cas d'Usage

### E-commerce

```yaml
shard_keys:
  orders: { customer_id: 1, order_date: 1 }
  products: { category: 1, product_id: 1 }
  cart: { session_id: "hashed" }
  reviews: { product_id: 1, created_at: 1 }

sizing:
  shards_minimum: 3
  growth_projection: "Plan for 3x growth first year"

Áâπpoints_cl√©s:
  - Pics de charge pr√©visibles (Black Friday, promotions)
  - Consid√©rer read preference secondary pour reporting
  - TTL pour sessions expir√©es
  - Monitoring sp√©cifique sur latence checkout
```

### SaaS Multi-Tenant

```yaml
shard_keys:
  documents: { tenant_id: 1, document_id: 1 }
  users: { tenant_id: 1, user_id: 1 }
  events: { tenant_id: "hashed", timestamp: 1 }

considerations:
  - Isolation par tenant garantie
  - Attention aux "whale customers" (gros clients)
  - Zone sharding potentiel pour compliance (GDPR)
  - Metrics par tenant pour billing

best_practices:
  - Alertes sur d√©s√©quilibre par tenant
  - Possibilit√© de shard d√©di√© pour gros clients
  - Rate limiting par tenant au niveau applicatif
```

### IoT / Time Series

```yaml
shard_keys:
  sensor_data: { sensor_id: "hashed", timestamp: 1 }
  aggregates: { metric_name: 1, interval: 1 }

Áâπconsiderations:
  - Volume massif d'√©critures
  - TTL agressif sur donn√©es brutes (7-30 jours)
  - Pr√©-agr√©gation (hourly, daily) dans collections s√©par√©es
  - Compression importante (WiredTiger snappy)

optimizations:
  - Batch inserts (1000+ documents)
  - Write concern w:1 pour throughput
  - Read preference secondary pour analytics
  - Capped collections pour last-N-values
```

### Analytics / Logs

```yaml
shard_keys:
  logs: { application: 1, timestamp: 1 }
  events: { event_type: "hashed", timestamp: 1 }

Áâπconsiderations:
  - Ingestion tr√®s haute (100K+ ops/sec)
  - Requ√™tes analytiques lourdes
  - R√©tention d√©finie (30-90 jours)

best_practices:
  - Shards d√©di√©s aux √©critures vs lectures (zone sharding)
  - allowDiskUse pour agr√©gations volumineuses
  - Index partiel sur champs rares
  - Consid√©rer Time Series Collections (MongoDB 5.0+)
```

---

## √âvolution et Scaling

### Quand Ajouter un Shard ?

```javascript
// Crit√®res de d√©cision pour scaling horizontal

function shouldAddShard(metrics) {
  var reasons = [];

  // 1. Utilisation du stockage
  if (metrics.avgStorageUsagePercent > 70) {
    reasons.push("Stockage > 70% sur average");
  }

  // 2. Charge CPU
  if (metrics.avgCpuPercent > 70) {
    reasons.push("CPU > 70% sur average");
  }

  // 3. Cache pressure
  if (metrics.wiredTigerCacheUsage > 90) {
    reasons.push("WiredTiger cache > 90%");
  }

  // 4. Distribution d√©s√©quilibr√©e
  if (metrics.chunkImbalance > 30) {
    reasons.push("D√©s√©quilibre chunks > 30%");
  }

  // 5. Latence queries
  if (metrics.queryLatencyP95 > 200) {
    reasons.push("P95 latency > 200ms");
  }

  // D√©cision
  if (reasons.length >= 2) {
    print("‚ö†Ô∏è  RECOMMANDATION : Ajouter un shard");
    print("\nRaisons :");
    reasons.forEach(r => print("  - " + r));
    print("\nActions :");
    print("  1. Provisionner nouveau shard (Replica Set)");
    print("  2. Ajouter au cluster : sh.addShard(...)");
    print("  3. Laisser le balancer redistribuer (plusieurs jours)");
    print("  4. Monitorer les migrations");
    return true;
  } else {
    print("‚úÖ Capacit√© actuelle suffisante");
    return false;
  }
}

// Exemple
shouldAddShard({
  avgStorageUsagePercent: 75,
  avgCpuPercent: 65,
  wiredTigerCacheUsage: 85,
  chunkImbalance: 15,
  queryLatencyP95: 180
});
```

### Migration vers MongoDB Atlas

```yaml
migration_strategy:

  √©valuation:
    - Estimer co√ªt Atlas vs self-hosted
    - V√©rifier features disponibles
    - Planifier downtime acceptable

  preparation:
    - Cr√©er cluster Atlas (m√™me version)
    - Tester connectivity
    - R√©pliquer en staging

  migration_methods:

    method_1_live_migration:
      tool: "mongomirror (Atlas)"
      downtime: "Minimal (< 1 minute)"
      steps:
        - Configure mongomirror
        - Start replication
        - Monitor lag
        - Cutover when lag < 1 second
      suitable: "Production avec HA requirement"

    method_2_backup_restore:
      tool: "mongodump + mongorestore"
      downtime: "Several hours"
      steps:
        - Stop writes
        - mongodump from source
        - mongorestore to Atlas
        - Validate
        - Switch connection string
      suitable: "Acceptable downtime window"

    method_3_change_streams:
      tool: "Custom script with change streams"
      downtime: "Minimal"
      complexity: "High"
      suitable: "Complex migrations with transformations"

  validation:
    - Compare document counts
    - Validate indexes
    - Test application queries
    - Performance benchmarks

  rollback_plan:
    - Keep source cluster running 7+ days
    - Document rollback procedure
    - Test rollback in staging
```

---

## Conclusion

Le sharding MongoDB est une technologie puissante mais exigeante qui n√©cessite :

- ‚úÖ **Conception rigoureuse** : Shard key optimale d√®s le d√©part
- ‚úÖ **D√©ploiement m√©thodique** : Pr√©-splitting, distribution, validation
- ‚úÖ **Monitoring proactif** : Alertes, dashboards, health checks
- ‚úÖ **Maintenance disciplin√©e** : Fen√™tres planifi√©es, proc√©dures document√©es
- ‚úÖ **S√©curit√© multi-niveaux** : Defense in depth, principe du moindre privil√®ge
- ‚úÖ **Performance continue** : Requ√™tes optimis√©es, index appropri√©s
- ‚úÖ **√âvolution planifi√©e** : Scaling horizontal quand n√©cessaire

**R√®gle d'or** : Le sharding n'est pas une solution miracle. Ne shardez que quand n√©cessaire (>100 GB ou >10K ops/sec), et uniquement apr√®s avoir optimis√© votre Replica Set existant.

**Investissement requis** : Le sharding demande une expertise technique significative. Assurez-vous que votre √©quipe est form√©e et que vous avez les ressources pour maintenir un cluster shard√© en production avant de vous lancer.

**Alternative moderne** : Pour beaucoup de cas d'usage, MongoDB Atlas avec auto-scaling peut √™tre une alternative plus simple et plus fiable qu'un cluster shard√© auto-g√©r√©.

---

## Ressources Finales

### Documentation Officielle

- [MongoDB Sharding Manual](https://docs.mongodb.com/manual/sharding/)
- [MongoDB Production Notes](https://docs.mongodb.com/manual/administration/production-notes/)
- [MongoDB Best Practices](https://docs.mongodb.com/manual/administration/production-checklist-operations/)

### Formation

- [MongoDB University - M103: Basic Cluster Administration](https://university.mongodb.com/courses/M103)
- [MongoDB University - M201: MongoDB Performance](https://university.mongodb.com/courses/M201)
- [MongoDB Certification](https://university.mongodb.com/certification)

### Communaut√©

- [MongoDB Community Forums](https://www.mongodb.com/community/forums/)
- [MongoDB User Groups](https://www.mongodb.com/user-groups)
- [Stack Overflow - mongodb tag](https://stackoverflow.com/questions/tagged/mongodb)

### Blogs et Articles

- [MongoDB Engineering Blog](https://www.mongodb.com/blog)
- [MongoDB Performance Best Practices](https://www.mongodb.com/basics/best-practices)

---

**F√©licitations !** Vous avez termin√© le chapitre sur le Sharding. Vous disposez maintenant des connaissances n√©cessaires pour concevoir, d√©ployer, et maintenir un cluster shard√© MongoDB en production avec confiance et expertise.

---


‚è≠Ô∏è [S√©curit√©](/11-securite/README.md)
