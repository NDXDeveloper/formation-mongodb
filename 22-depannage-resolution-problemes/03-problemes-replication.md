üîù Retour au [Sommaire](/SOMMAIRE.md)

# 22.3 Probl√®mes de R√©plication

## Vue d'ensemble

La r√©plication est au c≈ìur de la haute disponibilit√© MongoDB. Les probl√®mes de r√©plication peuvent compromettre la disponibilit√©, la coh√©rence des donn√©es et les performances. Cette section fournit des m√©thodologies compl√®tes pour diagnostiquer et r√©soudre tous les probl√®mes li√©s √† la r√©plication.

---

## Table des Mati√®res

1. [Replication Lag](#1-replication-lag)
2. [√âlections Fr√©quentes](#2-%C3%A9lections-fr%C3%A9quentes)
3. [Synchronisation √âchou√©e](#3-synchronisation-%C3%A9chou%C3%A9e)
4. [Oplog Insuffisant](#4-oplog-insuffisant)
5. [Probl√®mes de Heartbeat](#5-probl%C3%A8mes-de-heartbeat)
6. [Rollback](#6-rollback)
7. [Split-Brain](#7-split-brain)
8. [Configuration et Monitoring](#8-configuration-et-monitoring)

---

## 1. Replication Lag

### Sympt√¥mes

```
Secondaries falling behind primary
Read operations returning stale data
Increasing lag over time
Applications experiencing inconsistencies
```

### Causes Possibles

- Charge d'√©criture √©lev√©e sur le primary
- Secondaries sous-dimensionn√©s (CPU, RAM, I/O)
- R√©seau lent ou instable entre membres
- Op√©rations longues sur le primary
- Secondary occup√© par des lectures
- Index manquants sur secondary
- Oplog trop petit

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Mesurer le Replication Lag

```javascript
// Depuis le primary
rs.printReplicationInfo()

// Sortie exemple :
// configured oplog size:   990MB
// log length start to end: 2483secs (0.69hrs)
// oplog first event time:  Mon Jan 15 2024 10:15:20
// oplog last event time:   Mon Jan 15 2024 10:56:43
// now:                     Mon Jan 15 2024 11:00:00

// Depuis un secondary
rs.printSecondaryReplicationInfo()

// Sortie exemple :
// source: mongodb-secondary1:27017
//   syncedTo: Mon Jan 15 2024 10:55:30
//   120 secs (0.03 hrs) behind the primary
// source: mongodb-secondary2:27017
//   syncedTo: Mon Jan 15 2024 10:56:40
//   20 secs (0.01 hrs) behind the primary
```

**Interpr√©ter les r√©sultats :**

```javascript
// Analyse d√©taill√©e
function analyzeLag() {
  const status = rs.status()

  const primary = status.members.find(m => m.stateStr === "PRIMARY")
  const primaryOptime = primary.optimeDate

  status.members.forEach(member => {
    if (member.stateStr === "SECONDARY") {
      const lag = (primaryOptime - member.optimeDate) / 1000  // secondes

      console.log(`${member.name}:`)
      console.log(`  Lag: ${lag} seconds`)
      console.log(`  State: ${member.stateStr}`)
      console.log(`  Health: ${member.health}`)

      if (lag > 60) {
        console.log(`  ‚ö†Ô∏è WARNING: High lag detected!`)
      }
    }
  })
}

analyzeLag()
```

**Seuils d'alerte :**
- < 10 secondes : Normal
- 10-60 secondes : Surveiller
- 60-300 secondes : Attention
- > 300 secondes : Critique

#### √âtape 2 : Identifier la Cause du Lag

```javascript
// 1. V√©rifier l'√©tat g√©n√©ral du replica set
rs.status()

// 2. V√©rifier les op√©rations en cours sur le primary
db.currentOp({
  "active": true,
  "secs_running": {$gt: 10}
})

// 3. V√©rifier la charge du secondary
// Se connecter au secondary
db.getMongo().setReadPref("secondary")

db.currentOp({
  "active": true
})

// 4. V√©rifier les m√©triques de r√©plication
db.serverStatus().metrics.repl
```

#### √âtape 3 : Analyser les M√©triques R√©seau

```bash
# Latence entre primary et secondary
ping -c 10 mongodb-secondary1

# Bande passante
iperf3 -c mongodb-secondary1

# Paquets perdus
mtr mongodb-secondary1 --report

# Connexions r√©seau
netstat -an | grep :27017
```

```javascript
// M√©triques r√©seau MongoDB
db.serverStatus().network

// V√©rifier le buffer de r√©plication
db.serverStatus().metrics.repl.buffer
```

#### √âtape 4 : Analyser l'Oplog

```javascript
// Taille et utilisation de l'oplog
db.getReplicationInfo()

// Sortie :
// {
//   logSizeMB: 990,
//   usedMB: 543.2,
//   timeDiff: 2483,              // secondes couvertes
//   timeDiffHours: 0.69,
//   tFirst: "Mon Jan 15 2024...",
//   tLast: "Mon Jan 15 2024...",
//   now: "Mon Jan 15 2024..."
// }

// Analyser le contenu de l'oplog
use local
db.oplog.rs.find().sort({$natural: -1}).limit(10)

// Voir les plus grosses op√©rations
db.oplog.rs.aggregate([
  {$project: {
    ts: 1,
    op: 1,
    ns: 1,
    size: {$bsonSize: "$$ROOT"}
  }},
  {$sort: {size: -1}},
  {$limit: 20}
])
```

#### √âtape 5 : V√©rifier les Ressources du Secondary

```bash
# CPU
top -p $(pgrep mongod)

# M√©moire
free -h
cat /proc/$(pgrep mongod)/status | grep VmRSS

# I/O
iostat -x 1 5

# Depuis MongoDB
mongosh --host secondary-host --eval "db.serverStatus().mem"
mongosh --host secondary-host --eval "db.serverStatus().wiredTiger.cache"
```

---

### R√©solution Pas √† Pas

#### Solution 1 : R√©duire la Charge sur le Primary

```javascript
// 1. Identifier les op√©rations lourdes
db.currentOp({
  "secs_running": {$gt: 5},
  "op": {$in: ["insert", "update", "delete"]}
})

// 2. Optimiser les √©critures avec bulk operations
// ‚ùå MAUVAIS
for (let i = 0; i < 10000; i++) {
  db.logs.insertOne({timestamp: new Date(), data: i})
}

// ‚úÖ BON
const docs = []
for (let i = 0; i < 10000; i++) {
  docs.push({timestamp: new Date(), data: i})
}
db.logs.insertMany(docs, {ordered: false})

// 3. Utiliser un write concern moins strict pour les donn√©es non critiques
db.logs.insertMany(docs, {
  writeConcern: {w: 1}  // Ne pas attendre les secondaries
})
```

#### Solution 2 : Augmenter les Ressources du Secondary

**Scaling vertical :**

```bash
# 1. Identifier le goulot d'√©tranglement
# CPU > 80% ‚Üí Augmenter CPU
# RAM < 20% libre ‚Üí Augmenter RAM
# I/O await > 10ms ‚Üí Disques plus rapides

# 2. Pour un secondary d'un replica set :
# a. Le mettre en mode RECOVERING
rs.stepDown(120)  # Sur le primary si c'est lui

# Depuis le secondary √† upgrader
db.adminCommand({replSetMaintenance: 1})

# b. Arr√™ter MongoDB
sudo systemctl stop mongod

# c. Upgrader les ressources (cloud, hardware, etc.)

# d. Red√©marrer
sudo systemctl start mongod

# e. Sortir du mode maintenance
db.adminCommand({replSetMaintenance: 0})

# f. V√©rifier la synchronisation
rs.status()
```

**Configuration WiredTiger optimis√©e :**

```yaml
# /etc/mongod.conf sur le secondary
storage:
  wiredTiger:
    engineConfig:
      cacheSizeGB: 16  # Augmenter le cache
      journalCompressor: snappy

    collectionConfig:
      blockCompressor: snappy
```

#### Solution 3 : Optimiser la R√©plication

**Cr√©er les m√™mes index sur les secondaries :**

```javascript
// V√©rifier les index sur le primary
db.collection.getIndexes()

// Se connecter au secondary
db.getMongo().setReadPref("secondary")

// V√©rifier les index manquants
// Cr√©er les index manquants en background
db.collection.createIndex({field: 1}, {background: true})

// Note : MongoDB 4.2+ build les index automatiquement
// sur tous les membres du replica set
```

**Ajuster les param√®tres de r√©plication :**

```javascript
// Augmenter le batch size de r√©plication
db.adminCommand({
  setParameter: 1,
  replBatchLimitBytes: 104857600  // 100MB (d√©faut: 100MB)
})

// Augmenter le nombre de threads de r√©plication
db.adminCommand({
  setParameter: 1,
  replWriterThreadCount: 16  // D√©faut: varie selon CPU
})
```

#### Solution 4 : Augmenter la Taille de l'Oplog

```javascript
// 1. V√©rifier la taille actuelle
db.getReplicationInfo()

// 2. Calculer la nouvelle taille n√©cessaire
// Formule : Taille = (Taux d'√©criture MB/s) √ó (Temps de r√©cup√©ration voulu en secondes)
// Exemple : 10 MB/s √ó 3600s (1h) = 36000 MB = 36 GB

// 3. Redimensionner l'oplog (MongoDB 4.0+)
db.adminCommand({
  replSetResizeOplog: 1,
  size: 36000  // MB
})

// 4. V√©rifier
db.getReplicationInfo()
```

**Pour versions < 4.0 (proc√©dure complexe) :**

```javascript
// 1. Arr√™ter le membre en mode standalone
sudo systemctl stop mongod

// 2. D√©marrer sans r√©plication
mongod --dbpath /var/lib/mongodb --port 37017

// 3. Se connecter
mongosh --port 37017

// 4. Cr√©er un nouveau oplog
use local
db.temp.drop()
db.temp.save({size: 36000})  // MB
db.oplog.rs.drop()
db.runCommand({
  create: "oplog.rs",
  capped: true,
  size: 36000 * 1024 * 1024
})

// 5. Red√©marrer en mode replica set
sudo systemctl start mongod
```

#### Solution 5 : D√©dier un Secondary aux Lectures

```javascript
// Configuration d'un secondary pour analytics
cfg = rs.conf()

// Trouver l'index du membre
var memberIndex = cfg.members.findIndex(m => m.host === "analytics-secondary:27017")

// Augmenter la priorit√© √† 0 (ne peut pas devenir primary)
cfg.members[memberIndex].priority = 0

// Marquer comme hidden (invisible pour les lectures normales)
cfg.members[memberIndex].hidden = true

// Appliquer
rs.reconfig(cfg)

// Maintenant diriger les analytics vers ce secondary
const client = new MongoClient(uri, {
  readPreference: {
    mode: 'secondary',
    tags: [{analytics: 'true'}]
  }
})
```

---

## 2. √âlections Fr√©quentes

### Sympt√¥mes

```
Primary changes frequently
Applications experiencing connection resets
"Not master" errors
Replica set instability
Multiple elections in short period
```

### Causes Possibles

- Probl√®mes r√©seau (timeouts, paquets perdus)
- Heartbeat timeout trop court
- Primary surcharg√©
- Configuration de priorit√© inad√©quate
- Hardware instable
- Conflit de votes (membres pairs)

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Analyser l'Historique des √âlections

```javascript
// Voir les changements r√©cents de primary
rs.status().members.forEach(member => {
  console.log(`${member.name}: ${member.stateStr}`)
  if (member.electionTime) {
    console.log(`  Last election: ${member.electionTime}`)
  }
})

// Analyser les logs
// grep "election" /var/log/mongodb/mongod.log | tail -50

// Compter les √©lections r√©centes
db.getSiblingDB("local").replset.election.find().sort({ts: -1}).limit(20)
```

#### √âtape 2 : V√©rifier la Configuration du Replica Set

```javascript
// Configuration actuelle
var cfg = rs.conf()
printjson(cfg)

// Points √† v√©rifier :
// 1. Nombre de membres (impair recommand√©)
console.log("Member count:", cfg.members.length)

// 2. Settings d'√©lection
console.log("Election timeout:", cfg.settings.electionTimeoutMillis, "ms")
console.log("Heartbeat interval:", cfg.settings.heartbeatIntervalMillis, "ms")
console.log("Heartbeat timeout:", cfg.settings.heartbeatTimeoutSecs, "s")

// 3. Priorit√©s des membres
cfg.members.forEach(m => {
  console.log(`${m.host}: priority ${m.priority}`)
})

// 4. Votes
cfg.members.forEach(m => {
  console.log(`${m.host}: votes ${m.votes}`)
})
```

#### √âtape 3 : Analyser la Connectivit√© R√©seau

```bash
# Tester la latence entre tous les membres
for host in mongodb1 mongodb2 mongodb3; do
  echo "Testing $host"
  ping -c 10 $host | grep avg
  echo "---"
done

# V√©rifier les timeouts
mtr --report --report-cycles 100 mongodb-primary

# Statistiques r√©seau MongoDB
mongosh --eval "db.serverStatus().network"
```

```javascript
// Depuis MongoDB, v√©rifier les heartbeats
rs.status().members.forEach(member => {
  console.log(`${member.name}:`)
  console.log(`  Last heartbeat: ${member.lastHeartbeat}`)
  console.log(`  Last heartbeat recv: ${member.lastHeartbeatRecv}`)
  console.log(`  Ping: ${member.pingMs}ms`)
  console.log(`  State: ${member.stateStr}`)
})
```

#### √âtape 4 : V√©rifier les Ressources Syst√®me

```bash
# Sur chaque membre du replica set
# CPU
top -bn1 | grep mongod

# M√©moire
free -h

# Swap (doit √™tre 0)
vmstat 1 5

# I/O
iostat -x 1 5

# Charge syst√®me
uptime
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Ajuster les Timeouts

```javascript
// Configuration actuelle
var cfg = rs.conf()

// Param√®tres par d√©faut :
// electionTimeoutMillis: 10000 (10 secondes)
// heartbeatIntervalMillis: 2000 (2 secondes)
// heartbeatTimeoutSecs: 10

// Pour r√©seau instable, augmenter les timeouts
cfg.settings = cfg.settings || {}
cfg.settings.electionTimeoutMillis = 20000    // 20 secondes
cfg.settings.heartbeatTimeoutSecs = 20        // 20 secondes
cfg.settings.heartbeatIntervalMillis = 4000   // 4 secondes

// Appliquer
rs.reconfig(cfg)

// V√©rifier
rs.conf().settings
```

**‚ö†Ô∏è Attention :**
- Timeouts trop longs = Failover plus lent
- Timeouts trop courts = √âlections inutiles
- Trouver le bon √©quilibre selon votre r√©seau

#### Solution 2 : Ajuster les Priorit√©s

```javascript
// D√©finir un primary pr√©f√©r√©
var cfg = rs.conf()

// Primary pr√©f√©r√© (priorit√© haute)
cfg.members[0].priority = 2
cfg.members[0].host  // "mongodb-primary:27017"

// Secondaries (priorit√© normale)
cfg.members[1].priority = 1
cfg.members[2].priority = 1

// Secondary de backup (priorit√© basse, ne devient primary qu'en dernier recours)
cfg.members[3].priority = 0.5

// Arbiter (priorit√© 0, ne peut jamais √™tre primary)
cfg.members[4].priority = 0

// Appliquer
rs.reconfig(cfg)
```

**Configuration recommand√©e :**

```javascript
// Datacenter principal (priorit√© haute)
cfg.members[0].priority = 2  // Primary pr√©f√©r√©
cfg.members[1].priority = 1  // Secondary local

// Datacenter secondaire (priorit√© basse)
cfg.members[2].priority = 0.5  // Ne devient primary qu'en cas de d√©sastre

// Arbiter (datacenter tiers)
cfg.members[3].priority = 0
cfg.members[3].arbiterOnly = true
```

#### Solution 3 : Assurer un Nombre Impair de Membres

```javascript
// Probl√®me : Nombre pair de membres votants
rs.conf().members.length  // 4 membres

// Solution 1 : Ajouter un arbiter (l√©ger, pas de donn√©es)
rs.addArb("mongodb-arbiter:27017")

// Solution 2 : Retirer un membre (si possible)
rs.remove("mongodb-secondary3:27017")

// Solution 3 : Ajuster les votes
var cfg = rs.conf()
cfg.members[3].votes = 0  // Membre non-votant
rs.reconfig(cfg)

// V√©rification
rs.conf().members.filter(m => m.votes === 1).length  // Doit √™tre impair
```

#### Solution 4 : Isoler les Probl√®mes R√©seau

**Configuration avec zones :**

```javascript
var cfg = rs.conf()

// D√©finir les zones g√©ographiques
cfg.members[0].tags = {datacenter: "dc1", rack: "r1"}
cfg.members[1].tags = {datacenter: "dc1", rack: "r2"}
cfg.members[2].tags = {datacenter: "dc2", rack: "r1"}

// Configuration de write concern pour √©viter les √©lections
cfg.settings = cfg.settings || {}
cfg.settings.getLastErrorModes = {
  multiDatacenter: {datacenter: 2}  // Au moins 2 datacenters
}

rs.reconfig(cfg)

// Utiliser dans les √©critures
db.collection.insertOne(doc, {
  writeConcern: {w: "multiDatacenter"}
})
```

**Priorit√© de lecture par localit√© :**

```javascript
// Node.js - Pr√©f√©rer les membres locaux
const client = new MongoClient(uri, {
  readPreference: {
    mode: 'nearest',
    maxStalenessSeconds: 90
  }
})
```

#### Solution 5 : Monitorer et Alerter

```javascript
// Script de monitoring des √©lections
function monitorElections() {
  const status = rs.status()

  // V√©rifier la stabilit√© du primary
  const primary = status.members.find(m => m.stateStr === "PRIMARY")

  if (!primary) {
    console.error("ALERT: No primary elected!")
    return
  }

  const uptimeSeconds = status.members.reduce((max, m) => {
    if (m.stateStr === "PRIMARY") {
      return m.uptime
    }
    return max
  }, 0)

  if (uptimeSeconds < 300) {  // Moins de 5 minutes
    console.warn(`WARNING: Recent election, primary uptime: ${uptimeSeconds}s`)
  }

  // V√©rifier les heartbeats
  status.members.forEach(member => {
    if (member.pingMs > 1000) {  // > 1 seconde
      console.warn(`WARNING: High ping to ${member.name}: ${member.pingMs}ms`)
    }
  })
}

// Ex√©cuter toutes les minutes
setInterval(monitorElections, 60000)
```

---

## 3. Synchronisation √âchou√©e

### Sympt√¥mes

```
Secondary stuck in STARTUP2 or RECOVERING
"Initial sync failed" errors
Secondary not catching up
Continuous resync attempts
```

### Causes Possibles

- Oplog trop court (donn√©es expir√©es avant sync compl√®te)
- Probl√®mes r√©seau pendant la synchronisation
- Espace disque insuffisant
- Collections tr√®s volumineuses
- Index builds longs
- Permissions insuffisantes

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Identifier l'√âtat du Membre

```javascript
// √âtat du replica set
rs.status()

// V√©rifier l'√©tat sp√©cifique
rs.status().members.forEach(member => {
  if (member.stateStr !== "PRIMARY" && member.stateStr !== "SECONDARY") {
    console.log(`${member.name}: ${member.stateStr}`)
    console.log(`  Error: ${member.lastHeartbeatMessage}`)
    console.log(`  Uptime: ${member.uptime}s`)
  }
})
```

#### √âtape 2 : Analyser les Logs

```bash
# Rechercher les erreurs de synchronisation
grep -i "sync\|initial sync\|replication" /var/log/mongodb/mongod.log | tail -100

# Messages cl√©s √† rechercher :
# - "initial sync failed"
# - "replSet error"
# - "oplog rolled over"
# - "too stale to catch up"
```

```javascript
// Voir les d√©tails depuis MongoDB
db.adminCommand({getLog: "global"}).log.filter(line =>
  line.includes("sync") || line.includes("replication")
).slice(-50)
```

#### √âtape 3 : V√©rifier l'Oplog

```javascript
// Sur le primary
db.getReplicationInfo()

// Sortie importante :
// timeDiff: dur√©e couverte par l'oplog en secondes
// Si timeDiff est court et que la sync initiale prend plus longtemps
// ‚Üí Probl√®me !

// Calculer si l'oplog est suffisant
// Temps de sync estim√© (en heures) > timeDiff en heures
// ‚Üí Augmenter l'oplog
```

#### √âtape 4 : V√©rifier les Ressources

```bash
# Espace disque
df -h /var/lib/mongodb

# V√©rifier les permissions
ls -la /var/lib/mongodb
# Doit appartenir √† mongodb:mongodb

# M√©moire disponible
free -h

# V√©rifier si le processus mongod tourne
ps aux | grep mongod
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Resynchronisation Forc√©e

```javascript
// ‚ö†Ô∏è ATTENTION : Efface toutes les donn√©es du membre

// 1. Se connecter au secondary probl√©matique
mongo --host secondary-problem:27017

// 2. Arr√™ter MongoDB
use admin
db.shutdownServer()

// 3. Supprimer les donn√©es (shell syst√®me)
sudo rm -rf /var/lib/mongodb/*
# Garder la configuration !

// 4. Red√©marrer MongoDB
sudo systemctl start mongod

// 5. Le secondary va automatiquement commencer une initial sync
// Surveiller les logs
tail -f /var/log/mongodb/mongod.log

// 6. V√©rifier la progression
rs.status()
```

#### Solution 2 : Utiliser une Copie Physique (Faster Resync)

```bash
# M√©thode plus rapide pour grandes bases de donn√©es

# 1. Sur le secondary qui fonctionne bien
sudo systemctl stop mongod

# 2. Copier les donn√©es vers le nouveau secondary
sudo rsync -av --progress \
  /var/lib/mongodb/ \
  new-secondary:/var/lib/mongodb/

# 3. Sur le nouveau secondary
sudo chown -R mongodb:mongodb /var/lib/mongodb

# 4. D√©marrer MongoDB
sudo systemctl start mongod

# 5. Le secondary va rattraper uniquement le delta de l'oplog
# Beaucoup plus rapide qu'une initial sync compl√®te
```

#### Solution 3 : Augmenter l'Oplog Avant Sync

```javascript
// Si l'oplog est trop court

// 1. Sur le primary, augmenter l'oplog
db.adminCommand({
  replSetResizeOplog: 1,
  size: 51200  // 50 GB (ajuster selon vos besoins)
})

// 2. V√©rifier
db.getReplicationInfo()
// timeDiff devrait √™tre plus grand maintenant

// 3. Relancer la sync sur le secondary
// Voir Solution 1
```

#### Solution 4 : Synchronisation par √âtapes

```javascript
// Pour bases tr√®s volumineuses

// 1. Cr√©er un backup depuis le primary
mongodump --host primary:27017 --out /backup/mongodb

// 2. Restaurer sur le secondary
mongorestore --host secondary:27017 /backup/mongodb

// 3. Le secondary va maintenant seulement rattraper l'oplog
// Beaucoup plus rapide

// 4. V√©rifier
rs.status()
```

#### Solution 5 : Configurer le Build Index en Arri√®re-Plan

```javascript
// Si la sync √©choue √† cause des index builds

// Sur le secondary, temporairement
db.adminCommand({
  setParameter: 1,
  maxIndexBuildMemoryUsageMegabytes: 1024  // 1GB pour index builds
})

// Cr√©er les index en arri√®re-plan
db.collection.createIndex({field: 1}, {background: true})
```

---

## 4. Oplog Insuffisant

### Sympt√¥mes

```
"Oplog too small" warnings
Members frequently resyncing
Unable to recover from downtime
Rapid oplog rollover
```

### Causes Possibles

- Taux d'√©criture √©lev√©
- Oplog dimensionn√© trop petit
- Op√©rations volumineuses
- Pas de monitoring de l'oplog

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Analyser l'Oplog

```javascript
// Informations sur l'oplog
db.getReplicationInfo()

// Sortie :
// {
//   logSizeMB: 990,           // Taille configur√©e
//   usedMB: 850,              // Utilis√© actuellement
//   timeDiff: 3600,           // Secondes couvertes (1h)
//   timeDiffHours: 1,
//   tFirst: "...",            // Premier √©v√©nement
//   tLast: "...",             // Dernier √©v√©nement
//   now: "..."
// }

// R√àGLE : timeDiffHours doit √™tre > temps de r√©cup√©ration maximum pr√©vu
// Recommandation : Au moins 24-72 heures

// Calculer le taux d'√©criture
var oplogSize = db.getReplicationInfo().logSizeMB
var oplogHours = db.getReplicationInfo().timeDiffHours
var writeRateMBPerHour = oplogSize / oplogHours

print("Write rate: " + writeRateMBPerHour.toFixed(2) + " MB/hour")
```

#### √âtape 2 : Analyser le Contenu de l'Oplog

```javascript
// Voir les plus grosses op√©rations
use local
db.oplog.rs.aggregate([
  {$project: {
    ts: 1,
    op: 1,
    ns: 1,
    o: 1,
    size: {$bsonSize: "$$ROOT"}
  }},
  {$sort: {size: -1}},
  {$limit: 20}
]).forEach(doc => {
  print(`${doc.ns} - ${doc.op} - ${(doc.size / 1024).toFixed(2)} KB`)
})

// Analyser la distribution des op√©rations
db.oplog.rs.aggregate([
  {$group: {
    _id: "$op",
    count: {$sum: 1},
    avgSize: {$avg: {$bsonSize: "$$ROOT"}}
  }},
  {$sort: {count: -1}}
])
```

#### √âtape 3 : Monitoring Continu

```javascript
// Script de monitoring de l'oplog
function monitorOplog() {
  var info = db.getReplicationInfo()

  var utilizationPercent = (info.usedMB / info.logSizeMB) * 100
  var hoursRemaining = info.timeDiffHours

  console.log(`Oplog utilization: ${utilizationPercent.toFixed(1)}%`)
  console.log(`Time coverage: ${hoursRemaining.toFixed(1)} hours`)

  if (hoursRemaining < 24) {
    console.error(`CRITICAL: Oplog covers less than 24 hours!`)
  } else if (hoursRemaining < 48) {
    console.warn(`WARNING: Oplog covers less than 48 hours`)
  }

  return {
    utilizationPercent,
    hoursRemaining,
    sizeMB: info.logSizeMB,
    usedMB: info.usedMB
  }
}

monitorOplog()
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Redimensionner l'Oplog (MongoDB 4.0+)

```javascript
// Calcul de la nouvelle taille
// Formule : Taille = Taux d'√©criture (MB/h) √ó Heures souhait√©es

// Exemple :
// Taux actuel : 500 MB/heure
// Couverture souhait√©e : 72 heures
// Nouvelle taille : 500 √ó 72 = 36,000 MB = 36 GB

// Redimensionner (peut se faire en ligne !)
db.adminCommand({
  replSetResizeOplog: 1,
  size: 36000  // MB
})

// V√©rifier
db.getReplicationInfo()

// Note : Faire sur tous les membres
// Primary
db.adminCommand({replSetResizeOplog: 1, size: 36000})

// Chaque secondary
db.getMongo().setReadPref("secondary")
db.adminCommand({replSetResizeOplog: 1, size: 36000})
```

#### Solution 2 : Optimiser les √âcritures

```javascript
// R√©duire les op√©rations volumineuses dans l'oplog

// ‚ùå PROBL√àME : Updates sans $set cr√©ent des op√©rations √©normes
db.users.updateOne(
  {_id: 123},
  {
    name: "John",
    email: "john@example.com",
    profile: {/* gros objet */},
    history: [/* gros array */]
  }
)
// L'oplog contient TOUT le document !

// ‚úÖ SOLUTION : Utiliser $set pour updates partiels
db.users.updateOne(
  {_id: 123},
  {$set: {
    "profile.city": "Paris",
    "lastLogin": new Date()
  }}
)
// L'oplog contient seulement les champs modifi√©s

// ‚ùå PROBL√àME : Deletes puis re-inserts
db.users.deleteOne({_id: 123})
db.users.insertOne({_id: 123, /* nouveau document */})
// 2 op√©rations dans l'oplog

// ‚úÖ SOLUTION : replaceOne
db.users.replaceOne(
  {_id: 123},
  {/* nouveau document */}
)
// 1 seule op√©ration
```

#### Solution 3 : Partitionnement des Donn√©es

```javascript
// Pour bases tr√®s actives

// Au lieu d'une grosse collection :
db.logs.insertOne({/* entr√©e log */})

// Cr√©er des collections par p√©riode
db[`logs_${year}_${month}`].insertOne({/* entr√©e log */})

// Avec TTL sur les anciennes
db.logs_2023_01.createIndex(
  {createdAt: 1},
  {expireAfterSeconds: 7776000}  // 90 jours
)

// Avantage : Moins de pression sur l'oplog
```

#### Solution 4 : Monitoring et Alertes

```javascript
// Cr√©er une alerte sur la couverture de l'oplog
function checkOplogHealth() {
  var info = db.getReplicationInfo()
  var hoursRemaining = info.timeDiffHours

  if (hoursRemaining < 24) {
    // Alerte critique
    sendAlert({
      level: "critical",
      message: `Oplog covers only ${hoursRemaining.toFixed(1)} hours`,
      action: "Increase oplog size immediately"
    })
  } else if (hoursRemaining < 48) {
    // Alerte warning
    sendAlert({
      level: "warning",
      message: `Oplog covers ${hoursRemaining.toFixed(1)} hours`,
      action: "Consider increasing oplog size"
    })
  }

  return hoursRemaining
}

// V√©rifier toutes les heures
setInterval(checkOplogHealth, 3600000)
```

---

## 5. Probl√®mes de Heartbeat

### Sympt√¥mes

```
Members marked as unreachable
"Failed to send heartbeat" errors
Frequent state changes
Network timeout errors
```

### Causes Possibles

- Latence r√©seau √©lev√©e
- Paquets perdus
- Firewall bloquant les heartbeats
- Membres surcharg√©s
- Configuration DNS

---

### Diagnostic Pas √† Pas

#### √âtape 1 : V√©rifier les Heartbeats

```javascript
// √âtat des heartbeats
rs.status().members.forEach(member => {
  console.log(`${member.name}:`)
  console.log(`  State: ${member.stateStr}`)
  console.log(`  Health: ${member.health}`)
  console.log(`  Ping: ${member.pingMs}ms`)
  console.log(`  Last heartbeat: ${member.lastHeartbeat}`)
  console.log(`  Last heartbeat message: ${member.lastHeartbeatMessage}`)
  console.log(`---`)
})

// Configuration des heartbeats
rs.conf().settings
// {
//   heartbeatIntervalMillis: 2000,    // Fr√©quence
//   heartbeatTimeoutSecs: 10,         // Timeout
//   electionTimeoutMillis: 10000      // Timeout √©lection
// }
```

#### √âtape 2 : Tester la Connectivit√©

```bash
# Entre chaque paire de membres
# Depuis mongodb1
ping -c 100 mongodb2 | grep avg
ping -c 100 mongodb3 | grep avg

# Latence TCP sur port MongoDB
hping3 -S -p 27017 -c 100 mongodb2

# Test de bande passante
iperf3 -c mongodb2

# MTU path discovery
ping -M do -s 1472 -c 10 mongodb2
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Ajuster les Timeouts

```javascript
var cfg = rs.conf()

// Pour r√©seau stable (datacenter local)
cfg.settings.heartbeatIntervalMillis = 2000   // 2s
cfg.settings.heartbeatTimeoutSecs = 10        // 10s

// Pour r√©seau instable (g√©o-distribu√©)
cfg.settings.heartbeatIntervalMillis = 5000   // 5s
cfg.settings.heartbeatTimeoutSecs = 20        // 20s

rs.reconfig(cfg)
```

#### Solution 2 : Optimiser le R√©seau

```bash
# Augmenter les buffers TCP
sudo sysctl -w net.core.rmem_max=134217728
sudo sysctl -w net.core.wmem_max=134217728
sudo sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sudo sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"

# Rendre permanent
echo "net.core.rmem_max=134217728" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

---

## 6. Rollback

### Sympt√¥mes

```
"Rollback required" messages
Data inconsistencies after recovery
Rollback files created
Recent writes lost
```

### Causes Possibles

- Primary failure avant r√©plication
- Network partition
- Write concern insuffisant
- Rollback apr√®s √©lection

---

### Diagnostic Pas √† Pas

```javascript
// V√©rifier les rollbacks
db.serverStatus().metrics.repl.rollback

// Fichiers de rollback
// ls -lh /var/lib/mongodb/rollback/
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Utiliser Write Concern Majority

```javascript
// Pour donn√©es critiques
db.orders.insertOne(order, {
  writeConcern: {w: "majority"}
})

// Emp√™che les rollbacks car confirme la r√©plication
```

#### Solution 2 : R√©cup√©rer les Donn√©es du Rollback

```bash
# Les fichiers de rollback sont dans /var/lib/mongodb/rollback/
cd /var/lib/mongodb/rollback/

# Examiner le contenu
mongorestore --dir=./rollback_data/ --dryRun

# Restaurer si n√©cessaire
mongorestore --dir=./rollback_data/
```

---

## 7. Split-Brain

### Sympt√¥mes

```
Multiple primaries
Data divergence
Conflicting writes
Network partition
```

### Diagnostic

```javascript
// V√©rifier le nombre de primaries
db.adminCommand({isMaster: 1})

// Sur chaque membre
rs.status().members.filter(m => m.stateStr === "PRIMARY")
```

### R√©solution

```javascript
// Forcer une reconfiguration
var cfg = rs.conf()
cfg.version++
rs.reconfig(cfg, {force: true})
```

---

## 8. Configuration et Monitoring

### Configuration Optimale

```javascript
var cfg = rs.conf()

// Settings recommand√©s
cfg.settings = {
  heartbeatIntervalMillis: 2000,
  heartbeatTimeoutSecs: 10,
  electionTimeoutMillis: 10000,
  catchUpTimeoutMillis: -1,  // Pas de limite pour le catchup
  getLastErrorModes: {
    majority: {datacenter: 2}
  }
}

// Priorit√©s
cfg.members[0].priority = 2  // Primary pr√©f√©r√©
cfg.members[1].priority = 1  // Secondary normal
cfg.members[2].priority = 0.5  // Backup
cfg.members[3].priority = 0  // Arbiter
cfg.members[3].arbiterOnly = true

rs.reconfig(cfg)
```

### Script de Monitoring Complet

```javascript
function replicaSetHealthCheck() {
  var status = rs.status()
  var health = {
    timestamp: new Date(),
    replSetName: status.set,
    members: [],
    alerts: []
  }

  // Analyser chaque membre
  status.members.forEach(member => {
    var memberHealth = {
      name: member.name,
      state: member.stateStr,
      health: member.health,
      uptime: member.uptime,
      pingMs: member.pingMs
    }

    // Lag pour les secondaries
    if (member.stateStr === "SECONDARY") {
      var primary = status.members.find(m => m.stateStr === "PRIMARY")
      if (primary) {
        var lag = (primary.optimeDate - member.optimeDate) / 1000
        memberHealth.lagSeconds = lag

        if (lag > 60) {
          health.alerts.push(`${member.name}: High replication lag (${lag}s)`)
        }
      }
    }

    // Ping √©lev√©
    if (member.pingMs > 100) {
      health.alerts.push(`${member.name}: High ping (${member.pingMs}ms)`)
    }

    // Health
    if (member.health !== 1) {
      health.alerts.push(`${member.name}: Unhealthy (health: ${member.health})`)
    }

    health.members.push(memberHealth)
  })

  // V√©rifier qu'il y a un primary
  var primaries = health.members.filter(m => m.state === "PRIMARY")
  if (primaries.length === 0) {
    health.alerts.push("CRITICAL: No primary elected")
  } else if (primaries.length > 1) {
    health.alerts.push("CRITICAL: Multiple primaries detected")
  }

  return health
}

// Ex√©cuter et afficher
var health = replicaSetHealthCheck()
printjson(health)

if (health.alerts.length > 0) {
  print("\n‚ö†Ô∏è  ALERTS:")
  health.alerts.forEach(alert => print("  - " + alert))
}
```

---

## Checklist de D√©pannage R√©plication

### Diagnostic Rapide (5 minutes)

```bash
# 1. √âtat g√©n√©ral
mongosh --eval "rs.status()"

# 2. Configuration
mongosh --eval "rs.conf()"

# 3. Lag de r√©plication
mongosh --eval "rs.printSecondaryReplicationInfo()"

# 4. Oplog
mongosh --eval "db.getReplicationInfo()"

# 5. Heartbeats
mongosh --eval "rs.status().members.forEach(m => print(m.name + ': ' + m.pingMs + 'ms'))"
```

### Actions Correctives par Probl√®me

```
PROBL√àME: Replication Lag > 60s
‚Üí V√©rifier charge du primary
‚Üí V√©rifier ressources des secondaries
‚Üí Augmenter oplog si n√©cessaire
‚Üí Optimiser les requ√™tes

PROBL√àME: √âlections fr√©quentes
‚Üí Augmenter timeouts
‚Üí V√©rifier r√©seau (ping, paquets perdus)
‚Üí Ajuster priorit√©s
‚Üí Assurer nombre impair de votants

PROBL√àME: Synchronisation √©chou√©e
‚Üí V√©rifier oplog (taille suffisante)
‚Üí V√©rifier espace disque
‚Üí Forcer resync si n√©cessaire
‚Üí Utiliser copie physique pour grandes bases

PROBL√àME: Oplog insuffisant
‚Üí Calculer besoin (taux √ó heures)
‚Üí Redimensionner avec replSetResizeOplog
‚Üí Optimiser les √©critures
‚Üí Monitorer en continu

PROBL√àME: Heartbeat failures
‚Üí Tester latence r√©seau
‚Üí Augmenter timeouts
‚Üí V√©rifier firewall
‚Üí Optimiser configuration r√©seau
```

---

## Conclusion

La r√©plication MongoDB est robuste mais n√©cessite :

1. **Configuration appropri√©e** (timeouts, priorit√©s, oplog)
2. **Infrastructure stable** (r√©seau, ressources)
3. **Monitoring proactif** (lag, oplog, √©lections)
4. **Proc√©dures claires** pour chaque type de probl√®me

**Points critiques :**
- ‚úÖ Oplog dimensionn√© pour 48-72h minimum
- ‚úÖ Nombre impair de membres votants
- ‚úÖ Timeouts adapt√©s √† votre r√©seau
- ‚úÖ Write concern majority pour donn√©es critiques
- ‚úÖ Monitoring continu avec alertes

---


‚è≠Ô∏è [Probl√®mes de sharding](/22-depannage-resolution-problemes/04-problemes-sharding.md)
