üîù Retour au [Sommaire](/SOMMAIRE.md)

# 22.4 Probl√®mes de Sharding

## Vue d'ensemble

Le sharding MongoDB permet la distribution horizontale des donn√©es, mais introduit une complexit√© suppl√©mentaire qui peut g√©n√©rer des probl√®mes sp√©cifiques. Cette section fournit des m√©thodologies compl√®tes pour diagnostiquer et r√©soudre les probl√®mes li√©s aux clusters shard√©s.

---

## Table des Mati√®res

1. [Balancing Bloqu√©](#1-balancing-bloqu%C3%A9)
2. [Jumbo Chunks](#2-jumbo-chunks)
3. [Requ√™tes Scatter-Gather](#3-requ%C3%AAtes-scatter-gather)
4. [Hotspots sur Shards](#4-hotspots-sur-shards)
5. [Migration de Chunks √âchou√©e](#5-migration-de-chunks-%C3%A9chou%C3%A9e)
6. [Probl√®mes de Config Servers](#6-probl%C3%A8mes-de-config-servers)
7. [Probl√®mes de Mongos](#7-probl%C3%A8mes-de-mongos)
8. [Shard Key Inad√©quate](#8-shard-key-inad%C3%A9quate)

---

## 1. Balancing Bloqu√©

### Sympt√¥mes

```
Uneven data distribution across shards
Balancer not running
"Migrations paused" messages
Some shards full while others empty
Performance degradation on specific shards
```

### Causes Possibles

- Balancer d√©sactiv√©
- Fen√™tre de balancing restrictive
- Jumbo chunks bloquant le balancer
- Migrations √©chou√©es r√©p√©t√©es
- Locks emp√™chant les migrations
- Ressources insuffisantes sur les shards

---

### Diagnostic Pas √† Pas

#### √âtape 1 : V√©rifier l'√âtat du Balancer

```javascript
// Se connecter √† mongos
mongosh --host mongos:27017

// V√©rifier l'√©tat du balancer
sh.getBalancerState()
// true = activ√©, false = d√©sactiv√©

// √âtat d√©taill√© du balancer
sh.status()

// Informations sp√©cifiques sur le balancer
db.getSiblingDB("config").settings.findOne({_id: "balancer"})
// Sortie :
// {
//   _id: "balancer",
//   mode: "full",  // ou "off"
//   stopped: false
// }

// V√©rifier si le balancer est en cours d'ex√©cution
sh.isBalancerRunning()

// Voir les migrations r√©centes
db.getSiblingDB("config").changelog.find({
  what: {$in: ["moveChunk.start", "moveChunk.commit", "moveChunk.error"]}
}).sort({time: -1}).limit(20)
```

#### √âtape 2 : Analyser la Distribution des Chunks

```javascript
// Distribution globale
sh.status()

// Distribution d√©taill√©e par collection
db.getSiblingDB("config").collections.find().forEach(coll => {
  print("\n=== " + coll._id + " ===")

  // Compter les chunks par shard
  db.getSiblingDB("config").chunks.aggregate([
    {$match: {ns: coll._id}},
    {$group: {
      _id: "$shard",
      count: {$sum: 1}
    }},
    {$sort: {count: -1}}
  ]).forEach(printjson)
})

// Voir la taille des donn√©es par shard
db.getSiblingDB("config").shards.find().forEach(shard => {
  print("\n=== " + shard._id + " ===")
  var conn = new Mongo(shard.host)
  var stats = conn.getDB("mydb").stats()
  print("Data size: " + (stats.dataSize / 1024 / 1024 / 1024).toFixed(2) + " GB")
  print("Storage size: " + (stats.storageSize / 1024 / 1024 / 1024).toFixed(2) + " GB")
})
```

#### √âtape 3 : Identifier les Chunks Probl√©matiques

```javascript
// Trouver les jumbo chunks
db.getSiblingDB("config").chunks.find({
  jumbo: true
}).forEach(printjson)

// Chunks les plus volumineux (estimation par range)
db.getSiblingDB("config").chunks.aggregate([
  {$project: {
    ns: 1,
    shard: 1,
    min: 1,
    max: 1,
    jumbo: 1,
    // Estimation grossi√®re de la taille par la diff√©rence de cl√©s
    rangeSize: {$subtract: ["$max", "$min"]}
  }},
  {$sort: {rangeSize: -1}},
  {$limit: 20}
])
```

#### √âtape 4 : V√©rifier les Locks et Op√©rations en Cours

```javascript
// V√©rifier les locks de balancing
db.getSiblingDB("config").locks.find()

// Voir les migrations actives
db.getSiblingDB("config").changelog.find({
  what: "moveChunk.start",
  time: {$gt: new Date(Date.now() - 3600000)}  // Derni√®re heure
}).count()

// V√©rifier si des migrations sont bloqu√©es
db.currentOp({
  desc: {$regex: /^migrateThread/}
})
```

#### √âtape 5 : Analyser les Logs

```bash
# Logs du balancer (sur config server)
grep -i "balancer\|moveChunk" /var/log/mongodb/mongod.log | tail -100

# Messages cl√©s √† rechercher :
# - "balancer is disabled"
# - "no chunks need to be moved"
# - "migration failed"
# - "balancer window"
# - "jumbo chunk"
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Activer le Balancer

```javascript
// Activer le balancer
sh.startBalancer()

// V√©rifier
sh.getBalancerState()
// true

// Alternative : Modifier la configuration
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {stopped: false}},
  {upsert: true}
)

// V√©rifier que le balancer d√©marre
sh.isBalancerRunning()
// Attendre quelques secondes et v√©rifier √† nouveau

// Voir l'historique r√©cent
db.getSiblingDB("config").changelog.find({
  what: {$in: ["balancer.start", "balancer.round"]}
}).sort({time: -1}).limit(5)
```

#### Solution 2 : Ajuster la Fen√™tre de Balancing

```javascript
// V√©rifier la fen√™tre actuelle
db.getSiblingDB("config").settings.findOne({_id: "balancer"})

// D√©finir une fen√™tre de balancing (heures creuses)
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {
    activeWindow: {
      start: "01:00",  // 1h du matin
      stop: "06:00"    // 6h du matin
    }
  }},
  {upsert: true}
)

// Supprimer la fen√™tre (balancing 24/7)
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$unset: {activeWindow: ""}}
)

// Pour permettre le balancing imm√©diat (production avec pr√©caution)
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {mode: "full"}},
  {upsert: true}
)
```

#### Solution 3 : Forcer le Balancing

```javascript
// D√©placer un chunk manuellement
sh.moveChunk(
  "mydb.mycollection",
  {shardKey: "valueInChunk"},
  "shard0001"  // Destination shard
)

// Exemple concret
sh.moveChunk(
  "mydb.orders",
  {customerId: 12345},
  "shard0002"
)

// V√©rifier la progression
db.currentOp({
  desc: {$regex: /^migrateThread/}
})

// Diviser un gros chunk manuellement avant de le d√©placer
sh.splitAt("mydb.orders", {customerId: 50000})
sh.splitAt("mydb.orders", {customerId: 75000})
```

#### Solution 4 : Nettoyer les Locks Orphelins

```javascript
// ATTENTION : √Ä faire uniquement si aucune migration n'est en cours !

// V√©rifier d'abord qu'aucune migration n'est active
db.currentOp({desc: {$regex: /^migrateThread/}})

// Si vide, nettoyer les locks
db.getSiblingDB("config").locks.find()

// Supprimer les locks obsol√®tes (avec prudence)
db.getSiblingDB("config").locks.remove({
  state: 0,  // Lock non actif
  ts: {$lt: new Date(Date.now() - 900000)}  // Older than 15 min
})

// Red√©marrer le balancer
sh.stopBalancer()
sh.startBalancer()
```

#### Solution 5 : Optimiser les Param√®tres du Balancer

```javascript
// Augmenter la taille des chunks (si beaucoup de petits chunks)
db.getSiblingDB("config").settings.updateOne(
  {_id: "chunksize"},
  {$set: {value: 128}},  // 128 MB (d√©faut: 64 MB)
  {upsert: true}
)

// Param√®tres avanc√©s du balancer
db.adminCommand({
  setParameter: 1,
  // Nombre de chunks √† d√©placer en parall√®le
  migrateCloneInsertionBatchSize: 100,
  // D√©lai entre les migrations
  migrateCloneInsertionBatchDelayMS: 0
})

// Configuration pour environnements haute performance
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {
    // Permettre migrations pendant les pics (avec pr√©caution)
    attemptToBalanceJumboChunks: true
  }},
  {upsert: true}
)
```

---

## 2. Jumbo Chunks

### Sympt√¥mes

```
"Jumbo chunk" warnings in logs
Uneven data distribution
Balancer unable to move chunks
Single shard overloaded
Performance issues on specific shard
```

### Causes Possibles

- Shard key avec faible cardinalit√©
- Documents volumineux
- Croissance rapide d'une plage de valeurs
- Split automatique d√©sactiv√©
- Chunk size trop grand

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Identifier les Jumbo Chunks

```javascript
// Trouver tous les jumbo chunks
db.getSiblingDB("config").chunks.find({
  jumbo: true
}).forEach(chunk => {
  printjson({
    ns: chunk.ns,
    shard: chunk.shard,
    min: chunk.min,
    max: chunk.max
  })
})

// Compter par collection
db.getSiblingDB("config").chunks.aggregate([
  {$match: {jumbo: true}},
  {$group: {
    _id: "$ns",
    count: {$sum: 1}
  }},
  {$sort: {count: -1}}
])

// Estimer la taille des jumbo chunks
db.getSiblingDB("config").chunks.find({jumbo: true}).forEach(chunk => {
  // Se connecter au shard
  var shardConn = new Mongo(
    db.getSiblingDB("config").shards.findOne({_id: chunk.shard}).host
  )

  // Compter les documents dans le chunk
  var dbName = chunk.ns.split('.')[0]
  var collName = chunk.ns.split('.').slice(1).join('.')

  var count = shardConn.getDB(dbName)[collName].count({
    $and: [
      chunk.min,
      {$not: chunk.max}  // Chunk range
    ]
  })

  print(`${chunk.ns} on ${chunk.shard}: ~${count} documents`)
})
```

#### √âtape 2 : Analyser la Shard Key

```javascript
// Voir la shard key de la collection
db.getSiblingDB("config").collections.findOne({
  _id: "mydb.mycollection"
})

// Analyser la distribution des valeurs de shard key
db.mycollection.aggregate([
  {$group: {
    _id: "$shardKeyField",
    count: {$sum: 1}
  }},
  {$sort: {count: -1}},
  {$limit: 20}
])

// V√©rifier la cardinalit√©
db.mycollection.aggregate([
  {$group: {_id: "$shardKeyField"}},
  {$count: "uniqueValues"}
])
```

#### √âtape 3 : Analyser la Croissance

```javascript
// Voir la croissance r√©cente par chunk
db.getSiblingDB("config").changelog.aggregate([
  {$match: {
    what: "split",
    time: {$gt: new Date(Date.now() - 86400000)}  // 24h
  }},
  {$group: {
    _id: "$ns",
    splits: {$sum: 1}
  }},
  {$sort: {splits: -1}}
])

// Identifier les plages qui ne se divisent pas
db.getSiblingDB("config").chunks.find({
  ns: "mydb.orders",
  jumbo: true
}).forEach(chunk => {
  print(`Range: ${tojson(chunk.min)} to ${tojson(chunk.max)}`)
})
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Diviser Manuellement les Jumbo Chunks

```javascript
// Strat√©gie 1 : Split au milieu
// Identifier un chunk jumbo
var chunk = db.getSiblingDB("config").chunks.findOne({
  ns: "mydb.orders",
  jumbo: true
})

// Calculer une valeur de split (au milieu de la plage)
// Pour une shard key num√©rique
var midPoint = {
  customerId: (chunk.min.customerId + chunk.max.customerId) / 2
}

// Diviser
sh.splitAt("mydb.orders", midPoint)

// Strat√©gie 2 : Splits multiples
// Diviser en plusieurs chunks plus petits
sh.splitAt("mydb.orders", {customerId: 10000})
sh.splitAt("mydb.orders", {customerId: 20000})
sh.splitAt("mydb.orders", {customerId: 30000})
sh.splitAt("mydb.orders", {customerId: 40000})

// Strat√©gie 3 : Split automatique par find
// Laisser MongoDB trouver le meilleur point de division
sh.splitFind("mydb.orders", {customerId: 25000})

// V√©rifier les nouveaux chunks
db.getSiblingDB("config").chunks.find({
  ns: "mydb.orders",
  "min.customerId": {$gte: chunk.min.customerId},
  "max.customerId": {$lte: chunk.max.customerId}
}).count()
```

#### Solution 2 : Refactorer la Shard Key (Migration)

```javascript
// ‚ö†Ô∏è IMPORTANT : N√©cessite migration de donn√©es

// Approche 1 : Shard key compos√©e (ajouter de la cardinalit√©)
// Ancienne shard key : {customerId: 1}
// Nouvelle shard key : {customerId: 1, orderDate: 1}

// 1. Cr√©er une nouvelle collection avec meilleure shard key
db.adminCommand({
  shardCollection: "mydb.orders_new",
  key: {customerId: 1, orderDate: 1}
})

// 2. Migrer les donn√©es (en batch)
var cursor = db.orders.find().batchSize(1000)
var batch = []

cursor.forEach(doc => {
  batch.push(doc)

  if (batch.length >= 1000) {
    db.orders_new.insertMany(batch, {ordered: false})
    batch = []
  }
})

if (batch.length > 0) {
  db.orders_new.insertMany(batch, {ordered: false})
}

// 3. V√©rifier l'int√©grit√©
db.orders.count()
db.orders_new.count()

// 4. Renommer (pendant une fen√™tre de maintenance)
db.orders.renameCollection("orders_old")
db.orders_new.renameCollection("orders")

// Approche 2 : Hashed shard key
db.adminCommand({
  shardCollection: "mydb.orders_new",
  key: {customerId: "hashed"}
})
// Distribution automatique uniforme
```

#### Solution 3 : Refactoring avec Resharding (MongoDB 5.0+)

```javascript
// MongoDB 5.0+ permet de changer la shard key en place !

// V√©rifier la version
db.version()

// Resharding avec nouvelle shard key
db.adminCommand({
  reshardCollection: "mydb.orders",
  key: {customerId: 1, orderDate: 1}
})

// Suivre la progression
db.getSiblingDB("config").reshardingOperations.find()

// Note : C'est une op√©ration lourde qui peut prendre du temps
// Planifier pendant une fen√™tre de maintenance
```

#### Solution 4 : Optimiser les Donn√©es

```javascript
// Strat√©gie 1 : Agr√©ger les petits documents
// Si beaucoup de petits documents cr√©ent un jumbo chunk

// Avant : 1 document par √©v√©nement
{_id: 1, userId: 123, event: "click", timestamp: ...}
{_id: 2, userId: 123, event: "view", timestamp: ...}
// ... 10 millions de petits documents

// Apr√®s : Agr√©ger par p√©riode
{
  _id: ObjectId(),
  userId: 123,
  date: ISODate("2024-01-15"),
  events: [
    {type: "click", timestamp: ...},
    {type: "view", timestamp: ...},
    // ... √©v√©nements de la journ√©e
  ],
  eventCount: 142
}

// Strat√©gie 2 : Externaliser les gros champs
// Si des documents individuels sont tr√®s gros

// Avant : Tout dans un document
{
  _id: 1,
  userId: 123,
  metadata: {...},
  largeData: "... plusieurs MB ..."
}

// Apr√®s : S√©parer les gros champs
// Collection principale
{
  _id: 1,
  userId: 123,
  metadata: {...},
  largeDataRef: "large_1"
}

// Collection pour gros champs (s√©par√©e)
{
  _id: "large_1",
  data: "... plusieurs MB ..."
}
```

#### Solution 5 : Autoriser la Migration des Jumbo Chunks

```javascript
// ‚ö†Ô∏è ATTENTION : Peut causer des probl√®mes de performance

// Permettre temporairement la migration des jumbo chunks
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {attemptToBalanceJumboChunks: true}},
  {upsert: true}
)

// Surveiller attentivement
db.currentOp({
  desc: {$regex: /^migrateThread/}
})

// D√©sactiver apr√®s √©quilibrage
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {attemptToBalanceJumboChunks: false}}
)
```

---

## 3. Requ√™tes Scatter-Gather

### Sympt√¥mes

```
Slow queries despite indexes
High network traffic between mongos and shards
All shards contacted for simple queries
Poor query performance in sharded environment
```

### Causes Possibles

- Requ√™tes sans shard key
- Shard key non incluse dans les filtres
- Requ√™tes sur champs non-sharded
- Mauvaise conception de l'application

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Identifier les Requ√™tes Scatter-Gather

```javascript
// Activer le profiler sur mongos
db.setProfilingLevel(1, {slowms: 100})

// Analyser les requ√™tes
db.system.profile.find({
  "command.shardVersion": {$exists: true}
}).sort({ts: -1}).limit(20).forEach(doc => {
  print("Query:", tojson(doc.command.filter))
  print("Shards used:", doc.nShards || "N/A")
  print("Millis:", doc.millis)
  print("---")
})

// Requ√™tes qui touchent tous les shards
db.system.profile.find({
  nShards: {$eq: db.getSiblingDB("config").shards.count()}
}).count()
```

#### √âtape 2 : Analyser avec explain()

```javascript
// Requ√™te sans shard key
db.orders.find({status: "pending"}).explain("executionStats")

// Sortie importante :
// {
//   "queryPlanner": {
//     "winningPlan": {
//       "stage": "SHARD_MERGE",  // ‚ö†Ô∏è Scatter-gather !
//       "shards": [
//         {shardName: "shard0000", ...},
//         {shardName: "shard0001", ...},
//         {shardName: "shard0002", ...}
//         // Tous les shards !
//       ]
//     }
//   },
//   "executionStats": {
//     "nReturned": 100,
//     "totalDocsExamined": 1000000,  // Examiner beaucoup de docs
//     "totalKeysExamined": 1000000,
//     "executionTimeMillis": 2543
//   }
// }

// Requ√™te avec shard key
db.orders.find({
  customerId: 12345,  // Shard key !
  status: "pending"
}).explain("executionStats")

// Sortie avec targeting :
// {
//   "queryPlanner": {
//     "winningPlan": {
//       "stage": "SINGLE_SHARD",  // ‚úÖ Un seul shard !
//       "shards": [
//         {shardName: "shard0001", ...}
//       ]
//     }
//   }
// }
```

#### √âtape 3 : Analyser les Patterns d'Acc√®s

```javascript
// Identifier les collections probl√©matiques
db.system.profile.aggregate([
  {$match: {
    op: "query",
    nShards: {$gt: 1}  // Multi-shard queries
  }},
  {$group: {
    _id: "$ns",
    count: {$sum: 1},
    avgShards: {$avg: "$nShards"},
    avgTime: {$avg: "$millis"}
  }},
  {$sort: {count: -1}}
])
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Inclure la Shard Key dans les Requ√™tes

```javascript
// ‚ùå MAUVAIS : Requ√™te sans shard key
db.orders.find({status: "pending"})
// Scatter-gather sur tous les shards

// ‚úÖ BON : Requ√™te avec shard key
db.orders.find({
  customerId: 12345,  // Shard key
  status: "pending"
})
// Targeted query sur un seul shard

// ‚ùå MAUVAIS : Update sans shard key
db.orders.updateMany(
  {status: "pending"},
  {$set: {status: "processing"}}
)
// Impacte tous les shards

// ‚úÖ BON : Update avec shard key
db.orders.updateMany(
  {
    customerId: 12345,  // Shard key
    status: "pending"
  },
  {$set: {status: "processing"}}
)
```

#### Solution 2 : Refactorer le Code Applicatif

```javascript
// Pattern 1 : Lookup par utilisateur
// Application Node.js

// ‚ùå MAUVAIS
async function getUserOrders(status) {
  // Cherche dans tous les shards !
  return await db.orders.find({status: status}).toArray()
}

// ‚úÖ BON
async function getUserOrders(userId, status) {
  // Targeted query
  return await db.orders.find({
    customerId: userId,  // Shard key
    status: status
  }).toArray()
}

// Pattern 2 : Recherche globale n√©cessaire
// Utiliser l'agr√©gation avec $facet pour combiner r√©sultats

// ‚úÖ OPTIMAL : Pr√©-calculer les agr√©gations
// Collection orders_by_status (maintenue par triggers)
{
  _id: "pending",
  orderIds: [123, 456, 789, ...],
  count: 3,
  lastUpdated: ISODate("...")
}

// Requ√™te rapide cibl√©e
db.orders_by_status.findOne({_id: "pending"})
```

#### Solution 3 : Cr√©er des Index Secondaires

```javascript
// Pour requ√™tes sans shard key mais fr√©quentes

// Cr√©er un index sur le champ fr√©quemment requ√™t√©
db.orders.createIndex({status: 1, orderDate: -1})

// Maintenant la requ√™te est plus rapide m√™me en scatter-gather
db.orders.find({status: "pending"}).sort({orderDate: -1})

// Note : Toujours moins performant qu'une targeted query,
// mais mieux que sans index
```

#### Solution 4 : Utiliser des Zones (Zone Sharding)

```javascript
// Pour isoler certaines requ√™tes sur des shards sp√©cifiques

// D√©finir des zones g√©ographiques
sh.addShardToZone("shard0000", "EU")
sh.addShardToZone("shard0001", "US")
sh.addShardToZone("shard0002", "ASIA")

// D√©finir les ranges
sh.updateZoneKeyRange(
  "mydb.orders",
  {customerId: MinKey, region: "EU"},
  {customerId: MaxKey, region: "EU"},
  "EU"
)

sh.updateZoneKeyRange(
  "mydb.orders",
  {customerId: MinKey, region: "US"},
  {customerId: MaxKey, region: "US"},
  "US"
)

// Requ√™tes avec r√©gion = targeted vers un zone
db.orders.find({
  region: "EU",
  status: "pending"
})
// Touche seulement les shards de la zone EU
```

#### Solution 5 : Monitoring et Alertes

```javascript
// Monitorer le ratio scatter-gather
function monitorScatterGather() {
  var stats = db.system.profile.aggregate([
    {$match: {op: "query"}},
    {$group: {
      _id: null,
      totalQueries: {$sum: 1},
      scatterGatherQueries: {
        $sum: {$cond: [{$gt: ["$nShards", 1]}, 1, 0]}
      }
    }},
    {$project: {
      totalQueries: 1,
      scatterGatherQueries: 1,
      percentage: {
        $multiply: [
          {$divide: ["$scatterGatherQueries", "$totalQueries"]},
          100
        ]
      }
    }}
  ]).toArray()[0]

  if (stats.percentage > 30) {  // > 30% scatter-gather
    console.warn(`WARNING: ${stats.percentage.toFixed(1)}% scatter-gather queries`)
  }

  return stats
}

monitorScatterGather()
```

---

## 4. Hotspots sur Shards

### Sympt√¥mes

```
One shard overloaded while others idle
Uneven write distribution
Performance bottleneck on specific shard
Increased latency for certain operations
CPU/Memory spike on one shard
```

### Causes Possibles

- Shard key monotone (auto-increment, timestamp)
- Concentration de valeurs populaires
- √âcriture s√©quentielle
- Mauvais choix de shard key
- Zone sharding mal configur√©

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Identifier le Hotspot

```javascript
// V√©rifier la distribution des op√©rations
db.getSiblingDB("config").shards.find().forEach(shard => {
  print("\n=== " + shard._id + " ===")

  var conn = new Mongo(shard.host.split(',')[0])  // Premier membre du replica set
  var serverStatus = conn.getDB("admin").serverStatus()

  print("Ops per second:")
  print("  Insert:", serverStatus.opcounters.insert)
  print("  Query:", serverStatus.opcounters.query)
  print("  Update:", serverStatus.opcounters.update)
  print("  Delete:", serverStatus.opcounters.delete)

  print("Connections:", serverStatus.connections.current)

  print("Memory:")
  print("  Resident:", serverStatus.mem.resident, "MB")
})

// Distribution des chunks (devrait √™tre √©quilibr√©e)
db.getSiblingDB("config").chunks.aggregate([
  {$match: {ns: "mydb.orders"}},
  {$group: {
    _id: "$shard",
    chunks: {$sum: 1}
  }},
  {$sort: {chunks: -1}}
])

// Distribution de la taille des donn√©es
db.adminCommand({
  dbStats: 1,
  scale: 1024 * 1024 * 1024  // GB
})
```

#### √âtape 2 : Analyser la Shard Key

```javascript
// Voir la shard key
db.getSiblingDB("config").collections.findOne({
  _id: "mydb.orders"
}).key

// Analyser la distribution des valeurs
db.orders.aggregate([
  {$sample: {size: 10000}},  // √âchantillon
  {$group: {
    _id: "$customerId",  // Shard key
    count: {$sum: 1}
  }},
  {$sort: {count: -1}},
  {$limit: 20}
])

// Identifier les valeurs tr√®s fr√©quentes (hotspot potentiel)
```

#### √âtape 3 : Analyser les Patterns d'√âcriture

```javascript
// Voir o√π vont les nouvelles insertions
db.getSiblingDB("config").changelog.find({
  what: "multi-split",
  time: {$gt: new Date(Date.now() - 3600000)}  // Derni√®re heure
}).forEach(entry => {
  print("Namespace:", entry.ns)
  print("Shard:", entry.details.before.shard)
  print("Number of splits:", entry.details.number)
})

// Pour shard key monotone (probl√®me)
db.orders.find().sort({_id: -1}).limit(10)
// Si tous les nouveaux docs vont sur le m√™me chunk/shard = hotspot
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Utiliser Hashed Shard Key

```javascript
// Pour √©viter hotspots sur shard keys monotones

// ‚ùå PROBL√àME : Shard key s√©quentielle
db.adminCommand({
  shardCollection: "mydb.orders",
  key: {orderId: 1}  // Auto-increment = hotspot !
})
// Toutes les nouvelles insertions vont sur le dernier chunk

// ‚úÖ SOLUTION : Hashed shard key
db.adminCommand({
  shardCollection: "mydb.orders_new",
  key: {orderId: "hashed"}
})
// Distribution automatique uniforme

// Migration des donn√©es (voir section Jumbo Chunks)
```

#### Solution 2 : Shard Key Compos√©e

```javascript
// Ajouter de la cardinalit√© pour √©viter hotspots

// ‚ùå PROBL√àME : Seulement timestamp
db.adminCommand({
  shardCollection: "mydb.events",
  key: {timestamp: 1}  // Monotone = hotspot
})

// ‚úÖ SOLUTION : Composer avec un champ al√©atoire/distribu√©
db.adminCommand({
  shardCollection: "mydb.events_new",
  key: {userId: 1, timestamp: 1}  // userId distribue la charge
})

// ‚úÖ ALTERNATIVE : Ajouter un champ randomis√©
// Ajouter un champ √† l'insertion
{
  _id: ObjectId("..."),
  timestamp: ISODate("..."),
  shardBucket: Math.floor(Math.random() * 100),  // 0-99
  data: {...}
}

db.adminCommand({
  shardCollection: "mydb.events_new",
  key: {shardBucket: 1, timestamp: 1}
})
```

#### Solution 3 : Pr√©-split des Chunks

```javascript
// Pour nouvelles collections avec charge pr√©visible

// Cr√©er des chunks vides r√©partis sur tous les shards
// Shard key : {userId: 1}

// Obtenir les limites de userId (min/max)
var minUserId = 1
var maxUserId = 1000000

// Calculer les points de split
var numShards = db.getSiblingDB("config").shards.count()
var numSplits = numShards * 10  // 10 chunks par shard

for (var i = 1; i < numSplits; i++) {
  var splitPoint = minUserId + (i * (maxUserId - minUserId) / numSplits)
  sh.splitAt("mydb.users", {userId: Math.floor(splitPoint)})
}

// Forcer la distribution
sh.startBalancer()
```

#### Solution 4 : Ajuster les Zones pour √âquilibrer

```javascript
// Red√©finir les zones pour mieux distribuer

// Identifier les plages probl√©matiques
db.getSiblingDB("config").chunks.aggregate([
  {$match: {ns: "mydb.orders"}},
  {$group: {
    _id: {
      shard: "$shard",
      min: "$min"
    },
    count: {$sum: 1}
  }},
  {$sort: {count: -1}}
])

// Ajuster les zones
sh.removeShardFromZone("shard0000", "HOT_ZONE")
sh.addShardToZone("shard0001", "HOT_ZONE")

// Red√©finir les ranges
sh.updateZoneKeyRange(
  "mydb.orders",
  {customerId: 0},
  {customerId: 500000},
  "ZONE_1"
)

sh.updateZoneKeyRange(
  "mydb.orders",
  {customerId: 500000},
  {customerId: 1000000},
  "ZONE_2"
)
```

#### Solution 5 : Scaling Horizontal du Shard

```javascript
// Si un shard est d√©finitivement plus charg√©

// Ajouter un nouveau shard
sh.addShard("shard0003/mongodb-shard03:27017")

// Forcer la redistribution des chunks du shard surcharg√©
var overloadedShard = "shard0000"
var targetShard = "shard0003"

// D√©placer manuellement des chunks
db.getSiblingDB("config").chunks.find({
  ns: "mydb.orders",
  shard: overloadedShard
}).limit(10).forEach(chunk => {
  sh.moveChunk(
    "mydb.orders",
    chunk.min,
    targetShard
  )
})

// Ou laisser le balancer faire le travail
sh.startBalancer()
```

---

## 5. Migration de Chunks √âchou√©e

### Sympt√¥mes

```
"Migration failed" errors in logs
Chunks stuck in migration
Data temporarily unavailable
Balancer repeatedly failing
Rollback of migrations
```

### Causes Possibles

- Ressources insuffisantes (CPU, m√©moire, disque)
- Timeout de migration
- Conflit de locks
- Erreurs r√©seau
- Documents orphelins

---

### Diagnostic Pas √† Pas

#### √âtape 1 : Identifier les Migrations √âchou√©es

```javascript
// Voir l'historique des migrations
db.getSiblingDB("config").changelog.find({
  what: {$in: ["moveChunk.start", "moveChunk.commit", "moveChunk.error"]}
}).sort({time: -1}).limit(50).forEach(entry => {
  print(entry.time, "-", entry.what, "-", entry.ns)
  if (entry.details && entry.details.errmsg) {
    print("  Error:", entry.details.errmsg)
  }
})

// Compter les √©checs r√©cents
db.getSiblingDB("config").changelog.count({
  what: "moveChunk.error",
  time: {$gt: new Date(Date.now() - 3600000)}  // Derni√®re heure
})

// Migrations en cours
db.currentOp({
  desc: {$regex: /^migrateThread/}
})
```

#### √âtape 2 : Analyser les Logs

```bash
# Sur le shard source et destination
grep -i "migrate\|moveChunk" /var/log/mongodb/mongod.log | tail -100

# Messages cl√©s :
# - "migration failed"
# - "error applying batch"
# - "destination failed with"
# - "migration commit failed"
# - "range deletion timed out"
```

#### √âtape 3 : V√©rifier les Ressources

```bash
# Sur chaque shard pendant une migration
# CPU
top -p $(pgrep mongod)

# M√©moire
free -h

# Espace disque
df -h /var/lib/mongodb

# I/O
iostat -x 1 5

# R√©seau
iftop -i eth0
```

---

### R√©solution Pas √† Pas

#### Solution 1 : Augmenter les Timeouts

```javascript
// Param√®tres de migration
db.adminCommand({
  setParameter: 1,
  // Timeout pour la phase de clone (d√©faut: 30 min)
  migrationCloneTimeout: 7200000,  // 2 heures (ms)
  // Timeout pour le catchup (d√©faut: pas de timeout)
  maxTimeMS: 3600000  // 1 heure
})

// Sur le shard source et destination
```

#### Solution 2 : Nettoyer les Documents Orphelins

```javascript
// Documents orphelins = chunks migr√©s mais pas nettoy√©s

// Sur chaque shard, ex√©cuter le nettoyage
db.adminCommand({
  cleanupOrphaned: "mydb.orders",
  startingFromKey: {customerId: MinKey}
})

// R√©p√©ter jusqu'√† ce qu'il n'y ait plus d'orphelins
// Sortie :
// {
//   ok: 1,
//   stoppedAtKey: {customerId: ...}
// }

// Si stoppedAtKey existe, continuer depuis ce point
db.adminCommand({
  cleanupOrphaned: "mydb.orders",
  startingFromKey: {customerId: <derni√®re valeur>}
})
```

#### Solution 3 : R√©duire la Charge Pendant Migration

```javascript
// Ralentir le balancer
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {
    // Limiter √† 1 migration √† la fois
    _secondaryThrottle: true,
    _waitForDelete: true
  }},
  {upsert: true}
)

// D√©finir une fen√™tre de migration
db.getSiblingDB("config").settings.updateOne(
  {_id: "balancer"},
  {$set: {
    activeWindow: {
      start: "02:00",
      stop: "05:00"
    }
  }}
)
```

#### Solution 4 : Forcer la Fin d'une Migration Bloqu√©e

```javascript
// ‚ö†Ô∏è ATTENTION : Peut causer des incoh√©rences

// 1. Identifier la migration bloqu√©e
db.currentOp({desc: {$regex: /^migrateThread/}})

// 2. Tuer l'op√©ration
db.killOp(<opid>)

// 3. Nettoyer les m√©tadonn√©es
db.getSiblingDB("config").migrations.deleteMany({
  _id: {$exists: true}
})

// 4. Red√©marrer le balancer
sh.stopBalancer()
sh.startBalancer()
```

#### Solution 5 : Migration Manuelle Contr√¥l√©e

```javascript
// Pour migrations critiques, faire manuellement

// 1. D√©sactiver le balancer
sh.stopBalancer()

// 2. Identifier le chunk √† migrer
var chunk = db.getSiblingDB("config").chunks.findOne({
  ns: "mydb.orders",
  shard: "shard0000",
  // Filtrer selon besoin
})

// 3. Migrer avec monitoring
sh.moveChunk("mydb.orders", chunk.min, "shard0001")

// 4. V√©rifier
db.getSiblingDB("config").chunks.findOne({_id: chunk._id})

// 5. Nettoyer les orphelins
db.adminCommand({cleanupOrphaned: "mydb.orders"})

// 6. R√©activer le balancer
sh.startBalancer()
```

---

## 6. Probl√®mes de Config Servers

### Sympt√¥mes

```
Unable to connect to config server
Metadata operations failing
"No config server available" errors
Sharding operations blocked
Cannot add/remove shards
```

### Diagnostic

```javascript
// √âtat du config server replica set
db.getSiblingDB("config").hello()

// Membres du config replica set
use config
rs.status()

// V√©rifier la configuration
rs.conf()
```

### R√©solution

```javascript
// Si config server down, d√©marrer en mode standalone
// Reconstruire le replica set si n√©cessaire

// Sur le config server survivant
rs.initiate({
  _id: "configReplSet",
  configsvr: true,
  members: [
    {_id: 0, host: "config1:27019"},
    {_id: 1, host: "config2:27019"},
    {_id: 2, host: "config3:27019"}
  ]
})
```

---

## 7. Probl√®mes de Mongos

### Sympt√¥mes

```
Cannot connect to mongos
Routing errors
"No primary detected" errors
Slow query routing
Connection pool exhaustion
```

### Diagnostic

```javascript
// √âtat de mongos
db.adminCommand({serverStatus: 1})

// Voir les shards connus
sh.status()

// Connexions
db.serverStatus().connections
```

### R√©solution

```bash
# Red√©marrer mongos
sudo systemctl restart mongos

# V√©rifier la configuration
cat /etc/mongos.conf

# Tester la connexion
mongosh --host mongos:27017 --eval "db.adminCommand({hello: 1})"
```

---

## 8. Shard Key Inad√©quate

### Sympt√¥mes

```
Poor query performance
Uneven data distribution
Frequent jumbo chunks
Many scatter-gather queries
Hotspots on specific shards
```

### Diagnostic

```javascript
// Analyser l'efficacit√© de la shard key actuelle
// 1. Cardinalit√©
db.orders.aggregate([
  {$group: {_id: "$customerId"}},
  {$count: "uniqueValues"}
])

// 2. Distribution
db.getSiblingDB("config").chunks.aggregate([
  {$match: {ns: "mydb.orders"}},
  {$group: {_id: "$shard", count: {$sum: 1}}},
  {$sort: {count: -1}}
])

// 3. Fr√©quence d'acc√®s
db.system.profile.aggregate([
  {$match: {
    ns: "mydb.orders",
    "command.filter.customerId": {$exists: true}
  }},
  {$count: "targetedQueries"}
])
```

### R√©solution

**Crit√®res d'une bonne shard key :**

```
1. Cardinalit√© √©lev√©e (nombreuses valeurs uniques)
2. Fr√©quence faible (valeurs r√©parties uniform√©ment)
3. Monotonie √©vit√©e (pas auto-increment ni timestamp seul)
4. Incluse dans la plupart des requ√™tes
```

**Options :**

```javascript
// Option 1 : Hashed shard key
{shardKeyField: "hashed"}

// Option 2 : Compound shard key
{field1: 1, field2: 1}

// Option 3 : Avec randomisation
{bucket: 1, timestamp: 1}
// o√π bucket = Math.floor(Math.random() * 100)
```

---

## Checklist Globale Sharding

### Diagnostic Rapide (10 minutes)

```bash
# 1. √âtat du cluster
mongosh --host mongos --eval "sh.status()"

# 2. Balancer actif ?
mongosh --host mongos --eval "sh.isBalancerRunning()"

# 3. Distribution des chunks
mongosh --host mongos --eval "db.getSiblingDB('config').chunks.aggregate([{\$group: {_id: '\$shard', count: {\$sum: 1}}}, {\$sort: {count: -1}}])"

# 4. Jumbo chunks ?
mongosh --host mongos --eval "db.getSiblingDB('config').chunks.count({jumbo: true})"

# 5. Migrations r√©centes
mongosh --host mongos --eval "db.getSiblingDB('config').changelog.find({what: 'moveChunk.error'}).sort({time: -1}).limit(5)"
```

### Monitoring Continu

```javascript
// Script de monitoring sharding
function shardingHealthCheck() {
  print("=== Sharding Health Check ===\n")

  // 1. Nombre de shards
  var numShards = db.getSiblingDB("config").shards.count()
  print("Shards:", numShards)

  // 2. Balancer
  print("\nBalancer:")
  print("  Active:", sh.isBalancerRunning())
  print("  State:", sh.getBalancerState())

  // 3. Distribution des chunks
  print("\nChunk distribution:")
  db.getSiblingDB("config").chunks.aggregate([
    {$group: {_id: "$shard", chunks: {$sum: 1}}},
    {$sort: {chunks: -1}}
  ]).forEach(s => {
    print(`  ${s._id}: ${s.chunks} chunks`)
  })

  // 4. Jumbo chunks
  var jumboCount = db.getSiblingDB("config").chunks.count({jumbo: true})
  print("\nJumbo chunks:", jumboCount)
  if (jumboCount > 0) {
    print("  ‚ö†Ô∏è  WARNING: Jumbo chunks detected")
  }

  // 5. Migrations r√©centes
  var recentErrors = db.getSiblingDB("config").changelog.count({
    what: "moveChunk.error",
    time: {$gt: new Date(Date.now() - 3600000)}
  })
  print("\nMigration errors (last hour):", recentErrors)
  if (recentErrors > 0) {
    print("  ‚ö†Ô∏è  WARNING: Recent migration failures")
  }

  print("\n=== End of Health Check ===")
}

shardingHealthCheck()
```

---

## Conclusion

Le sharding MongoDB offre une scalabilit√© horizontale mais n√©cessite :

1. **Shard key bien choisie** (cardinalit√©, distribution, queries)
2. **Monitoring actif** (balancer, distribution, migrations)
3. **Maintenance r√©guli√®re** (jumbo chunks, orphans)
4. **Configuration optimale** (balancer, timeouts, zones)

**Points critiques :**
- ‚úÖ Shard key avec haute cardinalit√© et distribution uniforme
- ‚úÖ √âviter les shard keys monotones (timestamp, auto-increment)
- ‚úÖ Inclure la shard key dans les requ√™tes (√©viter scatter-gather)
- ‚úÖ Balancer activ√© avec fen√™tre appropri√©e
- ‚úÖ Monitoring continu des jumbo chunks
- ‚úÖ Nettoyer les orphelins r√©guli√®rement

---


‚è≠Ô∏è [Corruption de donn√©es](/22-depannage-resolution-problemes/05-corruption-donnees.md)
