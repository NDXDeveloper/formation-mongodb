ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 22.6 RÃ©cupÃ©ration aprÃ¨s Panne

## Vue d'ensemble

La rÃ©cupÃ©ration aprÃ¨s panne est une compÃ©tence critique pour assurer la disponibilitÃ© et la continuitÃ© de service MongoDB. Cette section couvre les procÃ©dures complÃ¨tes de rÃ©cupÃ©ration pour tous les types de dÃ©ploiements, des serveurs standalone aux clusters shardÃ©s complexes.

---

## Table des MatiÃ¨res

1. [Types de Pannes](#1-types-de-pannes)
2. [ProcÃ©dures de DÃ©marrage](#2-proc%C3%A9dures-de-d%C3%A9marrage)
3. [RÃ©cupÃ©ration Serveur Standalone](#3-r%C3%A9cup%C3%A9ration-serveur-standalone)
4. [RÃ©cupÃ©ration Replica Set](#4-r%C3%A9cup%C3%A9ration-replica-set)
5. [RÃ©cupÃ©ration Cluster ShardÃ©](#5-r%C3%A9cup%C3%A9ration-cluster-shard%C3%A9)
6. [Disaster Recovery](#6-disaster-recovery)
7. [Failover et Bascule](#7-failover-et-bascule)
8. [Tests et Validation](#8-tests-et-validation)

---

## 1. Types de Pannes

### Classification des Pannes

#### 1.1 Panne d'ArrÃªt Propre

**CaractÃ©ristiques :**
- MongoDB arrÃªtÃ© correctement (shutdown command)
- Journal complet et cohÃ©rent
- DonnÃ©es cohÃ©rentes sur disque
- RedÃ©marrage simple

**Signes dans les logs :**
```
[timestamp] I CONTROL  [conn1] received shutdown command
[timestamp] I CONTROL  [conn1] shutting down
[timestamp] I STORAGE  [conn1] WiredTigerKVEngine shutting down
[timestamp] I STORAGE  [conn1] shutdown: closing all files...
[timestamp] I STORAGE  [conn1] shutdown: removing fs lock...
[timestamp] I CONTROL  [conn1] now exiting
[timestamp] I CONTROL  [conn1] shutdown: going to close listening sockets...
[timestamp] I CONTROL  [conn1] shutdown complete
```

#### 1.2 Panne d'ArrÃªt Brutal

**Causes :**
- ArrÃªt forcÃ© (kill -9)
- Panne Ã©lectrique
- Crash systÃ¨me
- Crash matÃ©riel

**CaractÃ©ristiques :**
- Journal potentiellement incomplet
- NÃ©cessite rÃ©cupÃ©ration du journal
- Peut nÃ©cessiter rÃ©paration

**Signes dans les logs :**
```
[timestamp] W STORAGE  [initandlisten] Detected unclean shutdown
[timestamp] I STORAGE  [initandlisten] Recovering data from the last clean checkpoint
[timestamp] I STORAGE  [initandlisten] WiredTiger message: recovery checkpoint
[timestamp] I STORAGE  [initandlisten] WiredTiger recovery in progress
```

#### 1.3 Panne MatÃ©rielle

**Types :**
- DÃ©faillance disque
- DÃ©faillance mÃ©moire
- DÃ©faillance rÃ©seau
- DÃ©faillance CPU/carte mÃ¨re

**Impacts :**
- Perte potentielle de donnÃ©es
- NÃ©cessite changement matÃ©riel
- Peut nÃ©cessiter restauration depuis backup

#### 1.4 Panne RÃ©seau

**Types :**
- Partition rÃ©seau
- Perte de connectivitÃ©
- Latence Ã©levÃ©e
- Saturation bande passante

**Impacts :**
- Ã‰lections dans replica set
- ProblÃ¨mes de rÃ©plication
- Timeouts d'application

#### 1.5 Panne de Datacenter

**ScÃ©narios :**
- Panne Ã©lectrique totale
- Catastrophe naturelle
- Incendie
- Inondation

**Impacts :**
- Perte complÃ¨te d'un site
- NÃ©cessite disaster recovery
- Bascule vers site secondaire

---

## 2. ProcÃ©dures de DÃ©marrage

### DÃ©marrage Standard

#### Ã‰tape 1 : VÃ©rifications PrÃ©-DÃ©marrage

```bash
#!/bin/bash
# Script de vÃ©rification prÃ©-dÃ©marrage

echo "=== MongoDB Pre-Start Checks ==="

# 1. VÃ©rifier que mongod n'est pas dÃ©jÃ  en cours d'exÃ©cution
if pgrep -x mongod > /dev/null; then
    echo "âœ— MongoDB is already running"
    exit 1
else
    echo "âœ“ No mongod process found"
fi

# 2. VÃ©rifier l'espace disque
DISK_USAGE=$(df -h /var/lib/mongodb | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt 90 ]; then
    echo "âš   WARNING: Disk usage is ${DISK_USAGE}%"
else
    echo "âœ“ Disk usage OK: ${DISK_USAGE}%"
fi

# 3. VÃ©rifier les permissions
if [ "$(stat -c '%U:%G' /var/lib/mongodb)" != "mongodb:mongodb" ]; then
    echo "âœ— Incorrect ownership on /var/lib/mongodb"
    echo "  Running: chown -R mongodb:mongodb /var/lib/mongodb"
    sudo chown -R mongodb:mongodb /var/lib/mongodb
else
    echo "âœ“ Ownership correct"
fi

# 4. VÃ©rifier l'existence du fichier de configuration
if [ ! -f /etc/mongod.conf ]; then
    echo "âœ— Configuration file not found: /etc/mongod.conf"
    exit 1
else
    echo "âœ“ Configuration file exists"
fi

# 5. VÃ©rifier la syntaxe de la configuration
mongod --config /etc/mongod.conf --validate > /dev/null 2>&1
if [ $? -eq 0 ]; then
    echo "âœ“ Configuration is valid"
else
    echo "âœ— Configuration is invalid"
    exit 1
fi

# 6. VÃ©rifier le lock file
if [ -f /var/lib/mongodb/mongod.lock ]; then
    if [ -s /var/lib/mongodb/mongod.lock ]; then
        echo "âš   WARNING: Non-empty lock file found"
        echo "  This may indicate an unclean shutdown"
    else
        echo "âœ“ Empty lock file (normal)"
    fi
fi

# 7. VÃ©rifier les logs prÃ©cÃ©dents
if [ -f /var/log/mongodb/mongod.log ]; then
    LAST_SHUTDOWN=$(grep "shutdown complete" /var/log/mongodb/mongod.log | tail -1)
    if [ -n "$LAST_SHUTDOWN" ]; then
        echo "âœ“ Previous shutdown was clean"
    else
        echo "âš   WARNING: Previous shutdown may have been unclean"
    fi
fi

echo ""
echo "Pre-start checks complete"
echo "Ready to start MongoDB"
```

#### Ã‰tape 2 : DÃ©marrage Normal

```bash
# DÃ©marrage via systemd (recommandÃ©)
sudo systemctl start mongod

# VÃ©rifier le dÃ©marrage
sudo systemctl status mongod

# Suivre les logs en temps rÃ©el
tail -f /var/log/mongodb/mongod.log

# Messages attendus :
# [timestamp] I CONTROL  [initandlisten] MongoDB starting
# [timestamp] I CONTROL  [initandlisten] db version v6.0.x
# [timestamp] I CONTROL  [initandlisten] OpenSSL version: ...
# [timestamp] I STORAGE  [initandlisten] WiredTiger opened
# [timestamp] I NETWORK  [listener] waiting for connections on port 27017
```

#### Ã‰tape 3 : Validation du DÃ©marrage

```javascript
// Se connecter Ã  MongoDB
mongosh

// VÃ©rifier la connectivitÃ©
db.adminCommand({ping: 1})
// {ok: 1}

// VÃ©rifier le statut du serveur
db.serverStatus()

// VÃ©rifier les bases de donnÃ©es
show dbs

// Pour un replica set, vÃ©rifier l'Ã©tat
rs.status()

// VÃ©rifier les opÃ©rations en cours
db.currentOp()

// VÃ©rifier les connexions
db.serverStatus().connections
```

---

## 3. RÃ©cupÃ©ration Serveur Standalone

### ScÃ©nario 1 : ArrÃªt Propre - RedÃ©marrage Simple

```bash
# Simple redÃ©marrage
sudo systemctl start mongod

# VÃ©rifier les logs
tail -50 /var/log/mongodb/mongod.log

# Tester la connexion
mongosh --eval "db.adminCommand({ping: 1})"
```

### ScÃ©nario 2 : ArrÃªt Brutal - RÃ©cupÃ©ration du Journal

```bash
# 1. Tenter un dÃ©marrage normal
sudo systemctl start mongod

# 2. Surveiller les logs pour la rÃ©cupÃ©ration
tail -f /var/log/mongodb/mongod.log

# Messages de rÃ©cupÃ©ration :
# [timestamp] I STORAGE  [initandlisten] Detected unclean shutdown
# [timestamp] I STORAGE  [initandlisten] Recovering from the journal
# [timestamp] I STORAGE  [initandlisten] Journal playback complete

# 3. Si le dÃ©marrage Ã©choue, vÃ©rifier les logs d'erreur
grep -i "error\|fatal\|exception" /var/log/mongodb/mongod.log | tail -20

# 4. Si corruption dÃ©tectÃ©e, voir section Corruption de DonnÃ©es
```

### ScÃ©nario 3 : Ã‰chec de DÃ©marrage - Mode Debug

```bash
# 1. ArrÃªter le service
sudo systemctl stop mongod

# 2. DÃ©marrer manuellement en mode verbose
sudo -u mongodb mongod \
  --config /etc/mongod.conf \
  --verbose \
  --logpath /tmp/mongod-debug.log

# 3. Analyser les logs dÃ©taillÃ©s
tail -f /tmp/mongod-debug.log

# 4. Si problÃ¨me identifiÃ© et rÃ©solu, redÃ©marrer normalement
sudo systemctl start mongod
```

### ScÃ©nario 4 : RÃ©cupÃ©ration avec --repair

**âš ï¸ Dernier recours - peut causer perte de donnÃ©es**

```bash
# 1. ArrÃªter MongoDB
sudo systemctl stop mongod

# 2. Sauvegarder l'Ã©tat actuel
sudo cp -r /var/lib/mongodb /var/lib/mongodb.backup.$(date +%Y%m%d_%H%M%S)

# 3. VÃ©rifier l'espace disque (besoin de 1.5x la taille des donnÃ©es)
df -h /var/lib/mongodb

# 4. Lancer la rÃ©paration
sudo -u mongodb mongod \
  --dbpath /var/lib/mongodb \
  --repair \
  --logpath /tmp/mongod-repair.log

# 5. Surveiller la progression
tail -f /tmp/mongod-repair.log

# 6. Une fois terminÃ©, redÃ©marrer normalement
sudo systemctl start mongod

# 7. Valider les donnÃ©es
mongosh --eval "
  db.adminCommand({listDatabases: 1}).databases.forEach(function(dbInfo) {
    if (!['admin', 'local', 'config'].includes(dbInfo.name)) {
      print('Validating database: ' + dbInfo.name);
      var db = db.getSiblingDB(dbInfo.name);
      db.getCollectionNames().forEach(function(coll) {
        var result = db[coll].validate();
        print('  ' + coll + ': ' + (result.valid ? 'OK' : 'INVALID'));
      });
    }
  });
"
```

---

## 4. RÃ©cupÃ©ration Replica Set

### Architecture et Points de DÃ©faillance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         REPLICA SET (rs0)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ PRIMARY  â”‚â”€â”€â”€â–¶â”‚SECONDARY1â”‚         â”‚
â”‚   â”‚  Node1   â”‚    â”‚  Node2   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚        â”‚               â”‚               â”‚
â”‚        â”‚               â”‚               â”‚
â”‚        â–¼               â–¼               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚SECONDARY2â”‚    â”‚ ARBITER  â”‚         â”‚
â”‚   â”‚  Node3   â”‚    â”‚  Node4   â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Points de dÃ©faillance :
1. Primary down â†’ Ã‰lection automatique
2. Secondary down â†’ RÃ©plication continue
3. Majority down â†’ Pas de primary (read-only)
4. Tous down â†’ RÃ©cupÃ©ration complÃ¨te
```

### ScÃ©nario 1 : Primary Down - Ã‰lection Automatique

```javascript
// Le replica set gÃ¨re automatiquement le failover

// 1. VÃ©rifier l'Ã©tat du replica set
rs.status()

// Sortie attendue :
// Un secondary devient primary automatiquement
// {
//   "members": [
//     {"name": "node1:27017", "stateStr": "DOWN"},
//     {"name": "node2:27017", "stateStr": "PRIMARY"},  // â† Nouveau primary
//     {"name": "node3:27017", "stateStr": "SECONDARY"}
//   ]
// }

// 2. RÃ©cupÃ©rer le nÅ“ud down
// Sur node1 :
sudo systemctl start mongod

// 3. Le nÅ“ud rejoindra automatiquement comme secondary
// VÃ©rifier
rs.status()
```

### ScÃ©nario 2 : Un Secondary Down - RÃ©cupÃ©ration Simple

```bash
# 1. Sur le secondary down
sudo systemctl start mongod

# 2. VÃ©rifier les logs
tail -f /var/log/mongodb/mongod.log

# Messages attendus :
# [timestamp] I REPL     [replication-0] Starting replication fetcher
# [timestamp] I REPL     [replication-0] Syncing from: node1:27017

# 3. VÃ©rifier le catch-up
mongosh --eval "rs.printSecondaryReplicationInfo()"

# Sortie :
# source: node2:27017
#   syncedTo: ...
#   0 secs (0 hrs) behind the primary
```

### ScÃ©nario 3 : Majority Down - Pas de Primary

```javascript
// Situation : Seulement 1 nÅ“ud sur 3 disponible
// = Pas de majoritÃ© = Pas de primary Ã©lu

// 1. VÃ©rifier l'Ã©tat
rs.status()

// Sortie :
// {
//   "members": [
//     {"name": "node1:27017", "stateStr": "SECONDARY"},  // Pas PRIMARY !
//     {"name": "node2:27017", "stateStr": "UNKNOWN"},
//     {"name": "node3:27017", "stateStr": "UNKNOWN"}
//   ]
// }

// 2. RÃ©cupÃ©rer les nÅ“uds down (prioritÃ©)
// Sur node2 et node3
sudo systemctl start mongod

// 3. Une fois la majoritÃ© revenue, Ã©lection automatique
// VÃ©rifier
rs.status()

// ALTERNATIVE : Forcer une reconfiguration (DANGER)
// âš ï¸ Seulement si impossible de rÃ©cupÃ©rer les autres nÅ“uds
// âš ï¸ Peut causer perte de donnÃ©es

// Sur le nÅ“ud survivant
cfg = rs.conf()
cfg.members = [cfg.members[0]]  // Garder seulement ce nÅ“ud
cfg.version++
rs.reconfig(cfg, {force: true})

// Le nÅ“ud devient primary immÃ©diatement
```

### ScÃ©nario 4 : Tous les NÅ“uds Down - RÃ©cupÃ©ration ComplÃ¨te

```bash
#!/bin/bash
# ProcÃ©dure de rÃ©cupÃ©ration complÃ¨te d'un replica set

echo "=== Replica Set Full Recovery ==="

# 1. Identifier le nÅ“ud avec les donnÃ©es les plus rÃ©centes
echo "Step 1: Checking all nodes for latest data"

for node in node1 node2 node3; do
  echo "Checking $node..."
  ssh $node "ls -la /var/lib/mongodb/journal/"
  ssh $node "grep 'shutdown' /var/log/mongodb/mongod.log | tail -1"
done

# Le nÅ“ud avec le journal le plus rÃ©cent = donnÃ©es les plus rÃ©centes

# 2. DÃ©marrer le nÅ“ud avec les donnÃ©es les plus rÃ©centes
echo "Step 2: Starting node with latest data"
LATEST_NODE="node1"  # Remplacer par le nÅ“ud identifiÃ©

ssh $LATEST_NODE "sudo systemctl start mongod"

# Attendre le dÃ©marrage
sleep 10

# 3. VÃ©rifier que le nÅ“ud dÃ©marre en standalone ou SECONDARY
ssh $LATEST_NODE "mongosh --eval 'db.adminCommand({isMaster: 1})'"

# 4. Si le nÅ“ud ne peut pas atteindre les autres membres,
#    forcer une reconfiguration temporaire
ssh $LATEST_NODE "mongosh" <<'EOF'
cfg = rs.conf()
cfg.members = cfg.members.filter(m => m.host === "node1:27017")
cfg.version++
rs.reconfig(cfg, {force: true})
EOF

# 5. DÃ©marrer les autres nÅ“uds
echo "Step 3: Starting remaining nodes"
for node in node2 node3; do
  ssh $node "sudo systemctl start mongod"
done

# 6. Ajouter les nÅ“uds au replica set
ssh $LATEST_NODE "mongosh" <<'EOF'
// Attendre que le nÅ“ud soit PRIMARY
while(rs.status().myState !== 1) {
  print("Waiting for PRIMARY state...")
  sleep(2000)
}

// Ajouter les autres membres
rs.add("node2:27017")
rs.add("node3:27017")

// VÃ©rifier
rs.status()
EOF

echo "Recovery complete"
```

### ScÃ©nario 5 : Oplog Trop Court - Resync Complet

```javascript
// Un secondary ne peut pas rattraper (oplog trop court)

// 1. Identifier le problÃ¨me
rs.printSecondaryReplicationInfo()

// Sortie :
// source: node2:27017
//   syncedTo: Thu Dec 07 2024 10:00:00 GMT+0000
//   ERROR: too stale to catch up

// 2. Sur le secondary problÃ©matique
use admin
db.shutdownServer()

// 3. Supprimer les donnÃ©es
// bash:
sudo rm -rf /var/lib/mongodb/*
# GARDER mongod.conf !

// 4. RedÃ©marrer
sudo systemctl start mongod

// 5. Initial sync automatique depuis le primary
// Surveiller les logs
tail -f /var/log/mongodb/mongod.log

// Messages attendus :
// [timestamp] I REPL     [replication-0] Starting initial sync
// [timestamp] I REPL     [replication-0] Cloning all databases
// [timestamp] I REPL     [replication-0] Initial sync done

// 6. VÃ©rifier
rs.status()
```

---

## 5. RÃ©cupÃ©ration Cluster ShardÃ©

### Architecture et Points de DÃ©faillance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SHARDED CLUSTER                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ MONGOS  â”‚  â”‚ MONGOS  â”‚  â”‚ MONGOS  â”‚        â”‚
â”‚   â”‚  App1   â”‚  â”‚  App2   â”‚  â”‚  App3   â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚
â”‚        â”‚            â”‚            â”‚             â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                     â”‚                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚             â”‚                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”               â”‚
â”‚         â”‚ CONFIG  â”‚   â”‚ CONFIG â”‚               â”‚
â”‚         â”‚ SERVER  â”‚   â”‚ SERVER â”‚               â”‚
â”‚         â”‚  (RS)   â”‚   â”‚  (RS)  â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜               â”‚
â”‚              â”‚            â”‚                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”              â”‚
â”‚         â”‚                       â”‚              â”‚
â”‚    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚
â”‚    â”‚ SHARD 1 â”‚  â”‚ SHARD 2â”‚  â”‚ SHARD 3â”‚         â”‚
â”‚    â”‚  (RS)   â”‚  â”‚  (RS)  â”‚  â”‚  (RS)  â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Points de dÃ©faillance critiques :
1. Config Servers â†’ MÃ©tadonnÃ©es inaccessibles
2. Tous les Mongos â†’ Applications ne peuvent pas router
3. Un Shard â†’ DonnÃ©es de ce shard inaccessibles
4. Balancer â†’ Distribution bloquÃ©e (non critique)
```

### ScÃ©nario 1 : Mongos Down

```bash
# Mongos est stateless - redÃ©marrage simple

# 1. Sur le serveur mongos
sudo systemctl start mongos

# 2. VÃ©rifier les logs
tail -50 /var/log/mongodb/mongos.log

# Messages attendus :
# [timestamp] I SHARDING [Balancer] about to contact config servers
# [timestamp] I SHARDING [Balancer] config servers and shards contacted successfully
# [timestamp] I NETWORK  [listener] waiting for connections on port 27017

# 3. Tester la connexion
mongosh --host mongos-server:27017 --eval "sh.status()"

# Si plusieurs mongos, les autres prennent le relais automatiquement
```

### ScÃ©nario 2 : Config Server Replica Set - RÃ©cupÃ©ration

```bash
# Config servers sont critiques - contiennent toutes les mÃ©tadonnÃ©es

# 1. VÃ©rifier l'Ã©tat du config server replica set
mongosh --host config-server:27019 --eval "rs.status()"

# 2. Si un membre down, rÃ©cupÃ©ration standard replica set
ssh config-server-1 "sudo systemctl start mongod"

# 3. Si majority down, rÃ©cupÃ©ration comme replica set normal
# Voir section "RÃ©cupÃ©ration Replica Set"

# 4. VÃ©rifier que mongos peut reconnecter
mongosh --host mongos-server:27017 --eval "sh.status()"
```

### ScÃ©nario 3 : Un Shard Down

```javascript
// Un shard (replica set) est down

// 1. Identifier le shard down
sh.status()

// Sortie :
// shards:
//   { "_id" : "shard0000", "host" : "shard0000/node1:27017,node2:27017" }
//   { "_id" : "shard0001", "host" : "shard0001/node1:27017,node2:27017" }  // DOWN
//   { "_id" : "shard0002", "host" : "shard0002/node1:27017,node2:27017" }

// 2. RÃ©cupÃ©rer le shard (procÃ©dure replica set)
// Sur les nÅ“uds du shard0001
// bash: sudo systemctl start mongod

// 3. VÃ©rifier le statut depuis mongos
sh.status()

// 4. Les donnÃ©es du shard redeviennent accessibles
// Les applications peuvent continuer
```

### ScÃ©nario 4 : Cluster Complet Down - RÃ©cupÃ©ration OrdonnÃ©e

```bash
#!/bin/bash
# ProcÃ©dure de rÃ©cupÃ©ration complÃ¨te d'un cluster shardÃ©

echo "=== Sharded Cluster Full Recovery ==="

# ORDRE CRITIQUE :
# 1. Config Servers (PREMIER)
# 2. Shards
# 3. Mongos (DERNIER)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 1 : Config Servers
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 1: Starting Config Servers..."

# DÃ©marrer le premier config server
ssh config-server-1 "sudo systemctl start mongod"
sleep 10

# VÃ©rifier qu'il dÃ©marre
ssh config-server-1 "mongosh --port 27019 --eval 'db.adminCommand({ping: 1})'"

# DÃ©marrer les autres config servers
for server in config-server-2 config-server-3; do
  ssh $server "sudo systemctl start mongod"
done

# Attendre que le replica set forme une majoritÃ©
sleep 20

# VÃ©rifier le config replica set
ssh config-server-1 "mongosh --port 27019 --eval 'rs.status()'" | grep "PRIMARY"

if [ $? -ne 0 ]; then
  echo "ERROR: Config servers did not elect a PRIMARY"
  exit 1
fi

echo "âœ“ Config Servers are up"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 2 : Shards
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 2: Starting Shards..."

for shard in shard0000 shard0001 shard0002; do
  echo "Starting $shard..."

  # DÃ©marrer tous les nÅ“uds du shard
  for node in node1 node2 node3; do
    ssh ${shard}-${node} "sudo systemctl start mongod"
  done

  sleep 10

  # VÃ©rifier le shard
  ssh ${shard}-node1 "mongosh --eval 'rs.status()'" | grep "PRIMARY"

  if [ $? -eq 0 ]; then
    echo "âœ“ $shard is up"
  else
    echo "âš  $shard may have issues"
  fi
done

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 3 : Mongos
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 3: Starting Mongos routers..."

for mongos in mongos-1 mongos-2 mongos-3; do
  ssh $mongos "sudo systemctl start mongos"
done

sleep 10

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 4 : Validation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 4: Validating cluster..."

# Tester depuis mongos
ssh mongos-1 "mongosh --eval 'sh.status()'" > /tmp/cluster-status.txt

# VÃ©rifier que tous les shards sont prÃ©sents
for shard in shard0000 shard0001 shard0002; do
  grep $shard /tmp/cluster-status.txt > /dev/null
  if [ $? -eq 0 ]; then
    echo "âœ“ $shard is visible from mongos"
  else
    echo "âœ— $shard is NOT visible from mongos"
  fi
done

echo ""
echo "Cluster recovery complete"
echo "Full status:"
cat /tmp/cluster-status.txt
```

### ScÃ©nario 5 : Config Server Corruption - Restauration Metadata

```bash
# Si les config servers sont corrompus, restaurer depuis backup

# 1. ArrÃªter tous les mongos
for mongos in mongos-1 mongos-2 mongos-3; do
  ssh $mongos "sudo systemctl stop mongos"
done

# 2. ArrÃªter les config servers
for server in config-server-1 config-server-2 config-server-3; do
  ssh $server "sudo systemctl stop mongod"
done

# 3. Restaurer le backup sur tous les config servers
for server in config-server-1 config-server-2 config-server-3; do
  ssh $server "
    sudo rm -rf /var/lib/mongodb-config/*
    sudo rsync -av /backup/config-server-latest/ /var/lib/mongodb-config/
    sudo chown -R mongodb:mongodb /var/lib/mongodb-config/
  "
done

# 4. RedÃ©marrer les config servers
for server in config-server-1 config-server-2 config-server-3; do
  ssh $server "sudo systemctl start mongod"
done

# 5. VÃ©rifier le replica set config
ssh config-server-1 "mongosh --port 27019 --eval 'rs.status()'"

# 6. RedÃ©marrer les mongos
for mongos in mongos-1 mongos-2 mongos-3; do
  ssh $mongos "sudo systemctl start mongos"
done

# 7. Valider
mongosh --host mongos-1:27017 --eval "sh.status()"
```

---

## 6. Disaster Recovery

### Plans de Disaster Recovery

#### RPO et RTO

**RPO (Recovery Point Objective) :**
- QuantitÃ© maximale de donnÃ©es pouvant Ãªtre perdues
- Exemples :
  - RPO = 0 : Aucune perte acceptable (rÃ©plication synchrone)
  - RPO = 5 min : Perte max de 5 minutes de donnÃ©es
  - RPO = 24h : Backup quotidien

**RTO (Recovery Time Objective) :**
- Temps maximal pour restaurer le service
- Exemples :
  - RTO = 5 min : Service rÃ©tabli en 5 minutes
  - RTO = 1h : Service rÃ©tabli en 1 heure
  - RTO = 4h : Service rÃ©tabli en 4 heures

#### StratÃ©gies par Niveau de CriticitÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CriticitÃ©    â”‚   RPO    â”‚   RTO    â”‚ Solution               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CRITIQUE     â”‚   0      â”‚  < 1 min â”‚ Multi-region RS        â”‚
â”‚              â”‚          â”‚          â”‚ + Auto-failover        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HAUTE        â”‚  < 5 min â”‚  < 5 min â”‚ Cross-DC RS            â”‚
â”‚              â”‚          â”‚          â”‚ + Automated recovery   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MOYENNE      â”‚  < 1h    â”‚  < 30min â”‚ Regular snapshots      â”‚
â”‚              â”‚          â”‚          â”‚ + Standby replica      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BASSE        â”‚  < 24h   â”‚  < 4h    â”‚ Daily backups          â”‚
â”‚              â”‚          â”‚          â”‚ + Manual recovery      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ProcÃ©dure DR ComplÃ¨te - Site Primaire Perdu

```bash
#!/bin/bash
# Disaster Recovery - Bascule vers site secondaire

echo "=== DISASTER RECOVERY PROCEDURE ==="
echo "Primary site lost - Failing over to secondary site"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 1 : Ã‰valuation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 1: Assessment"

# VÃ©rifier que le site primaire est vraiment inaccessible
ping -c 5 primary-site.example.com
if [ $? -eq 0 ]; then
  echo "WARNING: Primary site is responding!"
  echo "Confirm before proceeding with failover"
  read -p "Continue? (yes/no): " confirm
  if [ "$confirm" != "yes" ]; then
    exit 1
  fi
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 2 : Promotion du Site Secondaire
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 2: Promoting Secondary Site"

# Pour Replica Set gÃ©o-distribuÃ©
# Les secondaries du site secondaire vont Ã©lire un nouveau primary

# Si pas d'Ã©lection automatique (pas de majoritÃ©), forcer
ssh secondary-site-node1 "mongosh" <<'EOF'
// VÃ©rifier l'Ã©tat actuel
var status = rs.status()
printjson(status)

// Si pas de primary, forcer une reconfiguration
var cfg = rs.conf()

// Retirer les membres du site primaire (inaccessibles)
cfg.members = cfg.members.filter(m =>
  m.host.includes("secondary-site")
)

cfg.version++

// Forcer la reconfiguration
rs.reconfig(cfg, {force: true})

// Attendre l'Ã©lection
sleep(5000)

// VÃ©rifier
rs.status()
EOF

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 3 : Mise Ã  Jour DNS/Load Balancer
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 3: Updating DNS and Load Balancers"

# Pointer les applications vers le site secondaire
# (MÃ©thode dÃ©pend de votre infrastructure)

# Exemple avec DNS
# dig mongodb.example.com
# Mettre Ã  jour pour pointer vers secondary-site

# Exemple avec HAProxy
ssh load-balancer "
  # DÃ©sactiver backend primaire
  echo 'disable server mongodb/primary-site-1' | socat stdio /var/run/haproxy.sock

  # Activer backend secondaire
  echo 'enable server mongodb/secondary-site-1' | socat stdio /var/run/haproxy.sock
"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 4 : Validation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 4: Validation"

# Tester la connectivitÃ© depuis l'application
mongosh --host secondary-site-node1:27017 --eval "
  db.adminCommand({ping: 1})
  db.test_dr.insertOne({test: true, timestamp: new Date()})
  print('âœ“ Write successful on secondary site')
"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 5 : Notification
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 5: Notifications"

# Notifier les Ã©quipes
curl -X POST https://alerts.example.com/webhook \
  -d '{
    "event": "DR_FAILOVER",
    "site": "secondary",
    "timestamp": "'$(date -Iseconds)'",
    "status": "ACTIVE"
  }'

echo ""
echo "DISASTER RECOVERY COMPLETE"
echo "Site secondaire est maintenant le site actif"
echo ""
echo "TODO:"
echo "1. Investiguer la cause de la panne du site primaire"
echo "2. Planifier la restauration du site primaire"
echo "3. Communiquer avec les stakeholders"
echo "4. Documenter l'incident"
```

### Restauration du Site Primaire

```bash
#!/bin/bash
# Restauration du site primaire aprÃ¨s DR

echo "=== PRIMARY SITE RESTORATION ==="

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 1 : PrÃ©paration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 1: Preparation"

# VÃ©rifier que le site primaire est fonctionnel
ping -c 10 primary-site.example.com
if [ $? -ne 0 ]; then
  echo "ERROR: Primary site still unreachable"
  exit 1
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 2 : Resynchronisation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 2: Resyncing Primary Site"

# Sur les nÅ“uds du site primaire
for node in primary-site-node1 primary-site-node2 primary-site-node3; do
  ssh $node "
    # Nettoyer les anciennes donnÃ©es
    sudo systemctl stop mongod
    sudo rm -rf /var/lib/mongodb/*

    # RedÃ©marrer pour initial sync
    sudo systemctl start mongod
  "
done

# Les nÅ“uds vont faire une initial sync depuis le site secondaire (actif)

# Surveiller la progression
ssh primary-site-node1 "tail -f /var/log/mongodb/mongod.log" | grep "initial sync"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 3 : Validation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 3: Validation"

# VÃ©rifier que les nÅ“uds sont SECONDARY et Ã  jour
ssh primary-site-node1 "mongosh --eval 'rs.status()'" | grep "SECONDARY"

# VÃ©rifier le replication lag
ssh primary-site-node1 "mongosh --eval 'rs.printSecondaryReplicationInfo()'"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 4 : Failback (Optionnel)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Phase 4: Failback to Primary Site (Optional)"

read -p "Failback to primary site now? (yes/no): " failback

if [ "$failback" == "yes" ]; then
  # Augmenter la prioritÃ© des nÅ“uds du site primaire
  ssh secondary-site-node1 "mongosh" <<'EOF'
  cfg = rs.conf()

  // Augmenter prioritÃ© site primaire
  cfg.members.forEach(m => {
    if (m.host.includes("primary-site")) {
      m.priority = 2
    } else {
      m.priority = 1
    }
  })

  cfg.version++
  rs.reconfig(cfg)

  // Le primary va basculer automatiquement
EOF

  echo "Failback initiated. Monitor for new PRIMARY election..."

  # Attendre l'Ã©lection
  sleep 30

  # VÃ©rifier
  mongosh --host primary-site-node1:27017 --eval "rs.status()" | grep "PRIMARY"

  echo "âœ“ Failback complete. Primary site is now PRIMARY again."
fi

echo ""
echo "PRIMARY SITE RESTORATION COMPLETE"
```

---

## 7. Failover et Bascule

### Failover Automatique (Replica Set)

```javascript
// MongoDB gÃ¨re automatiquement le failover

// Configuration pour optimiser le failover
cfg = rs.conf()

// RÃ©duire les timeouts pour failover plus rapide
cfg.settings = {
  electionTimeoutMillis: 10000,    // 10s (dÃ©faut)
  heartbeatIntervalMillis: 2000,   // 2s
  heartbeatTimeoutSecs: 10         // 10s
}

// PrioritÃ©s pour contrÃ´ler quel membre devient primary
cfg.members[0].priority = 2  // PrÃ©fÃ©rÃ©
cfg.members[1].priority = 1  // Normal
cfg.members[2].priority = 0.5  // Dernier recours

rs.reconfig(cfg)

// Test de failover
// Sur le primary actuel
db.adminCommand({replSetStepDown: 60})  // Step down pour 60 secondes

// Un nouveau primary sera Ã©lu automatiquement
```

### Failover Manuel (Maintenance PlanifiÃ©e)

```javascript
// ProcÃ©dure pour maintenance sans downtime

// 1. Identifier le primary
rs.status().members.find(m => m.stateStr === "PRIMARY").name

// 2. Step down le primary (graceful)
db.adminCommand({
  replSetStepDown: 120,  // Reste secondary pendant 120 secondes
  force: false
})

// 3. Un secondary est Ã©lu primary automatiquement

// 4. Effectuer la maintenance sur l'ancien primary
// (RedÃ©marrage, upgrade, etc.)

// 5. Le nÅ“ud rejoint automatiquement comme secondary

// 6. Si besoin, le faire redevenir primary
// Augmenter sa prioritÃ© et attendre
```

### Bascule Applicative

```javascript
// Configuration application pour gÃ©rer le failover

// Node.js avec retry automatique
const { MongoClient } = require('mongodb')

const client = new MongoClient(uri, {
  // Retry writes automatiquement sur failover
  retryWrites: true,

  // Retry reads
  retryReads: true,

  // Write concern pour assurer durabilitÃ©
  writeConcern: {
    w: 'majority',
    wtimeout: 5000
  },

  // Read preference
  readPreference: 'primaryPreferred',  // Fallback sur secondary

  // Timeouts
  serverSelectionTimeoutMS: 5000,
  connectTimeoutMS: 10000,
  socketTimeoutMS: 45000
})

// Gestion des erreurs de failover
client.on('serverDescriptionChanged', (event) => {
  console.log('Server state changed:', event)
})

client.on('topologyDescriptionChanged', (event) => {
  console.log('Topology changed:', event)
  // Le driver reconnecte automatiquement
})

// Wrapper avec retry custom
async function executeWithRetry(operation, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await operation()
    } catch (error) {
      // Erreurs liÃ©es au failover
      if (error.message.includes('not master') ||
          error.message.includes('connection') ||
          error.code === 11600) {  // InterruptedDueToReplStateChange

        if (i < maxRetries - 1) {
          console.log(`Retry ${i + 1}/${maxRetries} after failover...`)
          await new Promise(resolve => setTimeout(resolve, 1000 * (i + 1)))
          continue
        }
      }
      throw error
    }
  }
}

// Utilisation
await executeWithRetry(async () => {
  return await db.collection('users').insertOne({name: "Alice"})
})
```

---

## 8. Tests et Validation

### Tests de RÃ©cupÃ©ration RÃ©guliers

```bash
#!/bin/bash
# Script de test de rÃ©cupÃ©ration (Ã  exÃ©cuter rÃ©guliÃ¨rement)

echo "=== MongoDB Recovery Drill ==="
echo "Date: $(date)"

TEST_ENV="test"  # Environnement de test

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 1 : Restauration depuis Backup
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Test 1: Backup Restoration"

# Prendre un backup
mongodump --host $TEST_ENV-mongodb:27017 --out /tmp/test-backup/

# "Corrompre" les donnÃ©es
mongosh --host $TEST_ENV-mongodb:27017 --eval "
  db.test_data.drop()
  print('Dropped test collection')
"

# Restaurer
mongorestore --host $TEST_ENV-mongodb:27017 --drop /tmp/test-backup/

# Valider
RESTORED_COUNT=$(mongosh --host $TEST_ENV-mongodb:27017 --quiet --eval "
  db.test_data.countDocuments({})
")

if [ $RESTORED_COUNT -gt 0 ]; then
  echo "âœ“ Backup restoration successful"
else
  echo "âœ— Backup restoration failed"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 2 : Failover Replica Set
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Test 2: Replica Set Failover"

# Step down le primary
PRIMARY=$(mongosh --host $TEST_ENV-mongodb:27017 --quiet --eval "
  rs.status().members.find(m => m.stateStr === 'PRIMARY').name
" | tail -1)

echo "Current primary: $PRIMARY"

# Forcer step down
mongosh --host $TEST_ENV-mongodb:27017 --eval "
  db.adminCommand({replSetStepDown: 60, force: true})
"

# Attendre nouvelle Ã©lection
sleep 15

# VÃ©rifier nouveau primary
NEW_PRIMARY=$(mongosh --host $TEST_ENV-mongodb:27017 --quiet --eval "
  rs.status().members.find(m => m.stateStr === 'PRIMARY').name
" | tail -1)

echo "New primary: $NEW_PRIMARY"

if [ "$PRIMARY" != "$NEW_PRIMARY" ]; then
  echo "âœ“ Failover successful"
else
  echo "âœ— Failover failed"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Test 3 : Recovery Time (RTO)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo "Test 3: Measuring RTO"

START_TIME=$(date +%s)

# ArrÃªter MongoDB
ssh $TEST_ENV-mongodb-1 "sudo systemctl stop mongod"

# Attendre que le service soit down
sleep 5

# RedÃ©marrer
ssh $TEST_ENV-mongodb-1 "sudo systemctl start mongod"

# Attendre que le service soit up
while ! mongosh --host $TEST_ENV-mongodb-1:27017 --eval "db.adminCommand({ping: 1})" > /dev/null 2>&1; do
  sleep 1
done

END_TIME=$(date +%s)
RTO=$((END_TIME - START_TIME))

echo "Recovery Time: ${RTO} seconds"

if [ $RTO -lt 60 ]; then
  echo "âœ“ RTO within acceptable range (< 60s)"
else
  echo "âš  RTO exceeds target"
fi

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Rapport Final
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
echo ""
echo "=== Recovery Drill Complete ==="
echo "Report saved to: /var/log/mongodb-recovery-drill-$(date +%Y%m%d).log"
```

### Checklist de Validation Post-RÃ©cupÃ©ration

```javascript
// Checklist complÃ¨te aprÃ¨s rÃ©cupÃ©ration

function postRecoveryValidation() {
  var results = {
    timestamp: new Date(),
    checks: []
  }

  // 1. ConnectivitÃ© de base
  try {
    db.adminCommand({ping: 1})
    results.checks.push({name: "Connectivity", status: "OK"})
  } catch (e) {
    results.checks.push({name: "Connectivity", status: "FAIL", error: e.message})
  }

  // 2. Statut du serveur
  try {
    var status = db.serverStatus()
    results.checks.push({
      name: "Server Status",
      status: "OK",
      uptime: status.uptime
    })
  } catch (e) {
    results.checks.push({name: "Server Status", status: "FAIL", error: e.message})
  }

  // 3. Replica Set (si applicable)
  try {
    var rsStatus = rs.status()
    var primary = rsStatus.members.find(m => m.stateStr === "PRIMARY")

    results.checks.push({
      name: "Replica Set",
      status: primary ? "OK" : "NO PRIMARY",
      members: rsStatus.members.length,
      primary: primary ? primary.name : "none"
    })
  } catch (e) {
    results.checks.push({name: "Replica Set", status: "NOT A REPLICA SET"})
  }

  // 4. Validation des collections
  var databases = db.adminCommand({listDatabases: 1}).databases
  databases.forEach(dbInfo => {
    if (!['admin', 'local', 'config'].includes(dbInfo.name)) {
      var database = db.getSiblingDB(dbInfo.name)

      database.getCollectionNames().forEach(collName => {
        try {
          var validation = database[collName].validate()
          results.checks.push({
            name: `Validation: ${dbInfo.name}.${collName}`,
            status: validation.valid ? "OK" : "INVALID",
            errors: validation.errors || []
          })
        } catch (e) {
          results.checks.push({
            name: `Validation: ${dbInfo.name}.${collName}`,
            status: "ERROR",
            error: e.message
          })
        }
      })
    }
  })

  // 5. Test CRUD
  try {
    var testDoc = {test: true, timestamp: new Date()}
    db.test_recovery.insertOne(testDoc)
    db.test_recovery.findOne({test: true})
    db.test_recovery.updateOne({test: true}, {$set: {updated: true}})
    db.test_recovery.deleteOne({test: true})
    db.test_recovery.drop()

    results.checks.push({name: "CRUD Operations", status: "OK"})
  } catch (e) {
    results.checks.push({name: "CRUD Operations", status: "FAIL", error: e.message})
  }

  // 6. Compter les Ã©checs
  var failures = results.checks.filter(c => c.status === "FAIL" || c.status === "INVALID")
  results.summary = {
    total: results.checks.length,
    passed: results.checks.length - failures.length,
    failed: failures.length
  }

  return results
}

// ExÃ©cuter
var validationResults = postRecoveryValidation()
printjson(validationResults)

if (validationResults.summary.failed === 0) {
  print("\nâœ“ All validation checks passed")
} else {
  print("\nâœ— " + validationResults.summary.failed + " validation checks failed")
}
```

---

## Checklist Globale de RÃ©cupÃ©ration

```
INCIDENT: Panne MongoDB dÃ©tectÃ©e

â˜ PHASE 1: Ã‰VALUATION IMMÃ‰DIATE (2 minutes)
  â˜ Identifier le type de panne
  â˜ Ã‰valuer l'impact (standalone/RS/sharded)
  â˜ VÃ©rifier les logs
  â˜ DÃ©terminer la criticitÃ©

â˜ PHASE 2: DIAGNOSTIC (5 minutes)
  â˜ Hardware OK ? (disque, mÃ©moire, rÃ©seau)
  â˜ Logs d'erreur ?
  â˜ ArrÃªt propre ou brutal ?
  â˜ DonnÃ©es accessibles ?
  â˜ Backup disponible ?

â˜ PHASE 3: DÃ‰CISION (2 minutes)
  â˜ RedÃ©marrage simple ?
  â˜ RÃ©cupÃ©ration du journal ?
  â˜ Failover RS ?
  â˜ Restauration backup ?
  â˜ DR complet ?

â˜ PHASE 4: EXÃ‰CUTION (15-120 minutes selon type)
  â˜ Suivre la procÃ©dure appropriÃ©e
  â˜ Documenter chaque action
  â˜ Monitorer la progression
  â˜ Communiquer status

â˜ PHASE 5: VALIDATION (15 minutes)
  â˜ Service disponible ?
  â˜ DonnÃ©es intÃ¨gres ? (validate)
  â˜ RÃ©plication OK ? (si RS)
  â˜ Performance normale ?
  â˜ Applications fonctionnelles ?

â˜ PHASE 6: POST-INCIDENT
  â˜ Analyse de cause racine
  â˜ Rapport d'incident
  â˜ Mise Ã  jour procÃ©dures
  â˜ Actions prÃ©ventives
  â˜ Retour d'expÃ©rience Ã©quipe
```

---

## Conclusion

La rÃ©cupÃ©ration aprÃ¨s panne nÃ©cessite :

1. **PrÃ©paration rigoureuse** : ProcÃ©dures documentÃ©es, testÃ©es rÃ©guliÃ¨rement
2. **Architecture rÃ©siliente** : Replica sets, backups, multi-site
3. **Monitoring proactif** : DÃ©tecter avant la panne critique
4. **Ã‰quipe formÃ©e** : ConnaÃ®tre les procÃ©dures, exercices rÃ©guliers
5. **Automatisation** : Scripts testÃ©s, playbooks

**Principes clÃ©s :**
- âœ… **Toujours utiliser Replica Set** (haute disponibilitÃ©)
- âœ… **Backups testÃ©s rÃ©guliÃ¨rement** (RPO/RTO connus)
- âœ… **ProcÃ©dures documentÃ©es et Ã  jour**
- âœ… **Drills de rÃ©cupÃ©ration mensuels**
- âœ… **Architecture multi-site** (DR)
- âœ… **Monitoring avec alertes**
- âœ… **Automatisation maximale**

**Temps de rÃ©cupÃ©ration typiques :**
- RedÃ©marrage simple : 10-30 secondes
- Failover automatique RS : 10-30 secondes
- RÃ©cupÃ©ration journal : 1-5 minutes
- Resync secondary : 30 minutes - 2 heures
- Restauration backup : 1-4 heures
- DR complet : 2-8 heures

**L'objectif est de minimiser le MTTR (Mean Time To Recovery).**

---


â­ï¸ [Analyse des logs d'erreurs](/22-depannage-resolution-problemes/07-analyse-logs-erreurs.md)
