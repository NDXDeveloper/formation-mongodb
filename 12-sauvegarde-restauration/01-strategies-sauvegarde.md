ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 12.1 StratÃ©gies de Sauvegarde

## Introduction

Une stratÃ©gie de sauvegarde efficace pour MongoDB nÃ©cessite une analyse approfondie des besoins mÃ©tier, des contraintes techniques et des objectifs de rÃ©cupÃ©ration. Il n'existe pas de solution universelle : chaque organisation doit dÃ©finir sa stratÃ©gie en fonction de sa criticitÃ© mÃ©tier, de son volume de donnÃ©es, de ses fenÃªtres de maintenance et de son budget.

Cette section prÃ©sente les diffÃ©rentes approches stratÃ©giques, leurs avantages, inconvÃ©nients et cas d'usage recommandÃ©s, ainsi que des exemples concrets d'implÃ©mentation.

## Types de Sauvegardes

### Sauvegarde ComplÃ¨te (Full Backup)

**DÃ©finition** : Copie intÃ©grale de toutes les donnÃ©es de la base Ã  un instant T.

**CaractÃ©ristiques** :
```yaml
Avantages:
  - Restauration simple et rapide (un seul fichier)
  - IndÃ©pendance totale (pas de chaÃ®ne de dÃ©pendances)
  - FacilitÃ© de vÃ©rification
  - IdÃ©al pour audit et archivage

InconvÃ©nients:
  - Consomme le plus d'espace disque
  - Temps de sauvegarde le plus long
  - Bande passante rÃ©seau importante
  - FenÃªtre d'exÃ©cution potentiellement problÃ©matique

Cas d'usage:
  - PremiÃ¨re sauvegarde d'un nouveau systÃ¨me
  - Base de rÃ©fÃ©rence hebdomadaire ou mensuelle
  - SystÃ¨mes avec peu de changements
  - Avant opÃ©rations majeures (migration, upgrade)
```

**ImplÃ©mentation** :
```bash
#!/bin/bash
# full_backup.sh - Sauvegarde complÃ¨te MongoDB

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backup/mongodb/full"
BACKUP_NAME="full_backup_${TIMESTAMP}"
MONGO_URI="mongodb://backup-user:password@localhost:27017/admin"

# CrÃ©ation du rÃ©pertoire
mkdir -p ${BACKUP_DIR}/${BACKUP_NAME}

# Sauvegarde complÃ¨te avec mongodump
mongodump \
  --uri="${MONGO_URI}" \
  --oplog \
  --gzip \
  --out="${BACKUP_DIR}/${BACKUP_NAME}" \
  2>&1 | tee ${BACKUP_DIR}/${BACKUP_NAME}/backup.log

# VÃ©rification
if [ $? -eq 0 ]; then
  # CrÃ©ation d'une archive
  tar -czf ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz \
    -C ${BACKUP_DIR} ${BACKUP_NAME}

  # Checksum
  sha256sum ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz \
    > ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz.sha256

  # MÃ©tadonnÃ©es
  cat > ${BACKUP_DIR}/${BACKUP_NAME}.metadata <<EOF
{
  "backup_type": "full",
  "timestamp": "${TIMESTAMP}",
  "size_bytes": $(stat -f%z ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz),
  "mongodb_version": "$(mongod --version | head -1)",
  "databases": $(mongo ${MONGO_URI} --quiet --eval "db.adminCommand('listDatabases').databases.length"),
  "status": "success"
}
EOF

  # Nettoyage du rÃ©pertoire temporaire
  rm -rf ${BACKUP_DIR}/${BACKUP_NAME}

  echo "âœ“ Full backup completed: ${BACKUP_NAME}.tar.gz"
else
  echo "âœ— Full backup failed"
  exit 1
fi
```

### Sauvegarde IncrÃ©mentale (Incremental Backup)

**DÃ©finition** : Sauvegarde uniquement des modifications depuis la derniÃ¨re sauvegarde (complÃ¨te ou incrÃ©mentale).

**Principe via Oplog** :
```javascript
// MongoDB utilise l'oplog pour les sauvegardes incrÃ©mentales
// L'oplog enregistre toutes les opÃ©rations d'Ã©criture

// Structure de l'oplog
{
  "ts": Timestamp(1701993600, 1),     // Timestamp de l'opÃ©ration
  "t": NumberLong(1),                  // Terme de rÃ©plication
  "h": NumberLong("1234567890"),       // Hash de l'opÃ©ration
  "v": 2,                              // Version oplog
  "op": "i",                           // Type: i=insert, u=update, d=delete
  "ns": "mydb.users",                  // Namespace
  "o": { "_id": ObjectId("..."), ... } // Objet de l'opÃ©ration
}
```

**ImplÃ©mentation** :
```bash
#!/bin/bash
# incremental_backup.sh - Sauvegarde incrÃ©mentale via oplog

BACKUP_DIR="/backup/mongodb/incremental"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
STATE_FILE="/var/lib/mongodb_backup/last_oplog_timestamp"

# Lire le dernier timestamp sauvegardÃ©
if [ -f "$STATE_FILE" ]; then
  LAST_TS=$(cat $STATE_FILE)
  echo "Last backup timestamp: $LAST_TS"
else
  echo "No previous backup found. Run full backup first."
  exit 1
fi

# Sauvegarder les entrÃ©es oplog depuis le dernier backup
mongo --quiet <<EOF
  use local
  db.oplog.rs.find({
    ts: { \$gt: Timestamp($LAST_TS) }
  }).forEach(function(doc) {
    printjson(doc);
  })
EOF > ${BACKUP_DIR}/oplog_${TIMESTAMP}.json

# Compression
gzip ${BACKUP_DIR}/oplog_${TIMESTAMP}.json

# Sauvegarder le nouveau timestamp
CURRENT_TS=$(mongo --quiet --eval "
  db = db.getSiblingDB('local');
  printjson(db.oplog.rs.find().sort({ts:-1}).limit(1).toArray()[0].ts);
")

echo "$CURRENT_TS" > $STATE_FILE

# MÃ©tadonnÃ©es
cat > ${BACKUP_DIR}/oplog_${TIMESTAMP}.metadata <<EOF
{
  "backup_type": "incremental",
  "timestamp": "${TIMESTAMP}",
  "from_ts": $LAST_TS,
  "to_ts": $CURRENT_TS,
  "size_bytes": $(stat -c%s ${BACKUP_DIR}/oplog_${TIMESTAMP}.json.gz)
}
EOF

echo "âœ“ Incremental backup completed: oplog_${TIMESTAMP}.json.gz"
```

**ChaÃ®ne de restauration** :
```bash
# Pour restaurer, il faut :
# 1. La derniÃ¨re sauvegarde complÃ¨te
# 2. Toutes les sauvegardes incrÃ©mentales dans l'ordre

# Exemple de restauration
mongorestore --gzip --archive=/backup/full/full_backup_20241201.tar.gz
mongorestore --oplogReplay --oplogFile=/backup/incremental/oplog_20241202.json.gz
mongorestore --oplogReplay --oplogFile=/backup/incremental/oplog_20241203.json.gz
# ... continuer pour chaque fichier incrÃ©mental
```

### Sauvegarde DiffÃ©rentielle (Differential Backup)

**DÃ©finition** : Sauvegarde de toutes les modifications depuis la derniÃ¨re sauvegarde complÃ¨te (pas depuis la derniÃ¨re diffÃ©rentielle).

**Comparaison** :
```
Chronologie des sauvegardes sur une semaine:

Full Backup:     [FULL]
                   â†“
IncrÃ©mentale:    [FULL]â†’[Inc1]â†’[Inc2]â†’[Inc3]â†’[Inc4]â†’[Inc5]â†’[Inc6]
Taille:          100%   5%     5%     5%     5%     5%     5%
Restauration:    1 + 6 fichiers nÃ©cessaires

DiffÃ©rentielle:  [FULL]â†’[Diff1]â†’[Diff2]â†’[Diff3]â†’[Diff4]â†’[Diff5]â†’[Diff6]
Taille:          100%   5%      10%     15%     20%     25%     30%
Restauration:    1 + 1 fichier nÃ©cessaire
```

**Avantages et inconvÃ©nients** :
```yaml
IncrÃ©mentale:
  avantages:
    - Sauvegardes plus rapides
    - Moins d'espace disque utilisÃ©
  inconvÃ©nients:
    - Restauration complexe (chaÃ®ne complÃ¨te)
    - Point de dÃ©faillance unique (perte d'une sauvegarde)

DiffÃ©rentielle:
  avantages:
    - Restauration plus simple (full + derniÃ¨re diff)
    - Plus tolÃ©rant aux erreurs
  inconvÃ©nients:
    - Sauvegardes de plus en plus longues
    - Plus d'espace disque que incrÃ©mentale
```

## StratÃ©gies par Environnement

### Environnement de Production Critique

**CaractÃ©ristiques** :
- RTO : < 1 heure
- RPO : < 15 minutes
- DisponibilitÃ© : 99.99% (4.38 min/mois)

**StratÃ©gie recommandÃ©e** :
```yaml
Architecture:
  - Replica Set 5 membres minimum
  - 1 membre dÃ©diÃ© backup (hidden, priority: 0)
  - GÃ©o-distribution sur 3 datacenters

Sauvegarde Continue:
  mÃ©thode: MongoDB Ops Manager / Atlas
  snapshot_interval: 15 minutes
  retention:
    snapshots: 48 heures
    daily: 30 jours
    weekly: 3 mois
    monthly: 1 an

Sauvegarde ComplÃ©mentaire:
  full_backup:
    frÃ©quence: hebdomadaire
    fenÃªtre: dimanche 02h00
    destination:
      - Local: SAN avec rÃ©plication
      - Remote: S3 Glacier Deep Archive

  incremental:
    frÃ©quence: toutes les 4 heures
    mÃ©thode: oplog export
    destination: S3 Standard-IA

Tests:
  restauration_partielle: hebdomadaire
  restauration_complÃ¨te: mensuel
  disaster_recovery_drill: trimestriel
```

**ImplÃ©mentation** :
```bash
#!/bin/bash
# production_critical_backup_strategy.sh

# Configuration
PRIMARY_BACKUP_DIR="/backup/primary"
REMOTE_BACKUP_BUCKET="s3://company-critical-backups"
BACKUP_MEMBER="mongodb-backup-node:27017"

# Fonction: Snapshot continu (via cron toutes les 15 min)
create_snapshot() {
  local timestamp=$(date +%Y%m%d_%H%M%S)
  local snapshot_name="snapshot_${timestamp}"

  # Utiliser le membre dÃ©diÃ© aux backups
  mongodump \
    --host=${BACKUP_MEMBER} \
    --oplog \
    --gzip \
    --archive=${PRIMARY_BACKUP_DIR}/${snapshot_name}.gz \
    --readPreference=secondary

  # Upload immÃ©diat vers S3
  aws s3 cp ${PRIMARY_BACKUP_DIR}/${snapshot_name}.gz \
    ${REMOTE_BACKUP_BUCKET}/snapshots/ \
    --storage-class STANDARD_IA

  # Garder local seulement 48h
  find ${PRIMARY_BACKUP_DIR} -name "snapshot_*.gz" -mmin +2880 -delete
}

# Fonction: Full backup hebdomadaire
weekly_full_backup() {
  local timestamp=$(date +%Y%m%d)
  local backup_name="full_${timestamp}"

  # Backup complet
  mongodump \
    --host=${BACKUP_MEMBER} \
    --oplog \
    --gzip \
    --archive=${PRIMARY_BACKUP_DIR}/${backup_name}.gz

  # Chiffrement avant upload
  gpg --encrypt --recipient backup@company.com \
    ${PRIMARY_BACKUP_DIR}/${backup_name}.gz

  # Upload vers Glacier
  aws s3 cp ${PRIMARY_BACKUP_DIR}/${backup_name}.gz.gpg \
    ${REMOTE_BACKUP_BUCKET}/weekly/ \
    --storage-class GLACIER

  # Validation
  aws s3api head-object \
    --bucket company-critical-backups \
    --key weekly/${backup_name}.gz.gpg

  if [ $? -eq 0 ]; then
    # Envoyer notification success
    send_notification "âœ“ Weekly backup completed: ${backup_name}"
  fi
}

# ExÃ©cution selon le type
case "$1" in
  snapshot)
    create_snapshot
    ;;
  weekly)
    weekly_full_backup
    ;;
  *)
    echo "Usage: $0 {snapshot|weekly}"
    exit 1
    ;;
esac
```

### Environnement de Production Standard

**CaractÃ©ristiques** :
- RTO : 2-4 heures
- RPO : 1-2 heures
- DisponibilitÃ© : 99.9% (43.8 min/mois)

**StratÃ©gie recommandÃ©e** :
```yaml
Architecture:
  - Replica Set 3 membres
  - Sauvegarde sur secondary

Sauvegarde:
  full_backup:
    frÃ©quence: quotidienne
    heure: 02h00 (hors pic)
    mÃ©thode: mongodump
    destination:
      - Local NAS: 7 jours
      - Cloud S3: 30 jours

  incremental:
    frÃ©quence: toutes les 6 heures
    mÃ©thode: oplog export
    rÃ©tention: 48 heures

Archivage:
  - Backup hebdomadaire conservÃ© 3 mois
  - Backup mensuel conservÃ© 1 an

Tests:
  restauration: bimensuel
  dr_drill: semestriel
```

**Script d'orchestration** :
```bash
#!/bin/bash
# standard_production_backup.sh

BACKUP_ROOT="/backup/mongodb"
DAILY_DIR="${BACKUP_ROOT}/daily"
WEEKLY_DIR="${BACKUP_ROOT}/weekly"
MONTHLY_DIR="${BACKUP_ROOT}/monthly"
S3_BUCKET="s3://company-backups/mongodb"

# DÃ©terminer le type de backup
DAY_OF_WEEK=$(date +%u)  # 1-7 (1=lundi, 7=dimanche)
DAY_OF_MONTH=$(date +%d)

if [ "$DAY_OF_MONTH" == "01" ]; then
  BACKUP_TYPE="monthly"
  TARGET_DIR="$MONTHLY_DIR"
  RETENTION_DAYS=365
elif [ "$DAY_OF_WEEK" == "7" ]; then
  BACKUP_TYPE="weekly"
  TARGET_DIR="$WEEKLY_DIR"
  RETENTION_DAYS=90
else
  BACKUP_TYPE="daily"
  TARGET_DIR="$DAILY_DIR"
  RETENTION_DAYS=7
fi

TIMESTAMP=$(date +%Y%m%d)
BACKUP_NAME="${BACKUP_TYPE}_${TIMESTAMP}"

echo "Executing ${BACKUP_TYPE} backup: ${BACKUP_NAME}"

# ExÃ©cution du backup
mkdir -p "$TARGET_DIR"

mongodump \
  --host=mongodb-secondary:27017 \
  --oplog \
  --gzip \
  --archive="${TARGET_DIR}/${BACKUP_NAME}.gz" \
  --numParallelCollections=4

if [ $? -eq 0 ]; then
  # MÃ©tadonnÃ©es
  SIZE=$(stat -c%s "${TARGET_DIR}/${BACKUP_NAME}.gz")
  cat > "${TARGET_DIR}/${BACKUP_NAME}.json" <<EOF
{
  "type": "${BACKUP_TYPE}",
  "timestamp": "$(date -Iseconds)",
  "size_bytes": ${SIZE},
  "size_human": "$(numfmt --to=iec-i --suffix=B ${SIZE})",
  "retention_days": ${RETENTION_DAYS}
}
EOF

  # Upload S3
  aws s3 cp "${TARGET_DIR}/${BACKUP_NAME}.gz" \
    "${S3_BUCKET}/${BACKUP_TYPE}/" \
    --storage-class STANDARD_IA \
    --metadata "backup-type=${BACKUP_TYPE},timestamp=${TIMESTAMP}"

  # Cleanup local selon rÃ©tention
  find "$TARGET_DIR" -name "*.gz" -mtime +${RETENTION_DAYS} -delete

  echo "âœ“ Backup completed successfully"
  exit 0
else
  echo "âœ— Backup failed"
  exit 1
fi
```

### Environnement de DÃ©veloppement/Test

**CaractÃ©ristiques** :
- RTO : 24 heures acceptable
- RPO : 24-48 heures
- Focus : CoÃ»t rÃ©duit, simplicitÃ©

**StratÃ©gie recommandÃ©e** :
```yaml
Architecture:
  - Instance standalone ou Replica Set 2 membres

Sauvegarde:
  full_backup:
    frÃ©quence: quotidienne (nuit)
    rÃ©tention: 7 jours local

  archivage:
    hebdomadaire: 1 mois
    pas de backup incrÃ©mental

Tests:
  restauration: aucun requis
  mais restauration ad-hoc frÃ©quente pour refresh
```

**Script simplifiÃ©** :
```bash
#!/bin/bash
# dev_backup_simple.sh

BACKUP_DIR="/backup/mongodb/dev"
RETENTION_DAYS=7

# Simple backup quotidien
mongodump \
  --host=localhost:27017 \
  --gzip \
  --archive="${BACKUP_DIR}/dev_$(date +%Y%m%d).gz"

# Cleanup simple
find "$BACKUP_DIR" -name "*.gz" -mtime +${RETENTION_DAYS} -delete
```

## StratÃ©gies par Volume de DonnÃ©es

### Petits Volumes (< 100 GB)

**Approche** : Backups complets frÃ©quents

```yaml
StratÃ©gie:
  type: Full backup uniquement
  frÃ©quence: Toutes les 4-6 heures
  mÃ©thode: mongodump
  durÃ©e_backup: 10-30 minutes

Avantages:
  - SimplicitÃ© maximale
  - Restauration rapide et fiable
  - Pas de gestion de chaÃ®ne

Configuration:
  compression: gzip -6 (balance vitesse/ratio)
  parallÃ©lisme: 4 collections simultanÃ©es
```

### Volumes Moyens (100 GB - 1 TB)

**Approche** : Full + incrÃ©mentale

```yaml
StratÃ©gie:
  full_backup:
    frÃ©quence: hebdomadaire
    durÃ©e_estimÃ©e: 2-6 heures
    fenÃªtre: weekend hors heures de pointe

  incremental:
    frÃ©quence: toutes les 4 heures
    via: oplog export
    durÃ©e: 5-15 minutes

Optimisations:
  - Backup sur secondary dÃ©diÃ©
  - Compression zstd (meilleur ratio)
  - ParallÃ©lisme Ã©levÃ© (8+ threads)
```

**Configuration optimisÃ©e** :
```bash
#!/bin/bash
# medium_volume_backup.sh

BACKUP_SIZE_ESTIMATE_GB=500
PARALLEL_COLLECTIONS=$((BACKUP_SIZE_ESTIMATE_GB / 50))  # 1 thread par 50GB

# Full backup optimisÃ©
mongodump \
  --host=backup-secondary:27017 \
  --oplog \
  --archive \
  --numParallelCollections=${PARALLEL_COLLECTIONS} \
  --readPreference=secondary \
  --readPreference='{ mode: "secondary", tagSets: [ { "backup": "true" } ] }' \
  | zstd -19 -T0 > /backup/full_$(date +%Y%m%d).zst

# Statistiques
echo "Backup completed in $SECONDS seconds"
echo "Size: $(du -h /backup/full_$(date +%Y%m%d).zst)"
```

### Gros Volumes (> 1 TB)

**Approche** : Snapshots systÃ¨me + oplog

```yaml
StratÃ©gie:
  snapshots:
    mÃ©thode: LVM/EBS snapshots
    frÃ©quence: quotidienne
    durÃ©e: minutes (Copy-on-Write)
    avantage: Pas d'impact performance

  oplog_continuous:
    export: continu (streaming)
    rÃ©tention: 7 jours
    objectif: Point-in-time recovery

  full_logical:
    frÃ©quence: mensuelle (archivage)
    mÃ©thode: mongodump avec streaming S3
```

**ImplÃ©mentation avec LVM** :
```bash
#!/bin/bash
# large_volume_snapshot_backup.sh

VOLUME_GROUP="vg_mongodb"
LOGICAL_VOLUME="lv_mongodb_data"
SNAPSHOT_NAME="snap_mongo_$(date +%Y%m%d_%H%M%S)"
SNAPSHOT_SIZE="50G"  # 10-20% de l'original
MOUNT_POINT="/mnt/mongo_snapshot"

# Phase 1: PrÃ©parer MongoDB
echo "Preparing MongoDB for snapshot..."
mongo admin --eval "
  db.fsyncLock();
  print('MongoDB locked for writes');
"

# Phase 2: CrÃ©er le snapshot LVM
echo "Creating LVM snapshot..."
lvcreate --size ${SNAPSHOT_SIZE} \
  --snapshot \
  --name ${SNAPSHOT_NAME} \
  /dev/${VOLUME_GROUP}/${LOGICAL_VOLUME}

# Phase 3: DÃ©bloquer MongoDB immÃ©diatement
mongo admin --eval "
  db.fsyncUnlock();
  print('MongoDB unlocked');
"

echo "Snapshot created. MongoDB downtime: ~1 second"

# Phase 4: Monter et copier (en arriÃ¨re-plan)
mkdir -p ${MOUNT_POINT}
mount -o ro /dev/${VOLUME_GROUP}/${SNAPSHOT_NAME} ${MOUNT_POINT}

# Copie vers stockage distant (pas de pression sur MongoDB)
rsync -av --progress ${MOUNT_POINT}/ /backup/snapshot_$(date +%Y%m%d)/ &

# Ou vers S3 avec streaming
tar -czf - -C ${MOUNT_POINT} . | \
  aws s3 cp - s3://backups/mongo_snapshot_$(date +%Y%m%d).tar.gz \
  --expected-size 1099511627776 &  # 1TB estimate

echo "Background copy initiated. Snapshot will be removed after completion."

# Cleanup automatique aprÃ¨s copie
wait
umount ${MOUNT_POINT}
lvremove -f /dev/${VOLUME_GROUP}/${SNAPSHOT_NAME}
echo "âœ“ Snapshot backup completed"
```

**Export oplog continu** :
```javascript
// continuous_oplog_export.js
// Ã€ exÃ©cuter comme daemon

const { MongoClient } = require('mongodb');
const fs = require('fs');
const zlib = require('zlib');

const uri = 'mongodb://backup-node:27017/?replicaSet=rs0';
const client = new MongoClient(uri);

async function exportOplogContinuously() {
  await client.connect();

  const db = client.db('local');
  const oplog = db.collection('oplog.rs');

  // Dernier timestamp traitÃ©
  let lastTs = await getLastProcessedTimestamp();

  // Watch oplog en temps rÃ©el
  const changeStream = oplog.watch([
    { $match: { ts: { $gt: lastTs } } }
  ], { fullDocument: 'updateLookup' });

  const outputStream = fs.createWriteStream(
    `/backup/oplog/oplog_${new Date().toISOString()}.json.gz`
  );
  const gzipStream = zlib.createGzip();
  gzipStream.pipe(outputStream);

  changeStream.on('change', (change) => {
    gzipStream.write(JSON.stringify(change) + '\n');
    lastTs = change.ts;

    // Sauvegarder progression toutes les 1000 ops
    if (Math.random() < 0.001) {
      saveLastProcessedTimestamp(lastTs);
    }
  });

  // Rotation quotidienne
  setInterval(() => {
    gzipStream.end();
    // CrÃ©er nouveau fichier...
  }, 24 * 60 * 60 * 1000);
}

exportOplogContinuously().catch(console.error);
```

## Matrices de DÃ©cision

### Matrice de SÃ©lection de StratÃ©gie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CritÃ¨re   â”‚  < 10GB  â”‚ 10-100GB â”‚100GB-1TB â”‚  > 1TB   â”‚  > 10TB    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MÃ©thode     â”‚mongodump â”‚mongodump â”‚mongodump â”‚ Snapshot â”‚ Snapshot   â”‚
â”‚ principale  â”‚          â”‚+ oplog   â”‚+ snapshotâ”‚+ oplog   â”‚+ streaming â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FrÃ©quence   â”‚ 4h       â”‚ 6h       â”‚ 12h      â”‚ 24h      â”‚ 24h        â”‚
â”‚ full        â”‚          â”‚          â”‚          â”‚          â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FrÃ©quence   â”‚ N/A      â”‚ 2h       â”‚ 4h       â”‚ Continu  â”‚ Continu    â”‚
â”‚ incrÃ©mental â”‚          â”‚          â”‚          â”‚          â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Impact      â”‚ Faible   â”‚ Moyen    â”‚ Moyen-   â”‚ Minimal  â”‚ Minimal    â”‚
â”‚ performance â”‚          â”‚          â”‚ Ã‰levÃ©    â”‚          â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ComplexitÃ©  â”‚ Simple   â”‚ Moyenne  â”‚ Ã‰levÃ©e   â”‚ TrÃ¨s     â”‚ Expert     â”‚
â”‚             â”‚          â”‚          â”‚          â”‚ Ã‰levÃ©e   â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Matrice RTO/RPO vs CoÃ»t

```
                    RPO (Recovery Point Objective)
                 â”‚
        24h      â”‚  Dev/Test      Standard        Business
                 â”‚  $              $$              $$$
                 â”‚  Daily          Daily+6h        4h snapshots
                 â”‚  mongodump      incremental     + oplog
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         4h      â”‚  Standard       Critical        Mission
                 â”‚  $$             $$$             $$$$
                 â”‚  Daily+4h       Hourly          15min snapshots
                 â”‚  incremental    snapshots       + streaming oplog
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         1h      â”‚  Critical       Mission         Ultra-Critical
                 â”‚  $$$            $$$$            $$$$$
                 â”‚  Hourly         15min           Continuous
                 â”‚  snapshots      snapshots       replication
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        < 15min  â”‚  Mission        Ultra-          Financial/
                 â”‚  $$$$           Critical        Healthcare
                 â”‚  15min          $$$$$           $$$$$$
                 â”‚  snapshots      Continuous      Multi-region
                 â”‚                 + geo-replic    active-active
                 â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      4h          1h         < 15min
                           RTO (Recovery Time Objective)
```

## StratÃ©gies Hybrides

### Approche Multi-Niveaux (RecommandÃ©e pour Production)

```yaml
Niveau 1 - Protection ImmÃ©diate (RPO: minutes):
  - Replica Set avec 3+ membres
  - Oplog sizing: 72h minimum
  - Read concern: majority
  - Write concern: { w: "majority", j: true }

Niveau 2 - Snapshots Rapides (RPO: 15min-4h):
  mÃ©thode: Cloud provider snapshots ou LVM
  frÃ©quence: toutes les 4 heures
  rÃ©tention: 48 heures
  restauration: 15-30 minutes

Niveau 3 - Backups Logiques (RPO: 24h):
  mÃ©thode: mongodump sur secondary
  frÃ©quence: quotidienne
  rÃ©tention: 30 jours
  restauration: 2-4 heures

Niveau 4 - Archives Long Terme (RPO: 1 semaine):
  mÃ©thode: mongodump compressÃ© + chiffrÃ©
  frÃ©quence: hebdomadaire
  destination: Glacier / Archive tier
  rÃ©tention: 1-7 ans
  restauration: 12-48 heures
```

**Orchestrateur multi-niveaux** :
```bash
#!/bin/bash
# hybrid_backup_orchestrator.sh

LEVEL1_MONITOR="/usr/local/bin/monitor_replication.sh"
LEVEL2_SNAPSHOT="/usr/local/bin/create_snapshot.sh"
LEVEL3_LOGICAL="/usr/local/bin/logical_backup.sh"
LEVEL4_ARCHIVE="/usr/local/bin/archive_backup.sh"

# Fonction de monitoring et dÃ©cision
check_and_execute() {
  local current_hour=$(date +%H)
  local day_of_week=$(date +%u)

  # Niveau 1: Monitoring continu
  $LEVEL1_MONITOR

  # Niveau 2: Snapshots toutes les 4h
  if [ $((current_hour % 4)) -eq 0 ]; then
    echo "Executing Level 2: Snapshot backup"
    $LEVEL2_SNAPSHOT
  fi

  # Niveau 3: Backup logique quotidien Ã  2h
  if [ "$current_hour" == "02" ]; then
    echo "Executing Level 3: Logical backup"
    $LEVEL3_LOGICAL
  fi

  # Niveau 4: Archive hebdomadaire dimanche Ã  3h
  if [ "$day_of_week" == "7" ] && [ "$current_hour" == "03" ]; then
    echo "Executing Level 4: Archive backup"
    $LEVEL4_ARCHIVE
  fi
}

# ExÃ©cution
check_and_execute

# Logger le rÃ©sultat
echo "[$(date)] Backup orchestration completed" >> /var/log/backup_orchestrator.log
```

### StratÃ©gie Geo-DistribuÃ©e

Pour les applications mondiales nÃ©cessitant rÃ©silience gÃ©ographique :

```yaml
Architecture:
  primary_region: us-east-1
    - Replica Set 5 membres
    - Snapshots toutes les 15 min â†’ S3 local

  secondary_region: eu-west-1
    - Replica Set 3 membres (delayed read replica)
    - ReÃ§oit snapshots via S3 cross-region replication

  tertiary_region: ap-southeast-1
    - Archive storage uniquement
    - ReÃ§oit backups hebdomadaires

Processus:
  1. Backup continu dans rÃ©gion primaire
  2. RÃ©plication asynchrone vers rÃ©gion secondaire
  3. Archive mensuelle vers rÃ©gion tertiaire

Avantages:
  - Protection contre catastrophe datacenter
  - ConformitÃ© rÃ©glementaire (localisation donnÃ©es)
  - RÃ©duction latence de restauration
```

**Configuration S3 cross-region** :
```bash
# Configurer la rÃ©plication cross-region
aws s3api put-bucket-replication \
  --bucket company-backups-us-east-1 \
  --replication-configuration '{
    "Role": "arn:aws:iam::123456:role/s3-replication",
    "Rules": [
      {
        "Status": "Enabled",
        "Priority": 1,
        "Filter": { "Prefix": "mongodb/" },
        "Destination": {
          "Bucket": "arn:aws:s3:::company-backups-eu-west-1",
          "ReplicationTime": {
            "Status": "Enabled",
            "Time": { "Minutes": 15 }
          },
          "Metrics": {
            "Status": "Enabled"
          }
        }
      }
    ]
  }'
```

## StratÃ©gies pour Sharded Clusters

Les clusters shardÃ©s nÃ©cessitent une coordination spÃ©ciale :

### Backup CohÃ©rent de Sharded Cluster

```yaml
DÃ©fi:
  - Multiples shards indÃ©pendants
  - Config servers critiques
  - CohÃ©rence inter-shards

Approche:
  1. ArrÃªter le balancer
  2. Backup de chaque shard
  3. Backup des config servers
  4. RedÃ©marrer le balancer
```

**Script coordonnÃ©** :
```bash
#!/bin/bash
# sharded_cluster_backup.sh

MONGOS_HOST="mongos:27017"
CONFIG_RS="configReplSet/config1:27019,config2:27019,config3:27019"
SHARDS=("shard1/shard1-a:27018" "shard2/shard2-a:27018" "shard3/shard3-a:27018")

BACKUP_DIR="/backup/sharded/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "Starting sharded cluster backup..."

# 1. ArrÃªter le balancer
echo "Stopping balancer..."
mongo $MONGOS_HOST --eval "sh.stopBalancer()" admin

# Attendre que toutes migrations soient terminÃ©es
while true; do
  ACTIVE=$(mongo $MONGOS_HOST --quiet --eval "sh.isBalancerRunning()" admin)
  if [ "$ACTIVE" == "false" ]; then
    break
  fi
  echo "Waiting for balancer to stop..."
  sleep 5
done

echo "Balancer stopped"

# 2. Backup des config servers
echo "Backing up config servers..."
mongodump \
  --host="$CONFIG_RS" \
  --oplog \
  --gzip \
  --out="$BACKUP_DIR/configdb"

# 3. Backup de chaque shard en parallÃ¨le
echo "Backing up shards..."
for shard in "${SHARDS[@]}"; do
  shard_name=$(echo $shard | cut -d'/' -f1)
  echo "  Backing up $shard_name..."

  mongodump \
    --host="$shard" \
    --oplog \
    --gzip \
    --out="$BACKUP_DIR/$shard_name" &
done

# Attendre tous les backups
wait

# 4. Sauvegarder mÃ©tadonnÃ©es du cluster
echo "Saving cluster metadata..."
mongo $MONGOS_HOST <<EOF > "$BACKUP_DIR/cluster_metadata.json"
  printjson({
    "config": db.getSiblingDB('config'),
    "shards": sh.status(),
    "databases": db.adminCommand({ listDatabases: 1 }),
    "timestamp": new Date()
  })
EOF

# 5. RedÃ©marrer le balancer
echo "Restarting balancer..."
mongo $MONGOS_HOST --eval "sh.startBalancer()" admin

# 6. CrÃ©er archive complÃ¨te
echo "Creating consolidated archive..."
tar -czf "$BACKUP_DIR.tar.gz" -C "$(dirname $BACKUP_DIR)" "$(basename $BACKUP_DIR)"

# 7. Checksum et mÃ©tadonnÃ©es
sha256sum "$BACKUP_DIR.tar.gz" > "$BACKUP_DIR.tar.gz.sha256"

cat > "$BACKUP_DIR.json" <<EOF
{
  "cluster_type": "sharded",
  "timestamp": "$(date -Iseconds)",
  "shards": ${#SHARDS[@]},
  "size_bytes": $(stat -c%s "$BACKUP_DIR.tar.gz"),
  "balancer_downtime_seconds": $SECONDS
}
EOF

echo "âœ“ Sharded cluster backup completed"
echo "  Location: $BACKUP_DIR.tar.gz"
echo "  Size: $(du -h $BACKUP_DIR.tar.gz | cut -f1)"
echo "  Balancer downtime: ${SECONDS}s"
```

## Optimisations de Performance

### ParallÃ©lisation Intelligente

```bash
#!/bin/bash
# parallel_backup_optimized.sh

# DÃ©tecter le nombre de CPU
NCPU=$(nproc)

# Estimer le nombre optimal de threads
# RÃ¨gle: 1 thread par 50GB de donnÃ©es, max 75% des CPU
DB_SIZE_GB=$(mongo --quiet --eval "
  db.adminCommand({ dbStats: 1, scale: 1024*1024*1024 }).dataSize
")

OPTIMAL_THREADS=$(echo "scale=0; $DB_SIZE_GB / 50" | bc)
MAX_THREADS=$(echo "scale=0; $NCPU * 0.75" | bc)

if [ $OPTIMAL_THREADS -gt $MAX_THREADS ]; then
  THREADS=$MAX_THREADS
else
  THREADS=$OPTIMAL_THREADS
fi

echo "Using $THREADS parallel threads for backup"

mongodump \
  --numParallelCollections=$THREADS \
  --gzip \
  --archive=/backup/optimized_$(date +%Y%m%d).gz
```

### Backup avec Limitation de Bande Passante

```bash
# Limiter l'impact rÃ©seau avec trickle ou tc
trickle -s -d 10000 -u 10000 \  # 10 MB/s max
  mongodump --gzip --archive | \
  ssh backup-server "cat > /backup/remote.gz"

# Ou avec rsync throttle
mongodump --gzip --out=/tmp/backup
rsync -avz --bwlimit=10000 /tmp/backup/ backup-server:/backup/
```

### Compression Adaptative

```bash
#!/bin/bash
# adaptive_compression.sh

BACKUP_SIZE_ESTIMATE=$(mongo --quiet --eval "
  db.adminCommand({ dbStats: 1 }).dataSize
")

# Choisir compression selon taille
if [ $BACKUP_SIZE_ESTIMATE -lt $((10 * 1024 * 1024 * 1024)) ]; then
  # < 10GB: compression maximale
  COMPRESSION="zstd -19"
  echo "Small dataset: using maximum compression"
elif [ $BACKUP_SIZE_ESTIMATE -lt $((100 * 1024 * 1024 * 1024)) ]; then
  # 10-100GB: balance
  COMPRESSION="gzip -6"
  echo "Medium dataset: using balanced compression"
else
  # > 100GB: compression rapide
  COMPRESSION="lz4"
  echo "Large dataset: using fast compression"
fi

mongodump --archive | $COMPRESSION > /backup/adaptive_$(date +%Y%m%d).compressed
```

## Checklist de SÃ©lection de StratÃ©gie

Utilisez cette checklist pour dÃ©finir votre stratÃ©gie :

```markdown
### Analyse des Besoins

- [ ] CriticitÃ© mÃ©tier dÃ©finie (dev/standard/critique/mission-critique)
- [ ] RTO dÃ©terminÃ© (4h/1h/15min/<15min)
- [ ] RPO dÃ©terminÃ© (24h/4h/1h/<15min)
- [ ] Volume de donnÃ©es estimÃ© (avec croissance 2 ans)
- [ ] FenÃªtres de maintenance identifiÃ©es
- [ ] Budget allouÃ© (infrastructure + stockage + bande passante)

### Contraintes Techniques

- [ ] Architecture MongoDB documentÃ©e (standalone/replica/sharded)
- [ ] CapacitÃ© rÃ©seau Ã©valuÃ©e (bande passante disponible)
- [ ] Stockage disponible calculÃ© (local + remote)
- [ ] Charge systÃ¨me analysÃ©e (CPU/RAM/IO pendant backup)
- [ ] ConformitÃ© rÃ©glementaire vÃ©rifiÃ©e (RGPD/HIPAA/SOX)

### SÃ©lection de MÃ©thode

- [ ] MÃ©thode principale choisie (mongodump/snapshot/ops manager)
- [ ] FrÃ©quence dÃ©finie (continue/4h/12h/24h)
- [ ] RÃ©tention planifiÃ©e (court/moyen/long terme)
- [ ] Compression sÃ©lectionnÃ©e (none/gzip/zstd/lz4)
- [ ] Chiffrement configurÃ© (transit + repos)
- [ ] Destination validÃ©e (local/S3/Glacier/multi-region)

### OpÃ©rations

- [ ] Scripts de backup crÃ©Ã©s et testÃ©s
- [ ] Monitoring configurÃ© (mÃ©triques + alertes)
- [ ] Automatisation dÃ©ployÃ©e (cron/kubernetes/ops manager)
- [ ] ProcÃ©dures de restauration documentÃ©es
- [ ] Tests de restauration planifiÃ©s
- [ ] DR drill organisÃ©
```

## Recommandations par Profil

### Startup / PME

```yaml
Recommandation: MongoDB Atlas (M10+)
Raison: Backup automatisÃ© intÃ©grÃ©, pas de gestion
CoÃ»t: 60-200â‚¬/mois tout inclus
RTO/RPO: 1h/15min (snapshots point-in-time)
```

### Entreprise Standard

```yaml
Recommandation: Self-hosted + S3
Architecture: Replica Set 3 membres
Backup: mongodump quotidien + oplog 6h
Stockage: 30j S3 Standard-IA + 1an Glacier
CoÃ»t: 500-2000â‚¬/mois (infra + stockage)
RTO/RPO: 4h/6h
```

### Entreprise Critique

```yaml
Recommandation: Ops Manager ou Atlas Dedicated
Architecture: Replica Set 5+ membres multi-AZ
Backup: Snapshots 15min + oplog streaming
Stockage: Multi-rÃ©gion avec rÃ©plication
CoÃ»t: 5000-20000â‚¬/mois
RTO/RPO: 30min/15min
```

### Enterprise Financial/Healthcare

```yaml
Recommandation: Atlas + Ops Manager hybrid
Architecture: Sharded cluster gÃ©o-distribuÃ©
Backup: Continuous + snapshots every 15min
ConformitÃ©: Chiffrement E2E + audit complet
Stockage: Immutable backups multi-region
CoÃ»t: 20000-100000â‚¬/mois
RTO/RPO: 15min/1min
```

## Conclusion

Le choix d'une stratÃ©gie de sauvegarde MongoDB doit Ãªtre guidÃ© par :

1. **Les objectifs mÃ©tier** (RTO/RPO) avant tout
2. **Le volume de donnÃ©es** et sa croissance anticipÃ©e
3. **Les contraintes techniques** de l'infrastructure
4. **Le budget disponible** (capex + opex)
5. **Les compÃ©tences de l'Ã©quipe** pour opÃ©rer la solution

Il n'existe pas de solution universelle. Les organisations matures adoptent gÃ©nÃ©ralement une **approche multi-niveaux** combinant plusieurs mÃ©thodes pour Ã©quilibrer coÃ»t, performance et fiabilitÃ©.

**Principe directeur** : Commencer simple, mesurer, puis optimiser progressivement. Une sauvegarde quotidienne testÃ©e vaut mieux qu'un systÃ¨me complexe non maÃ®trisÃ©.

---


â­ï¸ [mongodump et mongorestore](/12-sauvegarde-restauration/02-mongodump-mongorestore.md)
