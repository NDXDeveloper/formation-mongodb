ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 12.3 Sauvegarde de Replica Sets

## Introduction

Les Replica Sets constituent l'architecture de dÃ©ploiement standard pour MongoDB en production, offrant haute disponibilitÃ© et redondance des donnÃ©es. La sauvegarde d'un Replica Set nÃ©cessite une approche spÃ©cifique qui tire parti de sa topologie distribuÃ©e tout en minimisant l'impact sur les opÃ©rations de production.

Cette section dÃ©taille les stratÃ©gies, procÃ©dures et bonnes pratiques pour sauvegarder efficacement un Replica Set sans compromettre la disponibilitÃ© du service ni les performances applicatives.

## Architecture et ConsidÃ©rations

### Topologie Standard d'un Replica Set

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Replica Set (rs0)                       â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚   PRIMARY    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  SECONDARY   â”‚                 â”‚
â”‚   â”‚  mongo-1     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”‚  mongo-2     â”‚                 â”‚
â”‚   â”‚  (Writes)    â”‚  Oplog  â”‚  (Reads)     â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Replic â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                           â”‚                       â”‚
â”‚         â”‚         Oplog             â”‚                       â”‚
â”‚         â”‚       Replication         â”‚                       â”‚
â”‚         v                           v                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚  SECONDARY   â”‚         â”‚  SECONDARY   â”‚                 â”‚
â”‚   â”‚  mongo-3     â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  mongo-4     â”‚                 â”‚
â”‚   â”‚  (Reads)     â”‚         â”‚ (BACKUP)     â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                              hidden: true                   â”‚
â”‚                              priority: 0                    â”‚
â”‚                              tags: {backup: "true"}         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Clients â†’ Load Balancer â†’ PRIMARY (writes) + SECONDARIES (reads)
                           â†“
                       (Backup node isolated)
```

### Principes Fondamentaux

**RÃ¨gles d'or pour les backups de Replica Set** :

1. **Jamais sur le Primary** : Effectuer les backups sur un secondary pour ne pas impacter les Ã©critures
2. **CohÃ©rence point-in-time** : Utiliser `--oplog` pour garantir la cohÃ©rence transactionnelle
3. **Isolation du membre de backup** : Utiliser un membre cachÃ© (hidden) dÃ©diÃ© aux backups
4. **Monitoring du lag** : Surveiller le replication lag pendant les opÃ©rations
5. **Validation post-backup** : VÃ©rifier l'intÃ©gritÃ© et la complÃ©tude du backup

### Impact sur la RÃ©plication

Comprendre l'impact d'un backup sur la rÃ©plication :

```javascript
// Impact pendant le backup
{
  "backup_operation": {
    // Charge CPU
    "cpu_usage": "+30-50%",  // Sur le membre de backup
    "other_members": "minimal",

    // Charge I/O
    "disk_read": "Ã©levÃ©e",    // Lecture de tous les fichiers
    "disk_write": "minimale",

    // RÃ©seau
    "network_bandwidth": {
      "backup_node": "Ã©levÃ©e (si upload distant)",
      "replication": "peut augmenter si lag"
    },

    // RÃ©plication
    "oplog_lag": {
      "typical": "quelques secondes Ã  minutes",
      "max_acceptable": "< 10% de la fenÃªtre oplog",
      "alert_threshold": "5 minutes"
    }
  }
}
```

## Configuration d'un Membre de Backup DÃ©diÃ©

### Ajout d'un Membre CachÃ©

```javascript
// 1. Se connecter au Primary
mongo "mongodb://admin:password@mongo-primary:27017/admin?replicaSet=rs0"

// 2. RÃ©cupÃ©rer la configuration actuelle
cfg = rs.conf()

// 3. Ajouter un nouveau membre dÃ©diÃ© aux backups
cfg.members.push({
  _id: 4,                    // ID unique dans le replica set
  host: "mongo-backup:27017",
  priority: 0,               // Ne peut JAMAIS devenir Primary
  votes: 0,                  // Ne participe pas aux Ã©lections
  hidden: true,              // Invisible pour les clients
  slaveDelay: 0,            // Pas de dÃ©lai (sauf besoin spÃ©cifique)
  tags: {                    // Tags pour identification
    backup: "true",
    datacenter: "backup-dc",
    workload: "analytics"
  }
})

// 4. Appliquer la configuration
rs.reconfig(cfg)

// 5. VÃ©rifier la configuration
rs.conf().members.forEach(function(member) {
  if (member.tags && member.tags.backup === "true") {
    printjson({
      id: member._id,
      host: member.host,
      priority: member.priority,
      hidden: member.hidden,
      tags: member.tags
    });
  }
});

// RÃ©sultat attendu:
{
  "id": 4,
  "host": "mongo-backup:27017",
  "priority": 0,
  "hidden": true,
  "tags": {
    "backup": "true",
    "datacenter": "backup-dc",
    "workload": "analytics"
  }
}
```

### Configuration avec Delayed Secondary (Alternative)

Pour protection contre erreurs humaines, un membre delayed peut Ãªtre utile :

```javascript
// Membre delayed de 24h pour recovery d'erreurs
cfg = rs.conf()
cfg.members.push({
  _id: 5,
  host: "mongo-delayed:27017",
  priority: 0,
  votes: 0,
  hidden: true,
  slaveDelay: 86400,  // 24 heures de retard
  tags: {
    delayed: "true",
    recovery: "human-error"
  }
})
rs.reconfig(cfg)

// Usage: Backup sur le delayed member
// Avantage: Protection automatique contre suppressions accidentelles
// InconvÃ©nient: DonnÃ©es de J-1
```

### Validation de la Configuration

```bash
#!/bin/bash
# validate_backup_member.sh

MONGO_URI="mongodb://admin:password@mongo-primary:27017/admin?replicaSet=rs0"

echo "=== Replica Set Backup Configuration Validation ==="

# VÃ©rifier la prÃ©sence d'un membre de backup
mongo "$MONGO_URI" --quiet --eval "
  members = rs.conf().members;
  backupMembers = members.filter(function(m) {
    return m.tags && m.tags.backup === 'true';
  });

  if (backupMembers.length === 0) {
    print('âŒ ERROR: No backup member configured');
    quit(1);
  }

  backupMembers.forEach(function(m) {
    print('âœ“ Backup member found: ' + m.host);
    print('  Priority: ' + m.priority + (m.priority === 0 ? ' âœ“' : ' âœ— WARNING'));
    print('  Hidden: ' + m.hidden + (m.hidden ? ' âœ“' : ' âœ— WARNING'));
    print('  Votes: ' + m.votes);

    // VÃ©rifier l'Ã©tat
    status = rs.status().members.filter(function(s) {
      return s._id === m._id;
    })[0];

    print('  State: ' + status.stateStr);
    print('  Health: ' + status.health);

    if (status.stateStr !== 'SECONDARY') {
      print('  âš ï¸  WARNING: Member is not in SECONDARY state');
    }

    // VÃ©rifier le lag
    if (status.optimeDate && status.lastHeartbeat) {
      lag = (new Date() - status.optimeDate) / 1000;
      print('  Replication lag: ' + lag.toFixed(2) + 's' +
            (lag < 60 ? ' âœ“' : ' âš ï¸'));
    }
  });
"

if [ $? -eq 0 ]; then
  echo ""
  echo "âœ“ Backup member configuration is valid"
else
  echo ""
  echo "âœ— Backup member configuration has issues"
  exit 1
fi
```

## StratÃ©gies de Sauvegarde

### StratÃ©gie 1 : Backup depuis Secondary CachÃ©

**Approche recommandÃ©e pour production** :

```bash
#!/bin/bash
# backup_from_hidden_secondary.sh

set -euo pipefail

# Configuration
RS_NAME="rs0"
BACKUP_MEMBER="mongo-backup:27017"
BACKUP_DIR="/backup/mongodb/replica_set"
RETENTION_DAYS=30
MONGO_USER="backup-user"
MONGO_PASS="SecureBackupPass123"

# Logging
LOG_FILE="/var/log/mongodb_rs_backup.log"
exec 1> >(tee -a "$LOG_FILE")
exec 2>&1

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# Fonction: VÃ©rifier l'Ã©tat du membre
check_member_health() {
  local member=$1

  log "Checking health of backup member: $member"

  local state=$(mongo "mongodb://${MONGO_USER}:${MONGO_PASS}@${member}/admin" --quiet --eval "
    rs.status().members.filter(function(m) {
      return m.name === '${member}';
    })[0].stateStr;
  ")

  if [ "$state" != "SECONDARY" ]; then
    log "ERROR: Backup member is not in SECONDARY state (current: $state)"
    return 1
  fi

  # VÃ©rifier le lag de rÃ©plication
  local lag=$(mongo "mongodb://${MONGO_USER}:${MONGO_PASS}@${member}/admin" --quiet --eval "
    member = rs.status().members.filter(function(m) {
      return m.name === '${member}';
    })[0];
    print((new Date() - member.optimeDate) / 1000);
  ")

  log "Replication lag: ${lag}s"

  # Alert si lag > 5 minutes
  if (( $(echo "$lag > 300" | bc -l) )); then
    log "WARNING: Replication lag is high (${lag}s)"
  fi

  return 0
}

# Fonction: Backup principal
perform_backup() {
  local timestamp=$(date +%Y%m%d_%H%M%S)
  local backup_path="${BACKUP_DIR}/rs_backup_${timestamp}"
  local start_time=$(date +%s)

  log "=== Starting Replica Set Backup ==="
  log "Timestamp: $timestamp"
  log "Target: $BACKUP_MEMBER"

  # VÃ©rifier la santÃ© avant de commencer
  if ! check_member_health "$BACKUP_MEMBER"; then
    log "ERROR: Pre-backup health check failed"
    exit 1
  fi

  # CrÃ©er le rÃ©pertoire de backup
  mkdir -p "$backup_path"

  # Effectuer le backup avec oplog
  log "Executing mongodump with oplog..."

  if mongodump \
    --host="$BACKUP_MEMBER" \
    --username="$MONGO_USER" \
    --password="$MONGO_PASS" \
    --authenticationDatabase=admin \
    --oplog \
    --gzip \
    --numParallelCollections=8 \
    --readPreference='{ mode: "secondary", tagSets: [ { "backup": "true" } ] }' \
    --out="$backup_path"; then

    local end_time=$(date +%s)
    local duration=$((end_time - start_time))

    log "Backup completed in ${duration}s"

    # Sauvegarder les mÃ©tadonnÃ©es du Replica Set
    log "Saving replica set metadata..."
    mongo "mongodb://${MONGO_USER}:${MONGO_PASS}@${BACKUP_MEMBER}/admin" --quiet > \
      "$backup_path/replica_set_metadata.json" <<'EOF'
      printjson({
        replSetConfig: rs.conf(),
        replSetStatus: rs.status(),
        oplogInfo: db.getSiblingDB('local').oplog.rs.find().sort({$natural: -1}).limit(1).toArray()[0],
        timestamp: new Date(),
        serverVersion: db.version(),
        serverBuildInfo: db.serverBuildInfo()
      });
EOF

    # CrÃ©er archive compressÃ©e
    log "Creating archive..."
    tar -czf "${backup_path}.tar.gz" -C "$BACKUP_DIR" "$(basename $backup_path)"

    # Checksum
    sha256sum "${backup_path}.tar.gz" > "${backup_path}.tar.gz.sha256"

    # MÃ©tadonnÃ©es du backup
    local size=$(stat -c%s "${backup_path}.tar.gz")
    cat > "${backup_path}.json" <<EOF
{
  "backup_type": "replica_set",
  "replica_set_name": "$RS_NAME",
  "backup_member": "$BACKUP_MEMBER",
  "timestamp": "$(date -Iseconds)",
  "duration_seconds": $duration,
  "size_bytes": $size,
  "size_human": "$(numfmt --to=iec-i --suffix=B $size)",
  "mongodb_version": "$(mongo --quiet --eval 'db.version()')",
  "oplog_included": true
}
EOF

    # Nettoyage du rÃ©pertoire temporaire
    rm -rf "$backup_path"

    # Cleanup anciens backups
    log "Cleaning up old backups..."
    find "$BACKUP_DIR" -name "rs_backup_*.tar.gz" -mtime +$RETENTION_DAYS -delete
    find "$BACKUP_DIR" -name "rs_backup_*.json" -mtime +$RETENTION_DAYS -delete

    log "âœ“ Replica Set backup completed successfully"
    log "  Archive: ${backup_path}.tar.gz"
    log "  Size: $(numfmt --to=iec-i --suffix=B $size)"

    return 0
  else
    log "âœ— Backup failed"
    return 1
  fi
}

# Fonction: VÃ©rifier post-backup
post_backup_check() {
  log "Performing post-backup checks..."

  # VÃ©rifier que le membre est toujours en bonne santÃ©
  check_member_health "$BACKUP_MEMBER"

  # VÃ©rifier que la rÃ©plication continue
  local lag_after=$(mongo "mongodb://${MONGO_USER}:${MONGO_PASS}@${BACKUP_MEMBER}/admin" --quiet --eval "
    member = rs.status().members.filter(function(m) {
      return m.name === '${BACKUP_MEMBER}';
    })[0];
    print((new Date() - member.optimeDate) / 1000);
  ")

  log "Post-backup replication lag: ${lag_after}s"

  if (( $(echo "$lag_after > 600" | bc -l) )); then
    log "âš ï¸  WARNING: Replication lag increased significantly after backup"
  fi
}

# ExÃ©cution principale
main() {
  if perform_backup; then
    post_backup_check
    exit 0
  else
    log "Backup failed"
    exit 1
  fi
}

main "$@"
```

### StratÃ©gie 2 : Rolling Backup (Backup Rotatif)

Pour minimiser l'impact, effectuer des backups sur diffÃ©rents membres en rotation :

```bash
#!/bin/bash
# rolling_backup_strategy.sh

RS_MEMBERS=(
  "mongo-secondary-1:27017"
  "mongo-secondary-2:27017"
  "mongo-secondary-3:27017"
)

CURRENT_MEMBER_INDEX=0
STATE_FILE="/var/lib/mongodb_backup/rolling_backup_state"

# Lire l'index du dernier membre utilisÃ©
if [ -f "$STATE_FILE" ]; then
  CURRENT_MEMBER_INDEX=$(cat "$STATE_FILE")
fi

# SÃ©lectionner le prochain membre (rotation)
NEXT_MEMBER_INDEX=$(( (CURRENT_MEMBER_INDEX + 1) % ${#RS_MEMBERS[@]} ))
BACKUP_MEMBER="${RS_MEMBERS[$NEXT_MEMBER_INDEX]}"

echo "Selected member for backup: $BACKUP_MEMBER (index: $NEXT_MEMBER_INDEX)"

# Sauvegarder l'index pour la prochaine fois
echo "$NEXT_MEMBER_INDEX" > "$STATE_FILE"

# Effectuer le backup sur ce membre
mongodump \
  --host="$BACKUP_MEMBER" \
  --oplog \
  --gzip \
  --out="/backup/rolling_backup_$(date +%Y%m%d)"

echo "âœ“ Rolling backup completed on $BACKUP_MEMBER"
```

### StratÃ©gie 3 : Backup avec Freeze du Membre

Pour garantir zÃ©ro impact sur la production, temporairement "geler" le membre :

```bash
#!/bin/bash
# backup_with_freeze.sh

BACKUP_MEMBER="mongo-backup:27017"
MONGO_URI="mongodb://admin:pass@${BACKUP_MEMBER}/admin"
FREEZE_DURATION=3600  # 1 heure

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

# 1. Geler le membre (empÃªche Ã©lection pendant la pÃ©riode)
log "Freezing member for $FREEZE_DURATION seconds..."
mongo "$MONGO_URI" --eval "rs.freeze($FREEZE_DURATION)"

# 2. Optionnel: Passer en mode maintenance (pas de lectures)
log "Setting maintenance mode..."
mongo "$MONGO_URI" --eval "db.adminCommand({ replSetMaintenance: 1 })"

# 3. Effectuer le backup
log "Performing backup..."
mongodump \
  --host="$BACKUP_MEMBER" \
  --oplog \
  --gzip \
  --numParallelCollections=16 \
  --out="/backup/frozen_backup_$(date +%Y%m%d)"

BACKUP_EXIT_CODE=$?

# 4. Sortir du mode maintenance
log "Exiting maintenance mode..."
mongo "$MONGO_URI" --eval "db.adminCommand({ replSetMaintenance: 0 })"

# 5. DÃ©geler (automatique aprÃ¨s timeout, mais bonne pratique)
mongo "$MONGO_URI" --eval "rs.freeze(0)"

if [ $BACKUP_EXIT_CODE -eq 0 ]; then
  log "âœ“ Backup completed successfully"
else
  log "âœ— Backup failed"
  exit 1
fi
```

## Sauvegarde de l'Oplog

### Importance de l'Oplog dans les Replica Sets

```javascript
// L'oplog est critique pour:
// 1. CohÃ©rence point-in-time
// 2. Restauration avec replay
// 3. Synchronisation des membres

// VÃ©rifier la fenÃªtre oplog
use local
db.oplog.rs.find().sort({$natural: 1}).limit(1).pretty()  // Premier
db.oplog.rs.find().sort({$natural: -1}).limit(1).pretty() // Dernier

// Calculer la durÃ©e de l'oplog
firstOplog = db.oplog.rs.find().sort({$natural: 1}).limit(1).toArray()[0]
lastOplog = db.oplog.rs.find().sort({$natural: -1}).limit(1).toArray()[0]
oplogWindowHours = (lastOplog.ts.getTime() - firstOplog.ts.getTime()) / 3600

print("Oplog window: " + oplogWindowHours.toFixed(2) + " hours")
// Production: Minimum 48-72h recommandÃ©
```

### Export Continu de l'Oplog

Pour recovery point objectif minimal :

```javascript
// continuous_oplog_backup.js
const { MongoClient } = require('mongodb');
const fs = require('fs');
const zlib = require('zlib');

const config = {
  uri: 'mongodb://backup-node:27017/?replicaSet=rs0',
  database: 'local',
  collection: 'oplog.rs',
  outputDir: '/backup/oplog',
  rotationInterval: 3600000, // 1 heure
  compressionLevel: 6
};

class OplogBackup {
  constructor(config) {
    this.config = config;
    this.client = null;
    this.currentStream = null;
    this.lastTimestamp = null;
    this.stateFile = '/var/lib/mongodb_backup/oplog_state.json';
  }

  async connect() {
    this.client = await MongoClient.connect(this.config.uri, {
      readPreference: 'secondary',
      readPreferenceTags: [{ backup: 'true' }]
    });

    console.log('Connected to MongoDB for oplog streaming');
  }

  loadState() {
    try {
      const state = JSON.parse(fs.readFileSync(this.stateFile, 'utf8'));
      this.lastTimestamp = state.lastTimestamp;
      console.log(`Resuming from timestamp: ${this.lastTimestamp}`);
    } catch (err) {
      console.log('No previous state found, starting from current oplog');
      this.lastTimestamp = null;
    }
  }

  saveState(timestamp) {
    fs.writeFileSync(this.stateFile, JSON.stringify({
      lastTimestamp: timestamp,
      lastSaved: new Date().toISOString()
    }));
  }

  createOutputStream() {
    const filename = `oplog_${new Date().toISOString().replace(/[:.]/g, '-')}.jsonl.gz`;
    const filepath = `${this.config.outputDir}/${filename}`;

    const fileStream = fs.createWriteStream(filepath);
    const gzipStream = zlib.createGzip({ level: this.config.compressionLevel });

    gzipStream.pipe(fileStream);

    console.log(`Created new oplog file: ${filepath}`);
    return { gzipStream, filepath };
  }

  async streamOplog() {
    const db = this.client.db(this.config.database);
    const oplog = db.collection(this.config.collection);

    // Construire le filtre
    const filter = this.lastTimestamp
      ? { ts: { $gt: this.lastTimestamp } }
      : {};

    // CrÃ©er le change stream
    const changeStream = oplog.watch([
      { $match: filter }
    ], {
      fullDocument: 'updateLookup',
      startAtOperationTime: this.lastTimestamp
    });

    let { gzipStream, filepath } = this.createOutputStream();
    let rotationTimer = setTimeout(() => this.rotateStream(), this.config.rotationInterval);
    let operationCount = 0;

    console.log('Oplog streaming started...');

    changeStream.on('change', (change) => {
      // Ã‰crire l'opÃ©ration
      gzipStream.write(JSON.stringify(change) + '\n');

      // Mettre Ã  jour le timestamp
      this.lastTimestamp = change.ts;
      operationCount++;

      // Sauvegarder l'Ã©tat tous les 1000 ops
      if (operationCount % 1000 === 0) {
        this.saveState(this.lastTimestamp);
        console.log(`Processed ${operationCount} operations`);
      }
    });

    changeStream.on('error', (error) => {
      console.error('Error in change stream:', error);
      this.reconnect();
    });
  }

  async start() {
    await this.connect();
    this.loadState();
    await this.streamOplog();
  }
}

// DÃ©marrage
const backup = new OplogBackup(config);
backup.start().catch(console.error);

// Gestion des signaux
process.on('SIGINT', () => {
  console.log('Shutting down gracefully...');
  backup.saveState(backup.lastTimestamp);
  process.exit(0);
});
```

### Backup de l'Oplog avec mongodump

```bash
#!/bin/bash
# oplog_backup_dedicated.sh

BACKUP_DIR="/backup/oplog"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
STATE_FILE="/var/lib/mongodb_backup/last_oplog_timestamp"

# Lire le dernier timestamp sauvegardÃ©
if [ -f "$STATE_FILE" ]; then
  LAST_TS=$(cat "$STATE_FILE")
  echo "Last oplog timestamp: $LAST_TS"
else
  echo "No previous oplog backup found"
  LAST_TS="Timestamp(0, 0)"
fi

# Exporter l'oplog depuis le dernier timestamp
mongodump \
  --host="mongo-backup:27017" \
  --db=local \
  --collection=oplog.rs \
  --query="{\"ts\": {\"\$gt\": $LAST_TS}}" \
  --gzip \
  --out="$BACKUP_DIR/oplog_incremental_$TIMESTAMP"

# Sauvegarder le nouveau timestamp
CURRENT_TS=$(mongo "mongodb://mongo-backup:27017/local" --quiet --eval "
  db.oplog.rs.find().sort({ts: -1}).limit(1).toArray()[0].ts;
")

echo "$CURRENT_TS" > "$STATE_FILE"

echo "âœ“ Oplog backup completed: oplog_incremental_$TIMESTAMP"
echo "  Last timestamp: $CURRENT_TS"
```

## Monitoring et Validation

### Monitoring du Replication Lag

```bash
#!/bin/bash
# monitor_replication_lag.sh

MONGO_URI="mongodb://admin:pass@mongo-primary:27017/admin?replicaSet=rs0"
ALERT_THRESHOLD=300  # 5 minutes

while true; do
  echo "=== Replication Status - $(date) ==="

  mongo "$MONGO_URI" --quiet --eval "
    status = rs.status();
    primary = status.members.filter(m => m.stateStr === 'PRIMARY')[0];

    status.members.forEach(function(member) {
      if (member.stateStr === 'SECONDARY') {
        lag = (primary.optimeDate - member.optimeDate) / 1000;

        status_icon = 'âœ“';
        if (lag > $ALERT_THRESHOLD) {
          status_icon = 'âš ï¸';
        }
        if (member.health !== 1) {
          status_icon = 'âœ—';
        }

        print(status_icon + ' ' + member.name +
              ' | State: ' + member.stateStr +
              ' | Health: ' + member.health +
              ' | Lag: ' + lag.toFixed(2) + 's' +
              (member.tags ? ' | Tags: ' + JSON.stringify(member.tags) : ''));
      }
    });
  "

  sleep 10
done
```

### Dashboard de Monitoring avec Prometheus

```yaml
# prometheus_rules.yml
groups:
  - name: mongodb_replica_set_backup
    interval: 30s
    rules:
      # Alerte sur lag de rÃ©plication Ã©levÃ©
      - alert: MongoDBReplicationLagHigh
        expr: |
          mongodb_mongod_replset_member_replication_lag_seconds{
            state="SECONDARY",
            tags_backup="true"
          } > 300
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "MongoDB replication lag high on backup member"
          description: "Backup member {{ $labels.member }} has replication lag of {{ $value }}s"

      # Alerte si membre de backup n'est pas SECONDARY
      - alert: MongoDBBackupMemberNotHealthy
        expr: |
          mongodb_mongod_replset_member_state{
            tags_backup="true"
          } != 2
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "MongoDB backup member is not in SECONDARY state"
          description: "Backup member {{ $labels.member }} is in state {{ $value }}"

      # Alerte sur Ã©chec de backup
      - alert: MongoDBBackupFailed
        expr: |
          time() - mongodb_backup_last_success_timestamp > 86400
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MongoDB backup has not succeeded in 24h"
          description: "Last successful backup was {{ $value | humanizeDuration }} ago"
```

### Script de Validation ComplÃ¨te

```bash
#!/bin/bash
# validate_replica_set_backup.sh

BACKUP_FILE="/backup/mongodb/rs_backup_20241208.tar.gz"
TEST_DIR="/tmp/backup_validation_$$"
TEST_PORT=27099

echo "=== Replica Set Backup Validation ==="
echo "Backup file: $BACKUP_FILE"

# 1. VÃ©rifier l'intÃ©gritÃ© du fichier
echo -e "\n1. Checking file integrity..."
if [ -f "${BACKUP_FILE}.sha256" ]; then
  if sha256sum -c "${BACKUP_FILE}.sha256"; then
    echo "âœ“ Checksum valid"
  else
    echo "âœ— Checksum INVALID"
    exit 1
  fi
else
  echo "âš ï¸  No checksum file found"
fi

# 2. Extraire le backup
echo -e "\n2. Extracting backup..."
mkdir -p "$TEST_DIR"
tar -xzf "$BACKUP_FILE" -C "$TEST_DIR"

if [ $? -ne 0 ]; then
  echo "âœ— Failed to extract backup"
  exit 1
fi

# 3. VÃ©rifier la prÃ©sence de l'oplog
echo -e "\n3. Checking oplog..."
if [ -f "$TEST_DIR"/*/oplog.bson ]; then
  OPLOG_SIZE=$(stat -c%s "$TEST_DIR"/*/oplog.bson)
  echo "âœ“ Oplog found ($(numfmt --to=iec-i --suffix=B $OPLOG_SIZE))"
else
  echo "âœ— Oplog NOT found - backup may not be point-in-time consistent"
fi

# 4. VÃ©rifier les mÃ©tadonnÃ©es du replica set
echo -e "\n4. Checking replica set metadata..."
if [ -f "$TEST_DIR"/*/replica_set_metadata.json ]; then
  echo "âœ“ Replica set metadata found"

  RS_NAME=$(jq -r '.replSetConfig._id' "$TEST_DIR"/*/replica_set_metadata.json)
  MEMBER_COUNT=$(jq '.replSetStatus.members | length' "$TEST_DIR"/*/replica_set_metadata.json)

  echo "  Replica Set: $RS_NAME"
  echo "  Members: $MEMBER_COUNT"
else
  echo "âš ï¸  Replica set metadata not found"
fi

# 5. Test de restauration (optionnel, nÃ©cessite instance MongoDB test)
if command -v mongod &> /dev/null; then
  echo -e "\n5. Testing restoration (sample)..."

  # DÃ©marrer instance MongoDB temporaire
  TEMP_DBPATH="$TEST_DIR/test_mongod"
  mkdir -p "$TEMP_DBPATH"

  mongod --port $TEST_PORT --dbpath "$TEMP_DBPATH" \
    --logpath "$TEST_DIR/mongod.log" --fork

  sleep 5

  # Restaurer le backup
  mongorestore --port $TEST_PORT --drop --gzip \
    --dir="$TEST_DIR"/*/ \
    --nsInclude="testdb.*" 2>&1 | grep -i "done"

  if [ $? -eq 0 ]; then
    echo "âœ“ Test restoration successful"

    # VÃ©rifier quelques donnÃ©es
    mongo --port $TEST_PORT --quiet --eval "
      dbs = db.adminCommand({ listDatabases: 1 }).databases;
      print('Databases restored: ' + dbs.length);
    "
  else
    echo "âœ— Test restoration failed"
  fi

  # ArrÃªter MongoDB test
  mongod --port $TEST_PORT --dbpath "$TEMP_DBPATH" --shutdown
else
  echo -e "\n5. Skipping restoration test (mongod not available)"
fi

# Cleanup
rm -rf "$TEST_DIR"

echo -e "\nâœ“ Validation completed"
```

## Restauration de Replica Set

### Restauration ComplÃ¨te du Replica Set

```bash
#!/bin/bash
# restore_replica_set_complete.sh

set -euo pipefail

BACKUP_FILE="/backup/mongodb/rs_backup_20241208.tar.gz"
RS_NAME="rs0"
RS_MEMBERS=(
  "mongo-1:27017"
  "mongo-2:27017"
  "mongo-3:27017"
)

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

log "=== Replica Set Complete Restoration ==="
log "Backup: $BACKUP_FILE"
log "Replica Set: $RS_NAME"
log ""

# 1. Extraire le backup
TEMP_DIR="/tmp/rs_restore_$$"
mkdir -p "$TEMP_DIR"

log "Extracting backup..."
tar -xzf "$BACKUP_FILE" -C "$TEMP_DIR"
BACKUP_DIR=$(find "$TEMP_DIR" -type d -name "rs_backup_*" | head -1)

# 2. ArrÃªter tous les membres du replica set
log "Stopping all replica set members..."
for member in "${RS_MEMBERS[@]}"; do
  host=$(echo $member | cut -d: -f1)
  port=$(echo $member | cut -d: -f2)

  log "  Stopping $member..."
  mongo "mongodb://$member/admin" --eval "db.shutdownServer()" || true
done

sleep 5

# 3. Restaurer sur chaque membre
log "Restoring data to each member..."

for i in "${!RS_MEMBERS[@]}"; do
  member="${RS_MEMBERS[$i]}"
  host=$(echo $member | cut -d: -f1)
  port=$(echo $member | cut -d: -f2)

  log "  Restoring to member $((i+1)): $member"

  # Nettoyer le datadir (ATTENTION: destructif!)
  ssh "$host" "sudo rm -rf /var/lib/mongodb/*"

  # Restaurer
  mongorestore \
    --host="$member" \
    --drop \
    --oplogReplay \
    --gzip \
    --dir="$BACKUP_DIR" &
done

# Attendre toutes les restaurations
wait

log "All members restored"

# 4. DÃ©marrer le premier membre en mode standalone
log "Starting first member as standalone..."
PRIMARY="${RS_MEMBERS[0]}"
ssh "$(echo $PRIMARY | cut -d: -f1)" "sudo systemctl start mongod"

sleep 10

# 5. RÃ©initialiser le replica set
log "Re-initializing replica set..."
mongo "mongodb://$PRIMARY/admin" <<EOF
  // Initier le replica set
  rs.initiate({
    _id: "$RS_NAME",
    members: [
      { _id: 0, host: "${RS_MEMBERS[0]}" }
    ]
  });

  // Attendre que le member devienne PRIMARY
  sleep(5000);

  // Ajouter les autres membres
  $(for i in $(seq 1 $((${#RS_MEMBERS[@]} - 1))); do
    echo "rs.add({ host: '${RS_MEMBERS[$i]}', priority: 1 });"
    echo "sleep(2000);"
  done)

  // Afficher le statut
  rs.status();
EOF

# 6. Attendre la synchronisation
log "Waiting for replica set to stabilize..."
for attempt in {1..30}; do
  HEALTHY=$(mongo "mongodb://$PRIMARY/admin" --quiet --eval "
    status = rs.status();
    healthy = status.members.filter(m => m.health === 1).length;
    print(healthy);
  ")

  if [ "$HEALTHY" == "${#RS_MEMBERS[@]}" ]; then
    log "âœ“ All members are healthy"
    break
  fi

  log "  Waiting... ($HEALTHY/${#RS_MEMBERS[@]} healthy)"
  sleep 10
done

# 7. VÃ©rifier l'intÃ©gritÃ©
log "Verifying data integrity..."
mongo "mongodb://$PRIMARY/admin" --quiet --eval "
  db.adminCommand({ listDatabases: 1 }).databases.forEach(function(db) {
    if (db.name !== 'admin' && db.name !== 'local' && db.name !== 'config') {
      print('âœ“ Database: ' + db.name + ' - Size: ' +
            (db.sizeOnDisk / 1024 / 1024 / 1024).toFixed(2) + ' GB');
    }
  });
"

# Cleanup
rm -rf "$TEMP_DIR"

log ""
log "âœ“ Replica Set restoration completed successfully"
log "  Verify application connectivity before resuming production traffic"
```

### Restauration d'un Seul Membre

```bash
#!/bin/bash
# restore_single_member.sh

BACKUP_FILE="/backup/mongodb/rs_backup_20241208.tar.gz"
TARGET_MEMBER="mongo-2:27017"
RS_NAME="rs0"

log() {
  echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

log "=== Single Member Restoration ==="
log "Target: $TARGET_MEMBER"

# 1. Retirer le membre du replica set
log "Removing member from replica set..."
PRIMARY=$(mongo "mongodb://$TARGET_MEMBER/admin?replicaSet=$RS_NAME" --quiet --eval "
  rs.isMaster().primary;
")

mongo "mongodb://$PRIMARY/admin" --eval "
  cfg = rs.conf();
  cfg.members = cfg.members.filter(m => m.host !== '$TARGET_MEMBER');
  rs.reconfig(cfg, {force: true});
"

sleep 5

# 2. ArrÃªter le membre
log "Stopping target member..."
mongo "mongodb://$TARGET_MEMBER/admin" --eval "db.shutdownServer()" || true

sleep 5

# 3. Nettoyer et restaurer
host=$(echo $TARGET_MEMBER | cut -d: -f1)

log "Cleaning data directory..."
ssh "$host" "sudo rm -rf /var/lib/mongodb/*"

log "Extracting and restoring backup..."
TEMP_DIR="/tmp/restore_$$"
mkdir -p "$TEMP_DIR"
tar -xzf "$BACKUP_FILE" -C "$TEMP_DIR"

mongorestore \
  --host="$TARGET_MEMBER" \
  --drop \
  --oplogReplay \
  --gzip \
  --dir="$TEMP_DIR"/*

# 4. RÃ©ajouter au replica set
log "Re-adding member to replica set..."
mongo "mongodb://$PRIMARY/admin" --eval "
  rs.add('$TARGET_MEMBER');
"

log "âœ“ Member restored and re-added to replica set"
log "  Waiting for initial sync to complete..."

# Attendre la synchronisation
for i in {1..60}; do
  STATE=$(mongo "mongodb://$PRIMARY/admin" --quiet --eval "
    rs.status().members.filter(m => m.name === '$TARGET_MEMBER')[0].stateStr;
  ")

  if [ "$STATE" == "SECONDARY" ]; then
    log "âœ“ Member is now SECONDARY and in sync"
    break
  fi

  log "  State: $STATE (waiting...)"
  sleep 10
done

rm -rf "$TEMP_DIR"
```

## Automatisation et Orchestration

### Cron pour Backups Automatiques

```bash
# /etc/cron.d/mongodb-replica-set-backup

# Backup quotidien Ã  2h du matin
0 2 * * * mongodb /usr/local/bin/backup_replica_set.sh >> /var/log/mongodb_backup_cron.log 2>&1

# Backup hebdomadaire complet le dimanche Ã  3h
0 3 * * 0 mongodb /usr/local/bin/backup_replica_set_full.sh >> /var/log/mongodb_backup_cron.log 2>&1

# Export oplog toutes les 4 heures
0 */4 * * * mongodb /usr/local/bin/backup_oplog_incremental.sh >> /var/log/mongodb_oplog_cron.log 2>&1

# Validation quotidienne Ã  4h
0 4 * * * mongodb /usr/local/bin/validate_last_backup.sh >> /var/log/mongodb_validation_cron.log 2>&1

# Cleanup hebdomadaire le lundi Ã  5h
0 5 * * 1 mongodb /usr/local/bin/cleanup_old_backups.sh >> /var/log/mongodb_cleanup_cron.log 2>&1
```

### Kubernetes CronJob pour Replica Set

```yaml
# mongodb-rs-backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mongodb-replica-set-backup
  namespace: database
spec:
  schedule: "0 2 * * *"  # 2h du matin tous les jours
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: mongodb-backup
        spec:
          restartPolicy: OnFailure
          serviceAccountName: mongodb-backup

          # AffinitÃ© pour s'exÃ©cuter prÃ¨s du membre de backup
          affinity:
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - mongodb
                    - key: role
                      operator: In
                      values:
                      - backup
                  topologyKey: kubernetes.io/hostname

          containers:
          - name: backup
            image: mongo:7.0
            command:
            - /bin/bash
            - -c
            - |
              set -euo pipefail

              echo "=== MongoDB Replica Set Backup ==="
              echo "Timestamp: $(date -Iseconds)"

              # Identifier le membre de backup
              BACKUP_MEMBER=$(mongo "mongodb://mongo-0.mongo:27017,mongo-1.mongo:27017,mongo-2.mongo:27017/?replicaSet=rs0" \
                --username=$MONGO_USER \
                --password=$MONGO_PASSWORD \
                --authenticationDatabase=admin \
                --quiet --eval "
                  rs.status().members.filter(function(m) {
                    return m.tags && m.tags.backup === 'true';
                  })[0].name;
                ")

              echo "Backup member: $BACKUP_MEMBER"

              # Effectuer le backup
              BACKUP_NAME="rs_backup_$(date +%Y%m%d_%H%M%S).gz"

              mongodump \
                --host="$BACKUP_MEMBER" \
                --username=$MONGO_USER \
                --password=$MONGO_PASSWORD \
                --authenticationDatabase=admin \
                --oplog \
                --gzip \
                --numParallelCollections=8 \
                --readPreference='{ mode: "secondary", tagSets: [ { "backup": "true" } ] }' \
                --archive=/backup/$BACKUP_NAME

              # Upload vers S3
              echo "Uploading to S3..."
              aws s3 cp /backup/$BACKUP_NAME $S3_BUCKET/ \
                --storage-class STANDARD_IA \
                --metadata "backup-type=replica-set,timestamp=$(date +%s)"

              # MÃ©tadonnÃ©es
              cat > /backup/${BACKUP_NAME%.gz}.json <<EOF
              {
                "backup_type": "replica_set",
                "timestamp": "$(date -Iseconds)",
                "size_bytes": $(stat -c%s /backup/$BACKUP_NAME),
                "backup_member": "$BACKUP_MEMBER"
              }
              EOF

              aws s3 cp /backup/${BACKUP_NAME%.gz}.json $S3_BUCKET/

              echo "âœ“ Backup completed: $BACKUP_NAME"

            env:
            - name: MONGO_USER
              valueFrom:
                secretKeyRef:
                  name: mongodb-backup-credentials
                  key: username
            - name: MONGO_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-backup-credentials
                  key: password
            - name: S3_BUCKET
              value: "s3://company-backups/mongodb-replica-sets"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access_key_id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret_access_key

            volumeMounts:
            - name: backup-storage
              mountPath: /backup

            resources:
              requests:
                memory: "4Gi"
                cpu: "2"
              limits:
                memory: "8Gi"
                cpu: "4"

          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: mongodb-backup-pvc
```

## Cas Particuliers et Troubleshooting

### ScÃ©nario 1 : Backup Pendant une Ã‰lection

```bash
# GÃ©rer les Ã©lections pendant le backup
perform_backup_with_election_handling() {
  local max_attempts=3
  local attempt=0

  while [ $attempt -lt $max_attempts ]; do
    attempt=$((attempt + 1))

    echo "Backup attempt $attempt/$max_attempts"

    # VÃ©rifier qu'il y a un primary
    PRIMARY=$(mongo "$MONGO_URI" --quiet --eval "rs.isMaster().primary")

    if [ -z "$PRIMARY" ]; then
      echo "No PRIMARY found, waiting for election..."
      sleep 30
      continue
    fi

    # Effectuer le backup
    if mongodump --host="$BACKUP_MEMBER" --oplog --gzip --out=/backup/attempt_$attempt; then
      echo "âœ“ Backup successful"
      return 0
    else
      echo "âœ— Backup failed, retrying..."
      sleep 60
    fi
  done

  echo "âœ— Backup failed after $max_attempts attempts"
  return 1
}
```

### ScÃ©nario 2 : Recovery aprÃ¨s Split-Brain

```javascript
// Identifier et rÃ©soudre un split-brain
// Se connecter Ã  chaque membre indÃ©pendamment

// Membre 1
mongo mongodb://mongo-1:27017/admin
rs.status()
// Si state: PRIMARY mais isolated

// Forcer une nouvelle configuration depuis le vrai PRIMARY
cfg = rs.conf()
cfg.version++
rs.reconfig(cfg, {force: true})

// Restaurer depuis le backup le plus rÃ©cent si nÃ©cessaire
```

### ScÃ©nario 3 : Backup avec Oplog Ã‰puisÃ©

```bash
#!/bin/bash
# handle_oplog_exhaustion.sh

# VÃ©rifier la taille de la fenÃªtre oplog avant backup
check_oplog_window() {
  WINDOW_HOURS=$(mongo "mongodb://backup-node:27017/local" --quiet --eval "
    first = db.oplog.rs.find().sort({ts: 1}).limit(1).toArray()[0].ts.getTime();
    last = db.oplog.rs.find().sort({ts: -1}).limit(1).toArray()[0].ts.getTime();
    print((last - first) / 3600);
  ")

  echo "Oplog window: ${WINDOW_HOURS}h"

  if (( $(echo "$WINDOW_HOURS < 24" | bc -l) )); then
    echo "âš ï¸  WARNING: Oplog window < 24h, backup may not capture full history"
    echo "Consider increasing oplogSizeMB or performing backup more frequently"
  fi
}

check_oplog_window

# Augmenter la taille de l'oplog si nÃ©cessaire
# Note: NÃ©cessite MongoDB 4.0+
# mongo admin --eval "db.adminCommand({replSetResizeOplog: 1, size: 102400})"  # 100GB
```

## Checklist de Production

```markdown
### Avant le Backup

- [ ] VÃ©rifier l'Ã©tat du Replica Set (rs.status())
- [ ] Confirmer qu'un membre SECONDARY est disponible
- [ ] VÃ©rifier le replication lag (< 5 min)
- [ ] Confirmer la fenÃªtre oplog (> 48h recommandÃ©)
- [ ] VÃ©rifier l'espace disque disponible
- [ ] Confirmer que le balancer est actif (si sharded)
- [ ] VÃ©rifier les alertes de monitoring

### Pendant le Backup

- [ ] Monitorer le replication lag en temps rÃ©el
- [ ] Surveiller l'utilisation CPU/RAM/IO du membre
- [ ] VÃ©rifier que les autres membres restent sains
- [ ] Confirmer progression du backup (taille croissante)
- [ ] VÃ©rifier qu'aucune Ã©lection n'est en cours

### AprÃ¨s le Backup

- [ ] VÃ©rifier l'intÃ©gritÃ© du fichier (checksum)
- [ ] Valider la prÃ©sence de l'oplog dans le backup
- [ ] Confirmer la taille (cohÃ©rence avec historique)
- [ ] Upload vers stockage distant rÃ©ussi
- [ ] MÃ©tadonnÃ©es enregistrÃ©es
- [ ] Replication lag revenu Ã  la normale
- [ ] Tester restauration (pÃ©riodiquement)
- [ ] Documenter tout incident
```

## Conclusion

La sauvegarde de Replica Sets requiert une approche mÃ©thodique qui balance protection des donnÃ©es et continuitÃ© de service. Les points clÃ©s :

1. **Toujours backup sur un SECONDARY** - Jamais sur le PRIMARY
2. **Utiliser `--oplog`** - Pour cohÃ©rence point-in-time
3. **Membre dÃ©diÃ© aux backups** - Configuration hidden/priority:0
4. **Monitoring continu** - Replication lag et santÃ© du cluster
5. **Automatisation robuste** - Scripts avec gestion d'erreurs
6. **Tests rÃ©guliers** - Valider la recouvrabilitÃ©

Un Replica Set bien configurÃ© pour les backups combine haute disponibilitÃ© et recouvrabilitÃ© fiable, deux piliers essentiels de toute infrastructure de production MongoDB.

---


â­ï¸ [Sauvegarde de clusters shardÃ©s](/12-sauvegarde-restauration/04-sauvegarde-clusters-shardes.md)
