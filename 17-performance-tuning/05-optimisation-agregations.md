üîù Retour au [Sommaire](/SOMMAIRE.md)

# 17.5 Optimisation des Agr√©gations

## Introduction

Le framework d'agr√©gation MongoDB est un outil extr√™mement puissant pour le traitement et l'analyse de donn√©es, mais sa flexibilit√© s'accompagne de d√©fis de performance significatifs. Les pipelines d'agr√©gation peuvent varier de quelques millisecondes √† plusieurs heures d'ex√©cution selon leur conception. L'optimisation des agr√©gations en production n√©cessite une compr√©hension approfondie des optimisations automatiques, des patterns efficaces, et des limites du syst√®me.

Cette section explore les m√©thodologies expertes pour concevoir, analyser et optimiser les pipelines d'agr√©gation dans des environnements de production √† grande √©chelle.

## Anatomie d'un Pipeline d'Agr√©gation

### Architecture d'Ex√©cution

Un pipeline d'agr√©gation traverse plusieurs phases d'ex√©cution :

```javascript
// Pipeline example
db.orders.aggregate([
  { $match: { status: "completed" } },           // Stage 1
  { $lookup: { /* join customers */ } },         // Stage 2
  { $unwind: "$items" },                         // Stage 3
  { $group: { _id: "$customerId", total: { $sum: "$items.price" } } }, // Stage 4
  { $sort: { total: -1 } },                      // Stage 5
  { $limit: 100 }                                // Stage 6
])

// Phases d'ex√©cution :
// 1. Query Planning : Analyse et optimisation du pipeline
// 2. Index Selection : Choix des index pour stages initiaux
// 3. Pipeline Optimization : R√©organisation automatique des stages
// 4. Execution : Traitement stage par stage
// 5. Result Return : Retour des r√©sultats
```

**Flow de donn√©es** :
```
Collection ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí ... ‚Üí Stage N ‚Üí Results

Chaque stage :
- Input : Documents du stage pr√©c√©dent
- Processing : Transformation/filtrage/agr√©gation
- Output : Documents pour le stage suivant
```

### Optimisations Automatiques du Query Optimizer

MongoDB applique automatiquement plusieurs optimisations au pipeline. Comprendre ces optimisations permet de concevoir des pipelines plus efficaces.

#### Optimisation 1 : $match Pushdown

**Principe** : D√©placer $match le plus t√¥t possible dans le pipeline.

```javascript
// Pipeline initial (sous-optimal)
db.orders.aggregate([
  { $lookup: {
      from: "customers",
      localField: "customerId",
      foreignField: "_id",
      as: "customer"
  }},
  { $unwind: "$customer" },
  { $match: {
      status: "completed",
      "customer.country": "France"
  }}
])

// Optimis√© automatiquement par MongoDB
db.orders.aggregate([
  // $match sur orders d√©plac√© avant $lookup
  { $match: { status: "completed" } },
  { $lookup: {
      from: "customers",
      localField: "customerId",
      foreignField: "_id",
      as: "customer",
      // $match sur customers int√©gr√© dans le lookup pipeline
      pipeline: [
        { $match: { country: "France" } }
      ]
  }},
  { $unwind: "$customer" }
])

// Gain :
// - Moins de documents passent par $lookup
// - $lookup traite moins de documents customers
// - R√©duction drastique de la charge
```

**Impact mesurable** :
```
Avant optimisation :
- Orders matched : 1,000,000
- Lookups performed : 1,000,000
- Filters applied after : 100,000
- Time : 45 seconds

Apr√®s optimisation :
- Orders matched : 100,000 (90% filtr√© avant)
- Lookups performed : 100,000
- Filters applied after : 100,000
- Time : 4.5 seconds (10√ó improvement)
```

#### Optimisation 2 : $sort + $limit Fusion

```javascript
// Pipeline initial
db.products.aggregate([
  { $sort: { sales: -1 } },
  { $limit: 100 }
])

// Optimis√© automatiquement en "top-K sort"
// Au lieu de trier tous les documents puis prendre 100,
// maintient seulement les 100 meilleurs en m√©moire pendant le tri
// = Beaucoup plus efficace

// Algorithme :
// 1. Maintenir heap de 100 documents
// 2. Pour chaque nouveau document :
//    - Si meilleur que le pire du heap : remplacer
//    - Sinon : ignorer
// 3. Retourner les 100 du heap

// Complexit√© :
// Sans fusion : O(N log N) + m√©moire pour N documents
// Avec fusion : O(N log K) + m√©moire pour K documents (K=100)
```

**Performance comparison** :
```javascript
// Collection : 10,000,000 documents
// Query : Top 100

// Sans fusion (sort all then limit) :
// - Memory : ~2GB (tout en m√©moire)
// - Time : 25 seconds
// - Risk : Memory overflow si > 32MB sans allowDiskUse

// Avec fusion (top-K algorithm) :
// - Memory : ~500KB (seulement 100 docs)
// - Time : 3 seconds
// - Risk : None
```

#### Optimisation 3 : Pipeline Sequence Optimization

```javascript
// Pipeline initial
db.orders.aggregate([
  { $project: { customerId: 1, total: 1, items: 1 } },
  { $match: { total: { $gte: 100 } } }
])

// Optimis√© automatiquement
db.orders.aggregate([
  { $match: { total: { $gte: 100 } } },  // D√©plac√© avant $project
  { $project: { customerId: 1, total: 1, items: 1 } }
])

// Gain : Filter avant projection r√©duit le nombre de documents √† projeter
```

#### Optimisation 4 : $skip + $limit Coalescence

```javascript
// Pipeline initial
db.collection.aggregate([
  { $skip: 100 },
  { $limit: 50 }
])

// Optimis√© automatiquement
// Internalement trait√© comme :
// - Skip les 100 premiers
// - Retourner les 50 suivants
// - Ignorer le reste (early termination)
```

### Limites des Optimisations Automatiques

**Ce que MongoDB NE peut PAS optimiser automatiquement** :

```javascript
// 1. $lookup co√ªteux AVANT un $match s√©lectif
db.orders.aggregate([
  { $lookup: { /* millions de lookups */ } },
  { $match: { /* filtre 99% */ } }
])
// Solution : Inverse l'ordre manuellement

// 2. Multiples $project cons√©cutifs
db.collection.aggregate([
  { $project: { field1: 1, field2: 1, computed1: { $add: ["$a", "$b"] } } },
  { $project: { field1: 1, computed2: { $multiply: ["$computed1", 2] } } }
])
// Solution : Fusionner en un seul $project

// 3. $unwind suivi de $match qui pourrait √™tre avant
db.collection.aggregate([
  { $unwind: "$items" },  // Explose les documents
  { $match: { status: "active" } }  // Filtre sur le doc parent
])
// Solution : $match avant $unwind si le filtre est sur le document parent

// 4. R√©p√©tition de calculs co√ªteux
db.collection.aggregate([
  { $addFields: { expensive: { /* calcul complexe */ } } },
  { $match: { expensive: { $gt: 100 } } },
  { $group: { _id: "$category", avgExpensive: { $avg: "$expensive" } } }
])
// Optimal : Le calcul est fait une fois et r√©utilis√©
```

## Strat√©gies d'Optimisation des Stages Critiques

### Optimisation de $match

**R√®gle fondamentale** : $match doit √™tre le plus t√¥t possible et le plus s√©lectif possible.

#### $match avec Index

```javascript
// Pipeline optimal : $match peut utiliser un index
db.orders.aggregate([
  {
    $match: {
      customerId: ObjectId("..."),  // Index utilisable
      status: "completed",
      createdAt: { $gte: ISODate("2025-01-01") }
    }
  },
  { $group: { /* ... */ } }
])

// V√©rifier l'utilisation de l'index avec explain()
const explain = db.orders.explain("executionStats").aggregate([...]);

// Chercher dans explain :
// - executionStats.executionStages.stage === "IXSCAN"
// - Pas de COLLSCAN
```

**Index optimal pour $match** :
```javascript
// Cr√©er index compound suivant la r√®gle ESR
db.orders.createIndex({
  customerId: 1,    // Equality
  status: 1,        // Equality
  createdAt: -1     // Range
})

// Le $match utilise l'index efficacement
// - Scan direct aux documents du customerId
// - Filtre status dans ce subset
// - Range sur createdAt
```

#### $match Early Filtering

```javascript
// Calcul de la s√©lectivit√©
const totalDocs = db.orders.countDocuments();
const matchedDocs = db.orders.countDocuments({ status: "completed" });
const selectivity = matchedDocs / totalDocs;

print(`Selectivity: ${(selectivity * 100).toFixed(2)}%`);

// Si selectivity < 10% : $match est tr√®s b√©n√©fique
// Si selectivity > 50% : B√©n√©fice moindre mais toujours positif
```

**Pattern anti-performance** :
```javascript
// ‚ùå MAUVAIS : $match tardif
db.orders.aggregate([
  { $lookup: { /* join expensive */ } },
  { $unwind: "$items" },
  { $addFields: { /* calculations */ } },
  { $match: { status: "completed" } }  // Devrait √™tre en premier !
])

// Impact :
// - Tous les stages pr√©c√©dents traitent 100% des documents
// - Puis 90% sont jet√©s par le $match
// - Gaspillage massif de CPU et m√©moire

// ‚úÖ BON : $match en premier
db.orders.aggregate([
  { $match: { status: "completed" } },  // Filter 90% imm√©diatement
  { $lookup: { /* 10√ó moins de lookups */ } },
  { $unwind: "$items" },
  { $addFields: { /* 10√ó moins de calculs */ } }
])
```

### Optimisation de $lookup

Le $lookup est souvent le stage le plus co√ªteux d'un pipeline.

#### $lookup avec Pipeline vs Basic

**Basic $lookup** :
```javascript
// Syntaxe simple (foreign key join)
{
  $lookup: {
    from: "customers",
    localField: "customerId",
    foreignField: "_id",
    as: "customer"
  }
}

// Comportement :
// - Pour CHAQUE document d'orders
// - Ex√©cute : db.customers.find({ _id: order.customerId })
// - Performance : O(N) o√π N = nombre d'orders
```

**Pipeline $lookup** (plus flexible et optimisable) :
```javascript
{
  $lookup: {
    from: "customers",
    let: { custId: "$customerId" },
    pipeline: [
      { $match: {
          $expr: { $eq: ["$_id", "$$custId"] },
          status: "active"  // Filter additionnel c√¥t√© customers
      }},
      { $project: { name: 1, email: 1 } }  // Projection pour r√©duire data transfer
    ],
    as: "customer"
  }
}

// Avantages :
// - Filtrage c√¥t√© foreign collection (moins de data transferr√©e)
// - Projection (documents plus petits)
// - Peut utiliser index sur customers
```

#### $lookup Performance Optimization

**1. Index sur foreignField** :
```javascript
// ESSENTIEL : Index sur le champ de jointure
db.customers.createIndex({ _id: 1 })  // _id d√©j√† index√© par d√©faut

// Pour autres champs :
db.customers.createIndex({ customerId: 1 })

// Sans index : COLLSCAN pour chaque lookup = catastrophique
// Avec index : IXSCAN = rapide
```

**2. Reduce Lookup Count** :
```javascript
// Strat√©gie : Filter AVANT le $lookup
db.orders.aggregate([
  // ‚úÖ Filter first
  { $match: {
      status: "completed",
      createdAt: { $gte: ISODate("2025-01-01") }
  }},  // R√©duit de 1M √† 100k orders

  // Puis lookup seulement sur 100k au lieu de 1M
  { $lookup: { from: "customers", ... } }
])
```

**3. Batch Lookups avec $group** :
```javascript
// Anti-pattern : Lookup individuel pour chaque document
db.orders.aggregate([
  { $lookup: { from: "products", localField: "productId", ... } }
])
// = N lookups (N = nombre d'orders)

// Pattern optimis√© : Group puis lookup une fois
db.orders.aggregate([
  { $group: {
      _id: "$productId",
      orders: { $push: "$$ROOT" }
  }},
  { $lookup: {
      from: "products",
      localField: "_id",
      foreignField: "_id",
      as: "product"
  }},
  { $unwind: "$orders" },
  { $replaceRoot: {
      newRoot: {
        $mergeObjects: ["$orders", { product: { $arrayElemAt: ["$product", 0] } }]
      }
  }}
])
// = M lookups (M = nombre de produits uniques, M << N)
```

**4. Avoid Multiple $lookups** :
```javascript
// ‚ùå MAUVAIS : Multiple lookups s√©quentiels
db.orders.aggregate([
  { $lookup: { from: "customers", ... } },    // N lookups
  { $lookup: { from: "products", ... } },     // N lookups
  { $lookup: { from: "shipping", ... } }      // N lookups
])
// Total : 3N lookups

// ‚úÖ MEILLEUR : Embedded ou denormalization
// Option A : Embed donn√©es fr√©quentes dans orders
{
  _id: ObjectId("orderId"),
  customerName: "John",  // Embedded
  customerEmail: "john@example.com",
  productName: "Widget",
  // Lookup seulement pour d√©tails additionnels si n√©cessaire
}

// Option B : Pr√©-join dans une vue mat√©rialis√©e
db.createView("orders_enriched", "orders", [
  { $lookup: { from: "customers", ... } },
  { $lookup: { from: "products", ... } }
])

// Queries utilisent la vue pr√©-join√©e
db.orders_enriched.aggregate([...])
```

#### $lookup dans Sharded Clusters

**Probl√©matique** : $lookup peut d√©clencher des broadcasts co√ªteux.

```javascript
// Sur un cluster shard√©
db.orders.aggregate([
  { $match: { customerId: ObjectId("...") } },  // Targeted √† 1 shard
  { $lookup: {
      from: "products",  // Collection products shard√©e
      localField: "productId",
      foreignField: "_id",
      as: "product"
  }}
])

// Comportement :
// 1. $match ex√©cut√© sur le shard contenant l'order
// 2. $lookup doit contacter TOUS les shards de products
//    (car productId distribution inconnue)
// 3. = Scatter-gather pour chaque lookup

// Solution : Colocation ou denormalization
// Option 1 : Shard products par m√™me cl√© que orders
// Option 2 : Embed product info dans orders (denormalization)
```

### Optimisation de $group

$group est souvent le stage le plus consommateur de m√©moire.

#### Memory Management

**Limite par d√©faut** : 100 MB par stage

```javascript
// Pipeline d√©passant la limite
db.orders.aggregate([
  {
    $group: {
      _id: "$customerId",
      orders: { $push: "$$ROOT" }  // Accumule tous les documents
    }
  }
])

// Si trop de customers ou documents volumineux :
// Error : "Exceeded memory limit for $group"
```

**Solutions** :

**1. allowDiskUse** :
```javascript
db.orders.aggregate(
  [...],
  { allowDiskUse: true }
)

// Permet de d√©border sur disque si n√©cessaire
// Trade-off :
// - √âvite les erreurs m√©moire
// - Mais significativement plus lent (10-100√ó)
// - Utilise le temporary directory
```

**2. Reduce Document Size** :
```javascript
// ‚ùå MAUVAIS : Push documents complets
{
  $group: {
    _id: "$category",
    products: { $push: "$$ROOT" }  // Document entier
  }
}

// ‚úÖ BON : Push seulement les champs n√©cessaires
{
  $group: {
    _id: "$category",
    products: {
      $push: {
        productId: "$_id",
        name: "$name",
        price: "$price"
        // Seulement ce qui est n√©cessaire
      }
    }
  }
}

// R√©duction de m√©moire : 80-90% typiquement
```

**3. Pre-aggregation avec $project** :
```javascript
db.orders.aggregate([
  // R√©duire taille des documents AVANT $group
  { $project: {
      customerId: 1,
      total: 1,
      date: 1
      // Exclure champs non n√©cessaires
  }},
  { $group: {
      _id: "$customerId",
      totalSpent: { $sum: "$total" },
      orderCount: { $sum: 1 }
  }}
])
```

**4. Streaming Aggregation** :
```javascript
// Pour agr√©gations sur √©normes datasets
// Strat√©gie : Process par chunks

async function streamingAggregation() {
  const categories = await db.products.distinct("category");
  const results = [];

  for (const category of categories) {
    const categoryResult = await db.products.aggregate([
      { $match: { category: category } },  // Process une category √† la fois
      { $group: {
          _id: null,
          avgPrice: { $avg: "$price" },
          count: { $sum: 1 }
      }}
    ]).toArray();

    results.push({ category, ...categoryResult[0] });
  }

  return results;
}

// Avantages :
// - M√©moire constante (pas de pic)
// - Parall√©lisable
// Inconv√©nients :
// - Plus lent (multiple queries)
// - Code plus complexe
```

#### Accumulateurs Performants

Tous les accumulateurs n'ont pas le m√™me co√ªt.

**Performance par accumulateur** :

| Accumulateur | Co√ªt | M√©moire | Note |
|--------------|------|---------|------|
| $sum | Tr√®s faible | O(1) | ‚úÖ Optimal |
| $avg | Faible | O(1) | ‚úÖ Optimal |
| $min / $max | Faible | O(1) | ‚úÖ Optimal |
| $first / $last | Faible | O(1) | ‚úÖ Optimal |
| $push | √âlev√© | O(N) | ‚ö†Ô∏è Limiter la taille |
| $addToSet | Tr√®s √©lev√© | O(N) | ‚ö†Ô∏è D√©dupe co√ªteuse |
| $stdDevPop | Moyen | O(1) | Acceptable |

**Optimisations** :

```javascript
// ‚ùå MAUVAIS : $addToSet sur grandes collections
{
  $group: {
    _id: "$category",
    uniqueCustomers: { $addToSet: "$customerId" }  // Peut avoir millions
  }
}
// Co√ªt : O(N¬≤) pour la d√©duplication

// ‚úÖ MEILLEUR : Limiter ou utiliser alternative
{
  $group: {
    _id: "$category",
    customerCount: { $sum: 1 }  // Si seul le count est n√©cessaire
  }
}

// Ou si vraiment besoin de la liste :
db.orders.aggregate([
  { $group: {
      _id: { category: "$category", customerId: "$customerId" }
  }},
  { $group: {
      _id: "$_id.category",
      uniqueCustomers: { $push: "$_id.customerId" }
  }}
])
// D√©duplication faite par le premier $group (plus efficace)
```

### Optimisation de $sort

$sort sans index supportant est tr√®s co√ªteux.

#### Index-Supported Sort

```javascript
// Index existant
db.orders.createIndex({ customerId: 1, orderDate: -1 })

// ‚úÖ Sort support√© par index
db.orders.aggregate([
  { $match: { customerId: ObjectId("...") } },
  { $sort: { orderDate: -1 } }
])

// explain() montre :
// - Pas de stage SORT
// - IXSCAN avec direction: "backward"
// - executionTimeMillis tr√®s faible

// ‚ùå Sort NON support√© par index
db.orders.aggregate([
  { $match: { customerId: ObjectId("...") } },
  { $sort: { totalAmount: -1 } }  // Pas dans l'index
])

// explain() montre :
// - Stage SORT pr√©sent
// - memLimit: 33554432 (32 MB)
// - Risque d'√©chec si dataset > 32MB sans allowDiskUse
```

**Strat√©gie d'index pour sorts fr√©quents** :
```javascript
// Identifier les sorts fr√©quents
db.system.profile.aggregate([
  { $match: { "command.aggregate": { $exists: true } } },
  { $project: {
      collection: "$command.aggregate",
      sortFields: "$command.pipeline.$sort"
  }},
  { $unwind: "$sortFields" },
  { $group: {
      _id: "$sortFields",
      count: { $sum: 1 }
  }},
  { $sort: { count: -1 } }
])

// Cr√©er index pour les sorts les plus fr√©quents
```

#### Sort + Limit Optimization

```javascript
// Pipeline optimal : Sort + Limit fusionn√©s
db.products.aggregate([
  { $match: { category: "Electronics" } },
  { $sort: { sales: -1 } },
  { $limit: 100 }
])

// Algorithme top-K automatique
// M√©moire : Seulement 100 documents
// Pas de risque de memory overflow

// ‚ùå MAUVAIS : Sort apr√®s des stages co√ªteux
db.products.aggregate([
  { $lookup: { /* expensive */ } },
  { $unwind: "$details" },
  { $addFields: { /* calculations */ } },
  { $sort: { calculatedField: -1 } },  // Sort sur tous les documents expans√©s
  { $limit: 100 }
])

// ‚úÖ MEILLEUR : Reduce dataset avant sort si possible
db.products.aggregate([
  { $match: { /* filter */ } },
  { $sort: { baseField: -1 } },
  { $limit: 100 },  // R√©duire √† 100 t√¥t
  { $lookup: { /* lookup sur 100 au lieu de millions */ } },
  { $unwind: "$details" },
  { $addFields: { /* calculations sur 100 */ } }
])
```

### Optimisation de $unwind

$unwind peut exploser le nombre de documents √† traiter.

```javascript
// Document avec array
{
  _id: 1,
  customer: "John",
  items: [
    { product: "A", qty: 2 },
    { product: "B", qty: 1 },
    { product: "C", qty: 5 }
  ]
}

// Apr√®s $unwind
db.orders.aggregate([
  { $unwind: "$items" }
])

// R√©sultat : 3 documents
{ _id: 1, customer: "John", items: { product: "A", qty: 2 } }
{ _id: 1, customer: "John", items: { product: "B", qty: 1 } }
{ _id: 1, customer: "John", items: { product: "C", qty: 5 } }

// Si array moyen de 10 items :
// - Input : 1,000,000 orders
// - After $unwind : 10,000,000 documents
// = 10√ó augmentation
```

**Optimisations** :

**1. Filter BEFORE $unwind** :
```javascript
// ‚ùå MAUVAIS : Unwind puis filter
db.orders.aggregate([
  { $unwind: "$items" },
  { $match: { "items.product": "A" } }
])
// Unwind tous les items, puis garde seulement "A"

// ‚úÖ BON : Filter l'array avant unwind
db.orders.aggregate([
  { $addFields: {
      items: {
        $filter: {
          input: "$items",
          as: "item",
          cond: { $eq: ["$$item.product", "A"] }
        }
      }
  }},
  { $match: { items: { $ne: [] } } },  // Garde seulement orders avec items filtr√©s
  { $unwind: "$items" }
])
// Unwind seulement les items pertinents
```

**2. preserveNullAndEmptyArrays : false** :
```javascript
// Par d√©faut : false (bon)
{ $unwind: "$items" }

// Si array vide ou absent : document supprim√©
// = Moins de documents √† traiter dans stages suivants

// preserveNullAndEmptyArrays: true
{ $unwind: { path: "$items", preserveNullAndEmptyArrays: true } }

// Garde les documents m√™me si array vide
// = Plus de documents (performance impact)
// Utiliser seulement si n√©cessaire pour la logique m√©tier
```

**3. Avoid Multiple $unwinds** :
```javascript
// ‚ùå TR√àS MAUVAIS : Double $unwind
db.orders.aggregate([
  { $unwind: "$items" },        // Array de 10 items
  { $unwind: "$items.options" } // Chaque item a 5 options
])

// Explosion : 1 order ‚Üí 50 documents (10 √ó 5)
// 1,000,000 orders ‚Üí 50,000,000 documents

// ‚úÖ Alternative : Aggregation diff√©rente
// Utiliser $reduce, $map, ou $filter pour traiter les arrays sans unwind
```

## Strat√©gies Avanc√©es d'Optimisation

### Pipeline Splitting

Pour les pipelines longs, consid√©rer le splitting en plusieurs √©tapes.

```javascript
// Pipeline monolithique (dur √† optimiser et debug)
db.orders.aggregate([
  { $match: { /* ... */ } },
  { $lookup: { /* ... */ } },
  { $unwind: "$items" },
  { $group: { /* complex aggregation */ } },
  { $lookup: { /* another join */ } },
  { $project: { /* complex transformations */ } },
  { $sort: { /* ... */ } },
  { $limit: 100 }
])

// Strat√©gie : Split en √©tapes logiques avec materialization
// √âtape 1 : Pr√©-agr√©gation et storage
db.orders.aggregate([
  { $match: { /* ... */ } },
  { $lookup: { /* ... */ } },
  { $unwind: "$items" },
  { $group: { /* complex aggregation */ } },
  { $out: "orders_preaggregated" }  // Materialise dans collection temp
])

// √âtape 2 : Processing final sur dataset r√©duit
db.orders_preaggregated.aggregate([
  { $lookup: { /* another join - sur dataset plus petit */ } },
  { $project: { /* transformations */ } },
  { $sort: { /* ... */ } },
  { $limit: 100 }
])

// Avantages :
// - Chaque √©tape optimisable ind√©pendamment
// - Debugging plus facile
// - R√©utilisation de l'√©tape 1 pour diff√©rentes queries
// - R√©sultats interm√©diaires indexables

// Inconv√©nients :
// - Overhead du write interm√©diaire
// - Pas suitable pour real-time (latency accrue)
```

### Vues Mat√©rialis√©es avec $merge et $out

Pour agr√©gations co√ªteuses ex√©cut√©es fr√©quemment, mat√©rialiser les r√©sultats.

#### $out : Remplacement Complet

```javascript
// Recalcul complet et remplacement
db.orders.aggregate([
  {
    $group: {
      _id: "$customerId",
      totalSpent: { $sum: "$total" },
      orderCount: { $sum: 1 },
      lastOrderDate: { $max: "$orderDate" }
    }
  },
  { $out: "customer_stats" }  // Remplace la collection enti√®rement
])

// Utilisation :
// Au lieu de r√©-agr√©ger √† chaque fois :
db.customer_stats.find({ totalSpent: { $gte: 10000 } })

// Strat√©gie de refresh :
// - Cron job nocturne pour recalcul complet
// - Ou trigger sur changements significatifs
```

#### $merge : Mise √† Jour Incr√©mentale

```javascript
// Mise √† jour incr√©mentale (MongoDB 4.2+)
db.orders.aggregate([
  {
    $match: {
      orderDate: { $gte: lastProcessedDate }  // Seulement nouveaux orders
    }
  },
  {
    $group: {
      _id: "$customerId",
      newSpent: { $sum: "$total" },
      newOrders: { $sum: 1 }
    }
  },
  {
    $merge: {
      into: "customer_stats",
      on: "_id",  // Match sur customerId
      whenMatched: [  // Update des stats existantes
        {
          $set: {
            totalSpent: { $add: ["$totalSpent", "$newSpent"] },
            orderCount: { $add: ["$orderCount", "$newOrders"] },
            lastUpdated: "$$NOW"
          }
        }
      ],
      whenNotMatched: "insert"  // Insert si nouveau customer
    }
  }
])

// Avantages :
// - Incr√©mental = beaucoup plus rapide
// - Pas de recalcul complet
// - Stats toujours relativement √† jour

// Usage :
// Ex√©cution fr√©quente (ex: toutes les 5 minutes)
// Charge distribu√©e vs batch nocturne
```

**Exemple complet : Dashboard stats** :

```javascript
// Collection stats mat√©rialis√©e
db.createCollection("daily_stats", {
  validator: {
    $jsonSchema: {
      required: ["date", "metrics"],
      properties: {
        date: { bsonType: "date" },
        metrics: {
          bsonType: "object",
          properties: {
            totalRevenue: { bsonType: "double" },
            orderCount: { bsonType: "int" },
            avgOrderValue: { bsonType: "double" }
          }
        }
      }
    }
  }
})

// Index pour queries rapides
db.daily_stats.createIndex({ date: -1 })

// Pipeline de calcul (ex√©cut√© quotidiennement)
db.orders.aggregate([
  {
    $match: {
      orderDate: {
        $gte: ISODate("2025-01-15T00:00:00Z"),
        $lt: ISODate("2025-01-16T00:00:00Z")
      }
    }
  },
  {
    $group: {
      _id: null,
      totalRevenue: { $sum: "$total" },
      orderCount: { $sum: 1 }
    }
  },
  {
    $addFields: {
      avgOrderValue: { $divide: ["$totalRevenue", "$orderCount"] }
    }
  },
  {
    $project: {
      _id: 0,
      date: ISODate("2025-01-15"),
      metrics: {
        totalRevenue: "$totalRevenue",
        orderCount: "$orderCount",
        avgOrderValue: "$avgOrderValue"
      }
    }
  },
  {
    $merge: {
      into: "daily_stats",
      on: "date",
      whenMatched: "replace",
      whenNotMatched: "insert"
    }
  }
])

// Dashboard query : Instantan√© !
db.daily_stats.find().sort({ date: -1 }).limit(30)
// vs agr√©gation en temps r√©el : 10,000√ó plus rapide
```

### Optimisation dans Sharded Clusters

Les pipelines sur clusters shard√©s ont des consid√©rations sp√©ciales.

#### Pipeline Targeting

```javascript
// ‚úÖ TARGETED : Query contient la shard key
db.orders.aggregate([
  { $match: { customerId: ObjectId("...") } }  // Shard key
  // ...
])

// Ex√©cution :
// - mongos route vers 1 seul shard
// - Pipeline ex√©cut√© enti√®rement sur ce shard
// - R√©sultat retourn√© directement
// Performance : Optimale

// ‚ùå SCATTER-GATHER : Query sans shard key
db.orders.aggregate([
  { $match: { status: "completed" } }  // Pas la shard key
  // ...
])

// Ex√©cution :
// - mongos envoie le pipeline √† TOUS les shards
// - Chaque shard ex√©cute et retourne r√©sultats partiels
// - mongos merge les r√©sultats
// Performance : N√ó plus lent (N = nombre de shards)
```

#### Split Pipeline

MongoDB divise automatiquement certains pipelines entre shards et mongos.

```javascript
db.orders.aggregate([
  { $match: { status: "completed" } },
  { $group: { _id: "$category", total: { $sum: "$amount" } } },
  { $sort: { total: -1 } },
  { $limit: 10 }
])

// Ex√©cution split√©e :

// Sur chaque shard :
[
  { $match: { status: "completed" } },
  { $group: { _id: "$category", total: { $sum: "$amount" } } }
]
// Retour au mongos : R√©sultats partiels par category

// Sur mongos :
[
  { $group: { _id: "$_id", total: { $sum: "$total" } } },  // Merge des groupes
  { $sort: { total: -1 } },
  { $limit: 10 }
]

// Optimisations :
// - Agr√©gation parall√®le sur les shards
// - Mongos ne merge que les r√©sultats agr√©g√©s (petit dataset)
```

**Stages non-distribuables** (toujours sur mongos) :

- $out / $merge (avant MongoDB 4.4)
- $lookup (si foreign collection non sharded)
- $graphLookup
- $facet
- $collStats

```javascript
// Pipeline avec $lookup sur collection non-shard√©e
db.sharded_orders.aggregate([
  { $match: { /* ... */ } },  // Ex√©cut√© sur shards
  { $lookup: {                 // FORCE tout √† remonter √† mongos
      from: "unsharded_customers",
      // ...
  }},
  { $group: { /* ... */ } }   // Ex√©cut√© sur mongos
])

// Impact :
// - Tous les documents match√©s remontent √† mongos
// - Network overhead √©lev√©
// - Mongos devient bottleneck

// Solution : Shard aussi customers, ou embed customer data
```

## Analyse et Debugging des Pipelines

### explain() pour Aggregation

```javascript
const explain = db.collection.explain("executionStats").aggregate([...])

// Structure sp√©cifique aux aggregations
{
  "stages": [
    {
      "$cursor": {
        "queryPlanner": { /* plan pour le $cursor stage */ },
        "executionStats": { /* stats pour requ√™te initiale */ }
      }
    },
    { "$stage2": { /* info sur stage 2 */ } },
    // ...
  ],
  "serverInfo": { /* ... */ },
  "ok": 1
}
```

**Analyse du $cursor stage** :

```javascript
// Le $cursor stage repr√©sente la partie du pipeline
// qui peut utiliser les index (g√©n√©ralement $match initial)

const cursorStats = explain.stages[0].$cursor.executionStats;

// M√©triques critiques :
{
  totalDocsExamined: 150000,    // Documents examin√©s
  totalKeysExamined: 150000,    // Cl√©s d'index examin√©es
  nReturned: 10000,             // Documents retourn√©s au pipeline
  executionTimeMillis: 234,     // Temps du cursor

  executionStages: {
    stage: "IXSCAN",            // ‚úÖ Utilise un index
    indexName: "status_1_date_-1",
    // ...
  }
}

// Ratio d'efficacit√©
const efficiency = cursorStats.nReturned / cursorStats.totalDocsExamined;
// > 0.1 : Bon
// < 0.01 : Probl√©matique
```

**Analyse des stages suivants** :

```javascript
// Stages apr√®s $cursor n'ont g√©n√©ralement pas de stats d√©taill√©es
// mais on peut inf√©rer :

explain.stages.forEach((stage, idx) => {
  const stageName = Object.keys(stage)[0];

  if (stageName === "$group") {
    // V√©rifier si allowDiskUse a √©t√© n√©cessaire
    // V√©rifier la m√©moire utilis√©e (non directement disponible)
    print(`Stage ${idx}: $group - Monitor memory usage`);
  }

  if (stageName === "$lookup") {
    // Lookup est co√ªteux
    // V√©rifier si foreign collection est index√©e
    print(`Stage ${idx}: $lookup - Verify foreign index`);
  }

  if (stageName === "$sort") {
    // Sort sans index est co√ªteux
    print(`Stage ${idx}: $sort - Check if index-supported`);
  }
});
```

### Profiling des Agr√©gations

```javascript
// Activer profiler
db.setProfilingLevel(2)

// Ex√©cuter aggregation
db.orders.aggregate([...])

// Analyser dans system.profile
db.system.profile.find({
  "command.aggregate": { $exists: true },
  ns: "mydb.orders"
}).sort({ ts: -1 }).limit(10).pretty()

// M√©triques importantes :
{
  op: "command",
  ns: "mydb.orders",
  command: {
    aggregate: "orders",
    pipeline: [ /* ... */ ],
    cursor: {},
    allowDiskUse: false
  },

  // M√©triques critiques
  millis: 5234,              // Temps total
  planSummary: "IXSCAN ...", // Plan utilis√©
  docsExamined: 1500000,     // Documents examin√©s
  nreturned: 100,            // Documents retourn√©s

  // Ressources
  cursorExhausted: true,
  numYield: 234,             // Nombre de yields (contention)
  locks: { /* ... */ },      // Lock stats

  // Flow control (si cluster)
  flowControl: { /* ... */ }
}
```

**Identifier les agr√©gations lentes** :

```javascript
// Top 10 agr√©gations les plus lentes
db.system.profile.aggregate([
  {
    $match: {
      "command.aggregate": { $exists: true },
      millis: { $gte: 1000 }  // > 1 seconde
    }
  },
  {
    $project: {
      collection: "$command.aggregate",
      pipeline: "$command.pipeline",
      millis: 1,
      docsExamined: 1,
      nreturned: 1,
      planSummary: 1
    }
  },
  { $sort: { millis: -1 } },
  { $limit: 10 }
])
```

### Performance Testing M√©thodique

```javascript
// Framework de test de performance
class AggregationPerformanceTest {
  constructor(collection, pipeline, name) {
    this.collection = collection;
    this.pipeline = pipeline;
    this.name = name;
  }

  async run(iterations = 5) {
    const results = [];

    // Warm-up run
    await db[this.collection].aggregate(this.pipeline).toArray();

    // Timed runs
    for (let i = 0; i < iterations; i++) {
      const start = Date.now();
      const result = await db[this.collection].aggregate(this.pipeline).toArray();
      const duration = Date.now() - start;

      results.push({
        iteration: i + 1,
        duration: duration,
        resultCount: result.length
      });
    }

    // Statistics
    const durations = results.map(r => r.duration);
    const avg = durations.reduce((a, b) => a + b) / durations.length;
    const min = Math.min(...durations);
    const max = Math.max(...durations);

    return {
      name: this.name,
      iterations: iterations,
      avgMs: avg.toFixed(2),
      minMs: min,
      maxMs: max,
      resultCount: results[0].resultCount,
      results: results
    };
  }
}

// Usage
const test = new AggregationPerformanceTest(
  "orders",
  [
    { $match: { status: "completed" } },
    { $group: { _id: "$customerId", total: { $sum: "$amount" } } }
  ],
  "Customer Totals Aggregation"
);

const perfResults = await test.run(10);
printjson(perfResults);
```

## Patterns d'Optimisation par Cas d'Usage

### Real-Time Analytics

**Contrainte** : Latence < 100ms

```javascript
// ‚ùå MAUVAIS : Agr√©gation complexe en temps r√©el
db.events.aggregate([
  { $match: { timestamp: { $gte: last24Hours } } },
  { $group: { _id: "$eventType", count: { $sum: 1 } } },
  { $sort: { count: -1 } }
])
// Latence : 2-5 secondes sur dataset large

// ‚úÖ BON : Pr√©-agr√©gation avec incremental update
// Collection pr√©-agr√©g√©e mise √† jour en continu
db.event_stats_realtime.find().sort({ count: -1 })
// Latence : < 10ms

// Mise √† jour via change stream
const changeStream = db.events.watch();
changeStream.on('change', (change) => {
  if (change.operationType === 'insert') {
    db.event_stats_realtime.updateOne(
      { _id: change.fullDocument.eventType },
      { $inc: { count: 1 } },
      { upsert: true }
    );
  }
});
```

### Reporting/BI Queries

**Contrainte** : Throughput √©lev√©, latence acceptable (secondes)

```javascript
// Strat√©gie : Vues mat√©rialis√©es avec refresh quotidien
// Vue : Sales by region, product category
db.orders.aggregate([
  {
    $match: {
      orderDate: { $gte: startOfMonth, $lt: endOfMonth }
    }
  },
  {
    $group: {
      _id: {
        region: "$shippingAddress.region",
        category: "$productCategory"
      },
      revenue: { $sum: "$total" },
      orderCount: { $sum: 1 },
      avgOrderValue: { $avg: "$total" }
    }
  },
  {
    $out: "sales_by_region_category_monthly"
  }
])

// Index pour queries BI rapides
db.sales_by_region_category_monthly.createIndex({ "_id.region": 1 })
db.sales_by_region_category_monthly.createIndex({ "_id.category": 1 })
db.sales_by_region_category_monthly.createIndex({ revenue: -1 })

// BI tools query la vue mat√©rialis√©e : Instant !
```

### ETL / Batch Processing

**Contrainte** : Volume √©norme, pas de contrainte temps r√©el

```javascript
// Strat√©gie : Process par chunks avec allowDiskUse
const batchSize = 10000;
let processed = 0;

while (true) {
  const batch = await db.raw_data.aggregate([
    { $skip: processed },
    { $limit: batchSize },
    {
      $lookup: { /* enrichment */ }
    },
    {
      $project: { /* transformation */ }
    }
  ], {
    allowDiskUse: true,
    maxTimeMS: 300000  // 5 minutes timeout par batch
  }).toArray();

  if (batch.length === 0) break;

  // Insert dans collection de destination
  await db.processed_data.insertMany(batch);

  processed += batch.length;
  print(`Processed: ${processed} documents`);
}
```

## Checklist d'Optimisation

### Avant D√©ploiement

```
‚òê $match en premier stage (avec index supportant)
‚òê Projections pr√©coces pour r√©duire taille documents
‚òê $lookup minimis√© et avec index sur foreignField
‚òê $unwind uniquement si n√©cessaire, apr√®s filtrage
‚òê $group avec accumulateurs l√©gers ($sum, $avg vs $push)
‚òê $sort support√© par index ou fusionn√© avec $limit
‚òê explain() analys√© avec ratios acceptables
‚òê Test avec dataset production-like (taille et distribution)
‚òê allowDiskUse configur√© si n√©cessaire
‚òê Timeout appropri√© (maxTimeMS)
```

### Monitoring Production

```
‚òê Latence P95 des agr√©gations < seuil d√©fini
‚òê Aucune agr√©gation d√©passant memory limit
‚òê Pas d'utilisation excessive de allowDiskUse (disque lent)
‚òê Index usage valid√© via $indexStats
‚òê Profiler pour identifier regressions
‚òê Resource utilization (CPU, RAM, I/O) sous contr√¥le
```

### Maintenance R√©guli√®re

```
‚òê Review pipelines lents (profiler monthly)
‚òê Update vues mat√©rialis√©es (si utilis√©es)
‚òê Index optimization pour nouveaux patterns
‚òê Cleanup de collections temporaires ($out/$merge)
‚òê V√©rifier evolution des query patterns
```

## Conclusion

L'optimisation des agr√©gations MongoDB n√©cessite une approche syst√©matique :

1. **Conception** : Ordre optimal des stages (filter early, project early, reduce document count)
2. **Index** : Support pour $match et $sort stages
3. **Memory Management** : Accumulateurs l√©gers, allowDiskUse quand n√©cessaire
4. **Materialization** : Vues pour queries fr√©quentes et co√ªteuses
5. **Monitoring** : explain(), profiler, m√©triques de production

**Principes directeurs** :
- R√©duire le dataset le plus t√¥t possible
- Minimiser les stages co√ªteux ($lookup, $unwind)
- Exploiter les index au maximum
- Mat√©rialiser pour queries r√©p√©titives
- Mesurer et it√©rer

**ROI typiques** :
- $match optimization : 10-100√ó improvement
- Index-supported $sort : 10-50√ó improvement
- Proper $lookup strategy : 5-20√ó improvement
- Materialized views : 100-1000√ó improvement

L'excellence en optimisation d'agr√©gations vient de la compr√©hension profonde du co√ªt de chaque stage, de l'utilisation judicieuse des index, et de l'adaptation de la strat√©gie au cas d'usage sp√©cifique (real-time vs batch, read-heavy vs write-heavy).

---

**Points cl√©s √† retenir :**
- $match et $project en premier : R√©duire le dataset imm√©diatement
- Index pour $match et $sort : √âviter collection scans et in-memory sorts
- $lookup est co√ªteux : Minimiser, indexer foreignField, consid√©rer denormalization
- $group memory limit : Utiliser accumulateurs l√©gers, allowDiskUse si n√©cessaire
- Vues mat√©rialis√©es : Pour agr√©gations co√ªteuses et fr√©quentes
- explain() et profiler : Essentiels pour l'analyse et l'optimisation
- Pipeline splitting : Pour pipelines complexes, d√©composer et mat√©rialiser

‚è≠Ô∏è [Configuration du moteur WiredTiger](/17-performance-tuning/06-configuration-wiredtiger.md)
