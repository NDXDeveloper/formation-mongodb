ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 17.9 Compression des DonnÃ©es

## Introduction

La compression des donnÃ©es est un levier d'optimisation majeur dans MongoDB, offrant des gains substantiels en termes de coÃ»t de stockage, performance I/O et bande passante rÃ©seau. WiredTiger, le moteur de stockage par dÃ©faut depuis MongoDB 3.2, implÃ©mente plusieurs niveaux de compression permettant d'atteindre des ratios de 2Ã— Ã  10Ã— selon les donnÃ©es et algorithmes choisis.

Une stratÃ©gie de compression optimale peut rÃ©duire les coÃ»ts de stockage de 50-80% tout en amÃ©liorant les performances de 10-40% grÃ¢ce Ã  la rÃ©duction des I/O. Cependant, une mauvaise configuration peut dÃ©grader les performances de 20-50% en raison du surcoÃ»t CPU.

Cette section explore les mÃ©canismes de compression dans MongoDB, leurs impacts sur les performances, et les mÃ©thodologies d'optimisation pour diffÃ©rents profils de charge.

## Architecture de la Compression dans MongoDB

### Niveaux de Compression

MongoDB implÃ©mente la compression Ã  trois niveaux distincts :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Application Layer                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Network Compression (Wire Protocol)     â”‚
â”‚  Compressors: snappy, zlib, zstd               â”‚
â”‚  Documents compressÃ©s durant le transit        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MongoDB Server Layer                 â”‚
â”‚  Documents en clair en mÃ©moire (cache)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      WiredTiger Storage Engine                 â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Collection Block Compression          â”‚    â”‚
â”‚  â”‚  Compressors: snappy, zlib, zstd, none â”‚    â”‚
â”‚  â”‚  Blocks: 4-32 KB compressÃ©s            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Index Prefix Compression              â”‚    â”‚
â”‚  â”‚  Compression des prÃ©fixes communs      â”‚    â”‚
â”‚  â”‚  (Automatique, non configurable)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Journal Compression                   â”‚    â”‚
â”‚  â”‚  Compressor: snappy (dÃ©faut), zlib     â”‚    â”‚
â”‚  â”‚  WAL logs compressÃ©s                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Filesystem (Data Files)               â”‚
â”‚  *.wt files (compressed blocks)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flow de Compression/DÃ©compression

**OpÃ©ration Write** :
```
1. Application â†’ Document (JSON/BSON)
   â†“
2. Network compression (optionnel)
   â†“
3. MongoDB server â†’ Cache (uncompressed)
   â†“
4. WiredTiger â†’ Compression des blocks
   â†“
5. Disk â†’ Blocks compressÃ©s Ã©crits
```

**OpÃ©ration Read** :
```
1. Disk â†’ Lecture des blocks compressÃ©s
   â†“
2. WiredTiger â†’ DÃ©compression en cache
   â†“
3. MongoDB server â†’ Document disponible
   â†“
4. Network compression (optionnel)
   â†“
5. Application â†’ Document reÃ§u
```

**Point critique** : La compression/dÃ©compression se fait Ã  la frontiÃ¨re cache-disque, pas en cache. Le cache WiredTiger stocke les donnÃ©es dÃ©compressÃ©es pour un accÃ¨s rapide.

## Compression des Collections (Block Compression)

### Algorithmes Disponibles

MongoDB/WiredTiger supporte quatre compresseurs pour les collections :

#### snappy (Par DÃ©faut)

```yaml
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: snappy
```

**CaractÃ©ristiques** :
- Ratio : 2-3Ã— typiquement
- CPU overhead : TrÃ¨s faible (5-10%)
- Vitesse : TrÃ¨s rapide (compression ~250 MB/s, dÃ©compression ~500 MB/s)
- DÃ©veloppÃ© par Google
- OptimisÃ© pour la vitesse, pas le ratio

**Use cases** :
- Workload latency-sensitive
- CPU limitÃ©
- DonnÃ©es dÃ©jÃ  peu compressibles
- Production gÃ©nÃ©raliste (dÃ©faut optimal)

#### zlib

```yaml
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zlib
```

**CaractÃ©ristiques** :
- Ratio : 3-5Ã— typiquement
- CPU overhead : Moyen (15-30%)
- Vitesse : Moyen (compression ~100 MB/s, dÃ©compression ~300 MB/s)
- Niveaux de compression : 1-9 (WiredTiger utilise niveau 6)
- Standard de facto (RFC 1950)

**Use cases** :
- Storage contraint (coÃ»t Ã©levÃ©)
- DonnÃ©es hautement compressibles
- Workload read-heavy (dÃ©compression plus rapide que compression)
- RÃ©duction maximale du stockage prioritaire

#### zstd (RecommandÃ© Modern)

```yaml
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zstd
```

**CaractÃ©ristiques** :
- Ratio : 3-6Ã— typiquement (meilleur que zlib)
- CPU overhead : Faible-Moyen (10-20%)
- Vitesse : Rapide (compression ~200 MB/s, dÃ©compression ~600 MB/s)
- DÃ©veloppÃ© par Facebook
- Meilleur compromis ratio/performance
- Disponible depuis MongoDB 4.2

**Use cases** :
- **RecommandÃ© pour nouveaux dÃ©ploiements**
- Meilleur Ã©quilibre performance/ratio
- Remplace avantageusement snappy dans la plupart des cas
- Production moderne

#### none

```yaml
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: none
```

**CaractÃ©ristiques** :
- Ratio : 1Ã— (aucune compression)
- CPU overhead : 0%
- Vitesse : Maximale

**Use cases** :
- Latence ultra-faible critique (<1ms P99)
- DonnÃ©es non compressibles (images, vidÃ©os dÃ©jÃ  compressÃ©es)
- CPU extrÃªmement limitÃ©
- Storage rapide et abondant

### Benchmark Comparatif

**Setup de test** :
- Collection : 10M documents, 50GB uncompressed
- Documents : JSON structurÃ© typique (e-commerce orders)
- Hardware : 32 cores @ 3.5 GHz, 128GB RAM, NVMe SSD
- Workload : 60% read, 40% write

**RÃ©sultats** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Compressorâ”‚ On-Disk  â”‚Comp Ratioâ”‚Read P99  â”‚Write P99 â”‚ CPU Avg  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ none     â”‚ 50.0 GB  â”‚  1.0Ã—    â”‚  1.2 ms  â”‚  1.8 ms  â”‚  22%     â”‚
â”‚ snappy   â”‚ 18.5 GB  â”‚  2.7Ã—    â”‚  1.5 ms  â”‚  2.1 ms  â”‚  25%     â”‚
â”‚ zstd     â”‚ 12.3 GB  â”‚  4.1Ã—    â”‚  1.7 ms  â”‚  2.4 ms  â”‚  28%     â”‚
â”‚ zlib     â”‚ 10.8 GB  â”‚  4.6Ã—    â”‚  2.3 ms  â”‚  3.2 ms  â”‚  35%     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Throughput (ops/sec):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Compressorâ”‚ Inserts  â”‚ Queries  â”‚  Updates â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ none     â”‚  45,000  â”‚  65,000  â”‚  38,000  â”‚
â”‚ snappy   â”‚  42,000  â”‚  62,000  â”‚  36,000  â”‚
â”‚ zstd     â”‚  39,000  â”‚  59,000  â”‚  34,000  â”‚
â”‚ zlib     â”‚  32,000  â”‚  52,000  â”‚  28,000  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Analyse:
- snappy : Meilleur balance pour la plupart des cas
- zstd : Meilleur ratio avec overhead CPU raisonnable
- zlib : Ratio maximal mais impact performance significatif
- none : Seulement si latence ultra-critique ou data non compressible
```

### Configuration par Collection

```javascript
// Lors de la crÃ©ation de la collection
db.createCollection("highlyCompressible", {
  storageEngine: {
    wiredTiger: {
      configString: "block_compressor=zlib"
    }
  }
})

// Collection avec donnÃ©es dÃ©jÃ  compressÃ©es (images)
db.createCollection("imageMetadata", {
  storageEngine: {
    wiredTiger: {
      configString: "block_compressor=none"
    }
  }
})

// Collection critique latence
db.createCollection("realtimeEvents", {
  storageEngine: {
    wiredTiger: {
      configString: "block_compressor=snappy"
    }
  }
})
```

**Changement de compresseur** (nÃ©cessite rebuild) :

```javascript
// MÃ©thode 1 : Via aggregation $out
db.oldCollection.aggregate([
  { $match: {} },
  { $out: "newCollectionWithZstd" }
])

// MÃ©thode 2 : Dump/restore avec nouveau compressor
// 1. Configure nouveau compressor dans mongod.conf
// 2. mongodump --collection oldCollection
// 3. Drop oldCollection
// 4. mongorestore (collection recrÃ©Ã©e avec nouveau compressor)

// MÃ©thode 3 : En-place avec compact (pas de changement de compressor)
// compact prÃ©serve le compressor existant
db.runCommand({ compact: "collection" })
```

### Analyse de l'EfficacitÃ© de Compression

```javascript
function analyzeCompressionEfficiency(collectionName) {
  const coll = db.getCollection(collectionName);
  const stats = coll.stats();

  const analysis = {
    collection: collectionName,

    // Tailles
    uncompressedSizeGB: (stats.size / 1024 / 1024 / 1024).toFixed(2),
    compressedSizeGB: (stats.storageSize / 1024 / 1024 / 1024).toFixed(2),

    // Ratio
    compressionRatio: (stats.size / stats.storageSize).toFixed(2) + "Ã—",

    // Savings
    savedGB: ((stats.size - stats.storageSize) / 1024 / 1024 / 1024).toFixed(2),
    savedPercent: (((stats.size - stats.storageSize) / stats.size) * 100).toFixed(2) + "%",

    // Index
    indexSizeGB: (stats.totalIndexSize / 1024 / 1024 / 1024).toFixed(2),

    // Total on disk
    totalOnDiskGB: ((stats.storageSize + stats.totalIndexSize) / 1024 / 1024 / 1024).toFixed(2),

    // Document stats
    documentCount: stats.count,
    avgDocumentSizeKB: (stats.avgObjSize / 1024).toFixed(2),

    // Compressor
    compressor: "unknown"
  };

  // Extract compressor from creation string
  if (stats.wiredTiger && stats.wiredTiger.creationString) {
    const match = stats.wiredTiger.creationString.match(/block_compressor=(\w+)/);
    if (match) {
      analysis.compressor = match[1];
    }
  }

  // Assessment
  const ratio = parseFloat(analysis.compressionRatio);
  if (ratio < 1.5) {
    analysis.assessment = "âš ï¸ Poor compression - Data may be pre-compressed or binary";
  } else if (ratio < 2.5) {
    analysis.assessment = "Acceptable compression";
  } else if (ratio < 4) {
    analysis.assessment = "âœ… Good compression";
  } else {
    analysis.assessment = "âœ… Excellent compression";
  }

  return analysis;
}

// Usage
printjson(analyzeCompressionEfficiency("orders"));

// Analyse de toutes les collections
db.getCollectionNames().forEach(collName => {
  const analysis = analyzeCompressionEfficiency(collName);
  print(`\n${collName}:`);
  print(`  Compression: ${analysis.compressionRatio} (${analysis.compressor})`);
  print(`  Saved: ${analysis.savedGB} GB (${analysis.savedPercent})`);
  print(`  Assessment: ${analysis.assessment}`);
});
```

### Facteurs Affectant le Ratio de Compression

**Type de donnÃ©es** :

```javascript
// DonnÃ©es hautement compressibles (ratio 5-10Ã—)
{
  _id: ObjectId("..."),
  status: "completed",              // RÃ©pÃ©titif
  paymentMethod: "credit_card",     // RÃ©pÃ©titif
  currency: "EUR",                  // RÃ©pÃ©titif
  description: "Standard text...",  // Text compresse bien
  tags: ["sale", "promo", "new"]   // RÃ©pÃ©titif
}
// Compresseur optimal : zlib

// DonnÃ©es moyennement compressibles (ratio 2-4Ã—)
{
  _id: ObjectId("..."),
  userId: "user12345",
  timestamp: ISODate("2025-01-15T10:30:00Z"),
  metrics: {
    cpu: 45.2,
    memory: 67.8,
    disk: 23.1
  }
}
// Compresseur optimal : snappy ou zstd

// DonnÃ©es peu compressibles (ratio 1.1-1.5Ã—)
{
  _id: ObjectId("..."),
  imageData: BinData(0, "..."),    // DÃ©jÃ  compressÃ© (JPEG)
  videoUrl: "https://...",
  hash: "a7f3e9c2d1b8..."          // Random
}
// Compresseur optimal : none

// DonnÃ©es avec structure rÃ©pÃ©titive (ratio 6-12Ã—)
{
  _id: ObjectId("..."),
  metadata: {
    version: "1.0.0",
    environment: "production",
    datacenter: "us-east-1",
    application: "web-service"
  },
  // Beaucoup de clÃ©s identiques entre documents
}
// Compresseur optimal : zlib
```

**Patterns de compression** :

```javascript
// Excellent pour compression
const excellentCompressionPatterns = {
  repetitiveValues: true,        // Status, categories
  textFields: true,              // Descriptions, comments
  numericalSequences: true,      // IDs sÃ©quentiels
  timestampRanges: true,         // Dates proches
  structuredKeys: true,          // JSON avec mÃªme structure
  lowCardinality: true          // Peu de valeurs uniques
};

// Mauvais pour compression
const poorCompressionPatterns = {
  binaryData: true,             // Images, audio, vidÃ©o
  encryptedData: true,          // DonnÃ©es chiffrÃ©es
  randomStrings: true,          // UUIDs, hashes
  alreadyCompressed: true,      // Fichiers ZIP, GZIP
  highCardinality: true,        // Valeurs trÃ¨s variÃ©es
  sparseData: true              // Beaucoup de null
};
```

### Compression Block Size

WiredTiger utilise des blocks de taille variable (4KB Ã  32KB).

```javascript
// Configuration avancÃ©e (rare)
db.createCollection("tuned", {
  storageEngine: {
    wiredTiger: {
      configString: "block_compressor=zstd,allocation_size=32KB"
      // Augmenter allocation_size peut amÃ©liorer ratio mais augmente latency
    }
  }
})

// Trade-offs :
// - Block size plus grand : Meilleur ratio, mais plus de donnÃ©es Ã  dÃ©compresser
// - Block size plus petit : Latence plus faible, mais ratio rÃ©duit
// - DÃ©faut (4KB-32KB dynamique) : Optimal pour la plupart des cas
```

## Compression des Index

### Prefix Compression

Les index B-Tree utilisent automatiquement la prefix compression.

**Principe** :
```
Sans prefix compression :
â”œâ”€ "customer_12345"
â”œâ”€ "customer_12346"
â”œâ”€ "customer_12347"
â””â”€ "customer_12348"

Avec prefix compression :
â”œâ”€ "customer_1234" (prefix)
    â”œâ”€ "5"
    â”œâ”€ "6"
    â”œâ”€ "7"
    â””â”€ "8"

Ã‰conomie : ~70% sur cet exemple
```

**Configuration** :
```yaml
storage:
  wiredTiger:
    indexConfig:
      prefixCompression: true  # DÃ©faut : true
```

**Impact** :
```javascript
// Mesure de l'efficacitÃ© prefix compression
function analyzeIndexCompression(collectionName) {
  const coll = db.getCollection(collectionName);
  const stats = coll.stats();

  const indexes = coll.getIndexes();

  indexes.forEach(index => {
    const indexStats = coll.aggregate([
      { $indexStats: {} },
      { $match: { name: index.name } }
    ]).next();

    if (indexStats) {
      print(`\nIndex: ${index.name}`);
      print(`  Keys: ${JSON.stringify(index.key)}`);
      print(`  Size: ${(indexStats.size / 1024 / 1024).toFixed(2)} MB`);

      // Estimation du ratio (approximatif)
      const avgKeySize = 50; // Estimation moyenne
      const expectedUncompressed = stats.count Ã— avgKeySize;
      const compressionRatio = expectedUncompressed / indexStats.size;
      print(`  Est. compression: ${compressionRatio.toFixed(2)}Ã—`);
    }
  });
}

analyzeIndexCompression("orders");
```

**Cas oÃ¹ dÃ©sactiver prefix compression** :
```javascript
// TrÃ¨s rare, seulement si :
// 1. Index sur donnÃ©es trÃ¨s alÃ©atoires (UUID)
// 2. Performance de write critique
// 3. Ratio de compression nÃ©gligeable

db.runCommand({
  createIndexes: "collection",
  indexes: [
    {
      key: { randomField: 1 },
      name: "random_idx",
      storageEngine: {
        wiredTiger: {
          configString: "prefix_compression=false"
        }
      }
    }
  ]
})

// Impact : +10-20% espace, +2-5% write performance
```

### Index Storage Size

```javascript
function compareIndexSizes() {
  const collections = db.getCollectionNames();

  collections.forEach(collName => {
    const stats = db.getCollection(collName).stats();

    const dataToIndexRatio = stats.totalIndexSize / stats.size;

    print(`\n${collName}:`);
    print(`  Data: ${(stats.size / 1024 / 1024).toFixed(2)} MB`);
    print(`  Indexes: ${(stats.totalIndexSize / 1024 / 1024).toFixed(2)} MB`);
    print(`  Ratio: ${(dataToIndexRatio * 100).toFixed(2)}%`);

    if (dataToIndexRatio > 0.5) {
      print(`  âš ï¸ Indexes > 50% of data size - Review index strategy`);
    }
  });
}

compareIndexSizes();
```

## Compression du Journal

### Configuration du Journal

```yaml
storage:
  wiredTiger:
    engineConfig:
      journalCompressor: snappy  # snappy (dÃ©faut) ou zlib
```

**Trade-offs** :

| Compressor | Ratio | CPU | Write Latency | Use Case |
|------------|-------|-----|---------------|----------|
| **snappy** | 2-3Ã— | Low | Baseline | **DÃ©faut recommandÃ©** |
| zlib | 3-5Ã— | Medium | +10-15% | Storage trÃ¨s limitÃ© |

**Impact du journal compression** :
```
Journal compressÃ© rÃ©duit :
- Taille des fichiers journal (100 MB â†’ 30-50 MB)
- I/O de write du journal
- Bande passante rÃ©seau pour rÃ©plication (oplog basÃ© sur journal)

Mais ajoute :
- CPU overhead sur writes
- LÃ©gÃ¨re augmentation de latency de write

Recommandation : Garder snappy (dÃ©faut optimal)
Changer vers zlib seulement si storage journal trÃ¨s limitÃ©
```

## Compression RÃ©seau

### Wire Protocol Compression

MongoDB 3.6+ supporte la compression du wire protocol.

**Configuration** :

```yaml
# mongod.conf
net:
  compression:
    compressors: snappy,zstd,zlib
```

**Driver configuration** (exemple Node.js) :

```javascript
const { MongoClient } = require('mongodb');

const client = new MongoClient(uri, {
  compressors: ['zstd', 'snappy', 'zlib'],  // Ordre de prÃ©fÃ©rence
  zlibCompressionLevel: 6  // 1-9, dÃ©faut 6
});
```

**NÃ©gociation** :
```
1. Client envoie liste de compressors supportÃ©s
2. Server rÃ©pond avec premier compressor commun
3. Toutes les communications suivantes utilisent ce compressor

Ordre de prÃ©fÃ©rence typique : zstd > snappy > zlib
```

**Impact de la compression rÃ©seau** :

```javascript
// Benchmark rÃ©seau
function benchmarkNetworkCompression() {
  // Sans compression
  const start1 = Date.now();
  const docs1 = db.largeCollection.find().limit(10000).toArray();
  const time1 = Date.now() - start1;

  // Avec compression (configurer dans driver)
  const start2 = Date.now();
  const docs2 = db.largeCollection.find().limit(10000).toArray();
  const time2 = Date.now() - start2;

  const improvement = ((time1 - time2) / time1 * 100).toFixed(2);

  print(`Without compression: ${time1}ms`);
  print(`With compression: ${time2}ms`);
  print(`Improvement: ${improvement}%`);
}

// RÃ©sultats typiques :
// - LAN (latency faible) : 5-15% improvement
// - WAN (latency Ã©levÃ©e) : 30-60% improvement
// - Large documents : Plus de gain
// - Small documents : Moins de gain (overhead)
```

**Recommandations** :

```yaml
# LAN (datacenter local, latency <1ms)
net:
  compression:
    compressors: snappy  # Overhead CPU minimal

# WAN (inter-rÃ©gions, latency >10ms)
net:
  compression:
    compressors: zstd,snappy  # Ratio important

# Mobile / Limited bandwidth
net:
  compression:
    compressors: zlib,zstd,snappy  # Ratio maximal
```

**Monitoring de la compression rÃ©seau** :

```javascript
function analyzeNetworkCompression() {
  const serverStatus = db.serverStatus();

  if (serverStatus.network && serverStatus.network.compression) {
    const comp = serverStatus.network.compression;

    const analysis = {
      compressor: comp.snappy ? "snappy" : (comp.zstd ? "zstd" : (comp.zlib ? "zlib" : "none")),

      // Bytes statistics
      bytesIn: {
        uncompressed: comp.snappy ? comp.snappy.compressor.bytesIn : 0,
        compressed: comp.snappy ? comp.snappy.compressor.bytesOut : 0
      },

      bytesOut: {
        uncompressed: comp.snappy ? comp.snappy.decompressor.bytesIn : 0,
        compressed: comp.snappy ? comp.snappy.decompressor.bytesOut : 0
      }
    };

    // Calculate ratios
    if (analysis.bytesIn.uncompressed > 0) {
      analysis.compressionRatioIn =
        (analysis.bytesIn.uncompressed / analysis.bytesIn.compressed).toFixed(2) + "Ã—";
    }

    if (analysis.bytesOut.uncompressed > 0) {
      analysis.compressionRatioOut =
        (analysis.bytesOut.uncompressed / analysis.bytesOut.compressed).toFixed(2) + "Ã—";
    }

    return analysis;
  } else {
    return { status: "Network compression not configured or not supported" };
  }
}

printjson(analyzeNetworkCompression());
```

## StratÃ©gies de Compression par Workload

### Read-Heavy Workload

**CaractÃ©ristiques** :
- 90% reads, 10% writes
- Latence critique : P99 < 50ms
- Cache hit ratio : >95%

**StratÃ©gie optimale** :

```yaml
# mongod.conf
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: snappy  # DÃ©compression rapide prioritaire
    indexConfig:
      prefixCompression: true
    engineConfig:
      journalCompressor: snappy

net:
  compression:
    compressors: snappy,zstd  # Snappy en premier pour latence
```

**Rationale** :
- DÃ©compression frÃ©quente (90% reads)
- snappy dÃ©compresse 2Ã— plus vite que zstd, 3Ã— que zlib
- Ratio 2-3Ã— suffisant si cache bien dimensionnÃ©
- Overhead CPU minimal

**MÃ©triques de validation** :
```javascript
const serverStatus = db.serverStatus();
const cacheHitRatio =
  ((serverStatus.wiredTiger.cache["pages requested from the cache"] -
    serverStatus.wiredTiger.cache["pages read into cache"]) /
   serverStatus.wiredTiger.cache["pages requested from the cache"] * 100).toFixed(2);

print(`Cache Hit Ratio: ${cacheHitRatio}%`);
// Si >95% : DÃ©compression limitÃ©e, snappy optimal
// Si <90% : ConsidÃ©rer zstd pour rÃ©duire I/O
```

### Write-Heavy Workload

**CaractÃ©ristiques** :
- 30% reads, 70% writes
- Throughput critique : >10K writes/sec
- Storage growth : Rapide

**StratÃ©gie optimale** :

```yaml
# mongod.conf
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zstd  # Bon ratio, compression rapide
    indexConfig:
      prefixCompression: true
    engineConfig:
      journalCompressor: snappy  # Journal : vitesse prioritaire

net:
  compression:
    compressors: zstd,snappy
```

**Rationale** :
- Compression frÃ©quente (70% writes)
- zstd : Meilleur ratio avec overhead CPU acceptable
- RÃ©duit write amplification (moins de bytes sur disque)
- Journal : snappy car writes synchrones

**Impact mesurÃ©** :
```
Benchmark : 100K inserts/sec

snappy:
- Storage: 180 GB
- CPU avg: 45%
- Write P99: 5.2ms

zstd:
- Storage: 120 GB (33% saved)
- CPU avg: 52% (+7%)
- Write P99: 6.1ms (+17%)

Conclusion : zstd optimal si storage coÃ»teux
            snappy optimal si latency critique
```

### Storage-Constrained Workload

**CaractÃ©ristiques** :
- Storage trÃ¨s limitÃ© ou trÃ¨s coÃ»teux
- Dataset volumineux (multi-TB)
- Performance acceptable si latence raisonnable

**StratÃ©gie optimale** :

```yaml
# mongod.conf
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zlib  # Ratio maximal
    indexConfig:
      prefixCompression: true
    engineConfig:
      journalCompressor: zlib  # MÃªme journal compressÃ© max

net:
  compression:
    compressors: zlib,zstd,snappy
```

**Rationale** :
- Minimiser coÃ»t de stockage
- Ratio 4-5Ã— vs 2-3Ã— = 40-50% de saving additionnel
- CPU overhead acceptable si pas ultra-latency-sensitive

**ROI calculation** :
```javascript
function calculateCompressionROI() {
  const datasetGB = 1000;  // 1 TB
  const storageCostPerGB = 0.10;  // $0.10/GB/month

  const scenarios = {
    snappy: {
      ratio: 2.5,
      cpuOverhead: 5,
      latencyOverhead: 0
    },
    zstd: {
      ratio: 4.0,
      cpuOverhead: 15,
      latencyOverhead: 10
    },
    zlib: {
      ratio: 4.5,
      cpuOverhead: 30,
      latencyOverhead: 20
    }
  };

  Object.keys(scenarios).forEach(compressor => {
    const s = scenarios[compressor];
    const storageGB = datasetGB / s.ratio;
    const monthlyCost = storageGB Ã— storageCostPerGB;
    const saving = ((datasetGB - storageGB) Ã— storageCostPerGB);

    print(`\n${compressor}:`);
    print(`  Storage: ${storageGB.toFixed(2)} GB`);
    print(`  Monthly cost: $${monthlyCost.toFixed(2)}`);
    print(`  Monthly saving vs uncompressed: $${saving.toFixed(2)}`);
    print(`  CPU overhead: +${s.cpuOverhead}%`);
    print(`  Latency overhead: +${s.latencyOverhead}%`);
  });
}

calculateCompressionROI();

// RÃ©sultat exemple :
// snappy : $40/month, saving $60
// zstd : $25/month, saving $75 (+$15 vs snappy)
// zlib : $22/month, saving $78 (+$18 vs snappy)
//
// Si +$15/month saving > coÃ»t CPU additionnel â†’ zstd/zlib optimal
```

### Analytics / Reporting Workload

**CaractÃ©ristiques** :
- Scans complets frÃ©quents
- AgrÃ©gations complexes
- Latence moins critique (seconds acceptable)

**StratÃ©gie optimale** :

```yaml
# mongod.conf
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zlib  # I/O rÃ©duits critiques
    indexConfig:
      prefixCompression: true
    engineConfig:
      journalCompressor: snappy

net:
  compression:
    compressors: zstd,zlib  # Large result sets
```

**Rationale** :
- Scans lisent tout le dataset â†’ I/O dominant
- zlib rÃ©duit I/O de 4-5Ã— vs aucune compression
- CPU overhead acceptable pour analytics
- RÃ©duction massive du temps de scan

**Mesure d'impact** :
```javascript
// Benchmark : Full collection scan
function benchmarkScanWithCompression(collectionName) {
  const coll = db.getCollection(collectionName);

  // Warm up
  coll.find().limit(1000).toArray();

  // Test scan
  const start = Date.now();
  const count = coll.find({}, { _id: 1 }).toArray().length;
  const duration = Date.now() - start;

  const stats = coll.stats();
  const storageGB = stats.storageSize / 1024 / 1024 / 1024;

  print(`Collection: ${collectionName}`);
  print(`Documents: ${count}`);
  print(`Storage: ${storageGB.toFixed(2)} GB`);
  print(`Scan time: ${duration}ms`);
  print(`Throughput: ${(count / duration * 1000).toFixed(0)} docs/sec`);

  // Estimation si non compressÃ©
  const uncompressedGB = stats.size / 1024 / 1024 / 1024;
  const ratio = uncompressedGB / storageGB;
  const estimatedTimeUncompressed = duration Ã— ratio;

  print(`\nWith current compression (${ratio.toFixed(2)}Ã—):`);
  print(`  Actual scan: ${duration}ms`);
  print(`Est. without compression:`);
  print(`  Would be: ${estimatedTimeUncompressed.toFixed(0)}ms`);
  print(`  Improvement: ${((estimatedTimeUncompressed - duration) / estimatedTimeUncompressed * 100).toFixed(2)}%`);
}

benchmarkScanWithCompression("largeAnalyticsCollection");

// RÃ©sultat typique :
// Avec zlib (4.5Ã—) : Scan en 45 secondes
// Sans compression : EstimÃ© 150 secondes
// AmÃ©lioration : 70%
```

## Impact Performance DÃ©taillÃ©

### CPU Overhead

**Mesure du CPU overhead** :

```javascript
function measureCompressionCPU() {
  // Baseline : Collection sans compression
  const baselineOps = 50000;

  print("Running baseline (no compression)...");
  const start1 = Date.now();
  for (let i = 0; i < baselineOps; i++) {
    db.testNoComp.insertOne({ data: "x".repeat(1000), index: i });
  }
  const time1 = Date.now() - start1;

  // Test : Collection avec compression
  print("Running with compression...");
  const start2 = Date.now();
  for (let i = 0; i < baselineOps; i++) {
    db.testWithComp.insertOne({ data: "x".repeat(1000), index: i });
  }
  const time2 = Date.now() - start2;

  const overhead = ((time2 - time1) / time1 * 100).toFixed(2);

  print(`\nResults:`);
  print(`No compression: ${time1}ms`);
  print(`With compression: ${time2}ms`);
  print(`CPU overhead: ${overhead}%`);
}

// RÃ©sultats typiques :
// snappy : +5-10% CPU
// zstd : +10-20% CPU
// zlib : +25-40% CPU
```

### I/O Reduction

**Mesure de la rÃ©duction I/O** :

```javascript
function measureIOReduction(collectionName) {
  const stats = db.getCollection(collectionName).stats();
  const wt = db.serverStatus().wiredTiger;

  // Bloc manager stats
  const bytesRead = wt.block_manager["bytes read"];
  const bytesWritten = wt.block_manager["bytes written"];

  const uncompressedSize = stats.size;
  const compressedSize = stats.storageSize;
  const ratio = uncompressedSize / compressedSize;

  print(`Collection: ${collectionName}`);
  print(`Compression ratio: ${ratio.toFixed(2)}Ã—`);
  print(`\nI/O Impact:`);
  print(`  Uncompressed size: ${(uncompressedSize / 1024 / 1024).toFixed(2)} MB`);
  print(`  Compressed on disk: ${(compressedSize / 1024 / 1024).toFixed(2)} MB`);
  print(`  I/O reduced by: ${((uncompressedSize - compressedSize) / 1024 / 1024).toFixed(2)} MB`);
  print(`  Reduction: ${((1 - 1/ratio) * 100).toFixed(2)}%`);

  // IOPS estimation
  const avgBlockSize = 16384;  // 16KB
  const blocksUncompressed = Math.ceil(uncompressedSize / avgBlockSize);
  const blocksCompressed = Math.ceil(compressedSize / avgBlockSize);

  print(`\nIOPS Impact (estimated):`);
  print(`  Blocks without compression: ${blocksUncompressed}`);
  print(`  Blocks with compression: ${blocksCompressed}`);
  print(`  IOPS saved: ${blocksUncompressed - blocksCompressed} (${((1 - blocksCompressed/blocksUncompressed) * 100).toFixed(2)}%)`);
}

measureIOReduction("orders");
```

### Latency Impact

**Impact sur read latency** :

```
DÃ©composition read latency :

Sans compression :
â”œâ”€ Disk I/O : 5 ms (lecture 100 KB)
â”œâ”€ Transfer to cache : 0.1 ms
â””â”€ Return to client : 0.1 ms
Total : 5.2 ms

Avec compression 4Ã— (snappy) :
â”œâ”€ Disk I/O : 1.5 ms (lecture 25 KB)
â”œâ”€ DÃ©compression : 0.3 ms
â”œâ”€ Transfer to cache : 0.1 ms
â””â”€ Return to client : 0.1 ms
Total : 2.0 ms

AmÃ©lioration : 61% plus rapide !

Note : BÃ©nÃ©fice si I/O > CPU
Sur NVMe (I/O <1ms), bÃ©nÃ©fice rÃ©duit
```

**Impact sur write latency** :

```
Sans compression :
â”œâ”€ Cache write : 0.1 ms
â”œâ”€ Disk I/O : 3 ms (Ã©criture 100 KB)
â””â”€ Journal write : 2 ms
Total : 5.1 ms

Avec compression 4Ã— (snappy) :
â”œâ”€ Compression : 0.4 ms
â”œâ”€ Cache write : 0.1 ms
â”œâ”€ Disk I/O : 1 ms (Ã©criture 25 KB)
â””â”€ Journal write : 0.7 ms
Total : 2.2 ms

AmÃ©lioration : 57% plus rapide !

Note : Sur SSD rapide, overhead compression peut dÃ©passer gain I/O
```

## Monitoring et MÃ©triques

### MÃ©triques ClÃ©s

```javascript
function compressionMetricsDashboard() {
  const collections = db.getCollectionNames();
  const serverStatus = db.serverStatus();

  const dashboard = {
    timestamp: new Date(),

    // Global storage
    globalStorage: {
      dataUncompressedGB: 0,
      dataCompressedGB: 0,
      indexSizeGB: 0,
      totalOnDiskGB: 0,
      overallCompressionRatio: 0
    },

    // Per-collection breakdown
    collections: [],

    // I/O metrics
    io: {
      bytesReadGB: (serverStatus.wiredTiger.block_manager["bytes read"] / 1024 / 1024 / 1024).toFixed(2),
      bytesWrittenGB: (serverStatus.wiredTiger.block_manager["bytes written"] / 1024 / 1024 / 1024).toFixed(2),
      blocksRead: serverStatus.wiredTiger.block_manager["blocks read"],
      blocksWritten: serverStatus.wiredTiger.block_manager["blocks written"]
    }
  };

  // Aggregate collection stats
  collections.forEach(collName => {
    const stats = db.getCollection(collName).stats();

    const uncompressedGB = stats.size / 1024 / 1024 / 1024;
    const compressedGB = stats.storageSize / 1024 / 1024 / 1024;
    const indexGB = stats.totalIndexSize / 1024 / 1024 / 1024;
    const ratio = stats.size / stats.storageSize;

    dashboard.globalStorage.dataUncompressedGB += uncompressedGB;
    dashboard.globalStorage.dataCompressedGB += compressedGB;
    dashboard.globalStorage.indexSizeGB += indexGB;
    dashboard.globalStorage.totalOnDiskGB += compressedGB + indexGB;

    dashboard.collections.push({
      name: collName,
      uncompressedGB: uncompressedGB.toFixed(2),
      compressedGB: compressedGB.toFixed(2),
      indexGB: indexGB.toFixed(2),
      ratio: ratio.toFixed(2) + "Ã—",
      savedGB: (uncompressedGB - compressedGB).toFixed(2)
    });
  });

  // Calculate overall ratio
  dashboard.globalStorage.overallCompressionRatio =
    (dashboard.globalStorage.dataUncompressedGB /
     dashboard.globalStorage.dataCompressedGB).toFixed(2) + "Ã—";

  // Format global storage
  dashboard.globalStorage.dataUncompressedGB =
    dashboard.globalStorage.dataUncompressedGB.toFixed(2);
  dashboard.globalStorage.dataCompressedGB =
    dashboard.globalStorage.dataCompressedGB.toFixed(2);
  dashboard.globalStorage.indexSizeGB =
    dashboard.globalStorage.indexSizeGB.toFixed(2);
  dashboard.globalStorage.totalOnDiskGB =
    dashboard.globalStorage.totalOnDiskGB.toFixed(2);

  return dashboard;
}

const dashboard = compressionMetricsDashboard();
printjson(dashboard);

// Export pour monitoring externe
// print(JSON.stringify(dashboard));
```

### Alerting

**Prometheus-style alerting rules** :

```yaml
# Compression ratio trop faible
- alert: LowCompressionRatio
  expr: |
    mongodb_collection_compression_ratio < 1.5
  for: 1h
  labels:
    severity: info
  annotations:
    summary: "Low compression ratio detected"
    description: "Collection {{ $labels.collection }} has compression ratio < 1.5Ã—"

# Storage growth anormal
- alert: HighStorageGrowth
  expr: |
    rate(mongodb_storage_size_bytes[24h]) > 10737418240  # 10 GB/day
  labels:
    severity: warning
  annotations:
    summary: "High storage growth rate"
    description: "Storage growing > 10 GB/day"

# Overhead CPU compression Ã©levÃ©
- alert: HighCompressionCPU
  expr: |
    rate(mongodb_compression_cpu_time_seconds[5m]) > 0.5
  for: 15m
  labels:
    severity: warning
  annotations:
    summary: "High CPU time spent on compression"
```

## StratÃ©gies de Migration

### Migration vers un Nouveau Compressor

**Approche Progressive (RecommandÃ©e)** :

```javascript
// Ã‰tape 1 : CrÃ©er nouvelle collection avec nouveau compressor
db.createCollection("orders_new", {
  storageEngine: {
    wiredTiger: {
      configString: "block_compressor=zstd"
    }
  }
});

// Ã‰tape 2 : Copier les index
db.orders.getIndexes().forEach(function(index) {
  var key = index.key;
  delete index.key;
  delete index.ns;
  delete index.v;
  db.orders_new.createIndex(key, index);
});

// Ã‰tape 3 : Migration des donnÃ©es (par batch)
var batchSize = 10000;
var cursor = db.orders.find().noCursorTimeout();
var batch = [];

while (cursor.hasNext()) {
  batch.push(cursor.next());

  if (batch.length >= batchSize) {
    db.orders_new.insertMany(batch, { ordered: false });
    batch = [];
    print("Migrated " + db.orders_new.countDocuments() + " documents");
  }
}

// Dernier batch
if (batch.length > 0) {
  db.orders_new.insertMany(batch, { ordered: false });
}

cursor.close();

// Ã‰tape 4 : Validation
var oldCount = db.orders.countDocuments();
var newCount = db.orders_new.countDocuments();
print(`Old collection: ${oldCount} documents`);
print(`New collection: ${newCount} documents`);
assert(oldCount === newCount, "Document count mismatch!");

// Ã‰tape 5 : Comparaison de taille
var oldStats = db.orders.stats();
var newStats = db.orders_new.stats();
print("\nCompression comparison:");
print(`Old (snappy): ${(oldStats.storageSize / 1024 / 1024).toFixed(2)} MB`);
print(`New (zstd): ${(newStats.storageSize / 1024 / 1024).toFixed(2)} MB`);
print(`Saving: ${((oldStats.storageSize - newStats.storageSize) / oldStats.storageSize * 100).toFixed(2)}%`);

// Ã‰tape 6 : Cutover (downtime minimal)
// 1. Stop writes to old collection
// 2. Migrate delta since start
// 3. Rename collections
db.orders.renameCollection("orders_old");
db.orders_new.renameCollection("orders");

// Ã‰tape 7 : Validation post-cutover
// Test queries, verify performance

// Ã‰tape 8 : Drop old collection (aprÃ¨s validation)
// db.orders_old.drop();
```

**Approche Blue-Green** :

```javascript
// Configuration replica set pour migration
// Primary : Ancienne config (snappy)
// New Secondary : Nouvelle config (zstd)

// 1. Ajouter nouveau secondary avec nouveau compressor
rs.add({
  host: "mongo4.example.com:27017",
  priority: 0,
  votes: 0
})

// 2. Sur mongo4, configurer nouveau compressor
// mongod.conf sur mongo4 :
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zstd

// 3. Initial sync (peut prendre heures/jours)
// Monitor : rs.status()

// 4. Une fois sync complÃ¨te, comparer tailles
db.adminCommand({ serverStatus: 1 }).wiredTiger

// 5. Step down primary
rs.stepDown(120)

// 6. Nouveau primary avec zstd devient actif
// 7. Autres membres initial sync et obtiennent zstd
```

### A/B Testing de Compresseurs

```javascript
// Framework pour A/B test
class CompressionABTest {
  constructor(sourceCollection, testDuration = 3600) {
    this.source = sourceCollection;
    this.duration = testDuration;
    this.results = {
      snappy: { collection: sourceCollection + "_test_snappy" },
      zstd: { collection: sourceCollection + "_test_zstd" },
      zlib: { collection: sourceCollection + "_test_zlib" }
    };
  }

  async setup() {
    // CrÃ©er collections de test avec diffÃ©rents compressors
    for (const [compressor, config] of Object.entries(this.results)) {
      db.createCollection(config.collection, {
        storageEngine: {
          wiredTiger: {
            configString: `block_compressor=${compressor}`
          }
        }
      });

      // Copier sample de donnÃ©es
      const sample = db.getCollection(this.source)
        .aggregate([{ $sample: { size: 100000 } }])
        .toArray();

      db.getCollection(config.collection).insertMany(sample);
    }
  }

  async runBenchmark() {
    for (const [compressor, config] of Object.entries(this.results)) {
      const coll = db.getCollection(config.collection);

      // Mesures
      const startTime = Date.now();

      // Read test
      const readStart = Date.now();
      coll.find().limit(10000).toArray();
      config.readLatency = Date.now() - readStart;

      // Write test
      const writeStart = Date.now();
      for (let i = 0; i < 1000; i++) {
        coll.insertOne({ test: i, data: "x".repeat(1000) });
      }
      config.writeLatency = Date.now() - writeStart;

      // Storage
      const stats = coll.stats();
      config.storageSize = stats.storageSize;
      config.compressionRatio = (stats.size / stats.storageSize).toFixed(2);
    }
  }

  async cleanup() {
    for (const config of Object.values(this.results)) {
      db.getCollection(config.collection).drop();
    }
  }

  getResults() {
    return this.results;
  }
}

// Usage
const test = new CompressionABTest("orders");
await test.setup();
await test.runBenchmark();
printjson(test.getResults());
await test.cleanup();
```

## Best Practices

### Checklist de Configuration

```
â˜ Analyse du type de donnÃ©es
  â˜ Identifier donnÃ©es hautement compressibles (text, rÃ©pÃ©titif)
  â˜ Identifier donnÃ©es peu compressibles (binary, random)
  â˜ Mesurer distribution des types

â˜ DÃ©finir prioritÃ©s
  â˜ Latence vs Storage cost
  â˜ CPU disponible
  â˜ I/O capabilities

â˜ Choisir compresseurs
  â˜ Collection : snappy (latence) | zstd (balance) | zlib (ratio)
  â˜ Index : prefix compression enabled (dÃ©faut)
  â˜ Journal : snappy (recommandÃ©)
  â˜ Network : selon WAN/LAN

â˜ Tester en staging
  â˜ Benchmark avec donnÃ©es rÃ©elles
  â˜ Mesurer latency P99
  â˜ Mesurer CPU usage
  â˜ Mesurer storage savings

â˜ Monitoring production
  â˜ Compression ratio par collection
  â˜ Storage growth rate
  â˜ CPU overhead
  â˜ Latency impact

â˜ Optimisation continue
  â˜ Review trimestriel
  â˜ A/B testing de nouveaux compressors
  â˜ Ajustement selon Ã©volution workload
```

### DÃ©cision Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario                â”‚ snappy   â”‚ zstd     â”‚ zlib     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Low latency required    â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜†â˜†    â”‚ â˜…â˜†â˜†â˜†â˜†    â”‚
â”‚ Storage constrained     â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚
â”‚ CPU limited             â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜†â˜†    â”‚ â˜…â˜†â˜†â˜†â˜†    â”‚
â”‚ Write heavy             â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚
â”‚ Read heavy              â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚
â”‚ Analytics workload      â”‚ â˜…â˜…â˜†â˜†â˜†    â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚
â”‚ General purpose         â”‚ â˜…â˜…â˜…â˜…â˜†    â”‚ â˜…â˜…â˜…â˜…â˜…    â”‚ â˜…â˜…â˜…â˜†â˜†    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Recommendation :
- DÃ©faut 2020-2024 : snappy
- DÃ©faut 2025+ : zstd (meilleur compromis)
- Storage critique : zlib
- Ultra low-latency : none (rare)
```

### Erreurs Courantes

```javascript
// âŒ ERREUR 1 : Compression uniforme
// Appliquer mÃªme compressor Ã  toutes les collections
storage:
  wiredTiger:
    collectionConfig:
      blockCompressor: zlib  // Trop agressif pour tout

// âœ… CORRECT : StratÃ©gie par collection
db.createCollection("realtime", {
  storageEngine: { wiredTiger: { configString: "block_compressor=snappy" } }
});
db.createCollection("analytics", {
  storageEngine: { wiredTiger: { configString: "block_compressor=zlib" } }
});

// âŒ ERREUR 2 : Ignorer le monitoring
// Configurer compression puis oublier

// âœ… CORRECT : Monitoring continu
// Alerter si compression ratio < 1.5Ã— (donnÃ©es possiblement prÃ©-compressÃ©es)
// Review mensuel des mÃ©triques

// âŒ ERREUR 3 : Migration sans test
// Changer compressor directement en production

// âœ… CORRECT : Test puis migration progressive
// 1. A/B test en staging
// 2. Mesurer impact
// 3. Migration collection par collection
// 4. Validation Ã  chaque Ã©tape

// âŒ ERREUR 4 : Ignorer network compression
// Oublier de configurer compression rÃ©seau

// âœ… CORRECT : Activer network compression
net:
  compression:
    compressors: zstd,snappy  // Surtout pour WAN
```

## Conclusion

La compression des donnÃ©es dans MongoDB offre des bÃ©nÃ©fices significatifs :

**Gains mesurables** :
- Storage : 50-80% de rÃ©duction (ratio 2-5Ã—)
- I/O : 50-75% de rÃ©duction
- Network : 30-70% de rÃ©duction (selon latency)
- CoÃ»ts : 40-70% d'Ã©conomies storage

**Trade-offs** :
- CPU : +5-30% selon compressor
- Latency : -20% Ã  +20% selon I/O/CPU balance
- ComplexitÃ© : Configuration et monitoring additionnels

**Recommandations clÃ©s** :
1. **zstd pour nouveaux dÃ©ploiements** (meilleur compromis 2025+)
2. **snappy pour ultra low-latency** (<5ms P99)
3. **zlib pour storage critique** (coÃ»ts Ã©levÃ©s)
4. **none seulement si justifiÃ©** (latence <1ms ou data prÃ©-compressÃ©e)

**MÃ©thodologie** :
1. Analyser les donnÃ©es (type, compressibilitÃ©)
2. DÃ©finir prioritÃ©s (latence, storage, CPU)
3. Tester avec donnÃ©es rÃ©elles
4. DÃ©ployer progressivement
5. Monitorer et ajuster

La compression n'est pas une configuration "set and forget". Elle nÃ©cessite un monitoring continu et des ajustements rÃ©guliers basÃ©s sur l'Ã©volution du workload et des technologies (nouveaux compresseurs, hardware plus rapide).

---

**Points clÃ©s Ã  retenir :**
- zstd est le meilleur choix moderne (ratio + performance)
- snappy reste optimal pour latence critique
- Compression = Storage savings + I/O reduction - CPU overhead
- Tester avant dÃ©ploiement (impact performance variable)
- Monitoring continu essentiel (ratio, CPU, latency)
- StratÃ©gie diffÃ©rente par collection selon workload
- Network compression crucial pour WAN
- Migration progressive nÃ©cessaire pour changement de compressor

â­ï¸ [Read/Write splitting](/17-performance-tuning/10-read-write-splitting.md)
