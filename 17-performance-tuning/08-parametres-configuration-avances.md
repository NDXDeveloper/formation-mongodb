üîù Retour au [Sommaire](/SOMMAIRE.md)

# 17.8 Param√®tres de Configuration Avanc√©s

## Introduction

Au-del√† du dimensionnement mat√©riel et de l'optimisation des requ√™tes, les param√®tres de configuration syst√®me et MongoDB jouent un r√¥le crucial dans les performances en production. Une configuration optimale peut am√©liorer les performances de 20-50% et pr√©venir des probl√®mes critiques de stabilit√© et de scalabilit√©.

Cette section explore les param√®tres avanc√©s de MongoDB et du syst√®me d'exploitation, leurs impacts sur les performances, et les m√©thodologies pour les optimiser selon diff√©rents profils de charge. Une configuration incorrecte peut d√©grader les performances ou causer des instabilit√©s subtiles difficiles √† diagnostiquer.

## Architecture de Configuration MongoDB

### Hi√©rarchie de Configuration

MongoDB utilise plusieurs sources de configuration avec ordre de priorit√© :

```
1. Command-line options (--parameter)
   ‚Üì (override)
2. Configuration file (mongod.conf)
   ‚Üì (override)
3. Runtime parameters (setParameter)
   ‚Üì (override)
4. Default values (compiled-in)
```

**Persistance** :
- Command-line : Non persistant (red√©marrage requis)
- Configuration file : Persistant
- Runtime parameters : Non persistant (sauf si setParameter: { persist: true })

### Structure mongod.conf

```yaml
# /etc/mongod.conf - Structure compl√®te

# Processus et systemd
processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongod.pid
  timeZoneInfo: /usr/share/zoneinfo

# R√©seau
net:
  port: 27017
  bindIp: 0.0.0.0
  maxIncomingConnections: 65536
  ipv6: false
  compression:
    compressors: snappy,zstd,zlib

# S√©curit√©
security:
  authorization: enabled
  keyFile: /etc/mongodb/keyfile

# Stockage
storage:
  dbPath: /var/lib/mongodb
  journal:
    enabled: true
    commitIntervalMs: 100
  directoryPerDB: true
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 20
      journalCompressor: snappy
      directoryForIndexes: false
    collectionConfig:
      blockCompressor: zstd
    indexConfig:
      prefixCompression: true

# Op√©rations
operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 100
  slowOpSampleRate: 1.0

# R√©plication
replication:
  oplogSizeMB: 51200
  replSetName: rs0
  enableMajorityReadConcern: true

# Sharding
sharding:
  clusterRole: shardsvr

# Logging
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  logRotate: reopen
  verbosity: 0
  component:
    accessControl:
      verbosity: 0
    command:
      verbosity: 0

# SetParameter
setParameter:
  enableLocalhostAuthBypass: false
  authenticationMechanisms: SCRAM-SHA-256
```

## Param√®tres de Performance Critiques

### Connexions et Concurrence

#### maxIncomingConnections

**Description** : Nombre maximum de connexions simultan√©es accept√©es.

```yaml
net:
  maxIncomingConnections: 65536  # D√©faut : 65536 (ou ulimit)
```

**Calcul optimal** :
```
maxIncomingConnections = (Expected Peak Connections √ó 1.5) + Reserved

O√π :
- Expected Peak : Connexions applicatives + admin + monitoring
- 1.5 : Marge pour pics
- Reserved : 50-100 pour admin/monitoring

Exemple :
- Application : 2000 connexions peak
- Monitoring : 20
- Admin : 10
- Calcul : (2000 √ó 1.5) + 30 = 3030
‚Üí Configurer : 4000
```

**Impact sur les ressources** :
```javascript
// Chaque connexion consomme :
// - Thread : 1 (OS thread)
// - RAM : 1-2 MB (stack + buffers)
// - File descriptors : 1-3

// Exemple avec 4000 connexions :
// - Threads : 4000
// - RAM : 4-8 GB
// - FDs : 4000-12000

// V√©rifier limites OS
db.serverStatus().connections
{
  current: 2345,
  available: 1655,
  totalCreated: 45678,
  active: 892,
  threaded: 2345
}
```

**Probl√®mes courants** :
```javascript
// Connection exhaustion
if (serverStatus.connections.available < 100) {
  print("WARNING: Less than 100 connections available");
  print("Consider:");
  print("1. Increase maxIncomingConnections");
  print("2. Optimize connection pooling in apps");
  print("3. Investigate connection leaks");
}

// Trop de connexions idle
const idleRatio = (serverStatus.connections.current - serverStatus.connections.active)
                  / serverStatus.connections.current;
if (idleRatio > 0.7) {
  print("INFO: >70% connections idle");
  print("Consider reducing pool size in applications");
}
```

#### Connection Pool (Application Side)

```javascript
// Configuration driver (exemple Node.js)
const client = new MongoClient(uri, {
  maxPoolSize: 100,          // Maximum connections dans le pool
  minPoolSize: 10,           // Minimum connections maintenues
  maxIdleTimeMS: 300000,     // 5 minutes idle avant fermeture
  waitQueueTimeoutMS: 10000, // Timeout si pool exhausted
  serverSelectionTimeoutMS: 30000
});

// Calcul optimal poolSize par instance app :
// maxPoolSize = (Expected Concurrent Ops √ó 1.2) / Number of App Instances

// Exemple :
// - 1000 ops/sec concurrent
// - 10 app instances
// - maxPoolSize = (1000 √ó 1.2) / 10 = 120 par instance
```

### Ticket System (Concurrency Control)

WiredTiger utilise un syst√®me de tickets pour contr√¥ler la concurrence.

**Configuration** :
```javascript
// Param√®tres par d√©faut (MongoDB 4.0+)
db.adminCommand({
  setParameter: 1,
  wiredTigerConcurrentReadTransactions: 128,  // Read tickets
  wiredTigerConcurrentWriteTransactions: 128  // Write tickets
})

// MongoDB 3.x avait des valeurs plus basses (128 par d√©faut)
// MongoDB 4.0+ : Valeurs dynamiques bas√©es sur CPU cores
```

**Calcul automatique** :
```
MongoDB 4.0+ :
Read tickets = 128 (fixe)
Write tickets = 128 (fixe)

MongoDB 5.0+ consid√®re les cores mais garde 128 comme optimal
```

**Monitoring des tickets** :
```javascript
function analyzeTicketUtilization() {
  const serverStatus = db.serverStatus();
  const wt = serverStatus.wiredTiger;

  const analysis = {
    readTickets: {
      available: wt.concurrentTransactions.read.available,
      out: wt.concurrentTransactions.read.out,
      totalTickets: wt.concurrentTransactions.read.totalTickets,
      utilization: ((wt.concurrentTransactions.read.out /
                     wt.concurrentTransactions.read.totalTickets) * 100).toFixed(2) + "%"
    },

    writeTickets: {
      available: wt.concurrentTransactions.write.available,
      out: wt.concurrentTransactions.write.out,
      totalTickets: wt.concurrentTransactions.write.totalTickets,
      utilization: ((wt.concurrentTransactions.write.out /
                     wt.concurrentTransactions.write.totalTickets) * 100).toFixed(2) + "%"
    }
  };

  // Assessment
  const issues = [];

  if (wt.concurrentTransactions.read.available < 10) {
    issues.push("‚ö†Ô∏è Low read tickets available - potential queueing");
  }

  if (wt.concurrentTransactions.write.available < 10) {
    issues.push("‚ö†Ô∏è Low write tickets available - potential queueing");
  }

  analysis.assessment = issues.length === 0 ?
    "‚úÖ Ticket system healthy" : issues;

  return analysis;
}

printjson(analyzeTicketUtilization());
```

**Tuning tickets** (Rare) :
```javascript
// Augmenter seulement si :
// 1. Tickets constamment exhausted (available = 0)
// 2. Queue depth √©lev√© persistant
// 3. Serveur sous-utilis√© (CPU < 50%)

// Augmentation progressive
db.adminCommand({
  setParameter: 1,
  wiredTigerConcurrentReadTransactions: 256,  // Doubled
  wiredTigerConcurrentWriteTransactions: 256
})

// Monitoring apr√®s changement :
// - CPU usage (devrait augmenter)
// - Latency (devrait baisser si CPU available)
// - Throughput (devrait augmenter)

// ATTENTION : Trop de tickets = CPU thrashing
// Ne jamais d√©passer 512 sans testing approfondi
```

### Journal et Durabilit√©

#### commitIntervalMs

**Impact** : Fr√©quence de flush du journal sur disque.

```yaml
storage:
  journal:
    commitIntervalMs: 100  # 50-500ms
```

**Trade-offs par valeur** :

| Valeur | Durabilit√© | Write Latency | Throughput | Use Case |
|--------|------------|---------------|------------|----------|
| 50ms | Excellent | +20% | -15% | Financial, critical data |
| 100ms | Bon | Baseline | Baseline | **D√©faut recommand√©** |
| 200ms | Acceptable | -15% | +20% | Write-heavy, tol√®re perte 200ms |
| 500ms | Risqu√© | -30% | +40% | Non recommand√© production |

**Combinaison avec Write Concern** :
```javascript
// j:true force flush imm√©diat (ignore commitIntervalMs)
db.collection.insertOne(
  { data: "critical" },
  { writeConcern: { w: "majority", j: true } }
)
// Latency : +5-10ms mais garantie durabilit√© maximale

// j:false (default) respecte commitIntervalMs
db.collection.insertOne(
  { data: "non-critical" },
  { writeConcern: { w: 1, j: false } }
)
// Latency minimale, perte possible en cas de crash
```

#### directoryPerDB et directoryForIndexes

**directoryPerDB** :
```yaml
storage:
  directoryPerDB: true  # Une directory par database
```

Avantages :
- Organisation claire
- Backup par database facilit√©
- Peut utiliser diff√©rents filesystems par database

Inconv√©nients :
- Plus de file descriptors
- Fragmentation filesystem possible

**directoryForIndexes** :
```yaml
storage:
  wiredTiger:
    engineConfig:
      directoryForIndexes: true  # S√©pare index des donn√©es
```

Avantages :
- Peut placer index sur storage plus rapide
- Meilleure organisation
- Debug facilit√©

Inconv√©nients :
- Complexit√© accrue
- N√©cessite configuration filesystem

**Configuration optimale pour I/O** :
```yaml
# Sc√©nario : NVMe pour index, SATA SSD pour data
storage:
  dbPath: /data/mongodb  # SATA SSD
  directoryPerDB: true
  wiredTiger:
    engineConfig:
      directoryForIndexes: true

# Puis cr√©er symlink des index vers NVMe
# ln -s /nvme/mongodb/mydb/index /data/mongodb/mydb/index
```

### Oplog Configuration

#### oplogSizeMB

**Calcul optimal** :
```javascript
// Formule
oplogSizeMB = Write Rate (MB/h) √ó Window (hours) √ó Safety Factor

// Exemple
const writeRateMBPerHour = 2048;  // 2 GB/h
const windowHours = 48;           // 48h window target
const safetyFactor = 1.5;

const oplogSize = writeRateMBPerHour √ó windowHours √ó safetyFactor;
// = 2048 √ó 48 √ó 1.5 = 147,456 MB = 144 GB

// Configuration
replication:
  oplogSizeMB: 147456
```

**Validation** :
```javascript
function analyzeOplogCapacity() {
  const oplogs = db.getSiblingDB("local").oplog.rs;
  const oplogStats = oplogs.stats();

  // Window actuelle
  const firstEntry = oplogs.find().sort({$natural: 1}).limit(1).next();
  const lastEntry = oplogs.find().sort({$natural: -1}).limit(1).next();
  const windowHours = (lastEntry.ts.getTime() - firstEntry.ts.getTime()) / 3600000;

  // Taux de remplissage
  const oplogSizeGB = oplogStats.maxSize / 1024 / 1024 / 1024;
  const oplogUsedGB = oplogStats.size / 1024 / 1024 / 1024;
  const fillRate = (oplogUsedGB / oplogSizeGB * 100).toFixed(2);

  // Write rate
  const writeMBPerHour = (oplogUsedGB √ó 1024) / windowHours;

  const analysis = {
    oplogSizeGB: oplogSizeGB.toFixed(2),
    currentWindowHours: windowHours.toFixed(2),
    fillRate: fillRate + "%",
    writeMBPerHour: writeMBPerHour.toFixed(2),

    // Projections
    timeToFull: ((oplogSizeGB - oplogUsedGB) / (writeMBPerHour / 1024)).toFixed(2) + " hours",

    recommendation: ""
  };

  if (windowHours < 24) {
    analysis.recommendation = "‚ö†Ô∏è Increase oplog - window < 24h";
  } else if (windowHours < 48) {
    analysis.recommendation = "Consider increasing oplog to 48h+ window";
  } else {
    analysis.recommendation = "‚úÖ Oplog size adequate";
  }

  return analysis;
}

printjson(analyzeOplogCapacity());
```

**Redimensionnement de l'oplog** :
```javascript
// MongoDB 4.0+ : Redimensionnement online
db.adminCommand({
  replSetResizeOplog: 1,
  size: 147456  // Nouvelle taille en MB
})

// Pr√©-4.0 : N√©cessite shutdown et rebuild
// 1. Backup
// 2. Shutdown secondary
// 3. D√©marrer en standalone
// 4. Recr√©er oplog avec nouvelle taille
// 5. Red√©marrer en replica set
// 6. R√©p√©ter pour chaque membre
```

## Param√®tres de Profiling et Monitoring

### Operation Profiling

```yaml
operationProfiling:
  mode: slowOp           # off | slowOp | all
  slowOpThresholdMs: 100 # Seuil en ms
  slowOpSampleRate: 1.0  # 0.0-1.0 (100% = tout profiler)
  filter: '{ op: { $in: ["query", "update", "remove"] } }'
```

**Configuration par environnement** :

```yaml
# Production
operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 100
  slowOpSampleRate: 0.1   # 10% sampling pour r√©duire overhead

# Staging / Debug
operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 50
  slowOpSampleRate: 1.0   # 100% pour debugging

# Performance testing
operationProfiling:
  mode: all               # Tous les ops
  slowOpThresholdMs: 0
  slowOpSampleRate: 1.0
```

**Runtime adjustment** :
```javascript
// Activer temporairement profiling complet
db.setProfilingLevel(2, { slowms: 0, sampleRate: 1.0 })

// Apr√®s 5 minutes, revenir √† slowOp
db.setProfilingLevel(1, { slowms: 100, sampleRate: 0.1 })

// Analyse des ops
db.system.profile.find().sort({ ts: -1 }).limit(10).pretty()
```

### Logging Verbosity

```yaml
systemLog:
  verbosity: 0  # 0-5, 0 = minimal
  component:
    accessControl:
      verbosity: 1    # Auth logs
    command:
      verbosity: 0
    control:
      verbosity: 0
    ftdc:
      verbosity: 0
    geo:
      verbosity: 0
    index:
      verbosity: 0
    network:
      verbosity: 0
    query:
      verbosity: 1    # Query logs pour debugging
    replication:
      verbosity: 0
    sharding:
      verbosity: 0
    storage:
      verbosity: 0
    write:
      verbosity: 0
    transaction:
      verbosity: 0
```

**Debugging specific components** :
```javascript
// Augmenter verbosity temporairement
db.adminCommand({
  setParameter: 1,
  logComponentVerbosity: {
    query: { verbosity: 2 },
    replication: { verbosity: 1 }
  }
})

// Apr√®s investigation
db.adminCommand({
  setParameter: 1,
  logComponentVerbosity: {
    query: { verbosity: 0 },
    replication: { verbosity: 0 }
  }
})
```

**Log rotation** :
```yaml
systemLog:
  logRotate: reopen  # rename | reopen

# reopen : Compatible avec logrotate
# rename : MongoDB g√®re la rotation (limite √† 10 fichiers)
```

Configuration logrotate :
```bash
# /etc/logrotate.d/mongodb
/var/log/mongodb/*.log {
  daily
  rotate 7
  compress
  delaycompress
  notifempty
  create 0640 mongodb mongodb
  sharedscripts
  postrotate
    /bin/kill -SIGUSR1 $(cat /var/run/mongodb/mongod.pid)
  endscript
}
```

## Param√®tres Runtime (setParameter)

### Param√®tres Persistants

MongoDB 4.2+ supporte setParameter persistant :

```javascript
// Persistent (survit au red√©marrage)
db.adminCommand({
  setParameter: 1,
  internalQueryExecMaxBlockingSortBytes: 335544320,  // 320 MB
  persist: true
})

// V√©rifier la persistance
db.adminCommand({ getParameter: "*" })
```

### Param√®tres de Query Execution

#### internalQueryExecMaxBlockingSortBytes

**Description** : M√©moire maximale pour in-memory sorts.

```javascript
// D√©faut : 33554432 (32 MB)
// Recommand√© pour analytics : 100-500 MB

db.adminCommand({
  setParameter: 1,
  internalQueryExecMaxBlockingSortBytes: 335544320,  // 320 MB
  persist: true
})

// Impact :
// - Sorts plus volumineux possibles sans allowDiskUse
// - Mais risque OOM si trop √©lev√© et nombreux sorts concurrents

// Calcul optimal :
// maxSortMemory = Available RAM / Expected Concurrent Sorts / 2

// Exemple :
// - RAM disponible : 64 GB
// - Sorts concurrents : 50
// - maxSortMemory = (64 √ó 1024) / 50 / 2 = 655 MB
```

#### internalQueryPlannerMaxIndexedSolutions

**Description** : Nombre max de plans d'index consid√©r√©s par le query planner.

```javascript
// D√©faut : 64
// Augmenter si beaucoup d'index et queries complexes

db.adminCommand({
  setParameter: 1,
  internalQueryPlannerMaxIndexedSolutions: 128
})

// Impact :
// + : Meilleur plan trouv√© si nombreux index
// - : Planning time augmente
```

#### internalQueryExecYieldIterations

**Description** : Nombre d'it√©rations avant yielding.

```javascript
// D√©faut : 128
// Yielding permet √† d'autres op√©rations de s'ex√©cuter

db.adminCommand({
  setParameter: 1,
  internalQueryExecYieldIterations: 64  // Plus de yielding
})

// Augmenter (256, 512) si :
// - Queries rapides dominant
// - Contention faible

// Diminuer (32, 64) si :
// - Queries longues monopolisent
// - Contention √©lev√©e
```

### Param√®tres de R√©plication

#### replWriterThreadCount

**Description** : Nombre de threads pour appliquer l'oplog sur secondaries.

```javascript
// D√©faut : 16 (ajust√© automatiquement)
// Range : 1-256

db.adminCommand({
  setParameter: 1,
  replWriterThreadCount: 32
})

// Calcul :
// replWriterThreadCount = min(Cores / 2, 32)

// Exemple : 48 cores
// = min(24, 32) = 24

// Augmenter si :
// - Replication lag sur secondaries
// - Write heavy workload
// - Secondaries ont CPU disponible

// Ne pas augmenter excessivement :
// - Overhead de coordination
// - Rendements d√©croissants au-del√† de 32
```

#### replBatchLimitBytes

**Description** : Taille max d'un batch d'oplog.

```javascript
// D√©faut : 100 MB

db.adminCommand({
  setParameter: 1,
  replBatchLimitBytes: 209715200  // 200 MB
})

// Augmenter pour :
// - R√©duire le nombre de batches
// - Am√©liorer throughput si bandwidth √©lev√©

// Attention :
// - Plus de m√©moire utilis√©e
// - Latency accrue si batch trop gros
```

### Param√®tres de Sharding

#### chunkSize

**Description** : Taille des chunks en MB.

```javascript
// D√©faut : 64 MB depuis MongoDB 3.4
// Range : 1-1024 MB

// Sur mongos ou config server
db.getSiblingDB("config").settings.updateOne(
  { _id: "chunksize" },
  { $set: { value: 128 } },
  { upsert: true }
)

// Calcul optimal :
// chunkSize = Dataset per Shard / Target Chunks per Shard

// Exemple :
// - 1 TB par shard
// - Target : 10,000 chunks
// - chunkSize = 1024 GB / 10000 = 102 MB ‚Üí 128 MB

// Augmenter (128-256 MB) si :
// - Large documents
// - Migration overhead trop √©lev√©

// Diminuer (32-64 MB) si :
// - Small documents
// - Meilleure distribution n√©cessaire
// - Jumbo chunks probl√®me
```

#### mongos Query Parameters

```javascript
// Sur mongos
db.adminCommand({
  setParameter: 1,

  // Timeout pour queries scatter-gather
  cursorTimeoutMillis: 600000,  // 10 minutes

  // Taille max du batch retourn√©
  internalQueryExecYieldPeriodMS: 10,

  // Connection pool vers shards
  ShardingTaskExecutorPoolMaxSize: 500
})
```

### Param√®tres de S√©curit√©

#### maxSessionsPerUser

**Description** : Sessions max par user (MongoDB 3.6+).

```javascript
db.adminCommand({
  setParameter: 1,
  maxSessions: 1000000,        // Global max sessions
  maxSessionsPerUser: 10000    // Par user
})

// Ajuster selon :
// - Nombre d'utilisateurs
// - Pattern d'utilisation (web vs batch)
```

## Configuration Syst√®me d'Exploitation

### Filesystem

**XFS recommand√©** (vs ext4) :

```bash
# Cr√©ation filesystem XFS
mkfs.xfs -f /dev/sdb

# Mount options optimales
mount -o noatime,nodiratime,nobarrier /dev/sdb /var/lib/mongodb

# /etc/fstab
/dev/sdb /var/lib/mongodb xfs noatime,nodiratime,nobarrier 0 0

# noatime : Ne pas update access time (r√©duit writes)
# nodiratime : Ne pas update directory access time
# nobarrier : D√©sactive les barri√®res (si UPS ou RAID avec BBU)
```

**Filesystem performance verification** :
```bash
# Test I/O
fio --name=mongodb-io-test \
    --filename=/var/lib/mongodb/test \
    --size=10G \
    --direct=1 \
    --rw=randrw \
    --rwmixread=60 \
    --bs=16k \
    --ioengine=libaio \
    --iodepth=64 \
    --runtime=60 \
    --numjobs=4 \
    --group_reporting

# Targets :
# - IOPS : >10,000 (SSD), >50,000 (NVMe)
# - Latency avg : <10ms (SSD), <1ms (NVMe)
```

### Kernel Parameters (sysctl)

```bash
# /etc/sysctl.d/mongodb.conf

# Network tuning
net.core.somaxconn = 4096                    # Socket listen backlog
net.ipv4.tcp_fin_timeout = 30                # TIME_WAIT timeout
net.ipv4.tcp_keepalive_time = 120            # Keepalive interval
net.ipv4.tcp_max_syn_backlog = 4096          # SYN backlog
net.ipv4.tcp_syncookies = 1                  # SYN flood protection

# Memory management
vm.swappiness = 1                            # Minimize swapping
vm.dirty_ratio = 15                          # Dirty pages threshold
vm.dirty_background_ratio = 5                # Background flush threshold
vm.zone_reclaim_mode = 0                     # NUMA memory reclaim

# File descriptors
fs.file-max = 98000                          # Global max FDs

# Apply
sysctl -p /etc/sysctl.d/mongodb.conf
```

**Explication des param√®tres** :

```yaml
vm.swappiness = 1:
  Description: Tendance du kernel √† swapper
  Impact: 0 = jamais swap sauf OOM (risqu√©)
         1 = swap minimal (recommand√©)
         60 = d√©faut Linux (trop agressif)

vm.dirty_ratio = 15:
  Description: % RAM de dirty pages avant flush synchrone
  Impact: Plus bas = flush plus fr√©quent, moins de cache
         Plus haut = risque de flush brusque
  Recommand√©: 10-15% pour databases

net.core.somaxconn = 4096:
  Description: Queue de connexions en attente
  Impact: Doit √™tre >= maxIncomingConnections
         Trop bas = connexions rejet√©es sous charge
```

### Ulimits

```bash
# /etc/security/limits.d/mongodb.conf

mongodb soft nofile 64000    # File descriptors (soft limit)
mongodb hard nofile 64000    # File descriptors (hard limit)
mongodb soft nproc 64000     # Processes/threads
mongodb hard nproc 64000
mongodb soft memlock unlimited  # Memory locking
mongodb hard memlock unlimited
mongodb soft fsize unlimited    # File size
mongodb hard fsize unlimited

# V√©rification
sudo -u mongodb bash -c 'ulimit -a'
```

**Calcul nofile (file descriptors)** :
```
FD required = Connections + Data files + Index files + Journal + System overhead

Example :
- Connections : 5000
- Data files : 500
- Index files : 1000
- Journal : 10
- System : 500
Total = 7010

Recommand√© : 64000 (largement suffisant pour la plupart des cas)
```

**Validation des limites** :
```javascript
// Dans MongoDB
db.serverStatus().connections
// Si current approche de available, v√©rifier ulimits

// V√©rifier les FDs utilis√©s
db.adminCommand({ serverStatus: 1 }).connections

// Check OS level
// lsof -u mongodb | wc -l
// Should be well below ulimit nofile
```

### Transparent Huge Pages (THP)

**D√©sactivation obligatoire** :

```bash
# Check status
cat /sys/kernel/mm/transparent_hugepage/enabled
# [always] madvise never  ‚Üí MAUVAIS (enabled)
# always madvise [never]  ‚Üí BON (disabled)

# D√©sactivation temporaire
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/enabled
echo never | sudo tee /sys/kernel/mm/transparent_hugepage/defrag

# D√©sactivation permanente : systemd unit
# /etc/systemd/system/disable-thp.service
[Unit]
Description=Disable Transparent Huge Pages
DefaultDependencies=no
After=sysinit.target local-fs.target
Before=mongod.service

[Service]
Type=oneshot
ExecStart=/bin/sh -c 'echo never > /sys/kernel/mm/transparent_hugepage/enabled'
ExecStart=/bin/sh -c 'echo never > /sys/kernel/mm/transparent_hugepage/defrag'

[Install]
WantedBy=basic.target

# Activer
sudo systemctl daemon-reload
sudo systemctl enable disable-thp.service
sudo systemctl start disable-thp.service
```

**Pourquoi d√©sactiver THP** :
```
THP (2MB pages) vs Regular pages (4KB)

Probl√®mes avec THP pour MongoDB :
1. Compaction stalls : Kernel tente de cr√©er huge pages
   ‚Üí Freeze applicatif de plusieurs secondes

2. Memory fragmentation : Difficile de maintenir huge pages
   ‚Üí Performance impr√©visible

3. WiredTiger incompatibilit√© : Utilise fine-grained memory
   ‚Üí Overhead de gestion THP > b√©n√©fices

Impact mesur√© :
- Avec THP : Latency spikes jusqu'√† 10+ secondes
- Sans THP : Latency stable et pr√©visible

Conclusion : TOUJOURS d√©sactiver THP pour MongoDB
```

### NUMA (Non-Uniform Memory Access)

**Configuration pour serveurs multi-socket** :

```bash
# Check NUMA topology
numactl --hardware

# Option 1 : D√©sactiver NUMA (simple mais pas optimal)
# /etc/default/grub
GRUB_CMDLINE_LINUX="numa=off"
# update-grub && reboot

# Option 2 : Interleave NUMA (meilleur)
# /etc/systemd/system/mongod.service.d/numa.conf
[Service]
ExecStart=
ExecStart=/usr/bin/numactl --interleave=all /usr/bin/mongod --config /etc/mongod.conf

# Reload systemd
systemctl daemon-reload
systemctl restart mongod

# V√©rification
ps aux | grep mongod
# Should show : numactl --interleave=all mongod
```

**Impact NUMA** :
```
Sans optimisation NUMA :
- Memory access latency variable (local vs remote)
- Performance d√©gradation 20-40%
- Comportement impr√©visible

Avec interleaving :
- Memory distribu√© uniform√©ment
- Latency pr√©visible
- Performance stable

Note : Certains benchmarks montrent que numa=off peut √™tre
l√©g√®rement meilleur que interleave pour MongoDB, mais
interleave est g√©n√©ralement plus safe.
```

## Configurations par Environnement

### Production (High Availability)

```yaml
# mongod.conf - Production HA

# Network
net:
  port: 27017
  bindIp: 0.0.0.0
  maxIncomingConnections: 10000
  compression:
    compressors: snappy,zstd

# Security
security:
  authorization: enabled
  keyFile: /etc/mongodb/keyfile

# Storage
storage:
  dbPath: /var/lib/mongodb
  journal:
    enabled: true
    commitIntervalMs: 100
  directoryPerDB: true
  wiredTiger:
    engineConfig:
      cacheSizeGB: 40
      journalCompressor: snappy
    collectionConfig:
      blockCompressor: zstd
    indexConfig:
      prefixCompression: true

# Replication
replication:
  oplogSizeMB: 102400  # 100 GB
  replSetName: prod-rs0
  enableMajorityReadConcern: true

# Operations
operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 100
  slowOpSampleRate: 0.05  # 5% sampling

# Logging
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  logRotate: reopen
  verbosity: 0

# setParameter
setParameter:
  enableLocalhostAuthBypass: false
  cursorTimeoutMillis: 600000
  internalQueryExecMaxBlockingSortBytes: 104857600  # 100 MB
```

### Development / Testing

```yaml
# mongod.conf - Development

net:
  port: 27017
  bindIp: 127.0.0.1
  maxIncomingConnections: 1000

security:
  authorization: disabled  # Faciliter dev

storage:
  dbPath: /data/mongodb-dev
  journal:
    enabled: true
    commitIntervalMs: 100
  wiredTiger:
    engineConfig:
      cacheSizeGB: 2
    collectionConfig:
      blockCompressor: snappy

operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 50  # Plus strict pour identifier probl√®mes
  slowOpSampleRate: 1.0   # 100% pour debugging

systemLog:
  destination: file
  path: /var/log/mongodb/mongod-dev.log
  verbosity: 1  # Plus verbose
  component:
    query:
      verbosity: 2  # Debug queries
```

### Analytics / Reporting

```yaml
# mongod.conf - Analytics

net:
  port: 27017
  bindIp: 0.0.0.0
  maxIncomingConnections: 2000

storage:
  dbPath: /data/mongodb-analytics
  journal:
    enabled: true
    commitIntervalMs: 200  # Moins critique
  wiredTiger:
    engineConfig:
      cacheSizeGB: 120  # Maximiser pour datasets
    collectionConfig:
      blockCompressor: zlib  # Haute compression

operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 5000  # Queries longues normales

setParameter:
  internalQueryExecMaxBlockingSortBytes: 524288000  # 500 MB
  internalQueryExecYieldIterations: 256  # Plus de yielding
```

## Validation et Testing de Configuration

### Health Check Automatis√©

```javascript
function comprehensiveConfigAudit() {
  const serverStatus = db.serverStatus();
  const buildInfo = db.adminCommand({ buildInfo: 1 });
  const params = db.adminCommand({ getParameter: "*" });

  const audit = {
    timestamp: new Date(),
    version: buildInfo.version,

    // Check critiques
    checks: {
      thp: {
        description: "Transparent Huge Pages",
        // Note : Doit √™tre v√©rifi√© via OS, pas disponible dans MongoDB
        status: "MANUAL CHECK REQUIRED",
        command: "cat /sys/kernel/mm/transparent_hugepage/enabled"
      },

      ulimits: {
        description: "File descriptors",
        current: serverStatus.connections.current,
        available: serverStatus.connections.available,
        total: serverStatus.connections.current + serverStatus.connections.available,
        status: serverStatus.connections.available > 1000 ? "OK" : "WARNING"
      },

      numa: {
        description: "NUMA interleaving",
        status: "MANUAL CHECK REQUIRED",
        command: "ps aux | grep -E 'numactl.*mongod'"
      },

      cache: {
        description: "WiredTiger cache size",
        configuredGB: (serverStatus.wiredTiger.cache["maximum bytes configured"] / 1024 / 1024 / 1024).toFixed(2),
        usagePercent: ((serverStatus.wiredTiger.cache["bytes currently in the cache"] /
                        serverStatus.wiredTiger.cache["maximum bytes configured"]) * 100).toFixed(2),
        status: "OK"
      },

      journal: {
        description: "Journal enabled and configured",
        enabled: params.storage ? params.storage.journal.enabled : "CHECK mongod.conf",
        commitIntervalMs: params.storage ? params.storage.journal.commitIntervalMs : "CHECK mongod.conf",
        status: "OK"
      },

      profiling: {
        description: "Operation profiling",
        level: db.getProfilingLevel(),
        slowms: db.getProfilingStatus().slowms,
        status: db.getProfilingLevel() > 0 ? "ENABLED" : "DISABLED"
      },

      connections: {
        description: "Connection configuration",
        maxIncoming: "CHECK mongod.conf",  // Not available via serverStatus
        current: serverStatus.connections.current,
        available: serverStatus.connections.available,
        status: serverStatus.connections.available < 100 ? "WARNING - Low available" : "OK"
      },

      oplog: {
        description: "Oplog sizing",
        sizeGB: "CHECK via rs.printReplicationInfo()",
        windowHours: "CHECK via rs.printReplicationInfo()",
        status: "MANUAL CHECK REQUIRED"
      }
    },

    // Recommandations
    recommendations: []
  };

  // Generate recommendations
  if (audit.checks.cache.usagePercent > 95) {
    audit.recommendations.push("‚ö†Ô∏è Cache usage >95% - Consider increasing cache size");
  }

  if (audit.checks.ulimits.available < 1000) {
    audit.recommendations.push("‚ö†Ô∏è Low available connections - Check maxIncomingConnections and ulimits");
  }

  if (audit.checks.profiling.level === 0) {
    audit.recommendations.push("‚ÑπÔ∏è Profiling disabled - Consider enabling slowOp profiling");
  }

  if (audit.recommendations.length === 0) {
    audit.recommendations.push("‚úÖ Configuration appears healthy");
  }

  return audit;
}

printjson(comprehensiveConfigAudit());
```

### Performance Regression Testing

```javascript
// Framework de test de configuration
class ConfigurationTest {
  constructor(testName) {
    this.testName = testName;
    this.results = [];
  }

  async runLoadTest(operations = 10000) {
    const start = Date.now();

    // Mixed workload
    for (let i = 0; i < operations; i++) {
      if (i % 10 === 0) {
        // 10% writes
        await db.testCollection.insertOne({
          _id: i,
          data: "test" + i,
          timestamp: new Date()
        });
      } else {
        // 90% reads
        await db.testCollection.findOne({ _id: Math.floor(Math.random() * i) });
      }
    }

    const duration = Date.now() - start;
    const opsPerSecond = (operations / duration * 1000).toFixed(2);

    return {
      operations: operations,
      durationMs: duration,
      opsPerSecond: opsPerSecond
    };
  }

  async compareConfigurations(config1, config2) {
    print(`Testing configuration: ${config1.name}`);
    // Apply config1
    // Run test
    const result1 = await this.runLoadTest();

    print(`Testing configuration: ${config2.name}`);
    // Apply config2
    // Run test
    const result2 = await this.runLoadTest();

    // Compare
    const improvement = ((result2.opsPerSecond - result1.opsPerSecond) /
                         result1.opsPerSecond * 100).toFixed(2);

    return {
      config1: config1.name,
      result1: result1,
      config2: config2.name,
      result2: result2,
      improvement: improvement + "%"
    };
  }
}
```

## Checklist de Configuration Production

### Pre-Deployment

```
‚òê OS Configuration
  ‚òê Disable THP (verify: cat /sys/kernel/mm/transparent_hugepage/enabled)
  ‚òê Configure NUMA (numactl --interleave=all ou numa=off)
  ‚òê Set ulimits (nofile: 64000, nproc: 64000)
  ‚òê Sysctl tuning (swappiness=1, dirty_ratio=15)
  ‚òê XFS filesystem with noatime,nodiratime

‚òê MongoDB Configuration
  ‚òê WiredTiger cache sized appropriately (working set √ó 1.3)
  ‚òê Journal enabled with appropriate commitIntervalMs
  ‚òê Oplog sized for 48h+ window
  ‚òê maxIncomingConnections appropriate
  ‚òê directoryPerDB enabled for organization
  ‚òê Compression configured (zstd recommended)

‚òê Security
  ‚òê Authorization enabled
  ‚òê KeyFile configured (replica sets)
  ‚òê TLS/SSL configured
  ‚òê Network encryption enabled
  ‚òê Audit logging configured

‚òê Monitoring
  ‚òê Profiling configured (slowOp with sampling)
  ‚òê Log rotation configured
  ‚òê Metrics collection (Prometheus/Datadog)
  ‚òê Alerting rules defined

‚òê Replication
  ‚òê Replica set configured with odd members
  ‚òê Write concern defaults set
  ‚òê Read preference strategy defined
  ‚òê Priority and votes configured
```

### Post-Deployment Validation

```
‚òê Health checks
  ‚òê THP disabled verification
  ‚òê NUMA configuration active
  ‚òê Ulimits effective
  ‚òê Disk I/O performance (fio test)

‚òê MongoDB status
  ‚òê Replica set status healthy
  ‚òê Replication lag < 5s
  ‚òê Cache hit ratio > 95%
  ‚òê No connection exhaustion
  ‚òê Checkpoint duration < 10s

‚òê Load testing
  ‚òê Baseline performance established
  ‚òê P99 latency < target
  ‚òê Throughput meets requirements
  ‚òê No errors under load

‚òê Monitoring active
  ‚òê Metrics flowing to monitoring system
  ‚òê Alerts firing correctly
  ‚òê Dashboards populated
  ‚òê On-call procedures documented
```

## Conclusion

La configuration optimale de MongoDB n√©cessite une approche holistique combinant :

1. **Configuration MongoDB** : Param√®tres internes adapt√©s au workload
2. **Configuration OS** : Kernel tuning, filesystem, ulimits
3. **Validation** : Testing et monitoring continus
4. **Ajustement** : It√©ration bas√©e sur m√©triques production

**Param√®tres les plus impactants** (par ordre d'importance) :
1. THP disabled (impact : 10-50% performance)
2. WiredTiger cache size (impact : 2-10√ó sur latence)
3. NUMA configuration (impact : 20-40% sur serveurs multi-socket)
4. ulimits appropriate (impact : Stabilit√©)
5. Filesystem et mount options (impact : 10-20% I/O)
6. Journal commitIntervalMs (impact : 15-30% write latency)
7. Connection limits (impact : Scalabilit√©)

**Erreurs courantes** :
- THP enabled (cause #1 de latency spikes)
- Cache undersized (thrashing)
- ulimits trop bas (connection failures)
- NUMA mal configur√© (performance impr√©visible)
- commitIntervalMs trop √©lev√© (perte de donn√©es)
- Profiling en mode "all" en production (overhead 20%+)

**Bonnes pratiques** :
- Documenter toute configuration non-standard
- Tester changements en staging d'abord
- Monitorer l'impact de chaque changement
- Utiliser configuration management (Ansible, Chef, Puppet)
- Maintenir des configurations de r√©f√©rence par environnement
- Auditer configuration r√©guli√®rement (monthly)

La configuration optimale √©volue avec le workload et la croissance. Un audit p√©riodique et des ajustements bas√©s sur les m√©triques de production sont essentiels pour maintenir des performances optimales.

---

**Points cl√©s √† retenir :**
- THP disabled est OBLIGATOIRE (impact critique sur latence)
- WiredTiger cache = working set √ó 1.3 minimum
- ulimits : nofile 64000, nproc 64000
- NUMA : interleave ou d√©sactiv√© pour serveurs multi-socket
- XFS avec noatime recommand√©
- commitIntervalMs : 100ms optimal pour la plupart des cas
- Profiling : slowOp avec sampling (0.05-0.1) en production
- Configuration doit √™tre versionn√©e et test√©e avant d√©ploiement
- Monitoring des param√®tres aussi important que configuration initiale

‚è≠Ô∏è [Compression des donn√©es](/17-performance-tuning/09-compression-donnees.md)
