üîù Retour au [Sommaire](/SOMMAIRE.md)

# 17.10 Read/Write Splitting

## Introduction

Le read/write splitting est une strat√©gie d'optimisation fondamentale dans MongoDB qui exploite l'architecture des replica sets pour distribuer la charge entre les membres. En dirigeant intelligemment les lectures vers les secondaries et les √©critures vers le primary, il est possible d'augmenter le throughput global de 2-5√ó tout en r√©duisant la latence sous charge.

Cependant, cette strat√©gie introduit des compromis critiques en termes de coh√©rence des donn√©es (replication lag) et n√©cessite une compr√©hension approfondie des read preferences, write concerns, et des patterns d'acc√®s applicatifs.

Cette section explore les architectures de read/write splitting, leurs impl√©mentations, impacts sur les performances, et m√©thodologies d'optimisation pour diff√©rents profils de charge en production.

## Architecture du Replica Set

### Topologie et R√¥les

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Replica Set                        ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ   PRIMARY    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  SECONDARY 1 ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ oplog   ‚îÇ              ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ All Writes   ‚îÇ repl    ‚îÇ Read replica ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Some Reads   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ              ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ         ‚îÇ oplog                                       ‚îÇ
‚îÇ         ‚îÇ repl                                        ‚îÇ
‚îÇ         v                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ  ‚îÇ  SECONDARY 2 ‚îÇ                                     ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ                                     ‚îÇ
‚îÇ  ‚îÇ Read replica ‚îÇ                                     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
‚îÇ                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Application Layer:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   WRITES     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ   PRIMARY    ‚îÇ
‚îÇ              ‚îÇ         ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   READS      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ SECONDARIES  ‚îÇ
‚îÇ (eventual    ‚îÇ         ‚îÇ              ‚îÇ
‚îÇ  consistency)‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flow de Donn√©es

**Op√©ration Write** :
```
1. Application ‚Üí Primary (write)
   ‚Üì
2. Primary ‚Üí Oplog (local)
   ‚Üì
3. Primary ‚Üí Secondaries (oplog replication)
   ‚Üì
4. Secondaries ‚Üí Apply oplog entries
   ‚Üì
5. Replication lag = temps entre (2) et (4)
```

**Op√©ration Read** :
```
Read with readPreference: primary
‚îú‚îÄ Application ‚Üí Primary ‚Üí Response
‚îî‚îÄ Consistent, mais charge le Primary

Read with readPreference: secondary
‚îú‚îÄ Application ‚Üí Secondary ‚Üí Response
‚îî‚îÄ √âventuellement consistent, d√©charge le Primary
```

## Read Preferences

### Modes Disponibles

MongoDB offre 5 modes de read preference :

#### 1. primary (D√©faut)

```javascript
// Configuration
db.collection.find().readPref("primary")

// Comportement
- Toutes les lectures sur le Primary
- Coh√©rence forte garantie
- Aucune lecture stale possible
```

**Caract√©ristiques** :
```yaml
Avantages:
  - Coh√©rence forte (linearizable reads)
  - Aucun replication lag concern
  - Donn√©es toujours √† jour

Inconv√©nients:
  - Charge enti√®re sur Primary
  - Pas de scaling horizontal des reads
  - Primary devient bottleneck

Use cases:
  - Writes fr√©quents apr√®s reads (read-your-writes)
  - Coh√©rence critique (transactions financi√®res)
  - Workload write-heavy (peu de b√©n√©fice splitting)
```

**M√©triques typiques** :
```
Setup: 3-node replica set, 10K ops/sec (70% reads)

primary only:
- Primary CPU: 85%
- Secondary 1 CPU: 15% (oplog application)
- Secondary 2 CPU: 15%
- Read latency P99: 45ms

Conclusion: Primary satur√©, secondaries sous-utilis√©s
```

#### 2. primaryPreferred

```javascript
// Configuration
db.collection.find().readPref("primaryPreferred")

// Comportement
- Pr√©f√®re le Primary
- Fallback vers Secondary si Primary indisponible
```

**Caract√©ristiques** :
```yaml
Avantages:
  - Coh√©rence forte quand Primary disponible
  - Haute disponibilit√© (reads continuent en cas de failover)
  - Transition smooth pendant √©lection

Inconv√©nients:
  - Pas de distribution de charge en temps normal
  - Stale reads possibles pendant failover

Use cases:
  - Coh√©rence prioritaire mais HA requise
  - Protection contre downtime Primary
  - Transition progressive vers secondary reads
```

**Pattern typique** :
```javascript
// Application configuration
const client = new MongoClient(uri, {
  readPreference: 'primaryPreferred',
  maxStalenessSeconds: 120  // Accepter max 2 min de lag
});

// Sc√©nario : Primary failover
// 1. Primary down d√©tect√©
// 2. Driver route vers Secondary imm√©diatement
// 3. Reads continuent (avec possible staleness)
// 4. Nouvelle √©lection termin√©e
// 5. Retour vers nouveau Primary
```

#### 3. secondary

```javascript
// Configuration
db.collection.find().readPref("secondary")

// Comportement
- Toutes les lectures sur Secondaries uniquement
- Erreur si aucun Secondary disponible
```

**Caract√©ristiques** :
```yaml
Avantages:
  - D√©charge compl√®te du Primary pour reads
  - Scaling horizontal des reads
  - Primary d√©di√© aux writes

Inconv√©nients:
  - Eventual consistency (replication lag)
  - Possible stale reads
  - Erreur si tous Secondaries down

Use cases:
  - Read-heavy workload (>80% reads)
  - Eventual consistency acceptable
  - Analytics / Reporting s√©par√©s
  - Read scaling prioritaire
```

**Performance impact** :
```
Setup: 3-node replica set, 10K ops/sec (70% reads)

secondary reads:
- Primary CPU: 35% (3K writes + oplog)
- Secondary 1 CPU: 40% (3.5K reads + oplog)
- Secondary 2 CPU: 40% (3.5K reads + oplog)
- Read latency P99: 25ms (d√©chargement)

Am√©lioration: 2√ó distribution de charge
```

#### 4. secondaryPreferred (Recommand√© pour Read Splitting)

```javascript
// Configuration
db.collection.find().readPref("secondaryPreferred")

// Comportement
- Pr√©f√®re les Secondaries
- Fallback vers Primary si aucun Secondary disponible
```

**Caract√©ristiques** :
```yaml
Avantages:
  - Distribution de charge vers Secondaries
  - Haute disponibilit√© (fallback Primary)
  - Meilleur compromis pour read scaling

Inconv√©nients:
  - Eventual consistency (replication lag)
  - Stale reads possibles

Use cases:
  - Read-heavy avec HA requise
  - Scaling horizontal optimal
  - Production g√©n√©raliste avec forte charge read
```

**Configuration optimale** :
```javascript
const client = new MongoClient(uri, {
  readPreference: 'secondaryPreferred',
  maxStalenessSeconds: 90,  // Max 90s de lag acceptable

  // Tag sets pour routing intelligent
  readPreferenceTags: [
    { dc: 'local', usage: 'read' },  // Pr√©f√©rence 1
    { dc: 'local' },                 // Pr√©f√©rence 2
    {}                                // Fallback any
  ]
});
```

#### 5. nearest

```javascript
// Configuration
db.collection.find().readPref("nearest")

// Comportement
- Route vers le membre avec latence r√©seau la plus faible
- Peut inclure Primary ou Secondaries
```

**Caract√©ristiques** :
```yaml
Avantages:
  - Latence minimale (g√©ographique)
  - Optimal pour multi-datacenter
  - Distribution automatique bas√©e sur network latency

Inconv√©nients:
  - Peut surcharger le Primary si closest
  - Eventual consistency (si secondary)
  - Comportement variable selon topology

Use cases:
  - D√©ploiement multi-r√©gion
  - Latence critique (<10ms P99)
  - Applications distribu√©es g√©ographiquement
```

**Mesure de latency** :
```javascript
function measureMemberLatency() {
  const status = rs.status();

  const latencies = status.members.map(member => ({
    name: member.name,
    state: member.stateStr,
    pingMs: member.pingMs || 0,
    health: member.health
  }));

  print("Member Latencies:");
  latencies.forEach(m => {
    print(`  ${m.name}: ${m.pingMs}ms (${m.state})`);
  });

  return latencies;
}

measureMemberLatency();

// Exemple output:
// mongo1.dc1: 1ms (PRIMARY)
// mongo2.dc1: 2ms (SECONDARY)
// mongo3.dc2: 45ms (SECONDARY)
//
// nearest ‚Üí mongo1 ou mongo2 (selon load)
```

### Read Preference avec Tag Sets

**Configuration de tags** :
```javascript
// Configuration des tags sur les membres
cfg = rs.conf();

cfg.members[0].tags = { dc: "east", usage: "general" };
cfg.members[1].tags = { dc: "east", usage: "analytics" };
cfg.members[2].tags = { dc: "west", usage: "general" };

rs.reconfig(cfg);
```

**Utilisation dans application** :
```javascript
// Read general queries from east datacenter
db.collection.find().readPref(
  "secondaryPreferred",
  [
    { dc: "east", usage: "general" },  // Pr√©f√©rence 1
    { dc: "east" },                    // Pr√©f√©rence 2
    {}                                  // Fallback any
  ]
);

// Analytics queries vers secondary d√©di√©
db.collection.aggregate([...]).readPref(
  "secondary",
  [
    { usage: "analytics" }  // Seulement vers analytics secondary
  ]
);
```

**Architecture avec tags** :
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Replica Set avec Tag Sets                          ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Primary (dc: east, usage: general)                ‚îÇ
‚îÇ  ‚îú‚îÄ Writes: Toutes                                 ‚îÇ
‚îÇ  ‚îî‚îÄ Reads: Applications est + fallback             ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Secondary 1 (dc: east, usage: analytics)          ‚îÇ
‚îÇ  ‚îú‚îÄ Reads: Queries analytics/reporting             ‚îÇ
‚îÇ  ‚îî‚îÄ Isolated workload                              ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Secondary 2 (dc: west, usage: general)            ‚îÇ
‚îÇ  ‚îú‚îÄ Reads: Applications ouest                      ‚îÇ
‚îÇ  ‚îî‚îÄ Low-latency local reads                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Benefits:
- Workload isolation (analytics vs transactional)
- Geographic locality (latency optimization)
- Resource segregation (CPU, I/O)
```

### maxStalenessSeconds

Contr√¥le l'acceptable replication lag pour les reads.

```javascript
// Configuration
const client = new MongoClient(uri, {
  readPreference: 'secondaryPreferred',
  maxStalenessSeconds: 120  // Max 2 minutes de lag
});

// Comportement
- Driver mesure le replication lag de chaque Secondary
- Exclut les Secondaries avec lag > maxStalenessSeconds
- Route seulement vers Secondaries "fresh enough"
```

**Calcul du replication lag** :
```javascript
function calculateReplicationLag() {
  const status = rs.status();
  const primary = status.members.find(m => m.state === 1);
  const secondaries = status.members.filter(m => m.state === 2);

  const lags = secondaries.map(sec => {
    const lagMs = primary.optimeDate - sec.optimeDate;
    const lagSeconds = Math.floor(lagMs / 1000);

    return {
      member: sec.name,
      lagSeconds: lagSeconds,
      acceptable: (maxStalenessSeconds) => lagSeconds <= maxStalenessSeconds
    };
  });

  return lags;
}

const lags = calculateReplicationLag();
lags.forEach(lag => {
  print(`${lag.member}: ${lag.lagSeconds}s lag`);
  print(`  Acceptable for maxStaleness=90s: ${lag.acceptable(90)}`);
});
```

**Trade-offs maxStalenessSeconds** :

| Valeur | Coh√©rence | Disponibilit√© | Use Case |
|--------|-----------|---------------|----------|
| **90s** | Bonne | Excellente | **D√©faut recommand√©** |
| 30s | Meilleure | Bonne | Staleness sensible |
| 300s | Acceptable | Maximale | Haute disponibilit√© prioritaire |
| Non d√©fini | Variable | Maximale | Accepte tout lag |

## Write Concerns

### Impact sur Read/Write Splitting

Write concern d√©termine le niveau de garantie d'√©criture.

#### w: 1 (D√©faut)

```javascript
db.collection.insertOne(
  { data: "example" },
  { writeConcern: { w: 1 } }
);

// Comportement
- Acknowledge d√®s que Primary a √©crit
- N'attend PAS la r√©plication vers Secondaries
- Latence minimale
```

**Implications** :
```yaml
Avantages:
  - Latence write minimale
  - Throughput maximal
  - Primary non bloqu√© par replication

Inconv√©nients:
  - Risque de perte de donn√©es (failover avant replication)
  - Reads sur Secondary peuvent √™tre stale m√™me apr√®s write ack
  - Pas de read-after-write consistency si read sur Secondary

Sc√©nario probl√©matique:
1. Write vers Primary avec w:1
2. Primary acknowledge imm√©diatement
3. Application read depuis Secondary (pas encore r√©pliqu√©)
4. Read retourne ancienne valeur
   ‚Üí Read-after-write violation
```

#### w: "majority" (Recommand√© Production)

```javascript
db.collection.insertOne(
  { data: "example" },
  { writeConcern: { w: "majority" } }
);

// Comportement
- Acknowledge quand majorit√© des membres ont √©crit
- Garantit durabilit√© en cas de failover
- Latence accrue
```

**Implications** :
```yaml
Avantages:
  - Durabilit√© garantie (majorit√© a donn√©es)
  - Safe en cas de failover Primary
  - Meilleure coh√©rence pour reads

Inconv√©nients:
  - Latence write +5-50ms selon network
  - Throughput r√©duit
  - D√©pend de replication lag

Am√©lioration pour read splitting:
- Apr√®s write avec w:majority, donn√©es sur majorit√©
- Read depuis Secondary a meilleure chance d'√™tre consistent
- Mais pas de garantie absolue (depend de quel Secondary)
```

**Mesure d'impact** :
```javascript
// Benchmark write concern
async function benchmarkWriteConcern(iterations = 1000) {
  const results = {
    w1: { total: 0, avg: 0 },
    majority: { total: 0, avg: 0 }
  };

  // Test w:1
  let start = Date.now();
  for (let i = 0; i < iterations; i++) {
    await db.test.insertOne(
      { data: i },
      { writeConcern: { w: 1 } }
    );
  }
  results.w1.total = Date.now() - start;
  results.w1.avg = results.w1.total / iterations;

  // Test w:majority
  start = Date.now();
  for (let i = 0; i < iterations; i++) {
    await db.test.insertOne(
      { data: i + iterations },
      { writeConcern: { w: "majority" } }
    );
  }
  results.majority.total = Date.now() - start;
  results.majority.avg = results.majority.total / iterations;

  results.overhead = {
    ms: results.majority.avg - results.w1.avg,
    percent: ((results.majority.avg / results.w1.avg - 1) * 100).toFixed(2)
  };

  return results;
}

// R√©sultats typiques:
// w:1 ‚Üí 2ms avg
// w:majority ‚Üí 7ms avg
// Overhead: +5ms (+250%)
```

#### j: true (Journal)

```javascript
db.collection.insertOne(
  { data: "example" },
  { writeConcern: { w: 1, j: true } }
);

// Comportement
- Attend que write soit dans journal (WAL)
- Garantit durabilit√© en cas de crash mongod
```

**Impact** :
```
Sans j:true:
‚îú‚îÄ Write en cache WiredTiger
‚îú‚îÄ Acknowledge imm√©diat
‚îî‚îÄ Journal flush async (100ms)

Avec j:true:
‚îú‚îÄ Write en cache WiredTiger
‚îú‚îÄ Force flush vers journal
‚îú‚îÄ Acknowledge apr√®s journal write
‚îî‚îÄ +3-10ms latency

Overhead: +5-15% latency
B√©n√©fice: Durabilit√© crash
```

### Read Concern

Compl√©ment du write concern pour les reads.

#### available (D√©faut)

```javascript
db.collection.find().readConcern("available")

// Comportement
- Retourne donn√©es imm√©diatement disponibles
- Peut retourner donn√©es non commit√©es (orphaned)
- Performance maximale
```

#### local

```javascript
db.collection.find().readConcern("local")

// Comportement
- Retourne derni√®res donn√©es locales au membre
- Pas de garantie de durabilit√©
- Standard pour la plupart des reads
```

#### majority

```javascript
db.collection.find().readConcern("majority")

// Comportement
- Retourne seulement donn√©es r√©pliqu√©es sur majorit√©
- Garantit que donn√©es ne seront pas rollback
- Coh√©rence forte mais latence accrue
```

**Trade-off readConcern** :

```javascript
// Sc√©nario : Read apr√®s Write
// 1. Write avec w:1
db.orders.insertOne({ id: 123 }, { writeConcern: { w: 1 } });

// 2. Read imm√©diat depuis Secondary
// Option A : readConcern: local
db.orders.find({ id: 123 }).readConcern("local");
// ‚Üí Peut ne pas voir le document (pas encore r√©pliqu√©)

// Option B : readConcern: majority
db.orders.find({ id: 123 }).readConcern("majority");
// ‚Üí Attendra que document soit sur majorit√©
// ‚Üí Garantit visibilit√© si write √©tait w:majority

// Option C : Read depuis Primary
db.orders.find({ id: 123 }).readPref("primary");
// ‚Üí Voit toujours le document
// ‚Üí Mais charge le Primary
```

## Architectures de Read/Write Splitting

### Architecture 1 : Simple Read Splitting

**Configuration** :
```
Topology:
- 1 Primary
- 2 Secondaries (read replicas)

Strategy:
- Writes ‚Üí Primary (w:majority)
- Reads ‚Üí secondaryPreferred
- No dedicated secondaries
```

**Impl√©mentation** :
```javascript
// Configuration driver
const client = new MongoClient(uri, {
  replicaSet: 'rs0',
  readPreference: 'secondaryPreferred',
  readConcern: { level: 'local' },
  writeConcern: { w: 'majority', j: false }
});

// Application code
// Writes (automatic routing to primary)
await db.orders.insertOne({
  customerId: 123,
  total: 99.99
});

// Reads (automatic routing to secondary)
const orders = await db.orders.find({
  customerId: 123
}).toArray();
```

**Performance** :
```
Before splitting (all on Primary):
- Primary CPU: 80%
- Read latency P99: 35ms
- Write latency P99: 25ms
- Max throughput: 8K ops/sec

After splitting:
- Primary CPU: 40% (writes only)
- Secondary 1 CPU: 30%
- Secondary 2 CPU: 30%
- Read latency P99: 25ms (improved)
- Write latency P99: 28ms (w:majority overhead)
- Max throughput: 15K ops/sec (+87%)
```

### Architecture 2 : Dedicated Analytics Secondary

**Configuration** :
```
Topology:
- 1 Primary
- 1 Secondary (transactional reads)
- 1 Secondary (analytics, hidden, priority=0)

Strategy:
- Writes ‚Üí Primary
- Transactional reads ‚Üí Secondary 1
- Analytics/Reporting ‚Üí Secondary 2 (isolated)
```

**Configuration membre analytics** :
```javascript
cfg = rs.conf();

// Secondary 2 : Analytics
cfg.members[2].priority = 0;  // Ne peut pas devenir Primary
cfg.members[2].hidden = true;  // Cach√© du driver par d√©faut
cfg.members[2].tags = { usage: "analytics" };

rs.reconfig(cfg);
```

**Utilisation** :
```javascript
// Application transactionnelle (standard)
const transactionalClient = new MongoClient(uri, {
  readPreference: 'secondaryPreferred',
  // Exclut hidden members automatiquement
});

// Application analytics (explicit)
const analyticsClient = new MongoClient(uri, {
  readPreference: 'secondary',
  readPreferenceTags: [{ usage: 'analytics' }],
  directConnection: true  // Connect directement au member
});

// Analytics queries
const report = await analyticsDb.orders.aggregate([
  { $match: { date: { $gte: startDate } } },
  { $group: {
      _id: "$category",
      revenue: { $sum: "$total" }
  }},
  { $sort: { revenue: -1 } }
], { allowDiskUse: true });
```

**B√©n√©fices** :
```yaml
Isolation:
  - Analytics queries n'impactent pas transactional
  - Pas de contention CPU/RAM
  - allowDiskUse sans risque

Performance:
  - Primary: 100% writes
  - Secondary 1: 100% transactional reads
  - Secondary 2: 100% analytics (isolated)
  - No interference

Configuration:
  - Secondary 2 peut avoir config diff√©rente:
    * Plus de RAM (analytics cache)
    * Slower storage OK (batch OK)
    * Diff√©rent index strategy
```

### Architecture 3 : Geographic Distribution

**Configuration** :
```
Topology: Multi-datacenter
- Primary (DC1 - East)
- Secondary 1 (DC1 - East)
- Secondary 2 (DC2 - West)

Strategy:
- Writes ‚Üí Primary (DC1)
- East reads ‚Üí DC1 members (nearest)
- West reads ‚Üí DC2 member (nearest)
```

**Configuration avec tags** :
```javascript
cfg = rs.conf();

cfg.members[0].tags = { dc: "east", region: "us-east-1" };
cfg.members[1].tags = { dc: "east", region: "us-east-1" };
cfg.members[2].tags = { dc: "west", region: "us-west-1" };

rs.reconfig(cfg);
```

**Application routing** :
```javascript
// East coast application
const eastClient = new MongoClient(uri, {
  readPreference: 'nearest',
  readPreferenceTags: [
    { dc: 'east' },  // Prefer local DC
    {}               // Fallback any
  ],
  maxStalenessSeconds: 90
});

// West coast application
const westClient = new MongoClient(uri, {
  readPreference: 'nearest',
  readPreferenceTags: [
    { dc: 'west' },  // Prefer local DC
    {}               // Fallback any (cross-DC)
  ],
  maxStalenessSeconds: 90
});
```

**Latency impact** :
```
East application:
- Write latency: 5ms (primary local)
- Read latency: 2ms (secondary local)

West application:
- Write latency: 75ms (primary cross-DC)
- Read latency: 3ms (secondary local)

Improvement:
- Without splitting: All queries to East DC
  * West read latency: 70ms
- With splitting: Local reads
  * West read latency: 3ms (23√ó faster!)
```

### Architecture 4 : Priority-based Routing

**Configuration** :
```
Topology:
- Primary (high-end server)
- Secondary 1 (high-end, critical reads)
- Secondary 2 (standard, general reads)

Strategy:
- Critical reads ‚Üí Secondary 1
- General reads ‚Üí Secondary 2
- Writes ‚Üí Primary
```

**Tag configuration** :
```javascript
cfg = rs.conf();

cfg.members[1].tags = { tier: "premium", priority: "high" };
cfg.members[2].tags = { tier: "standard", priority: "normal" };

rs.reconfig(cfg);
```

**Application routing** :
```javascript
// Critical path (user-facing)
async function getCriticalData(userId) {
  return await db.users.findOne(
    { _id: userId },
    {
      readPreference: 'secondary',
      readPreferenceTags: [
        { priority: 'high' },  // Premium secondary
        {}                     // Fallback
      ]
    }
  );
}

// Background processing (less critical)
async function getAnalyticsData() {
  return await db.events.aggregate([...], {
    readPreference: 'secondary',
    readPreferenceTags: [
      { priority: 'normal' }  // Standard secondary
    ],
    allowDiskUse: true
  });
}
```

## Coh√©rence et Replication Lag

### Probl√©matique de la Coh√©rence

**Read-your-writes problem** :
```javascript
// Sc√©nario probl√©matique
async function problemScenario() {
  // 1. User cr√©e un order
  const result = await db.orders.insertOne(
    { userId: 123, total: 99.99 },
    { writeConcern: { w: 1 } }  // Ack imm√©diat
  );

  const orderId = result.insertedId;

  // 2. Imm√©diatement, redirect vers page order
  // 3. Read depuis Secondary (pas encore r√©pliqu√©)
  const order = await db.orders.findOne(
    { _id: orderId }
    // readPreference: secondaryPreferred (default)
  );

  // 4. order === null (Secondary lag = 100ms)
  // 5. User voit erreur "Order not found"

  return order;
}

// Solution 1 : Read from Primary pour read-after-write
async function solution1() {
  const result = await db.orders.insertOne({...});

  // Read from Primary explicitly
  const order = await db.orders.findOne(
    { _id: result.insertedId }
  ).readPref('primary');

  return order;
}

// Solution 2 : w:majority + maxStaleness
async function solution2() {
  const result = await db.orders.insertOne(
    {...},
    { writeConcern: { w: 'majority' } }  // Majorit√© a donn√©es
  );

  // Read avec maxStaleness court
  const order = await db.orders.findOne(
    { _id: result.insertedId }
  ).readPref('secondaryPreferred', null, { maxStalenessSeconds: 5 });

  return order;
}

// Solution 3 : Session causally consistent
async function solution3() {
  const session = client.startSession({ causalConsistency: true });

  try {
    const result = await db.orders.insertOne(
      {...},
      { session }
    );

    // Read voit le write pr√©c√©dent
    const order = await db.orders.findOne(
      { _id: result.insertedId },
      { session }
    );

    return order;
  } finally {
    await session.endSession();
  }
}
```

### Monitoring Replication Lag

**Script de monitoring** :
```javascript
function comprehensiveReplicationLagMonitoring() {
  const status = rs.status();
  const primary = status.members.find(m => m.state === 1);
  const secondaries = status.members.filter(m => m.state === 2);

  const monitoring = {
    timestamp: new Date(),
    primary: {
      name: primary.name,
      optime: primary.optimeDate
    },
    secondaries: [],
    metrics: {
      maxLagSeconds: 0,
      avgLagSeconds: 0,
      minLagSeconds: Infinity
    }
  };

  secondaries.forEach(sec => {
    const lagMs = primary.optimeDate - sec.optimeDate;
    const lagSeconds = lagMs / 1000;

    monitoring.secondaries.push({
      name: sec.name,
      lagSeconds: lagSeconds.toFixed(2),
      health: sec.health,
      state: sec.stateStr,
      pingMs: sec.pingMs || 0
    });

    // Update metrics
    if (lagSeconds > monitoring.metrics.maxLagSeconds) {
      monitoring.metrics.maxLagSeconds = lagSeconds;
    }
    if (lagSeconds < monitoring.metrics.minLagSeconds) {
      monitoring.metrics.minLagSeconds = lagSeconds;
    }
  });

  // Calculate average
  const totalLag = monitoring.secondaries.reduce((sum, s) =>
    sum + parseFloat(s.lagSeconds), 0);
  monitoring.metrics.avgLagSeconds =
    (totalLag / monitoring.secondaries.length).toFixed(2);

  // Assessment
  const maxLag = monitoring.metrics.maxLagSeconds;
  if (maxLag > 60) {
    monitoring.assessment = "‚ö†Ô∏è HIGH LAG (>60s) - Investigate replication";
  } else if (maxLag > 10) {
    monitoring.assessment = "WARNING: Elevated lag (>10s)";
  } else {
    monitoring.assessment = "‚úÖ Replication lag healthy";
  }

  return monitoring;
}

// Ex√©cution p√©riodique (toutes les 30s)
setInterval(() => {
  const lag = comprehensiveReplicationLagMonitoring();
  print(JSON.stringify(lag, null, 2));
}, 30000);
```

**Causes de replication lag** :

```javascript
function diagnoseReplicationLag() {
  const serverStatus = db.serverStatus();
  const replStatus = rs.status();

  const diagnosis = {
    possibleCauses: []
  };

  // 1. Network issues
  replStatus.members.forEach(member => {
    if (member.pingMs > 50) {
      diagnosis.possibleCauses.push({
        cause: "High network latency",
        member: member.name,
        pingMs: member.pingMs,
        recommendation: "Check network between members"
      });
    }
  });

  // 2. Secondary overloaded (read queries)
  const ops = serverStatus.opcounters;
  const totalOps = ops.query + ops.insert + ops.update + ops.delete;
  if (totalOps > 10000) {  // Threshold
    diagnosis.possibleCauses.push({
      cause: "Secondary overloaded with read queries",
      opsPerSec: totalOps,
      recommendation: "Reduce read load or add more secondaries"
    });
  }

  // 3. Slow oplog application
  const replMetrics = serverStatus.metrics?.repl;
  if (replMetrics && replMetrics.apply) {
    const applyBatchMs = replMetrics.apply.batchSize || 0;
    if (applyBatchMs > 1000) {
      diagnosis.possibleCauses.push({
        cause: "Slow oplog batch application",
        avgBatchMs: applyBatchMs,
        recommendation: "Check secondary performance (CPU, I/O)"
      });
    }
  }

  // 4. Disk I/O saturation
  const wtStats = serverStatus.wiredTiger;
  if (wtStats) {
    const checkpointMs = wtStats.transaction["transaction checkpoint most recent time (msecs)"];
    if (checkpointMs > 30000) {
      diagnosis.possibleCauses.push({
        cause: "Slow checkpoints (I/O bottleneck)",
        checkpointMs: checkpointMs,
        recommendation: "Upgrade storage or reduce write load"
      });
    }
  }

  // 5. Initial sync in progress
  replStatus.members.forEach(member => {
    if (member.stateStr === "STARTUP2") {
      diagnosis.possibleCauses.push({
        cause: "Initial sync in progress",
        member: member.name,
        recommendation: "Wait for initial sync to complete"
      });
    }
  });

  return diagnosis;
}

printjson(diagnoseReplicationLag());
```

### Strat√©gies de Mitigation

**1. Read from Primary pour critical paths** :
```javascript
// Router pattern
class SmartRouter {
  constructor(db) {
    this.db = db;
  }

  // Critical reads : Always from Primary
  async getCriticalData(query) {
    return await this.db.collection.find(query)
      .readPref('primary')
      .toArray();
  }

  // Non-critical reads : Secondary preferred
  async getNonCriticalData(query) {
    return await this.db.collection.find(query)
      .readPref('secondaryPreferred')
      .maxStaleness(90)
      .toArray();
  }

  // Analytics : Dedicated secondary
  async getAnalyticsData(pipeline) {
    return await this.db.collection.aggregate(pipeline, {
      readPreference: 'secondary',
      readPreferenceTags: [{ usage: 'analytics' }],
      allowDiskUse: true
    }).toArray();
  }
}
```

**2. Causal Consistency Sessions** :
```javascript
async function causallyConsistentWorkflow() {
  const session = client.startSession({
    causalConsistency: true
  });

  try {
    // Write
    await db.orders.insertOne(
      { userId: 123, total: 99.99 },
      { session }
    );

    // Read verra le write (m√™me si sur Secondary)
    const orders = await db.orders.find(
      { userId: 123 },
      { session }
    ).readPref('secondaryPreferred').toArray();

    // Update bas√© sur read pr√©c√©dent
    await db.users.updateOne(
      { _id: 123 },
      { $inc: { orderCount: 1 } },
      { session }
    );

    return orders;
  } finally {
    await session.endSession();
  }
}
```

**3. Application-level retry** :
```javascript
async function readWithRetry(query, maxRetries = 3) {
  let attempt = 0;

  while (attempt < maxRetries) {
    try {
      // Try Secondary first
      const result = await db.collection.findOne(query)
        .readPref('secondaryPreferred')
        .maxStaleness(30);

      if (result) {
        return result;
      }

      // Not found, might be replication lag
      attempt++;
      if (attempt < maxRetries) {
        await new Promise(resolve => setTimeout(resolve, 100));
      }
    } catch (error) {
      throw error;
    }
  }

  // Final attempt: Read from Primary
  return await db.collection.findOne(query).readPref('primary');
}
```

## Performance Monitoring

### M√©triques Cl√©s

```javascript
function readWriteSplittingDashboard() {
  const serverStatus = db.serverStatus();
  const replStatus = rs.status();

  const dashboard = {
    timestamp: new Date(),

    // Member status
    members: replStatus.members.map(m => ({
      name: m.name,
      state: m.stateStr,
      health: m.health,
      uptime: m.uptime,
      pingMs: m.pingMs || 0
    })),

    // Replication lag
    replicationLag: (() => {
      const primary = replStatus.members.find(m => m.state === 1);
      const secondaries = replStatus.members.filter(m => m.state === 2);

      return secondaries.map(sec => ({
        member: sec.name,
        lagSeconds: ((primary.optimeDate - sec.optimeDate) / 1000).toFixed(2)
      }));
    })(),

    // Operations distribution
    operations: {
      total: serverStatus.opcounters.query +
             serverStatus.opcounters.insert +
             serverStatus.opcounters.update +
             serverStatus.opcounters.delete,

      breakdown: {
        queries: serverStatus.opcounters.query,
        inserts: serverStatus.opcounters.insert,
        updates: serverStatus.opcounters.update,
        deletes: serverStatus.opcounters.delete
      },

      readWriteRatio: (
        serverStatus.opcounters.query /
        (serverStatus.opcounters.insert +
         serverStatus.opcounters.update +
         serverStatus.opcounters.delete)
      ).toFixed(2)
    },

    // Connections
    connections: {
      current: serverStatus.connections.current,
      available: serverStatus.connections.available,
      active: serverStatus.connections.active || 0
    },

    // Performance
    performance: {
      cachePressure: ((serverStatus.wiredTiger.cache["bytes currently in the cache"] /
                      serverStatus.wiredTiger.cache["maximum bytes configured"]) * 100).toFixed(2) + "%",

      ticketsAvailable: {
        read: serverStatus.wiredTiger.concurrentTransactions.read.available,
        write: serverStatus.wiredTiger.concurrentTransactions.write.available
      }
    }
  };

  // Assessment
  const issues = [];

  dashboard.replicationLag.forEach(lag => {
    if (parseFloat(lag.lagSeconds) > 60) {
      issues.push(`‚ö†Ô∏è High lag on ${lag.member}: ${lag.lagSeconds}s`);
    }
  });

  if (dashboard.connections.available < 100) {
    issues.push("‚ö†Ô∏è Low available connections");
  }

  if (parseFloat(dashboard.performance.cachePressure) > 95) {
    issues.push("‚ö†Ô∏è High cache pressure");
  }

  dashboard.assessment = issues.length === 0 ?
    "‚úÖ Read/Write splitting healthy" : issues;

  return dashboard;
}

// Monitoring p√©riodique
setInterval(() => {
  const dashboard = readWriteSplittingDashboard();
  // Export vers monitoring system (Prometheus, Datadog, etc.)
  print(JSON.stringify(dashboard));
}, 60000);  // Toutes les minutes
```

### Alerting Rules

**Prometheus-style alerts** :
```yaml
# High replication lag
- alert: HighReplicationLag
  expr: mongodb_replication_lag_seconds > 60
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "High replication lag detected"
    description: "Replication lag > 60s on {{ $labels.member }}"

# Secondary unhealthy
- alert: SecondaryUnhealthy
  expr: mongodb_member_health == 0
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: "Secondary member unhealthy"
    description: "Member {{ $labels.member }} is unhealthy"

# Unbalanced load
- alert: UnbalancedReadLoad
  expr: |
    stddev(mongodb_operations_reads_per_second) /
    avg(mongodb_operations_reads_per_second) > 0.5
  for: 10m
  labels:
    severity: info
  annotations:
    summary: "Unbalanced read load distribution"
    description: "Read load not evenly distributed across secondaries"

# High secondary CPU
- alert: HighSecondaryCPU
  expr: |
    mongodb_system_cpu_usage{state="secondary"} > 85
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "High CPU on secondary"
    description: "Secondary CPU usage > 85% - consider adding capacity"
```

## Best Practices et Recommandations

### Configuration Recommand√©e par Workload

**Read-heavy (>70% reads)** :
```javascript
const client = new MongoClient(uri, {
  replicaSet: 'rs0',

  // Read splitting agressif
  readPreference: 'secondaryPreferred',
  maxStalenessSeconds: 90,

  // Write concern balanced
  writeConcern: { w: 'majority', j: false },

  // Read concern standard
  readConcern: { level: 'local' }
});

// Expected improvement: 2-3√ó read throughput
```

**Write-heavy (>50% writes)** :
```javascript
const client = new MongoClient(uri, {
  replicaSet: 'rs0',

  // Minimal read splitting (Primary peut g√©rer)
  readPreference: 'primaryPreferred',

  // Write concern performance
  writeConcern: { w: 1, j: false },

  readConcern: { level: 'local' }
});

// Focus: Optimize write path, not read splitting
```

**Mixed with critical consistency** :
```javascript
const client = new MongoClient(uri, {
  replicaSet: 'rs0',

  // Moderate splitting
  readPreference: 'secondaryPreferred',
  maxStalenessSeconds: 30,  // Tighter staleness

  // Strong durability
  writeConcern: { w: 'majority', j: true },

  // Stronger consistency
  readConcern: { level: 'majority' }
});

// Use causal consistency sessions for workflows
```

### Checklist de D√©ploiement

```
‚òê Architecture Design
  ‚òê D√©finir read/write ratio du workload
  ‚òê Identifier queries critiques (consistency required)
  ‚òê D√©cider si dedicated analytics secondary n√©cessaire
  ‚òê Planifier topology (datacenter, tags)

‚òê Configuration
  ‚òê Read preference adapt√© au workload
  ‚òê maxStalenessSeconds appropri√© (30-90s)
  ‚òê Write concern pour durabilit√© vs performance
  ‚òê Tag sets si architecture complexe

‚òê Application Code
  ‚òê Utiliser sessions causal consistency si n√©cessaire
  ‚òê Diff√©rencier critical vs non-critical reads
  ‚òê Impl√©menter retry logic pour replication lag
  ‚òê Tester read-after-write scenarios

‚òê Monitoring
  ‚òê Replication lag alerting (<60s)
  ‚òê Load distribution metrics
  ‚òê Secondary health checks
  ‚òê Connection distribution tracking

‚òê Testing
  ‚òê Load testing avec read splitting activ√©
  ‚òê Failover scenarios (Primary down)
  ‚òê Replication lag simulation
  ‚òê Consistency validation (stale reads acceptables?)

‚òê Documentation
  ‚òê Read preference strategy document√©e
  ‚òê Consistency guarantees clairement d√©finis
  ‚òê Failover behavior compris par √©quipe
  ‚òê Monitoring runbook cr√©√©
```

### Erreurs Courantes

```javascript
// ‚ùå ERREUR 1 : Read splitting sans consid√©rer consistency
// Code assume strong consistency mais lit depuis Secondary
async function badPattern1() {
  const result = await db.orders.insertOne({ total: 99.99 });

  // Imm√©diatement apr√®s, lit depuis Secondary
  const order = await db.orders.findOne({ _id: result.insertedId });
  // readPreference: secondaryPreferred (default)

  // order peut √™tre null si lag > 0
  return order;
}

// ‚úÖ CORRECT : Use Primary ou causal consistency
async function goodPattern1() {
  const session = client.startSession({ causalConsistency: true });

  const result = await db.orders.insertOne(
    { total: 99.99 },
    { session }
  );

  const order = await db.orders.findOne(
    { _id: result.insertedId },
    { session }  // Verra le write
  );

  await session.endSession();
  return order;
}

// ‚ùå ERREUR 2 : Ignorer maxStalenessSeconds
const client = new MongoClient(uri, {
  readPreference: 'secondary'
  // Pas de maxStalenessSeconds ‚Üí Accepte lag infini
});

// ‚úÖ CORRECT : D√©finir limite acceptable
const client = new MongoClient(uri, {
  readPreference: 'secondary',
  maxStalenessSeconds: 90  // Max 90s de lag
});

// ‚ùå ERREUR 3 : Read splitting sur workload write-heavy
// 70% writes, 30% reads
const client = new MongoClient(uri, {
  readPreference: 'secondary'  // Peu de b√©n√©fice, overhead r√©plication
});

// ‚úÖ CORRECT : Garder Primary pour reads aussi
const client = new MongoClient(uri, {
  readPreference: 'primary'  // Plus simple et efficace
});

// ‚ùå ERREUR 4 : Pas de fallback strategy
const client = new MongoClient(uri, {
  readPreference: 'secondary'  // Erreur si tous Secondaries down
});

// ‚úÖ CORRECT : Use secondaryPreferred pour HA
const client = new MongoClient(uri, {
  readPreference: 'secondaryPreferred'  // Fallback Primary
});
```

## Conclusion

Le read/write splitting est une technique puissante pour am√©liorer les performances MongoDB :

**B√©n√©fices mesurables** :
- Throughput : +50-300% selon ratio read/write
- Latence reads : -20-40% (d√©chargement Primary)
- Scalabilit√© : Horizontale pour reads
- Isolation : Workloads s√©par√©s (analytics vs transactional)

**Trade-offs critiques** :
- Coh√©rence : Eventual consistency (replication lag)
- Complexit√© : Configuration et monitoring accrus
- Risques : Read-after-write violations si mal configur√©

**Recommandations cl√©s** :
1. **Analyser workload** : Read/write ratio, consistency requirements
2. **secondaryPreferred d√©faut** : Meilleur compromis HA + performance
3. **maxStalenessSeconds : 90s** : Balance coh√©rence/disponibilit√©
4. **Causal consistency sessions** : Pour workflows critiques
5. **Monitoring continu** : Replication lag < 10s optimal
6. **Tag sets** : Pour architectures avanc√©es (analytics, geo)

**Quand utiliser** :
- ‚úÖ Read-heavy (>60% reads)
- ‚úÖ High traffic n√©cessitant scaling
- ‚úÖ Analytics/Reporting s√©par√©s
- ‚úÖ Multi-datacenter deployment

**Quand √©viter** :
- ‚ùå Strong consistency absolue requise partout
- ‚ùå Write-heavy (>60% writes)
- ‚ùå Tr√®s faible traffic (overhead > b√©n√©fice)
- ‚ùå Replication lag chroniquement √©lev√©

Le read/write splitting n'est pas une solution universelle mais un outil puissant qui, correctement configur√© et monitor√©, peut transformer les performances d'une application MongoDB √† forte charge de lecture.

---

**Points cl√©s √† retenir :**
- secondaryPreferred est le meilleur compromis production (HA + performance)
- maxStalenessSeconds : 90s recommand√© (coh√©rence vs disponibilit√©)
- Causal consistency sessions pour read-after-write scenarios
- Monitoring replication lag essentiel (<10s optimal, <60s acceptable)
- Tag sets permettent architectures avanc√©es (analytics, geo-distribution)
- w:majority + readConcern:majority pour coh√©rence forte si n√©cessaire
- Test de failover obligatoire avant production
- Read splitting b√©n√©fique seulement si >60% reads

‚è≠Ô∏è [Caching strategies](/17-performance-tuning/11-caching-strategies.md)
