üîù Retour au [Sommaire](/SOMMAIRE.md)

# 17.1 Identification des Goulots d'√âtranglement

## Introduction

L'identification pr√©cise des goulots d'√©tranglement est la premi√®re et la plus critique √©tape de toute d√©marche d'optimisation. Une analyse erron√©e conduit √† des optimisations inefficaces, voire contre-productives. Dans un syst√®me MongoDB en production, les goulots peuvent se manifester √† diff√©rents niveaux : applicatif, base de donn√©es, syst√®me, r√©seau, ou architecture distribu√©e.

Cette section pr√©sente une m√©thodologie syst√©matique pour identifier, isoler et caract√©riser les goulots d'√©tranglement en environnement de production.

## M√©thodologie USE (Utilization, Saturation, Errors)

La m√©thodologie USE, d√©velopp√©e par Brendan Gregg, fournit un framework syst√©matique pour l'analyse de performance. Pour chaque ressource du syst√®me, on examine trois dimensions :

### Utilization (Utilisation)

Mesure du temps pendant lequel la ressource est occup√©e (busy time), exprim√©e en pourcentage.

**Ressources √† analyser :**
- **CPU** : Temps CPU utilis√© vs idle
- **M√©moire** : RAM utilis√©e vs disponible
- **Disque** : Temps pendant lequel le disque traite des requ√™tes
- **R√©seau** : Bande passante utilis√©e vs disponible
- **Connexions** : Connexions actives vs pool size maximum

**Interpr√©tation des seuils :**
```
< 70%  : Utilisation normale, capacit√© suffisante
70-85% : Zone d'attention, surveiller les tendances
85-95% : Zone critique, planifier l'augmentation de capacit√©
> 95%  : Saturation imminente, action imm√©diate requise
```

### Saturation (Saturation)

Degr√© auquel une ressource a du travail en attente qu'elle ne peut traiter imm√©diatement.

**Indicateurs de saturation :**
- **CPU** : Load average, run queue length
- **M√©moire** : Page faults, swap activity
- **Disque** : Queue depth, await time
- **R√©seau** : Dropped packets, retransmissions
- **MongoDB** : Queued operations (read/write queues)

**Saturation critique MongoDB :**
```javascript
// V√©rification des queues MongoDB
db.serverStatus().globalLock.currentQueue
{
  total: 0,     // Queue totale
  readers: 0,   // Op√©rations de lecture en attente
  writers: 0    // Op√©rations d'√©criture en attente
}

// Valeurs pr√©occupantes :
// readers/writers > 10 : Saturation l√©g√®re
// readers/writers > 50 : Saturation s√©v√®re
// readers/writers > 100 : Saturation critique
```

### Errors (Erreurs)

Compteurs d'erreurs qui peuvent indiquer des probl√®mes de ressources ou de configuration.

**Erreurs critiques √† surveiller :**
- **Connexions refus√©es** : Pool exhaustion, limite de connexions
- **Timeouts** : Network, operation, election
- **Assertion failures** : Bugs, corruption, invariants viol√©s
- **Replication errors** : Lag excessif, oplog overflow
- **OOM (Out of Memory)** : Killer process, allocation failures

## Identification par Sympt√¥me

### Sympt√¥me : Latence √âlev√©e

La latence √©lev√©e peut avoir de multiples causes. Une approche m√©thodique est n√©cessaire pour identifier la vraie cause racine.

#### Analyse de la Latence

**1. Localisation de la latence**

D√©composer la latence totale en ses composantes :

```
Latence totale = Latence r√©seau + Latence queue + Latence traitement + Latence I/O
```

**Outils de mesure :**
- **Application side** : APM (New Relic, Datadog), instrumentation custom
- **MongoDB side** : Profiler, currentOp, system.profile collection
- **Infrastructure** : Network monitoring, traceroute, ping latency

**2. Analyse des percentiles**

Ne jamais se fier uniquement √† la moyenne, qui masque les outliers :

```javascript
// Analyse des percentiles dans le profiler
db.system.profile.aggregate([
  { $match: { ts: { $gte: ISODate("2025-01-01") } } },
  { $group: {
      _id: "$command.find",
      count: { $sum: 1 },
      avgMs: { $avg: "$millis" },
      maxMs: { $max: "$millis" },
      percentiles: { $push: "$millis" }
  }},
  { $project: {
      count: 1,
      avgMs: 1,
      maxMs: 1,
      p50: { $arrayElemAt: ["$percentiles", { $multiply: [0.50, "$count"] }] },
      p95: { $arrayElemAt: ["$percentiles", { $multiply: [0.95, "$count"] }] },
      p99: { $arrayElemAt: ["$percentiles", { $multiply: [0.99, "$count"] }] }
  }}
])
```

**Interpr√©tation :**
- P50 √©lev√© : Probl√®me syst√©mique affectant la majorit√© des requ√™tes
- P95-P99 √©lev√©s mais P50 normal : Outliers, possiblement contention ou cold cache
- Max >> P99 : Requ√™tes pathologiques √† investiguer individuellement

#### Corr√©lation avec les Ressources Syst√®me

**Matrice de corr√©lation latence-ressources :**

| Sympt√¥me | CPU √âlev√© | RAM Faible | I/O √âlev√© | Network Lent |
|----------|-----------|------------|-----------|--------------|
| Latence constante haute | ‚úì | ‚úì | ‚úì | ‚úì |
| Pics de latence sporadiques | - | ‚úì (page faults) | ‚úì (contention) | ‚úì (congestion) |
| Latence croissante dans le temps | - | ‚úì (leak) | ‚úì (fragmentation) | - |
| Latence variable selon l'heure | ‚úì (batch jobs) | - | ‚úì (backups) | ‚úì (peak hours) |

### Sympt√¥me : Faible Throughput

Le throughput faible indique que le syst√®me ne traite pas suffisamment d'op√©rations par unit√© de temps.

#### Analyse du Throughput

**1. Mesure du throughput actuel**

```javascript
// Throughput via serverStatus
const stats1 = db.serverStatus();
// Attendre 60 secondes
sleep(60000);
const stats2 = db.serverStatus();

const opsPerSecond = {
  insert: (stats2.opcounters.insert - stats1.opcounters.insert) / 60,
  query: (stats2.opcounters.query - stats1.opcounters.query) / 60,
  update: (stats2.opcounters.update - stats1.opcounters.update) / 60,
  delete: (stats2.opcounters.delete - stats1.opcounters.delete) / 60,
  command: (stats2.opcounters.command - stats1.opcounters.command) / 60
};
```

**2. Identification des limiteurs de throughput**

**Lock contention** :
```javascript
db.serverStatus().locks
// Analyse de :
// - acquireCount : Nombre de fois o√π le lock a √©t√© acquis
// - acquireWaitCount : Nombre de fois o√π il a fallu attendre
// - timeAcquiringMicros : Temps total d'attente

// Ratio critique :
const lockContentionRatio = timeAcquiringMicros / (acquireCount * avgOperationTime);
// > 10% : Contention significative
```

**Connection exhaustion** :
```javascript
db.serverStatus().connections
{
  current: 850,      // Connexions actuelles
  available: 150,    // Connexions disponibles
  totalCreated: 12500
}

// Saturation si available < 10% de la limite configur√©e
```

**Write concern timeout** :
```javascript
// Recherche des timeouts de write concern dans les logs
db.adminCommand({
  getLog: "global"
}).log.filter(entry => entry.includes("writeConcern"))
```

### Sympt√¥me : Utilisation M√©moire Excessive

L'utilisation m√©moire excessive peut conduire √† du swapping et une d√©gradation catastrophique des performances.

#### Analyse de la M√©moire

**1. D√©composition de l'utilisation m√©moire**

```javascript
const memStats = db.serverStatus().mem;
{
  bits: 64,           // Architecture
  resident: 4096,     // RAM physique utilis√©e (MB)
  virtual: 8192,      // M√©moire virtuelle totale (MB)
  mapped: 0,          // Fichiers mapp√©s (MMAPv1 uniquement)
  mappedWithJournal: 0
}

const wtCache = db.serverStatus().wiredTiger.cache;
{
  "maximum bytes configured": 5368709120,  // 5 GB cache WiredTiger
  "bytes currently in the cache": 4829847552,  // Utilisation actuelle
  "tracked dirty bytes in the cache": 524288000,  // Donn√©es modifi√©es non √©crites
  "pages evicted by application threads": 1234,
  "pages read into cache": 567890,
  "pages written from cache": 123456
}
```

**2. Identification des fuites m√©moire**

**M√©thode de d√©tection :**
```bash
# Surveillance continue de la m√©moire r√©sidente
while true; do
  echo "$(date): $(mongo --quiet --eval 'db.serverStatus().mem.resident')" >> mem_tracking.log
  sleep 300  # Toutes les 5 minutes
done

# Analyse de la tendance
awk '{print $2, $3}' mem_tracking.log | \
  gnuplot -e "set terminal dumb; plot '-' with lines"
```

**Indicateurs de fuite :**
- Croissance lin√©aire continue sans plateau
- Pas de corr√©lation avec le volume de donn√©es ou le nombre de connexions
- Pas de r√©cup√©ration m√©moire apr√®s p√©riodes de faible activit√©

**3. Working Set Analysis**

```javascript
// Estimation du working set
db.serverStatus().wiredTiger.cache["bytes currently in the cache"]
+ db.serverStatus().wiredTiger.cache["tracked dirty bytes in the cache"]

// Comparaison avec la taille des donn√©es acc√©d√©es fr√©quemment
db.stats().dataSize + db.stats().indexSize
```

**Diagnostic :**
- Working set > RAM disponible : Thrashing inevitable, n√©cessite scaling vertical
- Cache eviction rate √©lev√© : Cache sous-dimensionn√©
- Dirty ratio > 20% : Write pressure √©lev√©e, checkpoint contention possible

### Sympt√¥me : I/O Disque √âlev√©

Le disque est souvent le composant le plus lent et peut devenir un goulot majeur.

#### Analyse des I/O

**1. M√©triques syst√®me I/O**

```bash
# iostat - Analyse d√©taill√©e
iostat -x 5 3
# M√©triques critiques :
# - %util : Utilisation du disque (> 80% = probl√©matique)
# - await : Latence moyenne des requ√™tes I/O (> 20ms = lent)
# - svctm : Temps de service (deprecated mais informatif)
# - r/s, w/s : Reads et writes par seconde
# - rkB/s, wkB/s : Throughput en KB/s

# Patterns probl√©matiques :
# - High await + Low throughput : Contention ou disque lent
# - High util + High r/s : Read-heavy workload mal optimis√©
# - High util + High w/s : Write-heavy workload ou checkpoint contention
```

**2. Analyse des patterns d'acc√®s MongoDB**

```javascript
// Statistiques de stockage WiredTiger
const storageStats = db.serverStatus().wiredTiger.blockManager;
{
  "blocks read": 1234567,
  "blocks written": 987654,
  "blocks read per second": 45.2,
  "blocks written per second": 67.8
}

// Ratio read/write
const rwRatio = storageStats["blocks read"] / storageStats["blocks written"];
// Indique le type de workload dominant
```

**3. Identification des requ√™tes I/O intensives**

```javascript
// Recherche des requ√™tes avec beaucoup de documents examin√©s
db.system.profile.find({
  docsExamined: { $gt: 10000 },
  millis: { $gt: 100 }
}).sort({ docsExamined: -1 }).limit(10)

// Analyse des collection scans
db.system.profile.find({
  "planSummary": "COLLSCAN"
}).count()
```

**Patterns typiques :**
- **Cold cache apr√®s red√©marrage** : Pics I/O initiaux puis stabilisation
- **Collection scans fr√©quents** : Absence d'index appropri√©s
- **Hot documents** : Petite portion de donn√©es acc√©d√©e tr√®s fr√©quemment
- **Working set too large** : Cache misses constants

### Sympt√¥me : CPU √âlev√©

L'utilisation CPU √©lev√©e peut indiquer des requ√™tes inefficaces ou un volume de traitement excessif.

#### Analyse CPU

**1. D√©composition de l'utilisation CPU**

```bash
# top/htop - Vue processus
# Identifier les threads MongoDB consommateurs :
# - mongod : Thread principal
# - conn* : Threads de connexion (un par connexion active)
# - ftdc : Full Time Diagnostic Data Capture
# - snapshot : Snapshot threads

# Analyse par thread
top -H -p $(pidof mongod)
```

**2. Analyse du CPU dans MongoDB**

```javascript
// Pas de m√©trique CPU directe dans serverStatus
// Mais analyse indirecte via :

// Operations lentes indiquant CPU bound
db.currentOp({
  "active": true,
  "secs_running": { "$gt": 1 },
  "microsecs_running": { "$gt": 1000000 }
})

// Agr√©gations complexes
db.system.profile.find({
  "command.aggregate": { $exists: true },
  "millis": { $gt: 1000 }
})
```

**3. Identification des causes**

**In-Memory Sorting** :
```javascript
// Sorts sans index, forc√©s en m√©moire
db.system.profile.find({
  "planSummary": /SORT/,
  "executionStats.executionStages.stage": "SORT",
  "executionStats.executionStages.memLimit": 33554432  // 32 MB limit
})
```

**Regex non-optimis√©es** :
```javascript
// Recherche des regex sans ancrage
db.system.profile.find({
  "command.filter": {
    $exists: true
  }
}).forEach(doc => {
  const filter = doc.command.filter;
  for (let key in filter) {
    if (filter[key].$regex && !filter[key].$regex.startsWith("^")) {
      printjson(doc);
    }
  }
})
```

**Agr√©gations complexes** :
```javascript
// Pipelines avec nombreux stages
db.system.profile.find({
  "command.pipeline": { $exists: true }
}).sort({ "command.pipeline.length": -1 })
```

## Analyse des Goulots Sp√©cifiques MongoDB

### Goulot : Index Inefficace ou Manquant

#### D√©tection

**1. Index Usage Statistics**

```javascript
// Statistiques d'utilisation des index
db.collection.aggregate([
  { $indexStats: {} }
])

// Indicateurs :
// - accesses.ops : Nombre d'acc√®s
// - accesses.since : Date du dernier acc√®s
// Index non utilis√©s : candidates pour suppression
```

**2. Analyse explain() syst√©matique**

```javascript
// Audit automatis√© des requ√™tes profil√©es
db.system.profile.find({
  millis: { $gt: 100 }
}).forEach(function(doc) {
  if (doc.command.find) {
    const explainOutput = db[doc.ns.split('.')[1]]
      .find(doc.command.filter)
      .explain("executionStats");

    // Calcul du ratio examined/returned
    const examined = explainOutput.executionStats.totalDocsExamined;
    const returned = explainOutput.executionStats.nReturned;
    const ratio = examined / (returned || 1);

    if (ratio > 10) {
      print(`Inefficient query - Ratio: ${ratio}`);
      printjson(doc.command);
    }
  }
});
```

**Seuils critiques :**
```
Ratio examined/returned :
< 2    : Excellent (index couvrant ou tr√®s s√©lectif)
2-10   : Acceptable
10-100 : Pr√©occupant (index sous-optimal)
> 100  : Critique (collection scan ou index inefficace)
```

### Goulot : Replication Lag

Le lag de r√©plication affecte la coh√©rence et peut saturer la bande passante r√©seau.

#### D√©tection et Analyse

**1. Mesure du lag**

```javascript
// Sur chaque membre du replica set
rs.status().members.forEach(function(member) {
  if (member.state === 2) {  // SECONDARY
    const lag = rs.status().members.find(m => m.state === 1).optimeDate - member.optimeDate;
    print(`Member ${member.name}: Lag = ${lag / 1000} seconds`);
  }
})
```

**2. Analyse de l'oplog**

```javascript
// Taille et fen√™tre de l'oplog
const replSetGetStatus = rs.status();
const oplogs = db.getSiblingDB("local").oplog.rs;

const firstEntry = oplogs.find().sort({$natural: 1}).limit(1).next();
const lastEntry = oplogs.find().sort({$natural: -1}).limit(1).next();

const windowHours = (lastEntry.ts.getTime() - firstEntry.ts.getTime()) / 3600;

print(`Oplog window: ${windowHours} hours`);
// Recommandation : > 24h pour tol√©rer les maintenances
```

**3. Identification des causes**

**Write volume excessif** :
```javascript
// Taux d'√©criture sur le primary
db.serverStatus().opcounters.insert +
db.serverStatus().opcounters.update +
db.serverStatus().opcounters.delete

// Comparaison avec la capacit√© de r√©plication des secondaries
```

**Secondary performance** :
```javascript
// Sur un secondary, v√©rifier la vitesse d'application de l'oplog
db.serverStatus().repl.apply.ops
```

**Network bandwidth** :
```bash
# Monitoring de la bande passante
iftop -i eth0
# ou
nload eth0

# V√©rifier si saturation r√©seau corr√®le avec augmentation du lag
```

### Goulot : Balancer Activity (Sharding)

Les migrations de chunks peuvent impacter significativement les performances.

#### D√©tection

**1. √âtat du balancer**

```javascript
sh.status()
// V√©rifier :
// - balancer state : actif ou non
// - currently-enabled : balancing windows
// - migrations : en cours ou en queue

// D√©tails des migrations
db.getSiblingDB("config").locks.find({state: 2})  // Locks de migration
```

**2. Impact des migrations**

```javascript
// Logs de migration
db.adminCommand({
  getLog: "global"
}).log.filter(entry => entry.includes("moveChunk"))

// M√©triques de migration
sh.status().shards.forEach(function(shard) {
  const shardStats = db.getSiblingDB("admin").runCommand({
    serverStatus: 1,
    sharding: 1
  });

  if (shardStats.sharding) {
    print(`Shard ${shard._id}:`);
    print(`  Migration in progress: ${shardStats.sharding.migrationInProgress}`);
  }
})
```

**3. Identification des probl√®mes**

**Jumbo chunks** :
```javascript
// Chunks non-splittables
db.getSiblingDB("config").chunks.find({
  jumbo: true
})

// Distribution des tailles de chunks
db.getSiblingDB("config").chunks.aggregate([
  { $group: {
      _id: "$ns",
      avgSize: { $avg: "$estimatedDataSize" },
      maxSize: { $max: "$estimatedDataSize" },
      jumboCount: {
        $sum: { $cond: ["$jumbo", 1, 0] }
      }
  }}
])
```

**Hotspots** :
```javascript
// Identification des shards surcharg√©s
sh.status().shards.forEach(function(shard) {
  const stats = db.getSiblingDB(shard.host).serverStatus();
  print(`${shard._id}: ${stats.opcounters.insert + stats.opcounters.update} writes/sec`);
})
```

### Goulot : Connection Pool Exhaustion

L'√©puisement du pool de connexions cr√©e du queuing c√¥t√© application.

#### D√©tection

**1. Monitoring des connexions**

```javascript
// Connexions actuelles vs limites
db.serverStatus().connections
{
  current: 850,
  available: 150,
  totalCreated: 15234,
  rejected: 12,  // Connexions rejet√©es
  active: 342    // Connexions avec op√©ration en cours
}

// Alerter si :
// - available < 10% de la limite
// - rejected > 0
// - active/current ratio < 0.3 (beaucoup de connexions idle)
```

**2. Analyse des connexions actives**

```javascript
// D√©tail des connexions
db.currentOp({ "$all": true }).inprog.forEach(function(op) {
  if (op.client) {
    print(`Client: ${op.client}, Op: ${op.op}, Secs: ${op.secs_running}`);
  }
})

// Regroupement par client
db.currentOp({ "$all": true }).inprog.reduce(function(acc, op) {
  if (op.client) {
    acc[op.client] = (acc[op.client] || 0) + 1;
  }
  return acc;
}, {})
```

**3. Configuration et tuning**

**Limites syst√®me** :
```bash
# V√©rification des limites OS
ulimit -n  # File descriptors
cat /proc/sys/net/ipv4/ip_local_port_range  # Ports disponibles
cat /proc/sys/net/ipv4/tcp_fin_timeout  # TIME_WAIT timeout
```

**Configuration MongoDB** :
```javascript
// Limite de connexions configur√©e
db.serverStatus().connections.maxPoolSize ||
  "unlimited (d√©faut: 65536)"

// Configurable via :
// mongod.conf : net.maxIncomingConnections
// ou --maxConns parameter
```

## Outils de Diagnostic Avanc√©s

### mongotop et mongostat

**mongotop** : Monitoring en temps r√©el du temps pass√© par collection

```bash
mongotop 5  # Refresh toutes les 5 secondes

# Interpr√©ter :
# - Total time : Temps total par collection
# - Read time : Temps en lecture
# - Write time : Temps en √©criture

# Identifier les collections "hot" consommant le plus de ressources
```

**mongostat** : Vue globale des op√©rations

```bash
mongostat --discover 5  # Tous les membres du replica set

# M√©triques cl√©s :
# - insert/query/update/delete : Ops par seconde par type
# - flushes : Nombre de flush du journal
# - mapped/vsize/res : Utilisation m√©moire
# - qrw : Queue read/write (indicateur de contention)
# - arw : Active reads/writes
# - net_in/net_out : Trafic r√©seau

# Pattern critique :
# qrw √©lev√© + arw faible = Bottleneck applicatif ou connexions
# qrw √©lev√© + arw √©lev√© = Saturation ressources MongoDB
```

### FTDC (Full Time Diagnostic Data Capture)

FTDC collecte automatiquement des m√©triques d√©taill√©es toutes les secondes.

**Localisation des fichiers** :
```bash
# Emplacement par d√©faut
ls -lh /var/log/mongodb/diagnostic.data/

# Fichiers metrics.* : Donn√©es brutes BSON compress√©es
```

**Extraction et analyse** :
```python
# Script Python pour parser FTDC
import bson
import gzip

def parse_ftdc(filename):
    with gzip.open(filename, 'rb') as f:
        while True:
            try:
                doc = bson.decode(f.read(4096))
                # Traiter doc...
                yield doc
            except:
                break

# Analyser les patterns temporels
for doc in parse_ftdc('metrics.2025-01-15T10-00-00Z-00000'):
    if 'serverStatus' in doc:
        print(doc['serverStatus']['opcounters'])
```

**M√©triques cl√©s dans FTDC** :
- serverStatus complet toutes les secondes
- replSetGetStatus pour replica sets
- M√©triques OS (CPU, m√©moire, disque)
- M√©triques r√©seau

### Profiler Granulaire

Configuration avanc√©e du profiler pour analyse cibl√©e.

**Profiler avec filtres** :
```javascript
// Profiler niveau 1 avec seuil et filtres
db.setProfilingLevel(1, {
  slowms: 100,
  filter: {
    op: { $in: ["query", "update", "remove"] },
    ns: /^mydb\.important/,  // Seulement certaines collections
    millis: { $gt: 50 }
  }
})

// Profiler avec sampling
db.setProfilingLevel(1, {
  slowms: 50,
  sampleRate: 0.1  // Profiler seulement 10% des op√©rations
})
```

**Analyse avanc√©e du profiler** :
```javascript
// Top requ√™tes par temps total consomm√©
db.system.profile.aggregate([
  { $match: { ts: { $gte: new ISODate("2025-01-15T00:00:00Z") } } },
  { $group: {
      _id: {
        op: "$op",
        ns: "$ns",
        pattern: "$command"
      },
      count: { $sum: 1 },
      totalMs: { $sum: "$millis" },
      avgMs: { $avg: "$millis" },
      maxMs: { $max: "$millis" }
  }},
  { $sort: { totalMs: -1 } },
  { $limit: 20 }
])
```

### Performance Advisor (Atlas)

Pour les d√©ploiements Atlas, le Performance Advisor fournit des recommandations automatiques.

**Types de recommandations** :
- **Index suggestions** : Index recommand√©s bas√©s sur les query patterns
- **Schema suggestions** : Anti-patterns d√©tect√©s dans la mod√©lisation
- **Query suggestions** : Requ√™tes sous-optimales identifi√©es

**Acc√®s programmatique** :
```javascript
// Via Atlas API
curl --user "$ATLAS_PUBLIC_KEY:$ATLAS_PRIVATE_KEY" \
  --digest \
  "https://cloud.mongodb.com/api/atlas/v1.0/groups/$GROUP_ID/processes/$HOST:$PORT/performanceAdvisor/suggestedIndexes"
```

## M√©thodologie de Diagnostic Holistique

### Approche en Couches

L'analyse doit progresser m√©thodiquement √† travers les couches du syst√®me :

**Couche 1 : Application**
- Patterns d'utilisation : Normal vs anomal
- Connection pooling configuration
- Query patterns et fr√©quence
- Retry logic et timeouts

**Couche 2 : Driver MongoDB**
- Configuration du driver (read preference, write concern)
- Connection pool metrics
- Command monitoring
- Driver version et compatibilit√©

**Couche 3 : MongoDB Server**
- Query performance (explain, profiler)
- Index effectiveness
- Lock contention
- Oplog et replication

**Couche 4 : Syst√®me d'Exploitation**
- CPU, m√©moire, disque, r√©seau
- Kernel parameters (TCP tuning, file descriptors)
- Transparent Huge Pages (THP) disabled
- NUMA architecture considerations

**Couche 5 : Infrastructure**
- Stockage (type, IOPS, latence)
- R√©seau (bande passante, latence inter-datacenter)
- Virtualisation overhead
- Cloud provider specifics

### Workflow de Diagnostic

```
1. OBSERVER
   ‚îî‚îÄ> Collecter m√©triques de toutes les couches
   ‚îî‚îÄ> Identifier les anomalies et patterns

2. ORIENTER
   ‚îî‚îÄ> Hypoth√®ses sur la cause racine
   ‚îî‚îÄ> Prioriser par probabilit√© et impact

3. D√âCIDER
   ‚îî‚îÄ> S√©lectionner l'hypoth√®se √† tester
   ‚îî‚îÄ> Planifier le test de validation

4. AGIR
   ‚îî‚îÄ> Impl√©menter le changement
   ‚îî‚îÄ> Mesurer l'impact
   ‚îî‚îÄ> Documenter les r√©sultats

5. R√âP√âTER
   ‚îî‚îÄ> Si non r√©solu, retour √† √©tape 1 avec nouvelles donn√©es
```

### Checklist de Diagnostic Rapide

Pour un diagnostic initial en situation d'urgence :

```
‚òê M√©triques syst√®me basiques (CPU, RAM, I/O)
  ‚îî‚îÄ> top, free, iostat, iftop

‚òê √âtat MongoDB global
  ‚îî‚îÄ> db.serverStatus()
  ‚îî‚îÄ> db.currentOp({ "$all": true })

‚òê R√©plication (si applicable)
  ‚îî‚îÄ> rs.status()
  ‚îî‚îÄ> rs.printSlaveReplicationInfo()

‚òê Sharding (si applicable)
  ‚îî‚îÄ> sh.status()
  ‚îî‚îÄ> Balancer state

‚òê Connexions
  ‚îî‚îÄ> db.serverStatus().connections
  ‚îî‚îÄ> currentOp pour voir connexions actives

‚òê Requ√™tes lentes en cours
  ‚îî‚îÄ> db.currentOp({ "active": true, "secs_running": { $gt: 1 } })

‚òê Logs r√©cents
  ‚îî‚îÄ> db.adminCommand({ getLog: "global" })
  ‚îî‚îÄ> Recherche d'erreurs, warnings, timeouts

‚òê Profiler (si activ√©)
  ‚îî‚îÄ> db.system.profile.find().sort({ts:-1}).limit(10)
```

## Patterns de Goulots Communs et Signatures

### Pattern : "Morning Rush"

**Signature** :
- Pics de latence syst√©matiques √† heures fixes (d√©but journ√©e)
- CPU et I/O √©lev√©s pendant 30-60 minutes
- Normalisation progressive ensuite

**Cause probable** :
- Cold cache apr√®s nuit de faible activit√©
- Batch processes schedul√©s
- Backup windows se terminant

**Diagnostic** :
```javascript
// V√©rifier cache warming rate
db.serverStatus().wiredTiger.cache["pages read into cache"]

// Identifier batch jobs
db.currentOp({
  "secs_running": { $gt: 300 },
  "op": { $in: ["update", "remove"] }
})
```

### Pattern : "Death by Thousand Cuts"

**Signature** :
- D√©gradation progressive sur plusieurs jours/semaines
- Aucun √©v√©nement d√©clencheur identifiable
- Ressources semblent OK mais performance d√©cline

**Causes probables** :
- Croissance des donn√©es au-del√† du working set
- Index fragmention
- Oplog window shrinking
- Memory leak lent

**Diagnostic** :
```javascript
// Tendance croissance donn√©es
db.collection.stats().size
db.collection.stats().count

// Fragmentation index
db.collection.stats().indexSizes
// Comparer avec taille th√©orique bas√©e sur le nombre de documents

// Memory leak check
// Monitoring continu de resident memory sur plusieurs jours
```

### Pattern : "Scatter-Gather Storm"

**Signature** :
- Latence √©lev√©e sur cluster shard√©
- Requ√™tes simples prenant temps excessif
- Network traffic √©lev√© entre mongos et shards

**Cause** :
- Requ√™tes ne contenant pas la shard key
- Forcing de broadcast queries

**Diagnostic** :
```javascript
// Sur mongos, analyse des requ√™tes
db.system.profile.find({
  "execStats.stage": "SHARD_MERGE"
}).forEach(function(doc) {
  print(`Query: ${JSON.stringify(doc.command.filter)}`);
  print(`Shards contacted: ${doc.execStats.nShards}`);
})
```

### Pattern : "Checkpoint Cascade"

**Signature** :
- Pics r√©guliers de write latency toutes les 60 secondes
- Corr√©lation avec checkpoint interval
- Dirty cache ratio √©lev√© avant les pics

**Cause** :
- WiredTiger checkpoint overhead
- Write pressure d√©passant checkpoint capacity

**Diagnostic** :
```javascript
db.serverStatus().wiredTiger.transaction.checkpoint
// Analyser :
// - "transaction checkpoint max time (msecs)"
// - "transaction checkpoint min time (msecs)"
// Si max >> average : checkpoints probl√©matiques

db.serverStatus().wiredTiger.cache["tracked dirty bytes in the cache"]
// Si > 20% du cache : write pressure √©lev√©e
```

## Conclusion

L'identification pr√©cise des goulots d'√©tranglement requiert :

1. **M√©thodologie rigoureuse** : Approche syst√©matique USE et en couches
2. **Observation multi-dimensionnelle** : M√©triques de toutes les couches du stack
3. **Pens√©e corr√©lative** : Lier les sympt√¥mes aux causes racines
4. **Outils appropri√©s** : Ma√Ætrise de mongostat, profiler, explain, FTDC
5. **Pattern recognition** : Reconna√Ætre les signatures de probl√®mes connus

L'identification n'est que la premi√®re √©tape. Une fois le goulot identifi√© avec certitude, les sections suivantes aborderont les techniques sp√©cifiques d'optimisation pour chaque type de goulot.

---

**Points cl√©s :**
- Utiliser la m√©thodologie USE pour l'analyse syst√©matique
- Ne jamais se fier aux moyennes seules, analyser les percentiles
- Corr√©ler les m√©triques de diff√©rentes couches pour identifier la vraie cause racine
- Documenter les patterns observ√©s pour reconnaissance future
- Valider les hypoth√®ses par des tests avant d'impl√©menter des changements en production

‚è≠Ô∏è [Analyse avec explain() approfondie](/17-performance-tuning/02-analyse-explain-approfondie.md)
