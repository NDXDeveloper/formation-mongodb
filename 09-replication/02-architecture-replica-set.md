üîù Retour au [Sommaire](/SOMMAIRE.md)

# 9.2 Architecture Replica Set

## Introduction

L'architecture Replica Set de MongoDB repr√©sente une impl√©mentation sophistiqu√©e du mod√®le de r√©plication single-leader, con√ßue pour offrir haute disponibilit√©, durabilit√© des donn√©es et √©volutivit√© en lecture dans un environnement de production. Cette section explore en profondeur les composants architecturaux, leurs interactions, et les principes de conception qui sous-tendent cette architecture distribu√©e.

## Vue d'Ensemble Architecturale

### D√©finition d'un Replica Set

Un **Replica Set** est un cluster de processus `mongod` interconnect√©s qui maintiennent collectivement le m√™me dataset √† travers un m√©canisme de r√©plication asynchrone. Contrairement √† une simple r√©plication master-slave, un Replica Set int√®gre nativement :

- **√âlection automatique** : S√©lection d√©mocratique d'un nouveau Primary en cas de d√©faillance
- **D√©tection de pannes** : M√©canisme de heartbeat pour identifier les n≈ìuds d√©faillants
- **Reconfiguration dynamique** : Ajout/suppression de membres sans arr√™t du service
- **Gestion de la topologie** : Adaptation automatique aux changements de configuration

### Composants Principaux

L'architecture d'un Replica Set repose sur plusieurs composants fondamentaux :

#### 1. N≈ìuds de Donn√©es (Data-Bearing Nodes)

**Primary Node** :
- Unique n≈ìud acceptant les op√©rations d'√©criture
- Maintient le dataset autoritatif
- G√©n√®re l'Oplog √† partir des √©critures
- Envoie des heartbeats aux autres membres
- Se d√©grade automatiquement (step down) s'il ne peut atteindre la majorit√©

**Secondary Nodes** :
- R√©pliquent les donn√©es depuis le Primary (ou une autre source)
- Appliquent les op√©rations de l'Oplog dans le m√™me ordre
- Peuvent servir les lectures (selon Read Preference)
- Participent aux √©lections en cas de panne du Primary
- Maintiennent leur propre copie de l'Oplog

#### 2. Arbiters (Optionnels)

Les **Arbiters** sont des membres sp√©ciaux du Replica Set qui :
- Participent aux √©lections (votent) mais ne stockent pas de donn√©es
- Ne peuvent jamais devenir Primary
- N√©cessitent des ressources minimales (RAM, CPU, disque)
- Servent de "tie-breaker" dans les topologies √† nombre pair de membres
- **Limitation critique** : Ne contribuent pas √† la durabilit√© des donn√©es

**Note importante** : L'utilisation d'Arbiters est g√©n√©ralement d√©conseill√©e en production au profit de v√©ritables Secondary nodes qui offrent √† la fois redondance et disponibilit√©.

#### 3. Membres Sp√©cialis√©s

**Hidden Members** :
- R√©pliquent les donn√©es mais sont invisibles aux applications
- Ne peuvent pas devenir Primary (priority: 0)
- Utilis√©s pour backup, reporting, analytics sans impact sur les op√©rations
- Votent dans les √©lections (sauf configuration contraire)

**Delayed Members** :
- R√©pliquent avec un d√©lai configurable (ex: 1 heure, 4 heures)
- Protection contre les erreurs humaines (suppression accidentelle)
- Doivent √™tre Hidden (car donn√©es obsol√®tes)
- Ne peuvent pas devenir Primary

**Non-Voting Members** :
- Participent √† la r√©plication mais ne votent pas aux √©lections
- Utiles au-del√† de 7 membres (limite de votants)
- Peuvent devenir Primary si configur√©s avec priority > 0

## Architecture de Communication

### Topologie R√©seau

Un Replica Set forme un **graphe complet** (fully connected mesh) o√π chaque membre peut communiquer directement avec tous les autres membres.

```
         [Primary]
        /    |    \
       /     |     \
      /      |      \
[Secondary] [Secondary] [Secondary]
     \      /  \      /
      \    /    \    /
       \  /      \  /
    (tous connect√©s entre eux)
```

**Implications** :
- Pas de point de passage oblig√© (pas de SPOF au niveau r√©seau)
- Latence directe entre n'importe quelle paire de n≈ìuds
- Complexit√© O(n¬≤) en termes de connexions (n membres)
- Chaque membre maintient n-1 connexions persistantes

### Protocole de Heartbeat

Les membres √©changent p√©riodiquement des messages de heartbeat pour :

#### Fr√©quence et Timing

- **heartbeatIntervalMillis** : 2000 ms (d√©faut)
  - Intervalle entre deux heartbeats cons√©cutifs
  - Chaque membre envoie un heartbeat √† tous les autres
  - Configurable mais rarement modifi√© en production

- **electionTimeoutMillis** : 10000 ms (d√©faut)
  - D√©lai sans heartbeat du Primary avant qu'un Secondary puisse initier une √©lection
  - Balance entre d√©tection rapide et stabilit√© (√©viter les √©lections intempestives)
  - Doit √™tre sup√©rieur √† heartbeatIntervalMillis √ó N pour tol√©rer quelques heartbeats perdus

- **heartbeatTimeoutSecs** : 10 s (d√©faut)
  - Timeout pour consid√©rer un heartbeat comme √©chou√©
  - Un membre est marqu√© comme inaccessible apr√®s ce d√©lai

#### Contenu des Heartbeats

Chaque message heartbeat transporte :
- **√âtat du membre** : PRIMARY, SECONDARY, RECOVERING, ARBITER, etc.
- **OpTime** : Position dans l'Oplog (term, timestamp) ‚Üí permet de mesurer le lag
- **Configuration version** : D√©tecte les d√©synchronisations de configuration
- **Health status** : Indicateurs de sant√© (charge, erreurs)
- **Election metadata** : Informations pour les protocoles d'√©lection

#### D√©tection de D√©faillances

Un membre est consid√©r√© comme d√©faillant si :
1. Aucun heartbeat re√ßu pendant `electionTimeoutMillis`
2. Les heartbeats √©chouent syst√©matiquement (timeout r√©seau)
3. Le membre rapporte explicitement une erreur fatale

**Strat√©gie de d√©tection** :
- **Phi Accrual Failure Detector** : MongoDB utilise une variation de cet algorithme adaptatif
- Accumule l'historique des heartbeats pour estimer la probabilit√© de d√©faillance
- √âvite les faux positifs dus aux variations temporaires de latence

### Canaux de R√©plication

La r√©plication des donn√©es s'effectue via plusieurs canaux logiques :

#### 1. Oplog Streaming

**M√©canisme** :
- Les Secondaries ouvrent un **tailable cursor** sur `local.oplog.rs` du Primary (ou d'une source de r√©plication)
- Le cursor reste ouvert et "suit" les nouvelles entr√©es au fur et √† mesure
- Transmission des op√©rations par batches pour optimiser la bande passante
- Compression optionnelle (snappy, zlib, zstd) pour r√©duire le trafic r√©seau

**Optimisations** :
- **Oplog batching** : Les Secondaries r√©cup√®rent plusieurs op√©rations en une seule requ√™te
- **Parallel application** : Depuis MongoDB 4.0, application parall√®le des op√©rations non conflictuelles
- **Streaming replication** : Introduit dans 4.2, r√©duit la latence en envoyant les donn√©es d√®s leur √©criture

#### 2. Initial Sync

Lorsqu'un nouveau Secondary rejoint ou qu'un Secondary est trop en retard (Oplog gap) :

**Phases de l'Initial Sync** :
1. **Clone des donn√©es** :
   - Copie compl√®te de toutes les bases (sauf `local`)
   - Utilise des cursors parall√®les pour acc√©l√©rer le transfert
   - Peut √™tre interrompu et repris (resumable depuis MongoDB 4.4)

2. **Application Oplog** :
   - Pendant le clone, les nouvelles op√©rations s'accumulent
   - Une fois le clone termin√©, application des op√©rations manqu√©es
   - Peut n√©cessiter plusieurs it√©rations si le d√©bit d'√©criture est √©lev√©

3. **Final catch-up** :
   - Application des derni√®res op√©rations
   - Transition vers l'√©tat SECONDARY
   - D√©but de la r√©plication continue

**Co√ªt** : L'Initial Sync est tr√®s co√ªteux en ressources (CPU, disque, r√©seau) et peut prendre des heures/jours pour de gros datasets.

#### 3. Replication Chaining (Cha√Ænage)

Par d√©faut, les Secondaries r√©pliquent depuis le Primary, mais le **chaining** permet √† un Secondary de r√©pliquer depuis un autre Secondary.

**Avantages** :
- R√©duit la charge r√©seau sur le Primary
- Optimise la bande passante dans les topologies multi-datacenters
- Permet des topologies hi√©rarchiques (ex: DC1 ‚Üí DC2 ‚Üí DC3)

**Configuration** :
- Activ√© par d√©faut (`settings.chainingAllowed: true`)
- MongoDB s√©lectionne automatiquement la meilleure source (nearest avec Oplog √† jour)

**Risques** :
- Augmente le replication lag (effet cascade)
- Complexifie le diagnostic en cas de probl√®me
- Peut √™tre d√©sactiv√© pour forcer la r√©plication depuis le Primary uniquement

## Architecture de Donn√©es

### Organisation du Stockage

Chaque membre d'un Replica Set maintient plusieurs bases et collections critiques :

#### Base de Donn√©es `local`

La base `local` est sp√©ciale : **elle n'est jamais r√©pliqu√©e**. Elle contient :

**local.oplog.rs** :
- Collection capped (taille fixe, FIFO)
- Journal ordonn√© de toutes les op√©rations de modification
- Taille configurable (`oplogSizeMB` au d√©marrage)
- Crucial pour la r√©plication et la r√©cup√©ration

**local.replset.minvalid** :
- Stocke l'OpTime minimal valide apr√®s un crash
- Garantit la coh√©rence apr√®s un red√©marrage
- Utilis√© pour √©viter la lecture de donn√©es partiellement appliqu√©es

**local.replset.oplogTruncateAfterPoint** :
- Point jusqu'o√π l'Oplog peut √™tre tronqu√© en toute s√©curit√©
- Utilis√© lors de la r√©cup√©ration apr√®s panne

**local.system.replset** :
- Configuration actuelle du Replica Set
- Versionn√©e pour d√©tecter les changements de configuration
- Partag√©e entre tous les membres (r√©pliqu√©e via un m√©canisme sp√©cial)

**local.startup_log** :
- Journal des d√©marrages du `mongod`
- Informations de diagnostic (version, configuration, hardware)

#### Bases de Donn√©es Utilisateur

Toutes les bases autres que `local` sont r√©pliqu√©es :
- Stockage WiredTiger avec journalisation
- Isolation MVCC (Multi-Version Concurrency Control)
- Compression configurable (snappy, zlib, zstd, none)
- Checkpoints p√©riodiques pour durabilit√©

### L'Oplog : Structure et S√©mantique

L'Oplog est au c≈ìur de l'architecture de r√©plication. Explorons sa structure en d√©tail.

#### Format des Entr√©es Oplog

Chaque op√©ration dans l'Oplog est un document BSON :

```javascript
{
  "ts": Timestamp(1638360000, 1),     // Timestamp BSON (secondes, ordinal)
  "t": NumberLong(3),                  // Term number (mandat du Primary)
  "h": NumberLong("8124378213847"),    // Hash de l'op√©ration (legacy)
  "v": 2,                              // Version du format Oplog
  "op": "i",                           // Type d'op√©ration (i, u, d, c, n)
  "ns": "mydb.users",                  // Namespace (base.collection)
  "ui": UUID("..."),                   // UUID de la collection
  "wall": ISODate("2021-12-01T12:00:00Z"), // Timestamp mural
  "o": {                               // Objet op√©ration
    "_id": ObjectId("..."),
    "name": "Alice",
    "email": "alice@example.com"
  },
  "o2": { "_id": ObjectId("...") }     // Objet crit√®re (pour updates)
}
```

#### Types d'Op√©rations (op)

- **"i"** (insert) : Insertion d'un document, `o` contient le document complet
- **"u"** (update) : Mise √† jour, `o` contient les modificateurs, `o2` le crit√®re de s√©lection
- **"d"** (delete) : Suppression, `o2` contient le crit√®re
- **"c"** (command) : Commande administrative (createIndex, dropCollection, etc.)
- **"n"** (noop) : Op√©ration no-op (keepalive, marqueurs)

#### Idempotence des Op√©rations

**Principe fondamental** : Toutes les op√©rations de l'Oplog doivent √™tre **idempotentes**, c'est-√†-dire qu'elles peuvent √™tre appliqu√©es plusieurs fois avec le m√™me r√©sultat.

**Transformation pour l'idempotence** :

Original (client) :
```javascript
db.counters.updateOne(
  { _id: "page_views" },
  { $inc: { count: 1 } }
)
```

Oplog (transform√©) :
```javascript
{
  "op": "u",
  "ns": "stats.counters",
  "o": { $v: 1, $set: { count: 42 } },  // Valeur finale, pas l'incr√©ment
  "o2": { _id: "page_views" }
}
```

Cette transformation garantit qu'appliquer l'op√©ration multiple fois (ex: apr√®s un crash) produit le m√™me √©tat final.

#### Dimensionnement de l'Oplog

La taille de l'Oplog est un param√®tre critique :

**Calcul de la taille** :
```
OplogSize (GB) = Taux_√©criture (GB/h) √ó Fen√™tre_souhait√©e (h) √ó Facteur_s√©curit√©
```

**Exemple** :
- Taux d'√©criture : 5 GB/h
- Fen√™tre souhait√©e : 24h (permettre une panne de 24h sans resync)
- Facteur de s√©curit√© : 2x
- **OplogSize = 5 √ó 24 √ó 2 = 240 GB**

**Taille par d√©faut** :
- 5% de l'espace disque disponible
- Minimum 1 GB, maximum 50 GB
- Souvent insuffisant pour la production

**Redimensionnement** :
- Depuis MongoDB 4.0 : `replSetResizeOplog` en ligne (sans red√©marrage)
- Avant 4.0 : n√©cessite arr√™t et modification des fichiers de donn√©es

## Topologies de Replica Set

### Topologie Standard : 3 Membres

La configuration la plus courante et recommand√©e :

```
    [Primary]
      /    \
     /      \
[Secondary] [Secondary]
```

**Propri√©t√©s** :
- **Quorum** : 2 membres sur 3 (majorit√©)
- **Tol√©rance aux pannes** : 1 membre peut tomber sans perte de disponibilit√©
- **Redondance** : 3 copies des donn√©es
- **√âlections** : Toujours possible m√™me avec 1 panne

**Cas d'usage** : Production standard, √©quilibre optimal entre co√ªt et r√©silience.

### Topologie √† 5 Membres

Pour une r√©silience accrue :

```
       [Primary]
      /    |    \
     /     |     \
[Sec1]  [Sec2]  [Sec3]  [Sec4]
```

**Propri√©t√©s** :
- **Quorum** : 3 membres sur 5
- **Tol√©rance aux pannes** : 2 membres peuvent tomber
- **Co√ªt** : 5 serveurs complets (ou Secondaries + Arbiters, d√©conseill√©)
- **Cas d'usage** : Applications critiques, multi-datacenters

### Topologie avec Arbiter (√Ä √âviter)

```
    [Primary]
      /    \
     /      \
[Secondary] [Arbiter]
```

**Probl√®mes** :
- Pas de redondance si le Primary tombe (1 seule copie des donn√©es r√©centes)
- Risque de perte de donn√©es avec `w: 1`
- Protection minimale contre la corruption

**Alternative recommand√©e** : 3 Secondaries complets, m√™me si plus co√ªteux.

### Topologie Multi-Datacenter

Configuration pour haute disponibilit√© g√©ographique :

#### Distribution 2-1

```
    Datacenter A              Datacenter B
    [Primary]                 [Secondary]
    [Secondary]
```

**Avantages** :
- Survie √† la perte d'un datacenter entier
- Latence locale pour les lectures (Read Preference: nearest)

**Inconv√©nients** :
- Si DC-A tombe, pas de quorum possible (1/3) ‚Üí read-only
- N√©cessite un 3√®me site ou un arbiter cloud pour le quorum

#### Distribution 2-2-1 (Recommand√©e)

```
    DC-A              DC-B            DC-C (Cloud)
   [Primary]        [Secondary]      [Secondary]
   [Secondary]      [Secondary]
```

**Avantages** :
- Survie √† la perte d'un DC complet avec quorum
- √âlections possibles apr√®s panne de DC
- Meilleure distribution g√©ographique

**Consid√©ration** : Latence inter-DC impacte le write concern "majority".

#### Distribution avec Priority

Contr√¥ler quel datacenter h√©berge le Primary :

```javascript
cfg = rs.conf()
cfg.members[0].priority = 2  // DC-A: priorit√© haute
cfg.members[1].priority = 2  // DC-A
cfg.members[2].priority = 1  // DC-B: priorit√© normale
cfg.members[3].priority = 1  // DC-B
cfg.members[4].priority = 0.5 // DC-C: priorit√© basse (backup)
rs.reconfig(cfg)
```

Le Primary sera pr√©f√©rentiellement dans DC-A (latence minimale pour les applications principales).

### Topologie avec Membres Hidden

Pour isoler les charges analytiques :

```
[Primary] ‚Üê [Secondary] ‚Üê [Secondary (Reporting)]
                ‚Üë              (Hidden, Priority: 0)
                |
            [Secondary]
```

**Configuration du membre Hidden** :
```javascript
cfg = rs.conf()
cfg.members[3].priority = 0
cfg.members[3].hidden = true
cfg.members[3].tags = { usage: "reporting" }
rs.reconfig(cfg)
```

**Cas d'usage** :
- Requ√™tes analytiques lourdes sans impact sur les op√©rations OLTP
- Backups continus via mongodump/mongorestore
- Extraction ETL vers data warehouses

### Topologie avec Delayed Member

Protection contre les erreurs humaines :

```
[Primary] ‚Üí [Secondary] ‚Üí [Secondary (Delayed 4h)]
                            (Hidden, Priority: 0,
                             SlaveDelay: 14400)
```

**Configuration** :
```javascript
cfg = rs.conf()
cfg.members[4].priority = 0
cfg.members[4].hidden = true
cfg.members[4].slaveDelay = 14400  // 4 heures en secondes
cfg.members[4].votes = 0  // Optionnel: ne vote pas
rs.reconfig(cfg)
```

**Cas d'usage** :
- Protection contre `db.collection.drop()` accidentel
- R√©cup√©ration de donn√©es supprim√©es par erreur (si d√©tect√© dans les 4h)
- Ne remplace PAS les backups r√©guliers

## √âlection et Consensus : Architecture du Protocole

### Protocole Raft dans MongoDB

MongoDB impl√©mente une variation du protocole Raft pour g√©rer le consensus et les √©lections.

#### √âtats des N≈ìuds

Chaque membre peut √™tre dans un des √©tats suivants :

1. **Leader (Primary)** :
   - Accepte les √©critures
   - Envoie des heartbeats r√©guliers (AppendEntries)
   - R√©plique l'Oplog vers les Followers

2. **Follower (Secondary)** :
   - R√©plique depuis le Leader ou une autre source
   - R√©pond aux heartbeats
   - Initie une √©lection si pas de heartbeat pendant `electionTimeout`

3. **Candidate** :
   - √âtat transitoire pendant une √©lection
   - Sollicite les votes des autres membres
   - Devient Leader si majorit√© de votes obtenus

#### M√©canisme d'√âlection

**D√©clencheurs d'√©lection** :
1. D√©marrage initial du Replica Set (tous en √©tat Candidate)
2. Perte de heartbeat du Primary pendant `electionTimeoutMillis`
3. Commande manuelle `rs.stepDown()`
4. Reconfigurations avec changement de priorit√©

**Processus d'√©lection** :

1. **Transition vers Candidate** :
   - Un Secondary ne recevant plus de heartbeat du Primary incr√©mente son **term number**
   - Entre en √©tat CANDIDATE
   - Vote pour lui-m√™me
   - Envoie des **RequestVote RPC** √† tous les autres membres

2. **Crit√®res de Vote** :
   Un membre vote pour un Candidate si :
   - Le term du Candidate est sup√©rieur ou √©gal au term local
   - Le membre n'a pas d√©j√† vot√© dans ce term
   - L'Oplog du Candidate est au moins aussi √† jour (bas√© sur OpTime)
   - La priorit√© du Candidate le permet (priority > 0)
   - Les contraintes de tags sont respect√©es (si configur√©es)

3. **Obtention de la Majorit√©** :
   - Si le Candidate re√ßoit la majorit√© des votes (quorum), il devient PRIMARY
   - Commence imm√©diatement √† envoyer des heartbeats (AppendEntries)
   - Accepte les √©critures

4. **√âchec d'√âlection** :
   - Si aucun Candidate n'obtient la majorit√© (split vote)
   - Nouveau timeout al√©atoire (`electionTimeout` √ó random[1.0, 1.5])
   - Nouvelle tentative d'√©lection (term incr√©ment√©)

**Randomisation** : Le timeout al√©atoire √©vite les √©lections synchronis√©es qui conduiraient √† des split votes r√©p√©t√©s.

#### Term Numbers

Le **term** est un compteur logique monotone qui :
- S'incr√©mente √† chaque nouvelle √©lection
- Identifie de mani√®re unique chaque "mandat" d'un Primary
- Est inclus dans chaque op√©ration de l'Oplog (champ `t`)
- Permet de d√©tecter les configurations obsol√®tes

**Propri√©t√© de s√©curit√©** : Pour un term donn√©, il existe au plus un Leader.

### Priorit√©s et √âligibilit√©

La **priority** influence la probabilit√© qu'un membre devienne Primary :

**Valeurs de priority** :
- **priority: 0** ‚Üí Jamais Primary (Hidden, Delayed, Arbiters)
- **0 < priority ‚â§ 1000** ‚Üí √âligible, valeur plus √©lev√©e = pr√©f√©r√©
- **priority par d√©faut** : 1.0

**Impact sur les √©lections** :
- Un membre avec priority √©lev√©e sollicite l'√©lection plus rapidement (timeout r√©duit)
- Les autres membres pr√©f√®rent voter pour les membres de priorit√© √©lev√©e
- Permet de contr√¥ler quel datacenter h√©berge le Primary

**Exemple** :
```javascript
cfg = rs.conf()
// DC principal (priorit√© haute)
cfg.members[0].priority = 2
cfg.members[1].priority = 2
// DC secondaire (priorit√© normale)
cfg.members[2].priority = 1
cfg.members[3].priority = 1
// DC distant (priorit√© basse, backup)
cfg.members[4].priority = 0.5
rs.reconfig(cfg)
```

### Votes et Quorum

#### Membres Votants vs Non-Votants

- **Votants** : Participent aux √©lections (max 7 dans un Replica Set)
- **Non-votants** : R√©pliquent mais ne votent pas (`votes: 0`)

**Limitation √† 7 votants** : Compromis entre temps de convergence et r√©silience.

#### Calcul du Quorum

**Formule** : Majorit√© = ‚åäN/2‚åã + 1 (o√π N = nombre de membres votants)

**Exemples** :
- 3 membres ‚Üí quorum = 2
- 5 membres ‚Üí quorum = 3
- 7 membres ‚Üí quorum = 4

**Implications** :
- Un Replica Set de 3 membres tol√®re la perte de 1 membre
- Un Replica Set de 5 membres tol√®re la perte de 2 membres
- **Ajouter un 4√®me membre n'am√©liore PAS la tol√©rance aux pannes** (quorum passe de 2 √† 3)

**Recommandation** : Toujours utiliser un nombre impair de membres votants.

## Architecture de Reconfiguration

### Configuration Dynamique

La configuration d'un Replica Set est stock√©e dans un document BSON versionn√© :

```javascript
{
  _id: "myReplicaSet",
  version: 3,  // Incr√©ment√© √† chaque modification
  members: [
    {
      _id: 0,
      host: "mongo1.example.com:27017",
      priority: 2,
      tags: { dc: "east", rack: "1" }
    },
    {
      _id: 1,
      host: "mongo2.example.com:27017",
      priority: 1,
      tags: { dc: "east", rack: "2" }
    },
    {
      _id: 2,
      host: "mongo3.example.com:27017",
      priority: 1,
      tags: { dc: "west", rack: "1" }
    }
  ],
  settings: {
    chainingAllowed: true,
    heartbeatIntervalMillis: 2000,
    electionTimeoutMillis: 10000,
    catchUpTimeoutMillis: -1,
    getLastErrorDefaults: { w: "majority", wtimeout: 5000 }
  }
}
```

### Processus de Reconfiguration

Toute modification de la configuration passe par `rs.reconfig()` :

**√âtapes** :
1. **Validation** : Le Primary valide la nouvelle configuration
2. **Propagation** : La configuration est r√©pliqu√©e comme une op√©ration sp√©ciale
3. **Application** : Chaque membre applique la nouvelle configuration
4. **Version** : Le num√©ro de version est incr√©ment√©

**Reconfiguration s√©curis√©e** :
- N√©cessite un Primary disponible (sauf `force: true`)
- La nouvelle configuration doit avoir un quorum valide
- Les membres existants doivent reconna√Ætre leur nouvelle identit√©

**Reconfiguration forc√©e** (`force: true`) :
- Utilis√©e en cas de perte de quorum (disaster recovery)
- Peut entra√Æner une perte de donn√©es (rollback)
- Ne doit √™tre utilis√©e qu'en dernier recours

### Tags et Read Preference

Les **tags** permettent d'annoter les membres pour le routage intelligent des requ√™tes :

```javascript
cfg = rs.conf()
cfg.members[0].tags = { dc: "nyc", usage: "production" }
cfg.members[1].tags = { dc: "nyc", usage: "production" }
cfg.members[2].tags = { dc: "sfo", usage: "analytics" }
rs.reconfig(cfg)
```

**Utilisation** :
```javascript
db.collection.find().readPref("secondary", [
  { dc: "sfo", usage: "analytics" }  // Pr√©f√©rence
])
```

Permet d'isoler les charges de travail et d'optimiser la latence r√©seau.

## Consid√©rations de Performance

### Impact de la Topologie sur les Performances

#### Latence R√©seau Inter-Membres

La latence entre n≈ìuds impacte directement :

- **Write Concern "majority"** : Le temps d'ACK est limit√© par le Secondary le plus lent
- **√âlections** : Une latence √©lev√©e augmente le temps de convergence
- **Replication Lag** : Les liens lents cr√©ent du retard de r√©plication

**Recommandations** :
- Latence intra-DC : < 1 ms (id√©al)
- Latence inter-DC : < 50 ms (acceptable pour w: majority)
- Latence inter-continent : > 100 ms (probl√©matique, n√©cessite tuning)

#### Bandwidth Requirements

**Estimation de la bande passante** :
```
BW (Mbps) = Taux_√©criture (MB/s) √ó 8 √ó Nombre_Secondaries √ó Overhead
```

**Exemple** :
- Taux d'√©criture : 10 MB/s
- 2 Secondaries
- Overhead (compression, protocole) : 1.5x
- **BW = 10 √ó 8 √ó 2 √ó 1.5 = 240 Mbps**

**Optimisations** :
- Compression Oplog (snappy, zstd) : r√©duction de 50-70%
- Replication chaining : r√©duit la charge sur le Primary
- D√©duplication implicite (batching)

### Dimensionnement des Ressources

#### CPU

- **Primary** : Charge √©lev√©e (√©critures, lectures, g√©n√©ration Oplog)
- **Secondaries** : Charge mod√©r√©e (application Oplog, lectures si Read Preference)
- **Recommandation** : CPU √©quivalent entre Primary et Secondaries pour faciliter le failover

#### RAM

- **Working Set** : Doit tenir en RAM pour performances optimales
- **WiredTiger Cache** : Par d√©faut 50% de RAM - 1 GB (configurable)
- **Recommandation** : RAM(Secondary) ‚â• RAM(Primary)

#### Disque

- **IOPS** : Crucial pour les √©critures et l'application de l'Oplog
- **Latency** : SSD fortement recommand√© en production
- **Espace** : Pr√©voir pour l'Oplog + donn√©es + indexes + marge (20%)

**Formule Oplog** :
```
Espace_Oplog = Max(
  5% √ó Espace_disque_total,
  Taux_√©criture √ó Fen√™tre_souhait√©e √ó 1.5
)
```

## Surveillance et Observabilit√©

### M√©triques Architecturales Cl√©s

**Sant√© du Replica Set** :
- √âtat de chaque membre (PRIMARY, SECONDARY, RECOVERING, etc.)
- Connectivit√© inter-membres (heartbeat success rate)
- Version de configuration (config version mismatch)

**Performances de R√©plication** :
- Replication lag par Secondary
- Oplog window (temps avant que l'Oplog ne soit plein)
- Throughput de r√©plication (ops/sec appliqu√©es)

**√âlections** :
- Nombre d'√©lections (haute fr√©quence = instabilit√©)
- Dur√©e des √©lections (> 30s = probl√®me)
- Raison des √©lections (heartbeat timeout, priority change, etc.)

### Commandes de Monitoring

```javascript
// √âtat complet du Replica Set
rs.status()

// Configuration actuelle
rs.conf()

// Informations sur la r√©plication (vue depuis un membre)
db.getReplicationInfo()  // Info sur l'Oplog local
db.printSlaveReplicationInfo()  // Lag des Secondaries

// Oplog window
db.getReplicationInfo().timeDiff  // En secondes
```

## S√©curit√© de l'Architecture

### Authentication Inter-Membres

Les membres d'un Replica Set doivent s'authentifier mutuellement via :

#### Keyfile Authentication

M√©thode simple pour environnements non critiques :
```bash
# G√©n√©rer une keyfile
openssl rand -base64 756 > /path/to/keyfile
chmod 400 /path/to/keyfile

# Configuration mongod
security:
  keyFile: /path/to/keyfile
  authorization: enabled
```

**Limitations** : S√©curit√© basique (shared secret), pas de r√©vocation granulaire.

#### x.509 Certificate Authentication

M√©thode recommand√©e pour la production :
```yaml
security:
  clusterAuthMode: x509
net:
  tls:
    mode: requireTLS
    certificateKeyFile: /path/to/member-cert.pem
    CAFile: /path/to/ca.pem
```

**Avantages** :
- Cryptographie √† cl√© publique (pas de secret partag√©)
- Identit√© v√©rifiable par certificat
- R√©vocation possible via CRL

### Encryption en Transit

Toutes les communications inter-membres doivent √™tre chiffr√©es :

```yaml
net:
  tls:
    mode: requireTLS
    certificateKeyFile: /path/to/cert.pem
    CAFile: /path/to/ca.pem
    allowConnectionsWithoutCertificates: false
```

**Protocoles** : TLS 1.2+ (TLS 1.3 recommand√©)

### Network Isolation

**Bonnes pratiques** :
- R√©seau priv√© d√©di√© pour la communication inter-membres
- Firewalls restreignant l'acc√®s au port MongoDB (27017)
- VPN ou peering priv√© pour les connexions inter-DC
- Pas d'exposition publique des membres du Replica Set

## Limitations et Contraintes Architecturales

### Limites Num√©riques

- **Maximum 50 membres** par Replica Set
- **Maximum 7 membres votants** (compromis consensus/performance)
- **Maximum 1 Arbiter** recommand√© (plus = inutile)
- **Taille maximale Oplog** : Limit√©e par l'espace disque disponible

### Contraintes de Conception

- **Single-Leader** : Un seul Primary, point de contention pour les √©critures
- **R√©plication asynchrone** : Replication lag in√©vitable (coh√©rence √©ventuelle)
- **Quorum requis** : Perte de la majorit√© ‚Üí cluster read-only
- **Latence inter-DC** : Impacte les performances avec w: "majority"

### Trade-offs Architecturaux

**Nombre de membres** :
- Plus de membres = plus de redondance mais plus de complexit√© et co√ªt
- 3 membres (production standard) vs 5 membres (haute criticit√©)

**Distribution g√©ographique** :
- Latence r√©duite (membres proches) vs r√©silience (distribution large)
- Compromis entre performance locale et disaster recovery

**Write Concern** :
- w:1 (rapide mais risqu√©) vs w:"majority" (lent mais durable)
- √Ä ajuster selon les besoins de l'application

## Conclusion

L'architecture Replica Set de MongoDB est une construction sophistiqu√©e qui int√®gre :

- **Consensus distribu√©** via Raft pour garantir l'√©lection s√ªre d'un leader unique
- **R√©plication asynchrone** avec Oplog idempotent pour la propagation efficace des donn√©es
- **D√©tection de pannes** via heartbeats et timeouts adaptatifs
- **Flexibilit√© topologique** pour s'adapter √† divers besoins (HA, DR, performance)
- **Configurabilit√© fine** via Write/Read Concerns, priorities, tags

**Points cl√©s √† retenir** :

1. Un Replica Set forme un **graphe complet** o√π chaque membre communique avec tous les autres
2. Le **protocole Raft** garantit qu'il n'existe qu'un seul Primary √† tout moment
3. L'**Oplog** est l'√©pine dorsale de la r√©plication, avec des propri√©t√©s d'idempotence et d'ordre total
4. La **topologie** doit √™tre con√ßue en fonction des besoins de disponibilit√©, latence et co√ªt
5. Les **membres sp√©cialis√©s** (Hidden, Delayed) permettent d'isoler les charges de travail
6. La **latence r√©seau** et la **bande passante** sont des facteurs critiques de performance
7. Le **quorum** (majorit√©) est essentiel pour les √©lections et le write concern "majority"

La ma√Ætrise de cette architecture est fondamentale pour concevoir des syst√®mes MongoDB r√©silients, performants et adapt√©s aux exigences de production. Les sections suivantes approfondiront les aspects op√©rationnels : types de membres, m√©canismes d'√©lection, gestion de l'Oplog, et pratiques de maintenance.

‚è≠Ô∏è [Membres d'un Replica Set](/09-replication/03-membres-replica-set.md)
