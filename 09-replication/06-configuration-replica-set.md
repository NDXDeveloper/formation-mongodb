ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 9.6 Configuration d'un Replica Set

## Introduction

La configuration d'un Replica Set est une Ã©tape critique qui dÃ©finit l'architecture de haute disponibilitÃ© de MongoDB. Une configuration appropriÃ©e garantit la rÃ©silience, les performances et la cohÃ©rence des donnÃ©es. Ce chapitre couvre l'ensemble du processus, des configurations de base aux architectures complexes multi-datacenter.

## Architecture de RÃ©fÃ©rence

### Topologie Standard Ã  3 NÅ“uds

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PRIMARY      â”‚  â† Ã‰critures
â”‚  mongodb-01     â”‚
â”‚  Priority: 2    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚SECONDARY â”‚ â”‚SECONDARY â”‚
â”‚mongodb-02â”‚ â”‚mongodb-03â”‚
â”‚Priority:1â”‚ â”‚Priority:1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Topologie Multi-Datacenter (5 NÅ“uds)

```
Datacenter 1 (Principal)          Datacenter 2 (DR)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    PRIMARY      â”‚              â”‚   SECONDARY     â”‚
â”‚  mongodb-dc1-01 â”‚              â”‚  mongodb-dc2-01 â”‚
â”‚  Priority: 10   â”‚              â”‚  Priority: 1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚                   â”‚   SECONDARY     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”           â”‚  mongodb-dc2-02 â”‚
â”‚SECONDARY â”‚ â”‚SECONDARY â”‚           â”‚  Priority: 1    â”‚
â”‚dc1-02    â”‚ â”‚dc1-03    â”‚           â”‚  votes: 0       â”‚
â”‚Priority:9â”‚ â”‚Priority:9â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## PrÃ©paration de l'Infrastructure

### PrÃ©requis MatÃ©riels

**Minimum pour Production** :

| Composant | SpÃ©cification Minimale | RecommandÃ© |
|-----------|----------------------|------------|
| CPU | 4 cores | 8+ cores |
| RAM | 8 GB | 16-32 GB |
| Disque | 100 GB SSD | 500 GB+ NVMe SSD |
| RÃ©seau | 1 Gbps | 10 Gbps |
| Latence rÃ©seau | < 50ms entre membres | < 10ms |

### PrÃ©requis SystÃ¨me

```bash
# 1. DÃ©sactiver Transparent Huge Pages (THP)
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag

# Rendre permanent (/etc/rc.local)
cat >> /etc/rc.local <<EOF
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
EOF

# 2. Limites de fichiers (ulimit)
cat >> /etc/security/limits.conf <<EOF
mongodb soft nofile 64000
mongodb hard nofile 64000
mongodb soft nproc 64000
mongodb hard nproc 64000
EOF

# 3. ParamÃ¨tres kernel
cat >> /etc/sysctl.conf <<EOF
# MongoDB recommandations
vm.swappiness = 1
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
net.core.somaxconn = 4096
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_time = 120
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 8
EOF

sysctl -p

# 4. DÃ©sactiver NUMA (si applicable)
numactl --interleave=all mongod --config /etc/mongod.conf

# 5. Synchronisation NTP
systemctl enable chronyd
systemctl start chronyd
```

### RÃ©solution DNS et Hosts

```bash
# /etc/hosts sur chaque serveur
cat >> /etc/hosts <<EOF
# Replica Set members
10.0.1.10   mongodb-01  mongodb-01.internal.company.com
10.0.1.11   mongodb-02  mongodb-02.internal.company.com
10.0.1.12   mongodb-03  mongodb-03.internal.company.com
EOF

# VÃ©rifier la rÃ©solution
ping -c 3 mongodb-01
ping -c 3 mongodb-02
ping -c 3 mongodb-03
```

## Configuration de Base

### Fichier mongod.conf (Format YAML)

#### Configuration Membre 1 (Primary)

```yaml
# /etc/mongod.conf - mongodb-01

# RÃ©seau
net:
  port: 27017
  bindIp: 0.0.0.0  # Production: spÃ©cifier IPs spÃ©cifiques
  maxIncomingConnections: 65536

# Stockage
storage:
  dbPath: /data/mongodb
  journal:
    enabled: true
    commitIntervalMs: 100
  directoryPerDB: true
  wiredTiger:
    engineConfig:
      cacheSizeGB: 8  # ~50% RAM disponible
      journalCompressor: snappy
      directoryForIndexes: true
    collectionConfig:
      blockCompressor: snappy
    indexConfig:
      prefixCompression: true

# Processus
systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  logRotate: reopen
  timeStampFormat: iso8601-utc
  component:
    replication:
      verbosity: 1

processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongod.pid
  timeZoneInfo: /usr/share/zoneinfo

# RÃ©plication
replication:
  replSetName: rs0
  oplogSizeMB: 10240  # 10 GB
  enableMajorityReadConcern: true

# SÃ©curitÃ© (Ã  activer aprÃ¨s configuration initiale)
# security:
#   authorization: enabled
#   keyFile: /etc/mongodb/keyfile
#   clusterAuthMode: keyFile

# Monitoring
operationProfiling:
  mode: slowOp
  slowOpThresholdMs: 100
  slowOpSampleRate: 1.0
```

#### Configuration Membres 2 et 3 (Secondary)

```yaml
# /etc/mongod.conf - mongodb-02 et mongodb-03
# Identique Ã  mongodb-01, sauf :

net:
  port: 27017
  bindIp: 0.0.0.0

# Adapter dbPath et logs si sur mÃªme machine (dev)
storage:
  dbPath: /data/mongodb  # ou /data/mongodb-02, /data/mongodb-03

systemLog:
  path: /var/log/mongodb/mongod.log  # ou mongod-02.log, mongod-03.log

replication:
  replSetName: rs0  # DOIT Ãªtre identique
```

### CrÃ©ation des RÃ©pertoires

```bash
# Sur chaque serveur
sudo mkdir -p /data/mongodb
sudo mkdir -p /var/log/mongodb
sudo mkdir -p /var/run/mongodb

sudo chown -R mongodb:mongodb /data/mongodb
sudo chown -R mongodb:mongodb /var/log/mongodb
sudo chown -R mongodb:mongodb /var/run/mongodb

sudo chmod 755 /data/mongodb
sudo chmod 755 /var/log/mongodb
```

## DÃ©marrage des Instances

### MÃ©thode 1 : Systemd (RecommandÃ©)

```bash
# Sur chaque serveur

# 1. DÃ©marrer MongoDB
sudo systemctl start mongod

# 2. VÃ©rifier le statut
sudo systemctl status mongod

# 3. Activer au dÃ©marrage
sudo systemctl enable mongod

# 4. VÃ©rifier les logs
sudo tail -f /var/log/mongodb/mongod.log

# 5. VÃ©rifier la connexion
mongosh --host localhost:27017
```

### MÃ©thode 2 : DÃ©marrage Manuel

```bash
# Sur chaque serveur
mongod --config /etc/mongod.conf

# Ou avec numactl
numactl --interleave=all mongod --config /etc/mongod.conf
```

### VÃ©rification Initiale

```bash
# Sur chaque serveur
mongosh --eval "db.serverStatus().ok"
# Devrait retourner : 1

# VÃ©rifier que replication est configurÃ©
mongosh --eval "db.serverStatus().repl"
# Devrait afficher : { setName: 'rs0', ... }
```

## Initialisation du Replica Set

### Connexion au Premier Membre

```bash
# Se connecter au serveur destinÃ© Ã  Ãªtre Primary
mongosh --host mongodb-01:27017
```

### Configuration Initiale Basique

```javascript
// Configuration minimale
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongodb-01:27017" },
    { _id: 1, host: "mongodb-02:27017" },
    { _id: 2, host: "mongodb-03:27017" }
  ]
})
```

**RÃ©ponse attendue** :
```javascript
{
  "ok": 1,
  "$clusterTime": {
    "clusterTime": Timestamp(1705320000, 1),
    "signature": { ... }
  },
  "operationTime": Timestamp(1705320000, 1)
}
```

### Configuration AvancÃ©e avec Options

```javascript
// Configuration complÃ¨te avec toutes les options
rs.initiate({
  _id: "rs0",
  version: 1,

  // Configuration globale
  settings: {
    // Ã‰lection
    electionTimeoutMillis: 10000,
    catchUpTimeoutMillis: -1,  // IllimitÃ©
    catchUpTakeoverDelayMillis: 30000,

    // Heartbeat
    heartbeatIntervalMillis: 2000,
    heartbeatTimeoutSecs: 10,

    // RÃ©plication
    chainingAllowed: true,

    // Write Concern par dÃ©faut (MongoDB 5.0+)
    getLastErrorDefaults: {
      w: "majority",
      wtimeout: 5000
    },

    // Mode de rÃ©plication
    getLastErrorModes: {
      datacenter: {
        dc: 2  // Au moins 2 datacenters
      }
    }
  },

  // Membres
  members: [
    {
      _id: 0,
      host: "mongodb-01:27017",
      priority: 2,       // PrioritÃ© Ã©levÃ©e
      votes: 1,
      tags: {
        dc: "primary",
        rack: "A",
        nodeType: "data"
      }
    },
    {
      _id: 1,
      host: "mongodb-02:27017",
      priority: 1,
      votes: 1,
      tags: {
        dc: "primary",
        rack: "B",
        nodeType: "data"
      }
    },
    {
      _id: 2,
      host: "mongodb-03:27017",
      priority: 1,
      votes: 1,
      tags: {
        dc: "primary",
        rack: "C",
        nodeType: "data"
      }
    }
  ]
})
```

### VÃ©rification de l'Initialisation

```javascript
// 1. VÃ©rifier le statut
rs.status()

// 2. VÃ©rifier la configuration
rs.conf()

// 3. Identifier le Primary
rs.isMaster()

// 4. Afficher les informations de rÃ©plication
rs.printReplicationInfo()
rs.printSecondaryReplicationInfo()
```

## Options de Configuration des Membres

### Structure ComplÃ¨te d'un Membre

```javascript
{
  _id: 0,                    // ID unique dans le Replica Set
  host: "mongodb-01:27017",  // Hostname:port

  // Votes et PrioritÃ©
  votes: 1,                  // 0 ou 1 (max 7 votants)
  priority: 1,               // 0-1000 (0 = ne peut Ãªtre Primary)

  // Tags (pour Read Preference et Write Concern)
  tags: {
    dc: "east",
    rack: "A1",
    nodeType: "data",
    region: "us-east-1",
    usage: "production"
  },

  // Options spÃ©ciales
  arbiterOnly: false,        // Arbiter (ne stocke pas de donnÃ©es)
  hidden: false,             // Invisible pour les applications
  slaveDelay: 0,             // DÃ©lai de rÃ©plication (secondes)
  buildIndexes: true,        // Construire les index

  // Horizons (multi-datacenter, cloud)
  horizons: {
    external: "mongodb-01.external.com:27017",
    internal: "10.0.1.10:27017"
  }
}
```

### Priority (PrioritÃ©)

ContrÃ´le la prÃ©fÃ©rence pour devenir Primary :

```javascript
// Exemple : 3 nÅ“uds avec prioritÃ©s diffÃ©rentes
{
  members: [
    {
      _id: 0,
      host: "mongodb-ssd-01:27017",
      priority: 10  // Serveur haute performance - prÃ©fÃ©rÃ©
    },
    {
      _id: 1,
      host: "mongodb-standard-01:27017",
      priority: 5   // Serveur standard
    },
    {
      _id: 2,
      host: "mongodb-backup:27017",
      priority: 0   // Ne devient JAMAIS Primary
    }
  ]
}
```

**RÃ¨gles** :
- Valeur par dÃ©faut : `1`
- Plage : `0` Ã  `1000`
- `priority: 0` â†’ Membre passif (ne peut pas Ãªtre Primary)
- Plus haute prioritÃ© â†’ plus de chances d'Ãªtre Ã©lu

### Votes

ContrÃ´le la participation aux Ã©lections :

```javascript
// Configuration avec membres non-votants
{
  members: [
    // Membres votants (max 7)
    { _id: 0, host: "mongodb-01:27017", votes: 1 },
    { _id: 1, host: "mongodb-02:27017", votes: 1 },
    { _id: 2, host: "mongodb-03:27017", votes: 1 },

    // Membres non-votants
    { _id: 3, host: "mongodb-analytics:27017", votes: 0, priority: 0 },
    { _id: 4, host: "mongodb-backup:27017", votes: 0, priority: 0 }
  ]
}
```

**Contraintes** :
- Maximum 7 membres votants par Replica Set
- Maximum 50 membres au total
- Un membre avec `votes: 0` ne participe pas aux Ã©lections

### Hidden Members (Membres CachÃ©s)

Membres invisibles pour les applications :

```javascript
{
  _id: 3,
  host: "mongodb-analytics:27017",
  priority: 0,      // Obligatoire
  hidden: true,     // CachÃ© des applications
  votes: 1,         // Peut voter
  tags: {
    usage: "analytics"
  }
}
```

**Cas d'usage** :
- Serveurs analytics/reporting
- Sauvegardes sans impact
- Traitement de donnÃ©es
- Tests de charge

**Note** : Les applications ne peuvent pas lire depuis un hidden member mÃªme avec `readPreference: secondary`.

### Delayed Members (Membres RetardÃ©s)

RÃ©plication avec dÃ©lai (protection contre erreurs) :

```javascript
{
  _id: 4,
  host: "mongodb-delayed:27017",
  priority: 0,        // Obligatoire
  hidden: true,       // RecommandÃ©
  slaveDelay: 3600,   // 1 heure de dÃ©lai
  votes: 1
}
```

**Avantages** :
- Protection contre suppressions accidentelles
- Point de restauration rÃ©cent
- DÃ©tection prÃ©coce de corruption

**Attention** : Le delayed member ne doit jamais devenir Primary (`priority: 0`).

### Arbiter

Membre lÃ©ger qui vote uniquement :

```javascript
{
  _id: 2,
  host: "arbiter:27017",
  arbiterOnly: true,
  votes: 1
}
```

**CaractÃ©ristiques** :
- Ne stocke aucune donnÃ©e
- Participe aux Ã©lections uniquement
- Ressources minimales
- Utile pour nombre pair de membres

**Ajout d'un arbiter** :
```javascript
rs.addArb("arbiter:27017")
```

**DÃ©conseillÃ©** : MongoDB recommande d'utiliser un vrai membre de donnÃ©es plutÃ´t qu'un arbiter.

### Tags (Ã‰tiquettes)

MÃ©tadonnÃ©es pour routage avancÃ© :

```javascript
{
  members: [
    {
      _id: 0,
      host: "mongodb-dc1-01:27017",
      tags: {
        dc: "east",
        region: "us-east-1",
        zone: "us-east-1a",
        nodeType: "ssd",
        workload: "oltp"
      }
    },
    {
      _id: 1,
      host: "mongodb-dc2-01:27017",
      tags: {
        dc: "west",
        region: "us-west-1",
        zone: "us-west-1a",
        nodeType: "ssd",
        workload: "oltp"
      }
    },
    {
      _id: 2,
      host: "mongodb-analytics:27017",
      tags: {
        dc: "east",
        region: "us-east-1",
        zone: "us-east-1b",
        nodeType: "standard",
        workload: "analytics"
      }
    }
  ]
}
```

**Utilisation** :
- Read Preference ciblÃ©e
- Write Concern multi-datacenter
- Isolation des workloads

### Horizons (Multi-RÃ©seau)

Pour environnements avec plusieurs rÃ©seaux (VPN, cloud hybride) :

```javascript
{
  _id: 0,
  host: "mongodb-01.internal:27017",
  horizons: {
    // RÃ©seau interne
    internal: "10.0.1.10:27017",

    // RÃ©seau externe (VPN)
    external: "mongodb-01.vpn.company.com:27017",

    // RÃ©seau public (avec TLS)
    public: "mongodb-01.cloud.provider.com:27017"
  }
}
```

**Connexion** :
```javascript
// Client interne
mongosh "mongodb://mongodb-01.internal:27017/?replicaSet=rs0"

// Client externe
mongosh "mongodb://mongodb-01.vpn.company.com:27017/?replicaSet=rs0&loadBalanced=false"
```

## ParamÃ¨tres Globaux (Settings)

### Configuration des Settings

```javascript
cfg = rs.conf()

// Modifier les settings
cfg.settings = {
  // Timeouts d'Ã©lection
  electionTimeoutMillis: 10000,           // 10 secondes
  catchUpTimeoutMillis: -1,               // IllimitÃ© (ou 30000 pour 30s)
  catchUpTakeoverDelayMillis: 30000,     // 30 secondes

  // Heartbeat
  heartbeatIntervalMillis: 2000,          // Non modifiable en pratique
  heartbeatTimeoutSecs: 10,

  // RÃ©plication
  chainingAllowed: true,                  // RÃ©plication chaÃ®nÃ©e

  // Write Concern par dÃ©faut (MongoDB 4.4+)
  getLastErrorDefaults: {
    w: "majority",
    wtimeout: 5000
  },

  // Write Concern personnalisÃ©s
  getLastErrorModes: {
    // Au moins un membre par datacenter
    multiDC: { dc: 2 },

    // Au moins un membre par zone
    multiZone: { zone: 3 },

    // Tous les membres critiques
    critical: { critical: 3 }
  }
}

// Appliquer la configuration
rs.reconfig(cfg)
```

### electionTimeoutMillis

Temps avant qu'un Secondary initie une Ã©lection :

```javascript
cfg.settings.electionTimeoutMillis = 10000  // 10 secondes (dÃ©faut)

// RÃ©seaux lents/WAN
cfg.settings.electionTimeoutMillis = 30000  // 30 secondes

// RÃ©seaux rapides/LAN
cfg.settings.electionTimeoutMillis = 5000   // 5 secondes
```

### catchUpTimeoutMillis

DurÃ©e maximale de la catchup phase aprÃ¨s Ã©lection :

```javascript
// IllimitÃ© (attend que le nouveau Primary soit Ã  jour)
cfg.settings.catchUpTimeoutMillis = -1

// Pas de catchup (risque de perte de donnÃ©es)
cfg.settings.catchUpTimeoutMillis = 0

// Timeout de 30 secondes
cfg.settings.catchUpTimeoutMillis = 30000
```

### chainingAllowed

Active/dÃ©sactive la rÃ©plication chaÃ®nÃ©e :

```javascript
// Activer (dÃ©faut)
cfg.settings.chainingAllowed = true

// DÃ©sactiver (tous lisent depuis le Primary)
cfg.settings.chainingAllowed = false
```

**Impact** :
- `true` : RÃ©duit la charge sur le Primary
- `false` : Garantit que tous les Secondary ont les mÃªmes donnÃ©es

### getLastErrorModes (Write Concern PersonnalisÃ©)

DÃ©finir des rÃ¨gles de write concern basÃ©es sur les tags :

```javascript
// Configuration des membres avec tags
cfg.members = [
  { _id: 0, host: "dc1-01:27017", tags: { dc: "dc1", critical: "yes" } },
  { _id: 1, host: "dc1-02:27017", tags: { dc: "dc1", critical: "yes" } },
  { _id: 2, host: "dc2-01:27017", tags: { dc: "dc2", critical: "no" } },
  { _id: 3, host: "dc2-02:27017", tags: { dc: "dc2", critical: "yes" } }
]

// DÃ©finir les modes
cfg.settings.getLastErrorModes = {
  // Ã‰criture sur au moins 2 datacenters
  multiDC: { dc: 2 },

  // Ã‰criture sur tous les membres critiques
  allCritical: { critical: 3 }
}

rs.reconfig(cfg)

// Utilisation
db.orders.insertOne(
  { orderId: 123 },
  { writeConcern: { w: "multiDC", wtimeout: 5000 } }
)
```

## Reconfiguration Dynamique

### RÃ©cupÃ©rer la Configuration

```javascript
// MÃ©thode 1 : rs.conf()
cfg = rs.conf()

// MÃ©thode 2 : Commande
cfg = db.adminCommand({ replSetGetConfig: 1 }).config
```

### Modifier la Configuration

```javascript
// 1. RÃ©cupÃ©rer
cfg = rs.conf()

// 2. Modifier (exemple : changer prioritÃ©)
cfg.members[0].priority = 10

// 3. Appliquer
rs.reconfig(cfg)
```

### Reconfig avec Force

Pour situations d'urgence (quorum perdu) :

```javascript
// ATTENTION : Peut causer des incohÃ©rences
rs.reconfig(cfg, { force: true })
```

**Utilisation** :
- MajoritÃ© des membres down
- RÃ©cupÃ©ration d'urgence uniquement
- Risque de split-brain

### Ajouter un Membre

```javascript
// MÃ©thode 1 : rs.add()
rs.add("mongodb-04:27017")

// MÃ©thode 2 : Avec options
rs.add({
  host: "mongodb-04:27017",
  priority: 1,
  votes: 1,
  tags: { dc: "west" }
})

// MÃ©thode 3 : Via rs.reconfig()
cfg = rs.conf()
cfg.members.push({
  _id: 3,
  host: "mongodb-04:27017",
  priority: 1,
  votes: 1
})
rs.reconfig(cfg)
```

### Supprimer un Membre

```javascript
// MÃ©thode 1 : rs.remove()
rs.remove("mongodb-04:27017")

// MÃ©thode 2 : Via rs.reconfig()
cfg = rs.conf()
cfg.members = cfg.members.filter(m => m.host !== "mongodb-04:27017")
rs.reconfig(cfg)
```

### Modifier un Membre Existant

```javascript
cfg = rs.conf()

// Trouver le membre
var member = cfg.members.find(m => m.host === "mongodb-02:27017")

// Modifier les propriÃ©tÃ©s
member.priority = 5
member.tags = { dc: "east", rack: "A" }

// Appliquer
rs.reconfig(cfg)
```

### Reconfiguration Non-Disruptive

Pour minimiser l'impact :

```javascript
// 1. Modifier un Secondary d'abord
cfg = rs.conf()
cfg.members[1].priority = 5
rs.reconfig(cfg)

// 2. Attendre la stabilisation
sleep(5000)

// 3. Modifier d'autres membres
cfg = rs.conf()
cfg.members[2].priority = 3
rs.reconfig(cfg)
```

## Configurations AvancÃ©es

### Configuration Multi-Datacenter

```javascript
rs.initiate({
  _id: "rs0",

  settings: {
    electionTimeoutMillis: 30000,  // 30s pour WAN
    heartbeatTimeoutSecs: 20,
    getLastErrorModes: {
      multiDC: { dc: 2 }
    }
  },

  members: [
    // Datacenter 1 (Principal)
    {
      _id: 0,
      host: "dc1-mongodb-01:27017",
      priority: 10,
      tags: { dc: "dc1", region: "us-east-1" }
    },
    {
      _id: 1,
      host: "dc1-mongodb-02:27017",
      priority: 9,
      tags: { dc: "dc1", region: "us-east-1" }
    },
    {
      _id: 2,
      host: "dc1-mongodb-03:27017",
      priority: 8,
      tags: { dc: "dc1", region: "us-east-1" }
    },

    // Datacenter 2 (DR)
    {
      _id: 3,
      host: "dc2-mongodb-01:27017",
      priority: 1,
      tags: { dc: "dc2", region: "us-west-1" }
    },
    {
      _id: 4,
      host: "dc2-mongodb-02:27017",
      priority: 1,
      votes: 0,  // Non-votant pour Ã©viter split-brain
      tags: { dc: "dc2", region: "us-west-1" }
    }
  ]
})
```

### Configuration AsymÃ©trique (PSA)

Primary-Secondary-Arbiter (non recommandÃ© pour production) :

```javascript
rs.initiate({
  _id: "rs0",
  members: [
    { _id: 0, host: "mongodb-primary:27017" },
    { _id: 1, host: "mongodb-secondary:27017" },
    { _id: 2, host: "arbiter:27017", arbiterOnly: true }
  ]
})
```

**ProblÃ¨mes** :
- Pas de haute disponibilitÃ© rÃ©elle
- Write concern "majority" = 2 nÅ“uds (Primary + Arbiter ou Secondary)
- Pas de protection contre perte du Primary

**Alternative recommandÃ©e** : PSS (3 data members)

### Configuration avec Delayed Backup

```javascript
rs.initiate({
  _id: "rs0",
  members: [
    // Membres principaux
    { _id: 0, host: "mongodb-01:27017", priority: 2 },
    { _id: 1, host: "mongodb-02:27017", priority: 1 },
    { _id: 2, host: "mongodb-03:27017", priority: 1 },

    // Membre retardÃ© (protection)
    {
      _id: 3,
      host: "mongodb-delayed:27017",
      priority: 0,
      hidden: true,
      slaveDelay: 3600,  // 1 heure
      tags: { backup: "delayed" }
    },

    // Membre analytics
    {
      _id: 4,
      host: "mongodb-analytics:27017",
      priority: 0,
      hidden: true,
      votes: 0,
      tags: { usage: "analytics" }
    }
  ]
})
```

### Configuration Cloud Hybride

```javascript
rs.initiate({
  _id: "rs0",

  members: [
    // On-premise (prioritÃ© haute)
    {
      _id: 0,
      host: "onprem-01.internal:27017",
      priority: 10,
      tags: { environment: "onprem", dc: "datacenter1" },
      horizons: {
        internal: "10.0.1.10:27017",
        vpn: "onprem-01.vpn.company.com:27017"
      }
    },
    {
      _id: 1,
      host: "onprem-02.internal:27017",
      priority: 9,
      tags: { environment: "onprem", dc: "datacenter1" }
    },

    // Cloud (DR)
    {
      _id: 2,
      host: "cloud-01.aws.company.com:27017",
      priority: 1,
      tags: { environment: "cloud", provider: "aws", region: "us-east-1" },
      horizons: {
        internal: "172.31.10.10:27017",
        public: "cloud-01.aws.company.com:27017"
      }
    }
  ],

  settings: {
    electionTimeoutMillis: 30000,  // Latence WAN
    getLastErrorModes: {
      distributed: { environment: 2 }  // Un membre on-prem + un cloud
    }
  }
})
```

## SÃ©curisation du Replica Set

### GÃ©nÃ©ration du KeyFile

```bash
# 1. GÃ©nÃ©rer le keyfile sur le Primary
openssl rand -base64 756 > /etc/mongodb/keyfile

# 2. DÃ©finir les permissions
chmod 400 /etc/mongodb/keyfile
chown mongodb:mongodb /etc/mongodb/keyfile

# 3. Copier sur tous les membres
scp /etc/mongodb/keyfile mongodb-02:/etc/mongodb/keyfile
scp /etc/mongodb/keyfile mongodb-03:/etc/mongodb/keyfile

# 4. Permissions sur les autres membres
ssh mongodb-02 "chmod 400 /etc/mongodb/keyfile && chown mongodb:mongodb /etc/mongodb/keyfile"
ssh mongodb-03 "chmod 400 /etc/mongodb/keyfile && chown mongodb:mongodb /etc/mongodb/keyfile"
```

### Activation de l'Authentification

```yaml
# /etc/mongod.conf sur tous les membres
security:
  authorization: enabled
  keyFile: /etc/mongodb/keyfile
  clusterAuthMode: keyFile
```

### CrÃ©ation des Utilisateurs

```javascript
// 1. Se connecter au Primary AVANT d'activer l'auth
mongosh --host mongodb-01:27017

// 2. CrÃ©er l'admin
use admin
db.createUser({
  user: "admin",
  pwd: "SecurePassword123!",
  roles: [
    { role: "root", db: "admin" }
  ]
})

// 3. CrÃ©er un utilisateur application
use myapp
db.createUser({
  user: "appuser",
  pwd: "AppPassword456!",
  roles: [
    { role: "readWrite", db: "myapp" }
  ]
})

// 4. CrÃ©er un utilisateur monitoring
use admin
db.createUser({
  user: "monitoring",
  pwd: "MonitorPass789!",
  roles: [
    { role: "clusterMonitor", db: "admin" },
    { role: "read", db: "local" }
  ]
})
```

### RedÃ©marrage avec Authentification

```bash
# Sur chaque membre
sudo systemctl restart mongod

# VÃ©rifier
mongosh --host mongodb-01:27017 -u admin -p SecurePassword123! --authenticationDatabase admin
```

### Connexion String avec Authentification

```javascript
// Node.js driver
const uri = "mongodb://appuser:AppPassword456!@mongodb-01:27017,mongodb-02:27017,mongodb-03:27017/myapp?replicaSet=rs0&authSource=myapp"

// mongosh
mongosh "mongodb://admin:SecurePassword123!@mongodb-01:27017,mongodb-02:27017,mongodb-03:27017/?replicaSet=rs0&authSource=admin"
```

## Validation et Tests

### VÃ©rifications Post-Configuration

```javascript
// 1. Statut global
rs.status()

// 2. Configuration
rs.conf()

// 3. Membres et Ã©tats
rs.status().members.forEach(m => {
  print(m.name + " - " + m.stateStr + " - Health: " + m.health)
})

// 4. Replication lag
rs.printSecondaryReplicationInfo()

// 5. Oplog
rs.printReplicationInfo()

// 6. Connexions
db.serverStatus().connections

// 7. RÃ©plication
db.serverStatus().repl
```

### Test de Failover

```javascript
// 1. Identifier le Primary
var primary = rs.status().members.find(m => m.state === 1)
print("Primary: " + primary.name)

// 2. Simuler la dÃ©faillance (sur le Primary)
db.adminCommand({ shutdown: 1 })

// 3. Observer l'Ã©lection (sur un Secondary)
while(true) {
  var status = rs.status()
  var newPrimary = status.members.find(m => m.state === 1)
  if (newPrimary) {
    print("New Primary: " + newPrimary.name)
    break
  }
  sleep(1000)
}

// 4. RedÃ©marrer l'ancien Primary
// Il rejoindra comme Secondary
```

### Test de Write Concern

```javascript
// Test majority
db.test.insertOne(
  { test: "majority" },
  { writeConcern: { w: "majority", wtimeout: 5000 } }
)

// Test custom mode (si configurÃ©)
db.test.insertOne(
  { test: "multiDC" },
  { writeConcern: { w: "multiDC", wtimeout: 5000 } }
)

// Test tous les membres
db.test.insertOne(
  { test: "all" },
  { writeConcern: { w: 3, wtimeout: 5000 } }
)
```

### Test de Read Preference

```javascript
// Primary
db.test.find().readPref("primary")

// Secondary
db.test.find().readPref("secondary")

// PrimaryPreferred
db.test.find().readPref("primaryPreferred")

// SecondaryPreferred
db.test.find().readPref("secondaryPreferred")

// Nearest
db.test.find().readPref("nearest")

// Avec tags
db.test.find().readPref("secondary", [{ dc: "east" }])
```

## Bonnes Pratiques

### 1. Topologie

âœ… **Recommandations** :
- Toujours un nombre impair de membres votants (3, 5, 7)
- PSS (3 data members) minimum pour production
- Ã‰viter PSA (Primary-Secondary-Arbiter)
- Maximum 7 votants, jusqu'Ã  50 membres totaux

âŒ **Ã€ Ã©viter** :
- 2 membres seulement (pas de majoritÃ© possible)
- Trop de membres (overhead de rÃ©plication)
- Arbiter si possible (prÃ©fÃ©rer un vrai membre)

### 2. PrioritÃ©s

```javascript
// âœ… Bon : PrioritÃ©s distinctes
{ priority: 10 }  // Serveur haute performance
{ priority: 5 }   // Serveur standard
{ priority: 1 }   // Serveur backup/DR

// âŒ Mauvais : Toutes les prioritÃ©s identiques
{ priority: 1 }
{ priority: 1 }
{ priority: 1 }
// Ã‰lections indÃ©terministes
```

### 3. GÃ©o-Distribution

```javascript
// âœ… Bon : MajoritÃ© dans le DC principal
DC1: 3 membres votants (majoritÃ©)
DC2: 2 membres (1 votant, 1 non-votant)

// âŒ Mauvais : RÃ©partition Ã©gale
DC1: 2 membres
DC2: 2 membres
// Perte de DC = perte de majoritÃ©
```

### 4. Write Concern par DÃ©faut

```javascript
// âœ… RecommandÃ© pour production
cfg.settings.getLastErrorDefaults = {
  w: "majority",
  wtimeout: 5000
}

// âŒ Ã€ Ã©viter
w: 1  // Risque de perte de donnÃ©es
```

### 5. Monitoring

```javascript
// Script de monitoring quotidien
function dailyReplicaSetCheck() {
  var checks = []

  // Check 1: Tous les membres sont UP
  var status = rs.status()
  status.members.forEach(m => {
    if (m.health !== 1 || (m.state !== 1 && m.state !== 2)) {
      checks.push("WARN: " + m.name + " is " + m.stateStr)
    }
  })

  // Check 2: Replication lag < 60s
  status.members.forEach(m => {
    if (m.state === 2) {  // SECONDARY
      var lag = (status.date - m.optimeDate) / 1000
      if (lag > 60) {
        checks.push("WARN: " + m.name + " lag is " + lag + "s")
      }
    }
  })

  // Check 3: Oplog window > 24h
  var replInfo = db.getReplicationInfo()
  var oplogHours = replInfo.timeDiff / 3600
  if (oplogHours < 24) {
    checks.push("WARN: Oplog window is " + oplogHours + "h")
  }

  return checks.length === 0 ? "All checks passed" : checks
}

dailyReplicaSetCheck()
```

## DÃ©pannage

### Membre Ne Rejoint Pas le Replica Set

**SymptÃ´mes** :
```
Member state: STARTUP or STARTUP2
```

**Solutions** :
```javascript
// 1. VÃ©rifier la connectivitÃ© rÃ©seau
rs.status().members

// 2. VÃ©rifier le keyfile (si auth activÃ©e)
// Doit Ãªtre identique sur tous les membres

// 3. VÃ©rifier les logs
db.adminCommand({ getLog: "global" })

// 4. Forcer une resynchronisation
// Sur le membre problÃ©matique
db.adminCommand({ resync: 1 })
```

### Ã‰lections FrÃ©quentes

**Diagnostic** :
```javascript
// VÃ©rifier les Ã©lections rÃ©centes
rs.status().members.forEach(m => {
  if (m.electionTime) {
    print(m.name + " - Last election: " + m.electionDate)
  }
})
```

**Solutions** :
- Augmenter `electionTimeoutMillis`
- VÃ©rifier la latence rÃ©seau
- VÃ©rifier les ressources (CPU, mÃ©moire)
- Stabiliser les prioritÃ©s

### Configuration Corrompue

**RÃ©cupÃ©ration** :
```javascript
// Sur le Primary
var cfg = {
  _id: "rs0",
  version: 1,
  members: [
    { _id: 0, host: "mongodb-01:27017" },
    { _id: 1, host: "mongodb-02:27017" },
    { _id: 2, host: "mongodb-03:27017" }
  ]
}

// Force reconfig (DANGER)
rs.reconfig(cfg, { force: true })
```

## Conclusion

La configuration d'un Replica Set MongoDB est un processus qui nÃ©cessite :

- âœ… **Planification minutieuse** de la topologie
- âœ… **ComprÃ©hension des options** de configuration
- âœ… **Tests rigoureux** avant la production
- âœ… **Monitoring continu** aprÃ¨s dÃ©ploiement
- âœ… **Documentation** des choix architecturaux

**Points clÃ©s** :
1. Toujours un nombre impair de membres votants
2. Configuration des prioritÃ©s selon l'infrastructure
3. Write Concern "majority" par dÃ©faut
4. SÃ©curisation avec keyFile/x.509
5. Tests de failover rÃ©guliers
6. Monitoring proactif des mÃ©triques clÃ©s

Une configuration correcte du Replica Set garantit la haute disponibilitÃ©, la cohÃ©rence des donnÃ©es et la rÃ©silience de votre infrastructure MongoDB.

â­ï¸ [Ajout et suppression de membres](/09-replication/07-ajout-suppression-membres.md)
