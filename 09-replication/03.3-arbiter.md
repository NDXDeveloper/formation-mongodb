üîù Retour au [Sommaire](/SOMMAIRE.md)

# 9.3.3 Arbiter

## Introduction

Un **Arbiter** est un type de membre sp√©cial dans un Replica Set MongoDB qui participe aux √©lections en fournissant un vote, mais qui ne maintient aucune copie des donn√©es. Les Arbiters ont √©t√© introduits dans les premi√®res versions de MongoDB comme une solution √©conomique pour maintenir un nombre impair de votants dans un Replica Set sans le co√ªt d'un serveur complet suppl√©mentaire.

Cependant, l'utilisation d'Arbiters est aujourd'hui **fortement d√©conseill√©e** par MongoDB Inc. et la communaut√© pour la plupart des d√©ploiements de production. Cette section explore le fonctionnement des Arbiters, leurs cas d'usage historiques, leurs limitations critiques, et les alternatives recommand√©es.

## D√©finition et Caract√©ristiques

### Propri√©t√©s Fondamentales

Un Arbiter poss√®de les caract√©ristiques suivantes :

**Ce qu'un Arbiter FAIT** :
- Participe aux √©lections du Primary en votant
- Maintient une connexion au Replica Set
- √âchange des heartbeats avec les autres membres
- D√©tecte les pannes et initie/participe aux √©lections

**Ce qu'un Arbiter NE FAIT PAS** :
- Ne stocke aucune donn√©e (aucune r√©plication)
- Ne peut jamais devenir Primary
- Ne peut pas servir de lectures
- Ne contribue pas √† la durabilit√© des donn√©es
- N'a pas d'Oplog

### Ressources Requises

Les Arbiters sont extr√™mement l√©gers en ressources :

**M√©moire** :
- Quelques dizaines de MB seulement (50-100 MB typique)
- Pas de cache WiredTiger n√©cessaire
- Pas de buffer pour l'Oplog

**CPU** :
- Charge minimale (< 1% d'un core)
- Uniquement pour heartbeats et votes

**Disque** :
- Quasi-nul (< 100 MB)
- Uniquement pour les fichiers de configuration
- Pas de stockage de donn√©es

**R√©seau** :
- Trafic minimal (heartbeats uniquement)
- Pas de r√©plication de donn√©es

**Exemple de d√©ploiement** :
```
Arbiter sur micro-instance cloud :
- 1 vCPU (shared)
- 512 MB RAM
- 5 GB disque
- Co√ªt : ~$5-10/mois (vs $50-200/mois pour Secondary complet)
```

## Architecture et Fonctionnement

### R√¥le dans le Quorum

L'Arbiter sert de **tie-breaker** (briseur d'√©galit√©) dans les sc√©narios √† nombre pair de membres :

#### Sc√©nario Sans Arbiter (Probl√©matique)

```
Configuration : 2 membres (1 Primary + 1 Secondary)

Quorum = 2/2 (majorit√© impossible si un membre tombe)

Panne du Primary :
  Primary : DOWN
  Secondary : UP, ne peut pas s'auto-√©lire (pas de majorit√© 1/2)
  ‚Üí Cluster en read-only, aucun nouveau Primary
```

#### Sc√©nario Avec Arbiter (Solution Historique)

```
Configuration : 2 membres + 1 Arbiter

Quorum = 2/3

Panne du Primary :
  Primary : DOWN
  Secondary : UP, vote pour lui-m√™me (1 vote)
  Arbiter : UP, vote pour Secondary (1 vote)
  ‚Üí Total : 2/3 votes ‚Üí Secondary devient Primary
  ‚Üí Cluster continue de fonctionner
```

L'Arbiter permet d'atteindre la majorit√© sans ajouter un troisi√®me membre complet.

### Participation aux √âlections

#### Vote de l'Arbiter

L'algorithme de vote d'un Arbiter est simplifi√© par rapport √† un Secondary :

**Crit√®res de vote** :

1. **Term Number** : Vote pour le Candidate avec le term le plus √©lev√©
2. **Pas de v√©rification OpTime** : L'Arbiter n'a pas d'Oplog donc ne peut pas v√©rifier la fra√Æcheur des donn√©es
3. **Priority** : Respecte les priorit√©s configur√©es
4. **Vote unique par term** : Ne vote qu'une fois par term

**Pseudo-code** :

```javascript
function arbiterVote(candidate) {
  // V√©rification du term
  if (candidate.term < this.currentTerm) {
    return REJECT  // Candidate obsol√®te
  }

  // V√©rification du vote d√©j√† √©mis
  if (this.votedFor !== null && this.votedFor !== candidate.id) {
    return REJECT  // D√©j√† vot√© pour quelqu'un d'autre
  }

  // V√©rification de l'√©ligibilit√© du Candidate
  if (candidate.priority === 0) {
    return REJECT  // Candidate non-√©ligible
  }

  // L'Arbiter ne peut PAS v√©rifier l'OpTime
  // ‚Üí Vote bas√© uniquement sur term et priority

  this.votedFor = candidate.id
  this.currentTerm = candidate.term
  return ACCEPT
}
```

**Probl√®me Critique** : L'Arbiter ne peut pas garantir que le Candidate √©lu a les donn√©es les plus r√©centes, contrairement aux Secondaries.

#### Heartbeats

L'Arbiter √©change des heartbeats comme les autres membres :

**Contenu des heartbeats de l'Arbiter** :
```javascript
{
  "set": "myReplicaSet",
  "state": 7,              // 7 = ARBITER
  "stateStr": "ARBITER",
  "health": 1,
  "configVersion": 3,
  "self": false
  // Pas d'optime (pas d'Oplog)
  // Pas de syncingTo (pas de r√©plication)
}
```

**Informations manquantes** (par rapport √† un Secondary) :
- Pas d'OpTime (l'Arbiter n'a pas de donn√©es)
- Pas de source de r√©plication
- Pas de replication lag

### Configuration et D√©ploiement

#### Ajout d'un Arbiter

**M√©thode 1 : Lors de l'initialisation du Replica Set**

```javascript
rs.initiate({
  _id: "myReplicaSet",
  members: [
    { _id: 0, host: "mongo1.example.com:27017" },     // Primary
    { _id: 1, host: "mongo2.example.com:27017" },     // Secondary
    { _id: 2, host: "arbiter.example.com:27018", arbiterOnly: true }
  ]
})
```

**M√©thode 2 : Ajout √† un Replica Set existant**

```javascript
// √âtape 1 : D√©marrer le processus mongod pour l'Arbiter
// mongod --replSet myReplicaSet --port 27018 --dbpath /var/lib/mongodb-arbiter

// √âtape 2 : Ajouter l'Arbiter depuis le Primary
rs.addArb("arbiter.example.com:27018")

// Ou de mani√®re plus explicite :
cfg = rs.conf()
cfg.members.push({
  _id: 2,
  host: "arbiter.example.com:27018",
  arbiterOnly: true
})
rs.reconfig(cfg)
```

#### Fichier de Configuration

```yaml
# /etc/mongod-arbiter.conf

# O√π stocker les donn√©es (minimal pour un Arbiter)
storage:
  dbPath: /var/lib/mongodb-arbiter
  journal:
    enabled: false  # Pas de journal pour un Arbiter (pas de donn√©es)

# Configuration r√©seau
net:
  port: 27018  # Port diff√©rent si co-localis√©
  bindIp: 0.0.0.0

# Replica Set
replication:
  replSetName: myReplicaSet

# S√©curit√© (important m√™me pour un Arbiter)
security:
  authorization: enabled
  keyFile: /etc/mongodb-keyfile

# Ressources minimales
systemLog:
  destination: file
  path: /var/log/mongodb/arbiter.log
```

**D√©marrage** :

```bash
mongod --config /etc/mongod-arbiter.conf
```

#### V√©rification

```javascript
rs.status()

// V√©rifier la pr√©sence de l'Arbiter
{
  "members": [
    { "_id": 0, "stateStr": "PRIMARY", ... },
    { "_id": 1, "stateStr": "SECONDARY", ... },
    {
      "_id": 2,
      "name": "arbiter.example.com:27018",
      "health": 1,
      "state": 7,
      "stateStr": "ARBITER",
      "uptime": 3600,
      "lastHeartbeat": ISODate("..."),
      "lastHeartbeatRecv": ISODate("..."),
      "pingMs": NumberLong(1),
      "configVersion": 3,
      "self": false
      // Notez l'absence d'optime et de syncingTo
    }
  ]
}
```

## Cas d'Usage Historiques

### Contraintes Budg√©taires (Ann√©es 2010)

**Contexte historique** :

Dans les premi√®res ann√©es de MongoDB (2009-2015), les Arbiters √©taient couramment recommand√©s pour :

**Sc√©nario typique** :
```
Besoin : Replica Set avec haute disponibilit√©
Budget : Limit√© (2 serveurs physiques uniquement)

Solution avec Arbiter :
  Server1 : Primary (hardware complet, stockage)
  Server2 : Secondary (hardware complet, stockage)
  Arbiter : Micro-instance cloud ($5/mois) ou co-localis√©

Quorum : 2/3 (tol√©rance √† 1 panne)
Co√ªt : ~50% de 3 serveurs complets
```

**Justification** :
- Serveurs physiques co√ªteux (milliers d'euros)
- Cloud computing moins mature et plus cher
- Petits datasets (< 100 GB typique)

### Environnements de Test/D√©veloppement

**Utilisation acceptable** :

```javascript
// Environnement de dev local
rs.initiate({
  _id: "devReplicaSet",
  members: [
    { _id: 0, host: "localhost:27017" },  // Primary
    { _id: 1, host: "localhost:27018" },  // Secondary
    { _id: 2, host: "localhost:27019", arbiterOnly: true }  // Arbiter
  ]
})
```

**Avantages en dev** :
- Simulation d'un Replica Set complet avec ressources minimales
- Test des √©lections et failovers
- Co√ªt nul (m√™me machine)

**Limitation** : Ne refl√®te pas fid√®lement un environnement de production moderne.

### Topologies Multi-Datacenters Asym√©triques

**Sc√©nario legacy** :

```
DC1 (Principal) : 2 membres (Primary + Secondary)
DC2 (Distant)   : 1 Arbiter

Probl√©matique :
  - Pas de budget pour un 3√®me serveur complet dans DC2
  - Besoin de quorum m√™me si DC1 tombe
  - Arbiter comme solution de compromis

Topologie :
  DC1 : [Primary] [Secondary]
  DC2 : [Arbiter]

Quorum : 2/3
Si DC1 tombe : Pas de nouveau Primary (seulement 1/3 disponible)
‚Üí Solution imparfaite mais √©conomique
```

**Probl√®me** : Cette topologie ne prot√®ge pas vraiment contre la perte de DC1.

## Limitations et Probl√®mes Critiques

### 1. Absence de Contribution √† la Durabilit√©

**Probl√®me fondamental** : L'Arbiter ne stocke aucune donn√©e.

#### Sc√©nario de Perte de Donn√©es

```
Configuration : 1 Primary + 1 Secondary + 1 Arbiter

√âtape 1 : √âcriture avec w: 1
  Client √©crit doc { _id: 1, value: "important" }
  Primary acknowledge imm√©diatement

√âtape 2 : R√©plication vers Secondary (asynchrone)
  En cours... (lag de quelques millisecondes)

√âtape 3 : Primary crash AVANT r√©plication compl√®te
  Primary : DOWN (doc perdu)
  Secondary : UP (doc pas encore r√©pliqu√©)
  Arbiter : UP (ne stocke rien)

√âtape 4 : √âlection
  Secondary √©lu nouveau Primary (vote de l'Arbiter)

R√©sultat : doc { _id: 1 } PERDU d√©finitivement
```

**Avec 3 Secondaries complets** :
- Probabilit√© de perte : Faible (tous les 3 doivent √™tre en retard)
- M√™me avec w: 1, g√©n√©ralement au moins 1 Secondary a la donn√©e

**Avec 1 Secondary + 1 Arbiter** :
- Probabilit√© de perte : √âlev√©e (d√©pend uniquement de la vitesse de r√©plication vers 1 Secondary)
- Fen√™tre de vuln√©rabilit√© plus large

#### Statistiques de Risque

**Mod√®le probabiliste** :

```
Hypoth√®ses :
  - Replication lag moyen : 50 ms
  - Taux de panne du Primary : 0.1% par jour (rare)
  - Taux d'√©criture : 1000 ops/sec

Fen√™tre de vuln√©rabilit√© :
  - Op√©rations non r√©pliqu√©es : 1000 ops/sec √ó 0.05 sec = 50 ops
  - Si Primary crash, 50 ops potentiellement perdues

Avec 1 Secondary + Arbiter :
  - Perte de donn√©es lors de crash : ~50 ops

Avec 2 Secondaries (pas d'Arbiter) :
  - Les 2 doivent √™tre en retard simultan√©ment
  - Probabilit√© de perte : ~1/1000 de la configuration Arbiter
```

### 2. Asym√©trie de Tol√©rance aux Pannes

**Probl√®me** : La tol√©rance d√©pend de QUEL membre tombe.

#### Configuration PSA (Primary-Secondary-Arbiter)

```
Membres : P (Primary), S (Secondary), A (Arbiter)

Sc√©nario 1 : Secondary tombe
  P : UP (peut continuer les √©critures)
  S : DOWN
  A : UP
  Quorum : 2/3 (P + A)
  ‚Üí Cluster op√©rationnel, mais AUCUNE redondance des donn√©es

Sc√©nario 2 : Primary tombe
  P : DOWN
  S : UP (devient Primary)
  A : UP
  Quorum : 2/3 (S + A)
  ‚Üí Cluster op√©rationnel, mais AUCUNE redondance des donn√©es

Sc√©nario 3 : Arbiter tombe
  P : UP
  S : UP
  A : DOWN
  Quorum : Impossible (2/2 n√©cessaire, pas de majorit√©)
  ‚Üí Cluster en READ-ONLY (pas de nouvelles √©critures)
```

**Paradoxe** : La perte de l'Arbiter (membre sans donn√©es) est PIRE que la perte d'un membre avec donn√©es.

#### Configuration PSS (Primary-Secondary-Secondary)

```
Membres : P, S1, S2

Sc√©nario 1 : N'importe quel Secondary tombe
  Quorum : 2/3
  ‚Üí Cluster op√©rationnel avec 2 copies des donn√©es

Sc√©nario 2 : Primary tombe
  Quorum : 2/3
  ‚Üí Cluster op√©rationnel avec 2 copies des donn√©es

R√©silience : Toujours 2 copies des donn√©es minimum
```

### 3. Votes Sans Connaissance de l'√âtat des Donn√©es

**Probl√®me** : L'Arbiter vote sans pouvoir v√©rifier l'OpTime des Candidates.

#### Sc√©nario de Split-Brain Potentiel

```
Configuration : P (OpTime: 1000), S1 (OpTime: 999), S2 (OpTime: 900), A

Partition r√©seau :
  Groupe 1 : P, S1
  Groupe 2 : S2, A

√âlections simultan√©es :
  - S1 sollicite vote (OpTime: 999)
  - S2 sollicite vote (OpTime: 900)

Vote de l'Arbiter :
  - Ne peut pas comparer les OpTimes
  - Vote bas√© sur term number uniquement
  - Peut voter pour S2 (donn√©es moins √† jour)

R√©sultat possible :
  - S2 √©lu malgr√© un retard de 100 op√©rations
  - Rollback massif si P revient
```

Un Secondary √† la place de l'Arbiter aurait vot√© pour S1 (OpTime plus r√©cent).

### 4. Probl√®mes de Maintenance et S√©curit√©

#### N√©gligence de l'Arbiter

Les Arbiters, √©tant "moins importants", sont souvent n√©glig√©s :

**Probl√®mes courants** :
- **Patches de s√©curit√©** : Pas mis √† jour r√©guli√®rement
- **Monitoring** : Moins surveill√© que les membres avec donn√©es
- **Hardware d√©grad√©** : Instance cloud arr√™t√©e par erreur
- **Configuration obsol√®te** : Version MongoDB diff√©rente

**Cons√©quence** : L'Arbiter tombe au pire moment (quand il est le plus n√©cessaire).

#### Surface d'Attaque

Un Arbiter compromis peut influencer les √©lections :

```
Sc√©nario d'attaque :
  1. Attaquant compromet l'Arbiter (souvent moins s√©curis√©)
  2. Contr√¥le son vote lors des √©lections
  3. Force l'√©lection d'un Secondary sp√©cifique (ou emp√™che l'√©lection)
  4. Cr√©e une fen√™tre d'attaque sur le cluster
```

**Mitigation** : S√©curiser l'Arbiter autant que les autres membres (mais souvent oubli√©).

### 5. Incompatibilit√© avec Certaines Fonctionnalit√©s

#### Write Concern "majority"

Avec un Arbiter, `w: "majority"` fonctionne diff√©remment :

```
Configuration PSA : Primary + Secondary + Arbiter

Quorum = 2/3

Write Concern "majority" :
  - Attend r√©plication sur 2/3 membres
  - Primary (1) + Secondary (1) = 2/3 ‚úì
  - L'Arbiter ne compte PAS (ne stocke pas de donn√©es)

Probl√®me :
  Si Secondary est lent ou down :
  - w: "majority" TIMEOUT (impossible d'atteindre 2 membres avec donn√©es)
  - Ou force √† attendre un Secondary unique (√©quivalent √† w: 2)
```

**Avec PSS** :
- w: "majority" attend 2/3 = Primary + au moins 1 Secondary
- Si un Secondary est down, l'autre peut satisfaire le quorum

### 6. Complexit√© de Raisonnement

**Probl√®me cognitif** : Les Arbiters compliquent le mod√®le mental.

**Questions fr√©quentes** :
- "Si l'Arbiter tombe, puis-je continuer √† √©crire ?" ‚Üí Non
- "Si le Secondary tombe, mes donn√©es sont-elles safe ?" ‚Üí Non (1 seule copie)
- "w: 'majority' signifie combien de copies ?" ‚Üí D√©pend de la config

**Avec une topologie uniforme (PSS)** : Raisonnement simple, comportement pr√©visible.

## Recommandations Modernes

### Alternative 1 : Trois Secondaries Complets (Recommand√©)

**Configuration recommand√©e** :

```javascript
rs.initiate({
  _id: "myReplicaSet",
  members: [
    { _id: 0, host: "mongo1.example.com:27017", priority: 2 },  // Primary pr√©f√©r√©
    { _id: 1, host: "mongo2.example.com:27017", priority: 1 },  // Secondary
    { _id: 2, host: "mongo3.example.com:27017", priority: 1 }   // Secondary
  ]
})
```

**Avantages** :

1. **Redondance compl√®te** : 3 copies des donn√©es
2. **Tol√©rance sym√©trique** : N'importe quel membre peut tomber
3. **w: "majority" fiable** : 2 copies garanties m√™me avec 1 panne
4. **Simplicit√©** : Comportement uniforme et pr√©visible
5. **Performance** : Scaling en lecture avec Read Preference

**Co√ªt** :
- 3 serveurs complets au lieu de 2.5 (2 + Arbiter)
- Surco√ªt : ~50% par rapport √† PSA
- **Justification** : Vaut largement la durabilit√© et la fiabilit√© accrues

### Alternative 2 : Cluster Shard√©

Pour les cas n√©cessitant vraiment d'optimiser les co√ªts :

**Architecture** :

```
Shard 1 (PSS) : 3 membres dans DC primaire
Config Servers : 3 membres l√©gers
Mongos : Stateless routers

Total : 6 serveurs (3 data + 3 config)
```

**Avantages** :
- √âvolutivit√© horizontale
- Pas d'Arbiters
- Redondance compl√®te

### Alternative 3 : Replica Set √† 5 Membres

Pour haute criticit√© :

```javascript
rs.initiate({
  _id: "myReplicaSet",
  members: [
    { _id: 0, host: "dc1-mongo1:27017", priority: 2 },
    { _id: 1, host: "dc1-mongo2:27017", priority: 2 },
    { _id: 2, host: "dc2-mongo1:27017", priority: 1 },
    { _id: 3, host: "dc2-mongo2:27017", priority: 1 },
    { _id: 4, host: "dc3-mongo1:27017", priority: 0.5 }  // DR
  ]
})
```

**Propri√©t√©s** :
- Quorum : 3/5
- Tol√©rance : 2 pannes simultan√©es
- Protection g√©ographique : Survit √† la perte d'un datacenter complet

### Alternative 4 : Cloud Tier Gratuit (D√©veloppement)

Pour les environnements non-production :

**MongoDB Atlas M0** :
- Replica Set complet (PSS)
- Gratuit jusqu'√† 512 MB
- Pas d'Arbiter n√©cessaire

**Avantages** :
- Simule une vraie configuration de production
- Pas de co√ªt
- Meilleure pratique pour le d√©veloppement

## Quand un Arbiter Peut √ätre Acceptable

### Cas Limites

**1. Environnements de D√©veloppement/Test Locaux**

```javascript
// Dev local avec ressources limit√©es
rs.initiate({
  _id: "localDev",
  members: [
    { _id: 0, host: "localhost:27017" },
    { _id: 1, host: "localhost:27018" },
    { _id: 2, host: "localhost:27019", arbiterOnly: true }
  ]
})
```

**Justification** : Simulation, pas de donn√©es critiques.

**2. Migrations Legacy**

Lorsqu'une infrastructure PSA existante doit √™tre migr√©e progressivement :

```javascript
// √âtape 1 : √âtat actuel (PSA)
members: [P, S, A]

// √âtape 2 : Ajouter un 3√®me Secondary
members: [P, S, A, S2]

// √âtape 3 : Retirer l'Arbiter
members: [P, S, S2]
```

**3. Contraintes Absolues (Rare)**

Tr√®s rares sc√©narios o√π 3 serveurs sont vraiment impossibles :
- Syst√®me embarqu√© avec 2 n≈ìuds physiques uniquement
- Contraintes r√©glementaires interdisant le cloud
- Budget gouvernemental fig√© ne permettant que 2 serveurs

**M√™me dans ces cas** : Envisager d'autres architectures (standalone avec backups fr√©quents).

## Migration Depuis une Architecture avec Arbiter

### Processus de Migration PSA ‚Üí PSS

**Objectif** : Remplacer l'Arbiter par un Secondary complet sans interruption.

#### √âtape 1 : Pr√©paration

```bash
# Provisionner un nouveau serveur
# Installer MongoDB (m√™me version que le cluster)
# Configurer le stockage et la s√©curit√©
```

#### √âtape 2 : Ajouter le Nouveau Secondary

```javascript
// Depuis le Primary
rs.add({
  _id: 3,
  host: "mongo4.example.com:27017",
  priority: 1,
  votes: 1
})

// V√©rifier la progression de l'initial sync
rs.status().members.find(m => m._id === 3)
```

**√âtat temporaire** : PSSA (4 membres dont 1 Arbiter)

#### √âtape 3 : Attendre la Synchronisation Compl√®te

```javascript
// Surveiller jusqu'√† ce que le nouveau membre soit en √©tat SECONDARY
watch -n 5 'mongo --quiet --eval "rs.status().members.find(m => m._id === 3).stateStr"'

// Attendre : STARTUP2 ‚Üí RECOVERING ‚Üí SECONDARY
```

#### √âtape 4 : Retirer l'Arbiter

```javascript
// Identifier l'Arbiter
rs.conf().members.filter(m => m.arbiterOnly === true)

// Retirer l'Arbiter
rs.remove("arbiter.example.com:27018")

// Ou par ID
cfg = rs.conf()
cfg.members = cfg.members.filter(m => m._id !== 2)  // ID de l'Arbiter
rs.reconfig(cfg)
```

#### √âtape 5 : V√©rification

```javascript
rs.status()

// V√©rifier :
// - 3 membres (pas d'Arbiter)
// - Tous en √©tat SECONDARY ou PRIMARY
// - Quorum = 2/3
```

#### √âtape 6 : D√©commissionnement de l'Arbiter

```bash
# Arr√™ter le processus mongod de l'Arbiter
sudo systemctl stop mongod-arbiter

# Supprimer les donn√©es (minimales)
sudo rm -rf /var/lib/mongodb-arbiter

# D√©sinstaller ou r√©affecter le serveur
```

### Migration avec Changement de Topologie

**De PSA √† topologie g√©ographiquement distribu√©e** :

```javascript
// √âtat initial : PSA dans un seul datacenter
members: [
  { _id: 0, host: "dc1-p", priority: 1 },
  { _id: 1, host: "dc1-s", priority: 1 },
  { _id: 2, host: "dc1-a", arbiterOnly: true }
]

// √âtat cible : PSS multi-DC
members: [
  { _id: 0, host: "dc1-p", priority: 2, tags: { dc: "dc1" } },
  { _id: 1, host: "dc1-s", priority: 2, tags: { dc: "dc1" } },
  { _id: 3, host: "dc2-s", priority: 1, tags: { dc: "dc2" } }
]

// Processus :
// 1. Ajouter dc2-s
// 2. Attendre synchronisation
// 3. Retirer Arbiter
```

**Avantages** :
- Protection g√©ographique
- Pas d'Arbiter
- Tol√©rance √† la perte d'un datacenter (si DC1 tombe, dc2-s existe mais pas de quorum)

**Pour vrai DR** : Ajouter un 3√®me Secondary dans DC2 ou DC3.

## Monitoring et Troubleshooting

### Surveillance d'un Arbiter

M√™me si d√©conseill√©s, les Arbiters existants doivent √™tre monitor√©s :

#### M√©triques Critiques

```javascript
rs.status().members.filter(m => m.arbiterOnly === true)

{
  "_id": 2,
  "name": "arbiter.example.com:27018",
  "health": 1,           // CRITIQUE : 0 = down
  "state": 7,            // 7 = ARBITER
  "stateStr": "ARBITER",
  "uptime": 86400,
  "lastHeartbeat": ISODate("..."),
  "lastHeartbeatRecv": ISODate("..."),
  "pingMs": NumberLong(5),  // Latence r√©seau
  "configVersion": 3
}
```

**Alertes** :
- `health: 0` ‚Üí Arbiter DOWN (critique si Secondary aussi down)
- `pingMs > 100` ‚Üí Latence r√©seau √©lev√©e (risque d'√©lections lentes)
- `lastHeartbeat` ancien ‚Üí Communication interrompue

### Probl√®mes Courants

#### Arbiter Inaccessible

**Sympt√¥mes** :
- `health: 0` dans rs.status()
- Messages dans les logs : "can't connect to arbiter"

**Diagnostic** :

```bash
# V√©rifier la connectivit√© r√©seau
ping arbiter.example.com
telnet arbiter.example.com 27018

# V√©rifier le processus mongod
ssh arbiter.example.com
ps aux | grep mongod

# V√©rifier les logs
tail -f /var/log/mongodb/arbiter.log
```

**Causes fr√©quentes** :
- Instance cloud arr√™t√©e (√©conomie de co√ªts mal comprise)
- Firewall bloquant le port
- Processus mongod crashed
- Configuration r√©seau chang√©e

**R√©solution** :
```bash
# Red√©marrer le mongod
sudo systemctl start mongod-arbiter
```

#### Arbiter avec Version Diff√©rente

**Probl√®me** : Arbiter sur MongoDB 4.4, cluster sur 5.0

**Sympt√¥mes** :
- √âlections √©chouent
- Incompatibilit√© de protocole

**R√©solution** :
```bash
# Mettre √† jour l'Arbiter √† la m√™me version
sudo apt-get install mongodb-org=5.0.X
sudo systemctl restart mongod-arbiter
```

#### Split Vote Persistant

Avec un Arbiter, les split votes sont plus fr√©quents :

**Sc√©nario** :
```
Primary tombe, 2 Secondaries se pr√©sentent simultan√©ment
Secondary 1 : Vote pour lui-m√™me (1)
Secondary 2 : Vote pour lui-m√™me (1)
Arbiter : Vote pour S1 (ou S2)

Si timing serr√© : Votes partag√©s ‚Üí Pas de majorit√© ‚Üí Nouvelle √©lection
```

**R√©solution** : Augmenter `electionTimeoutMillis` pour r√©duire les collisions :

```javascript
cfg = rs.conf()
cfg.settings.electionTimeoutMillis = 15000  // 15 secondes
rs.reconfig(cfg)
```

## Consid√©rations de S√©curit√©

### S√©curisation d'un Arbiter

M√™me sans donn√©es, l'Arbiter doit √™tre s√©curis√© :

#### Authentication

```yaml
# mongod-arbiter.conf
security:
  authorization: enabled
  keyFile: /etc/mongodb-keyfile  # M√™me keyfile que le cluster
```

**Important** : M√™me keyfile/certificat que les autres membres.

#### TLS/SSL

```yaml
net:
  tls:
    mode: requireTLS
    certificateKeyFile: /etc/ssl/arbiter.pem
    CAFile: /etc/ssl/ca.pem
```

#### Firewall

```bash
# Autoriser uniquement les membres du Replica Set
iptables -A INPUT -p tcp --dport 27018 -s 10.0.1.10 -j ACCEPT  # Primary
iptables -A INPUT -p tcp --dport 27018 -s 10.0.1.11 -j ACCEPT  # Secondary
iptables -A INPUT -p tcp --dport 27018 -j DROP
```

### Risques Sp√©cifiques

**1. Compromission de l'Arbiter** :
- Influence malveillante des √©lections
- D√©ni de service (refus de voter)

**2. Arbiter Fant√¥me** :
- Arbiter ancien toujours en ex√©cution apr√®s suppression de la config
- Peut perturber les √©lections

**3. Co-localisation** :
- Arbiter sur la m√™me machine qu'un autre membre ‚Üí Perte de r√©silience
- Si la machine tombe, perte de 2 votants

## Documentation et Bonnes Pratiques

### Documentation de la Topologie

**Dans la documentation d'infrastructure** :

```markdown
## Replica Set Configuration

Type: PSA (Primary-Secondary-Arbiter)
‚ö†Ô∏è WARNING: Configuration non-recommand√©e, migration vers PSS pr√©vue Q2 2025

Members:
- mongo1.example.com:27017 (Primary, DC1)
- mongo2.example.com:27017 (Secondary, DC1)
- arbiter.example.com:27018 (Arbiter, DC1)

Quorum: 2/3
Fault Tolerance: 1 membre (asym√©trique)
Data Copies: 2 (Primary + Secondary uniquement)

Known Limitations:
- Perte de donn√©es possible avec w:1 si Primary crash
- Cluster read-only si Arbiter tombe
- Pas de protection g√©ographique

Migration Plan: [Link to migration doc]
```

### Politique de D√©ploiement

**Interdiction par d√©faut** :

```yaml
# infrastructure-policy.yml

mongodb_replica_set:
  allowed_topologies:
    - PSS   # 3 data-bearing members
    - PSSSS # 5 data-bearing members

  prohibited_topologies:
    - PSA   # Primary-Secondary-Arbiter

  exceptions:
    - environment: development
      justification: "Local dev environment, no critical data"
    - environment: legacy-migration
      justification: "Temporary during migration to PSS"
      expiry_date: "2025-06-30"
```

### Code Review Checklist

```markdown
## MongoDB Replica Set Deployment Review

- [ ] No Arbiters used (unless explicitly justified)
- [ ] Minimum 3 data-bearing members
- [ ] All members have equivalent hardware
- [ ] Geographic distribution documented
- [ ] Write Concern defaults to "majority"
- [ ] Migration plan from PSA if applicable
- [ ] Monitoring alerts configured
- [ ] Backup strategy documented
```

## Conclusion

Les **Arbiters** dans MongoDB sont un h√©ritage des premi√®res ann√©es de la technologie, con√ßus comme une solution √©conomique pour maintenir un quorum dans les Replica Sets. Bien qu'ils soient toujours support√©s techniquement, leur utilisation est **fortement d√©conseill√©e** en production moderne pour plusieurs raisons critiques :

**Probl√®mes fondamentaux** :
1. **Absence de redondance** : Ne contribuent pas √† la durabilit√© des donn√©es
2. **Asym√©trie de tol√©rance** : La perte de l'Arbiter est pire que la perte d'un Secondary
3. **Risque de perte de donn√©es** : Fen√™tre de vuln√©rabilit√© √©largie avec w:1
4. **Complexit√©** : Comportement moins pr√©visible que les topologies uniformes
5. **Limitations fonctionnelles** : Probl√®mes avec w:"majority", votes sans connaissance OpTime

**Alternatives recommand√©es** :
- **Production** : 3 Secondaries complets (PSS) minimum
- **Haute criticit√©** : 5 membres avec distribution g√©ographique
- **D√©veloppement** : Atlas M0 gratuit ou PSS local
- **Legacy** : Migration planifi√©e vers PSS

**Points cl√©s √† retenir** :

1. **Ne jamais utiliser d'Arbiters en production** sauf contraintes absolues document√©es
2. Les **√©conomies de co√ªts** (~30%) ne justifient pas les risques de durabilit√©
3. Les topologies **uniformes (PSS)** sont plus simples, plus fiables et plus pr√©visibles
4. Si un Arbiter existe, **planifier sa migration** vers un Secondary complet
5. **S√©curiser et monitorer** les Arbiters existants en attendant leur remplacement

Dans l'√©cosyst√®me MongoDB moderne (2024-2025), avec l'accessibilit√© du cloud, la baisse des co√ªts de stockage, et les exigences accrues en durabilit√© des donn√©es, les Arbiters n'ont plus de justification valable en production. Investir dans un troisi√®me membre complet est toujours pr√©f√©rable pour la fiabilit√©, la simplicit√© et la tranquillit√© d'esprit op√©rationnelle.

‚è≠Ô∏è [Hidden](/09-replication/03.4-hidden.md)
