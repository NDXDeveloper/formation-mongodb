üîù Retour au [Sommaire](/SOMMAIRE.md)

# 9.1 Concepts de R√©plication

## Introduction

La r√©plication est un m√©canisme fondamental dans les syst√®mes de bases de donn√©es distribu√©es modernes, permettant de maintenir plusieurs copies des m√™mes donn√©es sur diff√©rents n≈ìuds d'un cluster. Dans MongoDB, la r√©plication ne se limite pas √† une simple duplication passive des donn√©es : elle constitue l'infrastructure m√™me qui garantit la disponibilit√©, la durabilit√© et la coh√©rence des donn√©es dans un environnement distribu√© potentiellement hostile (pannes mat√©rielles, partitions r√©seau, latences variables).

Cette section explore les concepts th√©oriques et pratiques qui sous-tendent la r√©plication dans MongoDB, en les situant dans le contexte plus large de la th√©orie des syst√®mes distribu√©s.

## D√©finition et Objectifs de la R√©plication

### D√©finition Formelle

La **r√©plication** est le processus de cr√©ation et de maintenance de copies multiples (r√©plicas) d'un ensemble de donn√©es sur diff√©rents n≈ìuds physiquement s√©par√©s d'un syst√®me distribu√©. Chaque r√©plica contient id√©alement une copie compl√®te et √† jour des donn√©es, bien que dans la pratique, un certain degr√© de divergence temporaire soit in√©vitable en raison des contraintes du r√©seau et du th√©or√®me CAP.

### Objectifs Fondamentaux

La r√©plication dans MongoDB poursuit plusieurs objectifs interconnect√©s :

#### 1. Haute Disponibilit√© (High Availability)

La haute disponibilit√© d√©signe la capacit√© d'un syst√®me √† rester op√©rationnel et accessible m√™me en pr√©sence de d√©faillances. Dans le contexte de MongoDB :

- **Tol√©rance aux pannes mat√©rielles** : Si un serveur h√©bergeant un r√©plica subit une d√©faillance mat√©rielle (disque dur, alimentation, carte m√®re), les autres r√©plicas continuent de servir les requ√™tes.

- **Maintenance sans interruption** : Les op√©rations de maintenance (mises √† jour syst√®me, patches de s√©curit√©, upgrades MongoDB) peuvent √™tre effectu√©es sur un n≈ìud √† la fois sans arr√™ter le service global via une technique appel√©e "rolling restart".

- **Failover automatique** : En cas de perte du n≈ìud Primary, le syst√®me d√©tecte automatiquement la d√©faillance et promeut un n≈ìud Secondary en nouveau Primary, g√©n√©ralement en quelques secondes (typiquement 2-12 secondes selon la configuration).

- **Disponibilit√© g√©ographique** : En distribuant les r√©plicas sur plusieurs datacenters ou r√©gions cloud, le syst√®me peut survivre √† des pannes r√©gionales enti√®res (catastrophes naturelles, coupures √©lectriques massives, pannes de datacenter).

#### 2. Durabilit√© des Donn√©es (Data Durability)

La durabilit√© garantit que les donn√©es committ√©es survivent aux pannes :

- **Redondance** : En maintenant N copies des donn√©es, la probabilit√© de perte totale diminue exponentiellement avec N (en supposant des d√©faillances ind√©pendantes).

- **Protection contre la corruption** : Si un r√©plica subit une corruption de donn√©es (corruption du syst√®me de fichiers, erreurs de m√©moire ECC), les autres r√©plicas fournissent une source fiable pour la restauration.

- **Fen√™tre de r√©cup√©ration √©tendue** : L'Oplog permet de "rejouer" les op√©rations sur plusieurs heures ou jours, facilitant la r√©cup√©ration point-in-time.

#### 3. √âvolutivit√© en Lecture (Read Scalability)

- **Distribution de charge** : Les applications peuvent distribuer les requ√™tes de lecture entre plusieurs r√©plicas, augmentant ainsi le d√©bit total de lecture du syst√®me.

- **Isolation des charges** : Les requ√™tes analytiques lourdes peuvent √™tre dirig√©es vers des r√©plicas d√©di√©s, prot√©geant le Primary des impacts sur les performances transactionnelles.

- **Proximit√© g√©ographique** : Les lectures peuvent √™tre servies depuis le r√©plica le plus proche g√©ographiquement de l'utilisateur, r√©duisant la latence.

#### 4. Isolation et S√©gr√©gation des Charges de Travail

- **Reporting et analytics** : Ex√©cution de requ√™tes complexes et co√ªteuses sur des Secondaries d√©di√©s sans impacter les op√©rations OLTP.

- **Backup op√©rationnels** : Utilisation de membres Hidden pour effectuer des sauvegardes sans perturber les performances du cluster.

- **Testing et d√©veloppement** : Des r√©plicas peuvent servir de source de donn√©es pour les environnements de test.

## Mod√®les de R√©plication : Contexte Th√©orique

### R√©plication Synchrone vs Asynchrone

La distinction entre r√©plication synchrone et asynchrone est cruciale pour comprendre les compromis de MongoDB :

#### R√©plication Synchrone (Synchronous Replication)

**Principe** : Lorsqu'une op√©ration d'√©criture est soumise, le syst√®me attend que tous (ou un quorum) de r√©plicas aient confirm√© avoir persist√© l'op√©ration avant de renvoyer un accus√© de r√©ception au client.

**Avantages** :
- Coh√©rence forte garantie : les lectures voient toujours les √©critures pr√©c√©dentes
- Pas de perte de donn√©es en cas de panne du Primary (si quorum est utilis√©)
- Simplicit√© du mod√®le mental pour les d√©veloppeurs

**Inconv√©nients** :
- Latence √©lev√©e : le temps de r√©ponse est limit√© par le r√©plica le plus lent
- Disponibilit√© r√©duite : si un r√©plica est inaccessible, les √©critures peuvent √™tre bloqu√©es
- Throughput limit√© par la capacit√© des r√©plicas les plus lents

**Exemples de syst√®mes** : Google Spanner (avec TrueTime), certaines configurations de MySQL avec r√©plication semi-synchrone.

#### R√©plication Asynchrone (Asynchronous Replication)

**Principe** : Le Primary accuse r√©ception de l'√©criture d√®s qu'elle est persist√©e localement, puis propage l'op√©ration aux Secondaries de mani√®re asynchrone, sans attendre leur confirmation.

**Avantages** :
- Latence minimale : le client n'attend que la persistance locale
- Haute performance : le d√©bit d'√©criture n'est pas limit√© par les r√©plicas
- Tol√©rance aux ralentissements : un r√©plica lent n'impacte pas les performances globales

**Inconv√©nients** :
- Fen√™tre de perte potentielle : les √©critures r√©centes peuvent √™tre perdues si le Primary tombe avant propagation
- Coh√©rence √©ventuelle : les lectures sur Secondaries peuvent voir des donn√©es obsol√®tes
- Complexit√© accrue pour l'application (gestion du replication lag)

**Impl√©mentation MongoDB** : Par d√©faut, MongoDB utilise la r√©plication asynchrone, mais offre une granularit√© fine via les Write Concerns.

#### Mod√®le Hybride de MongoDB : Write Concern

MongoDB transcende la dichotomie synchrone/asynchrone via le concept de **Write Concern**, permettant de sp√©cifier par op√©ration :

```javascript
db.collection.insertOne(
  { document },
  { writeConcern: { w: "majority", j: true, wtimeout: 5000 } }
)
```

Param√®tres :
- **w (write acknowledgement)** :
  - `w: 1` (d√©faut) - Asynchrone pur, ACK d√®s √©criture sur le Primary
  - `w: "majority"` - Semi-synchrone, ACK apr√®s r√©plication sur la majorit√©
  - `w: <nombre>` - ACK apr√®s r√©plication sur N membres
  - `w: 0` - Fire-and-forget, pas d'ACK (d√©conseill√©)

- **j (journal)** :
  - `j: true` - Garantit la persistance sur disque via le journal WiredTiger
  - `j: false` - ACK d√®s √©criture en m√©moire

- **wtimeout** : Timeout en millisecondes, √©vite le blocage ind√©fini

Ce mod√®le permet d'ajuster le compromis latence/durabilit√© au niveau de chaque op√©ration.

### Single-Leader vs Multi-Leader vs Leaderless

#### Single-Leader (Master-Slave / Primary-Secondary)

**Architecture MongoDB** : C'est le mod√®le adopt√© par MongoDB avec les Replica Sets.

**Caract√©ristiques** :
- Un seul n≈ìud (Primary) accepte les √©critures
- Les Secondaries r√©pliquent depuis le Primary
- Les lectures peuvent √™tre distribu√©es (selon Read Preference)

**Avantages** :
- Pas de conflits d'√©criture (une seule source de v√©rit√©)
- Mod√®le de coh√©rence plus simple
- Impl√©mentation relativement directe

**Inconv√©nients** :
- Point de contention unique pour les √©critures
- D√©pendance √† la disponibilit√© du Primary
- Latence pour les √©critures distantes g√©ographiquement du Primary

**Gestion des pannes** : √âlection automatique d'un nouveau Primary selon un protocole de consensus (Raft dans MongoDB).

#### Multi-Leader (Multi-Master)

**Principe** : Plusieurs n≈ìuds acceptent simultan√©ment les √©critures, avec r√©plication bidirectionnelle.

**Utilis√© dans** : CouchDB, Cassandra (dans une certaine mesure), syst√®mes de r√©plication multi-r√©gions.

**Challenges** :
- D√©tection et r√©solution de conflits complexe
- Coh√©rence √©ventuelle garantie uniquement
- Risque de divergence en cas de partition prolong√©e

**MongoDB et Multi-Leader** : MongoDB ne supporte pas nativement le multi-leader au niveau Replica Set, mais le sharding peut √™tre vu comme une forme de multi-leader partitionn√© (chaque shard ayant son propre Primary).

#### Leaderless (Quorum-Based)

**Principe** : Pas de leader d√©sign√©, les lectures et √©critures n√©cessitent un quorum de n≈ìuds.

**Utilis√© dans** : Dynamo, Cassandra, Riak.

**Caract√©ristiques** :
- W (write quorum) + R (read quorum) > N (total n≈ìuds) garantit la coh√©rence
- Haute disponibilit√© en √©criture m√™me avec pannes
- D√©tection de conflits via vector clocks ou last-write-wins

**MongoDB et Leaderless** : MongoDB n'utilise pas ce mod√®le, pr√©f√©rant le single-leader pour simplifier la coh√©rence.

## Concepts Cl√©s de la R√©plication MongoDB

### Le Replica Set : Unit√© Fondamentale

Un **Replica Set** est un groupe de processus `mongod` qui maintiennent le m√™me dataset. C'est l'abstraction de base de la r√©plication dans MongoDB.

**Composition typique** :
- 1 Primary : n≈ìud leader acceptant les √©critures
- N Secondaries : n≈ìuds followers r√©pliquant depuis le Primary
- Optionnel : Arbiters, Hidden members, Delayed members

**Invariant fondamental** : √Ä tout moment, il existe au plus un Primary dans un Replica Set (single-leader invariant).

### L'Oplog : Journal de R√©plication

L'**Oplog** (operations log) est une collection capped sp√©ciale (`local.oplog.rs`) qui enregistre toutes les op√©rations de modification dans l'ordre chronologique.

**Propri√©t√©s critiques** :

1. **Idempotence** : Chaque op√©ration de l'Oplog peut √™tre appliqu√©e plusieurs fois avec le m√™me r√©sultat. Par exemple :
   - `{ $set: { status: "active" } }` est idempotent
   - `{ $inc: { counter: 1 } }` est transform√© en `{ $set: { counter: <valeur_finale> } }`

2. **Ordre total** : Les op√©rations sont strictement ordonn√©es par timestamp (optime), garantissant une r√©plication dans le m√™me ordre.

3. **Compacit√©** : L'Oplog stocke l'√©tat final, pas les √©tapes interm√©diaires. Plusieurs mises √† jour d'un m√™me document sont consolid√©es.

4. **Circularit√©** : √âtant une collection capped, l'Oplog r√©utilise l'espace en √©crasant les entr√©es les plus anciennes.

**Optime** : Chaque op√©ration est marqu√©e par un optime, un tuple `(term, timestamp)` o√π :
- `term` : Num√©ro de mandat du Primary (incr√©ment√© √† chaque √©lection)
- `timestamp` : Horodatage BSON (8 octets : 4 pour epoch secondes, 4 pour ordinal)

### Protocole de Consensus : Raft

Depuis MongoDB 4.0, le protocole d'√©lection et de consensus est bas√© sur **Raft**, rempla√ßant l'ancien protocole propri√©taire inspir√© de Paxos.

**Principes Raft** :

1. **Leader Election** :
   - Les n≈ìuds peuvent √™tre dans trois √©tats : Leader, Follower, Candidate
   - Si un Follower ne re√ßoit pas de heartbeat du Leader pendant `electionTimeout`, il devient Candidate
   - Le Candidate sollicite les votes des autres n≈ìuds
   - Un Candidate devient Leader s'il obtient la majorit√© des votes

2. **Log Replication** :
   - Le Leader accepte les requ√™tes clients et les ajoute √† son log
   - Le Leader propage les entr√©es aux Followers via des AppendEntries RPC
   - Une entr√©e est committ√©e quand r√©pliqu√©e sur la majorit√©

3. **Safety** :
   - **Election Safety** : Au plus un Leader par term
   - **Leader Append-Only** : Le Leader n'√©crase jamais son log
   - **Log Matching** : Si deux logs contiennent une entr√©e avec m√™me index et term, tous les entr√©es pr√©c√©dentes sont identiques
   - **Leader Completeness** : Si une entr√©e est committ√©e dans un term, elle sera pr√©sente dans tous les Leaders futurs
   - **State Machine Safety** : Si un n≈ìud applique une entr√©e √† un index, aucun autre n≈ìud n'appliquera une entr√©e diff√©rente pour cet index

**Adaptation MongoDB** :
- Utilisation de la priorit√© pour influencer les √©lections
- Concept de "votes" (membres votants vs non-votants)
- Int√©gration avec Write Concern "majority"

### Heartbeats et D√©tection de D√©faillances

Les membres d'un Replica Set √©changent p√©riodiquement des messages **heartbeat** pour :

1. **Surveiller la sant√©** : D√©tecter les n≈ìuds inaccessibles ou d√©faillants
2. **Partager l'√©tat** : Propager les informations de configuration et d'√©tat
3. **Synchroniser la topologie** : Maintenir une vue coh√©rente du cluster

**Param√®tres cl√©s** :
- `heartbeatIntervalMillis` : 2000 ms par d√©faut (fr√©quence des heartbeats)
- `electionTimeoutMillis` : 10000 ms par d√©faut (timeout avant √©lection)
- `heartbeatTimeoutSecs` : 10 s par d√©faut (timeout de r√©ponse heartbeat)

**M√©canisme de d√©tection** :
- Si un membre ne re√ßoit pas de heartbeat du Primary pendant `electionTimeoutMillis`, il peut initier une √©lection
- Si le Primary ne peut communiquer avec une majorit√© de membres, il se d√©grade automatiquement en Secondary (step down)

### Replication Lag : In√©vitabilit√© et Gestion

Le **replication lag** (d√©calage de r√©plication) est le d√©lai entre l'application d'une op√©ration sur le Primary et sa propagation sur un Secondary.

**Causes du lag** :

1. **Latence r√©seau** : D√©lai physique de transmission des donn√©es
2. **Charge du Secondary** : Saturation CPU/disque du Secondary
3. **Volume d'√©critures** : D√©bordement si le taux d'√©criture d√©passe la capacit√© de r√©plication
4. **Op√©rations lourdes** : Indexes builds, large transactions
5. **Configuration sous-optimale** : Oplog trop petit, insuffisance de ressources

**Mesure du lag** :
```javascript
rs.status()
```
Champ `optimeDate` pour chaque membre : diff√©rence entre Primary et Secondary.

**Implications** :

- **Lectures obsol√®tes** : Les lectures sur Secondaries peuvent voir des donn√©es p√©rim√©es
- **Failover retard√©** : Un Secondary tr√®s en retard peut retarder l'√©lection
- **Fen√™tre de perte** : Lag important augmente le risque de perte en cas de panne du Primary

**Strat√©gies d'att√©nuation** :

1. **Write Concern "majority"** : Attend la r√©plication sur la majorit√© avant ACK
2. **Read Concern "majority"** : Lit uniquement les donn√©es r√©pliqu√©es sur la majorit√©
3. **Monitoring** : Alertes sur lag > seuil acceptable (ex: 10 secondes)
4. **Dimensionnement** : Oplog suffisamment large (couvre plusieurs heures d'√©critures)
5. **Hardware** : Secondaries avec ressources √©quivalentes au Primary
6. **Network** : Bande passante et latence ad√©quates entre n≈ìuds

### Coh√©rence et Isolation : Read et Write Concerns

MongoDB offre un contr√¥le granulaire sur les garanties de coh√©rence via Read et Write Concerns.

#### Write Concern

Contr√¥le le niveau d'accus√© de r√©ception requis pour les √©critures :

**w: 1** (d√©faut avant MongoDB 5.0)
- ACK d√®s √©criture sur le Primary (en m√©moire ou journal selon `j`)
- Minimum de latence
- Risque de perte si Primary tombe avant r√©plication

**w: "majority"** (d√©faut depuis MongoDB 5.0)
- ACK apr√®s r√©plication sur la majorit√© des membres votants
- Durabilit√© garantie : survit √† la panne d'une minorit√©
- Latence accrue (attente de la r√©plication)

**w: \<nombre\>**
- ACK apr√®s r√©plication sur N membres sp√©cifiques
- Utile pour garantir la r√©plication sur des membres g√©ographiquement distribu√©s

**w: 0**
- Fire-and-forget, pas d'ACK
- Performance maximale mais aucune garantie
- D√©conseill√© sauf pour donn√©es non critiques

**j: true/false**
- Contr√¥le la persistance sur disque via le journal WiredTiger
- `j: true` garantit la durabilit√© m√™me en cas de crash brutal
- `j: false` accepte l'√©criture en m√©moire (cache WiredTiger)

#### Read Concern

Contr√¥le le niveau de coh√©rence requis pour les lectures :

**local** (d√©faut)
- Lit les donn√©es locales du n≈ìud, sans garantie de durabilit√©
- Peut lire des donn√©es non r√©pliqu√©es (susceptibles d'√™tre rollback)
- Latence minimale

**available**
- Similaire √† `local` mais optimis√© pour les clusters shard√©s
- Peut retourner des documents orphelins (apr√®s migration de chunk)

**majority**
- Lit uniquement les donn√©es r√©pliqu√©es sur la majorit√©
- Garantit qu'une donn√©e lue ne sera pas rollback
- Combin√© avec `writeConcern: majority`, fournit une coh√©rence causale

**linearizable**
- Coh√©rence la plus forte : √©quivalent √† une lecture depuis un registre unique
- Garantit que la lecture voit toutes les √©critures pr√©c√©dentes (global order)
- Latence √©lev√©e, utilis√© pour les cas critiques (ex: leader election)
- Disponible uniquement pour les lectures sur Primary avec `maxTimeMS`

**snapshot**
- Lecture depuis un snapshot coh√©rent (point-in-time)
- Utilis√© dans les transactions multi-documents
- Garantit l'isolation entre transactions (snapshot isolation)

#### Combinaisons Typiques

**Coh√©rence √©ventuelle** (performance maximale) :
```javascript
{ writeConcern: { w: 1 }, readConcern: { level: "local" } }
```

**Coh√©rence forte** (durabilit√© maximale) :
```javascript
{ writeConcern: { w: "majority", j: true }, readConcern: { level: "majority" } }
```

**Coh√©rence lin√©arisable** (cas critiques) :
```javascript
{ writeConcern: { w: "majority", j: true }, readConcern: { level: "linearizable" } }
```

### Read Preference : Routage des Lectures

La **Read Preference** d√©termine quels membres du Replica Set peuvent servir les lectures.

**Modes** :

1. **primary** (d√©faut)
   - Toutes les lectures depuis le Primary
   - Coh√©rence maximale (voit toutes les √©critures)
   - Charge concentr√©e sur le Primary

2. **primaryPreferred**
   - Primary si disponible, sinon Secondary
   - Fallback automatique lors de maintenance/panne
   - Utile pour applications tol√©rantes au lag

3. **secondary**
   - Lectures exclusivement depuis les Secondaries
   - D√©charge le Primary
   - Risque de lectures obsol√®tes

4. **secondaryPreferred**
   - Secondaries si disponibles, sinon Primary
   - √âquilibre charge/disponibilit√©

5. **nearest**
   - Membre avec latence r√©seau la plus faible
   - Optimal pour r√©duire la latence globale
   - Peut √™tre Primary ou Secondary

**Tag Sets** : Permettent de cibler des membres sp√©cifiques selon des tags (datacenter, r√©gion, type de hardware).

## Th√©or√®me CAP et Positionnement de MongoDB

Le **th√©or√®me CAP** (Consistency, Availability, Partition tolerance), √©nonc√© par Eric Brewer, stipule qu'un syst√®me distribu√© ne peut garantir simultan√©ment :

- **C (Consistency)** : Tous les n≈ìuds voient les m√™mes donn√©es au m√™me moment
- **A (Availability)** : Chaque requ√™te re√ßoit une r√©ponse (succ√®s ou √©chec)
- **P (Partition tolerance)** : Le syst√®me continue de fonctionner malgr√© les partitions r√©seau

En pratique, **P est obligatoire** (les partitions r√©seau sont in√©vitables), donc le choix se r√©sume √† CP vs AP.

### MongoDB : CP ou AP ?

**MongoDB est configurable** sur le spectre CP-AP :

**Configuration CP** (Consistency prioritaire) :
```javascript
writeConcern: { w: "majority", j: true }
readConcern: { level: "majority" }
readPreference: "primary"
```
- Durant une partition, si le Primary ne peut atteindre la majorit√©, il se d√©grade ‚Üí indisponibilit√© temporaire en √©criture
- Les lectures voient toujours des donn√©es coh√©rentes
- Privil√©gie la coh√©rence sur la disponibilit√©

**Configuration AP** (Availability prioritaire) :
```javascript
writeConcern: { w: 1 }
readConcern: { level: "local" }
readPreference: "secondaryPreferred"
```
- Les √©critures r√©ussissent tant que le Primary est accessible (m√™me isol√©)
- Les lectures peuvent √™tre servies par n'importe quel membre accessible
- Privil√©gie la disponibilit√© sur la coh√©rence (coh√©rence √©ventuelle)

**En pratique**, MongoDB offre un continuum configurable, permettant d'ajuster les compromis selon les besoins de chaque op√©ration.

## Comparaison avec d'Autres Syst√®mes

### MongoDB vs MySQL Replication

**MySQL (r√©plication asynchrone classique)** :
- Single-leader (master-slave)
- R√©plication bas√©e sur binlog (similaire √† l'Oplog)
- Pas de failover automatique natif (n√©cessite ProxySQL, Orchestrator, etc.)
- Coh√©rence √©ventuelle sans contr√¥le granulaire

**MongoDB** :
- Single-leader avec √©lection automatique (Raft)
- Oplog idempotent et ordonn√©
- Failover automatique en secondes
- Write/Read Concerns pour contr√¥le granulaire de la coh√©rence

### MongoDB vs Cassandra

**Cassandra** :
- Leaderless (quorum-based)
- Haute disponibilit√© en √©criture (multi-master)
- Coh√©rence √©ventuelle par d√©faut (tunable avec quorum)
- R√©solution de conflits via timestamps (last-write-wins)

**MongoDB** :
- Single-leader par Replica Set
- Pas de conflits d'√©criture (un seul Primary)
- Coh√©rence configurable (local √† linearizable)
- Mod√®le plus simple pour les d√©veloppeurs

### MongoDB vs PostgreSQL (r√©plication synchrone)

**PostgreSQL** :
- Single-leader avec r√©plication streaming
- Support r√©plication synchrone (similaire √† w: "majority")
- Failover via outils externes (Patroni, repmgr)
- Performances √©lev√©es mais moins de flexibilit√© g√©ographique

**MongoDB** :
- R√©plication asynchrone par d√©faut, configurable
- Failover automatique int√©gr√©
- Architecture pens√©e pour la distribution g√©ographique
- Trade-off performance/coh√©rence plus fin

## Limitations et Consid√©rations

### Limitations Th√©oriques

1. **Overhead de la r√©plication** : Chaque √©criture doit √™tre propag√©e, consommant bande passante et CPU
2. **Replication lag in√©vitable** : La r√©plication asynchrone implique toujours un d√©calage temporel
3. **Complexit√© de consensus** : Le protocole Raft, bien que robuste, ajoute de la latence et de la complexit√©
4. **Borne sup√©rieure d'√©lection** : Le temps de failover est born√© par `electionTimeoutMillis` (g√©n√©ralement 10s)

### Limitations Pratiques MongoDB

1. **Maximum 50 membres** par Replica Set (dont 7 votants maximum)
2. **Co√ªt de w: "majority"** : Latence accrue, particuli√®rement sur WAN
3. **Oplog continu** : N√©cessite un dimensionnement soign√© pour √©viter les resync complets
4. **Pas de multi-leader natif** : Les √©critures sont toujours single-point (jusqu'au sharding)

### Consid√©rations Op√©rationnelles

1. **Topologie r√©seau** : La latence inter-n≈ìuds impacte directement les performances
2. **Dimensionnement mat√©riel** : Les Secondaries doivent avoir des ressources similaires au Primary
3. **Oplog sizing** : Calculer la taille en fonction du taux d'√©criture et de la fen√™tre de maintenance
4. **Monitoring** : Surveillance continue du lag, de la sant√© des membres, des √©lections

## Conclusion

La r√©plication dans MongoDB repose sur des fondements th√©oriques solides issus de la recherche en syst√®mes distribu√©s (consensus Raft, th√©or√®me CAP, mod√®les de coh√©rence), tout en offrant une flexibilit√© pratique via les Write Concerns, Read Concerns et Read Preferences.

**Points cl√©s √† retenir** :

1. MongoDB utilise un mod√®le **single-leader** (Primary-Secondary) avec √©lection automatique via Raft
2. La r√©plication est **asynchrone par d√©faut**, mais configurable finement via Write Concern
3. Le **th√©or√®me CAP** s'applique : MongoDB permet de choisir sur le spectre CP-AP selon les besoins
4. L'**Oplog** est l'√©pine dorsale de la r√©plication, garantissant l'ordre et l'idempotence
5. Le **replication lag** est une r√©alit√© in√©vitable, n√©cessitant monitoring et gestion proactive
6. Les **Read et Write Concerns** offrent un contr√¥le granulaire sur les compromis coh√©rence/performance/disponibilit√©

La ma√Ætrise de ces concepts est essentielle pour concevoir, d√©ployer et op√©rer des syst√®mes MongoDB en production, en particulier pour les applications critiques n√©cessitant haute disponibilit√© et durabilit√© des donn√©es.

Dans les sections suivantes, nous approfondirons l'architecture concr√®te des Replica Sets, les m√©canismes d'√©lection, la gestion de l'Oplog, et les aspects op√©rationnels de la r√©plication.

‚è≠Ô∏è [Architecture Replica Set](/09-replication/02-architecture-replica-set.md)
