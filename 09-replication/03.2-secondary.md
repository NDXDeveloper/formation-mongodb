üîù Retour au [Sommaire](/SOMMAIRE.md)

# 9.3.2 Secondary

## Introduction

Les **Secondaries** constituent la majorit√© des membres d'un Replica Set MongoDB, jouant un r√¥le fondamental dans la haute disponibilit√©, la durabilit√© des donn√©es et l'√©volutivit√© en lecture. Contrairement au Primary qui d√©tient le monopole des √©critures, les Secondaries maintiennent des copies synchronis√©es du dataset en r√©pliquant continuellement les op√©rations depuis le Primary (ou d'autres Secondaries via chaining).

Comprendre les m√©canismes internes des Secondaries, leurs √©tats, leurs modes de fonctionnement et leurs optimisations est essentiel pour exploiter pleinement les capacit√©s de r√©plication de MongoDB en production.

## R√¥le et Responsabilit√©s

### R√©plication des Donn√©es

La responsabilit√© primordiale d'un Secondary est de **maintenir une copie √† jour du dataset** en r√©pliquant les op√©rations depuis une source (g√©n√©ralement le Primary).

#### Processus de R√©plication Continue

Le cycle de r√©plication d'un Secondary s'articule autour de plusieurs √©tapes :

1. **Ouverture d'un Tailable Cursor** :
   - Le Secondary ouvre un cursor sur `local.oplog.rs` de sa source de r√©plication
   - Ce cursor reste ouvert ind√©finiment, suivant les nouvelles entr√©es au fur et √† mesure
   - Type de cursor : `CursorType.TAILABLE_AWAIT` (attend les nouvelles donn√©es)

2. **R√©cup√©ration par Batches** :
   - Les op√©rations Oplog sont r√©cup√©r√©es par lots (batches) plut√¥t qu'une par une
   - Taille de batch typique : 100-1000 op√©rations (configurable)
   - Optimise le transfert r√©seau et r√©duit la latence

3. **Application des Op√©rations** :
   - Chaque op√©ration est appliqu√©e localement dans l'ordre strict de l'Oplog
   - Utilise le m√™me moteur de stockage (WiredTiger) que le Primary
   - Garantit l'idempotence : une op√©ration peut √™tre r√©appliqu√©e sans effet de bord

4. **Mise √† Jour de l'OpTime** :
   - Apr√®s application, le Secondary met √† jour son `lastAppliedOpTime`
   - Communique cette position au Primary via heartbeats
   - Permet au Primary de calculer le commit point (pour `readConcern: "majority"`)

**Flux de r√©plication** :

```
Primary                          Secondary
   |                                |
   | 1. G√©n√®re op√©ration Oplog      |
   |    ts: (t:5, ts:1000)          |
   |                                |
   |                                | 2. Tailable cursor demande nouvelles ops
   |<-------------------------------|
   |                                |
   | 3. Envoie batch d'op√©rations   |
   |------------------------------->|
   |                                |
   |                                | 4. Applique op√©rations localement
   |                                | 5. Met √† jour lastAppliedOpTime
   |                                |
   |                                | 6. Heartbeat avec nouvelle position
   |<-------------------------------|
   |                                |
   | 7. Met √† jour commit point     |
   |    (si majorit√© atteinte)      |
```

#### Garanties de Coh√©rence

**Coh√©rence √©ventuelle par d√©faut** :
- Les Secondaries convergent vers l'√©tat du Primary de mani√®re asynchrone
- Il existe toujours un **replication lag** (d√©calage temporel)
- Les lectures sur Secondaries peuvent observer des donn√©es l√©g√®rement obsol√®tes

**Coh√©rence configurable via Read Concern** :
- `local` : Lit les donn√©es locales, quel que soit leur √©tat de r√©plication
- `majority` : Lit uniquement les donn√©es r√©pliqu√©es sur la majorit√© (commit point)
- `snapshot` : Lit depuis un snapshot coh√©rent (transactions)

### Participation aux √âlections

Les Secondaries votants participent activement au processus d'√©lection du Primary :

#### Droit de Vote

Un Secondary peut voter si :
- `votes: 1` (propri√©t√© de configuration)
- Il fait partie des 7 membres votants maximum du Replica Set
- Il est accessible par le r√©seau depuis les autres membres

**Crit√®res de vote** :

Lorsqu'un Secondary re√ßoit une requ√™te de vote d'un Candidate, il d√©cide de voter en fonction de :

1. **Term Number** :
   - Le Candidate doit avoir un term ‚â• au term actuel du Secondary
   - Garantit qu'un Candidate "obsol√®te" ne peut pas √™tre √©lu

2. **OpTime Freshness** :
   - L'Oplog du Candidate doit √™tre au moins aussi √† jour que celui du Secondary
   - Garantit qu'aucune donn√©e committ√©e n'est perdue lors de l'√©lection

3. **Priority** :
   - Les Candidates avec `priority: 0` ne peuvent jamais √™tre √©lus
   - Les membres votent pr√©f√©rentiellement pour les Candidates √† priorit√© √©lev√©e

4. **Vote Unique par Term** :
   - Chaque Secondary ne vote qu'une seule fois par term
   - √âvite les votes multiples qui mineraient le consensus

**Algorithme de vote** :

```javascript
function shouldVoteFor(candidate) {
  // V√©rification du term
  if (candidate.term < this.currentTerm) {
    return false  // Candidate obsol√®te
  }

  // V√©rification du vote d√©j√† √©mis
  if (this.votedFor !== null && this.votedFor !== candidate.id) {
    return false  // D√©j√† vot√© pour quelqu'un d'autre dans ce term
  }

  // V√©rification de la fra√Æcheur des donn√©es
  if (candidate.lastAppliedOpTime < this.lastAppliedOpTime) {
    return false  // Candidate pas √† jour
  }

  // V√©rification de l'√©ligibilit√©
  if (candidate.priority === 0) {
    return false  // Candidate non-√©ligible
  }

  // Vote positif
  this.votedFor = candidate.id
  this.currentTerm = candidate.term
  return true
}
```

#### Initiation d'√âlections

Un Secondary peut initier une √©lection lorsque :

**Perte du Primary** :
- Aucun heartbeat du Primary re√ßu pendant `electionTimeoutMillis` (10s par d√©faut)
- Le Secondary passe en √©tat CANDIDATE
- Incr√©mente son term et sollicite les votes

**Priorit√© Sup√©rieure** :
- Un Secondary avec `priority` √©lev√©e d√©tecte un Primary de priorit√© inf√©rieure
- Apr√®s un d√©lai (`priorityTakeoverDelay`), initie une √©lection
- Force le Primary actuel √† se d√©grader (step down)

**Exemple : Priority Takeover**

```
Configuration :
  Primary (mongo1): priority: 1
  Secondary (mongo2): priority: 5  // Priorit√© sup√©rieure

S√©quence :
  1. mongo2 rejoint le Replica Set (√©tait down)
  2. mongo2 d√©tecte que mongo1 (priority: 1) est Primary
  3. mongo2 attend priorityTakeoverDelay (d√©faut: 5 secondes)
  4. mongo2 initie une √©lection
  5. mongo1 re√ßoit la requ√™te de vote, d√©tecte la priorit√© sup√©rieure
  6. mongo1 se d√©grade volontairement (step down)
  7. mongo2 est √©lu Primary
```

### Candidature au R√¥le de Primary

Un Secondary √©ligible (`priority > 0`) peut devenir Primary lors d'une √©lection :

#### Conditions de Candidature

1. **√âligibilit√©** : `priority > 0`
2. **Droit de vote** : `votes: 1` (g√©n√©ralement)
3. **Donn√©es √† jour** : OpTime pas trop en retard par rapport au Primary perdu
4. **Connectivit√©** : Capable de communiquer avec la majorit√© des membres

#### Processus de Candidature

**Transition vers CANDIDATE** :

```
SECONDARY ‚Üí CANDIDATE

D√©clencheurs :
- electionTimeout expir√© (pas de heartbeat du Primary)
- Priorit√© sup√©rieure d√©tect√©e (priority takeover)

Actions :
1. Incr√©menter currentTerm
2. Voter pour soi-m√™me
3. Envoyer RequestVote RPC √† tous les membres
4. Attendre les r√©ponses de vote
```

**Obtention de la Majorit√©** :

Si le Candidate re√ßoit la majorit√© des votes :

```
CANDIDATE ‚Üí PRIMARY

Actions :
1. Annoncer le statut PRIMARY via heartbeats
2. Commencer √† accepter les √©critures
3. G√©n√©rer l'Oplog pour les nouvelles op√©rations
4. Les autres membres se synchronisent sur le nouveau Primary
```

**√âchec de Candidature** :

Si aucune majorit√© n'est atteinte (split vote, autre Candidate √©lu) :

```
CANDIDATE ‚Üí SECONDARY

Actions :
1. D√©tecter qu'un autre membre est devenu Primary
2. Revenir √† l'√©tat SECONDARY
3. Commencer √† r√©pliquer depuis le nouveau Primary
```

### Service des Lectures

Les Secondaries peuvent servir les requ√™tes de lecture selon la **Read Preference** configur√©e :

#### Read Preference Modes

**secondary** :
- Toutes les lectures sont dirig√©es vers les Secondaries
- D√©charge compl√®tement le Primary
- Risque de lire des donn√©es obsol√®tes (replication lag)

```javascript
db.getMongo().setReadPref("secondary")
db.collection.find({ ... })  // Ex√©cut√© sur un Secondary
```

**secondaryPreferred** :
- Pr√©f√®re les Secondaries, fallback sur Primary si aucun Secondary disponible
- √âquilibre entre d√©charge du Primary et disponibilit√©

**nearest** :
- Lit depuis le membre le plus proche (latence r√©seau minimale)
- Peut √™tre le Primary ou un Secondary

#### Isolation avec Tags

Les Secondaries peuvent √™tre tagu√©s pour isoler diff√©rentes charges de travail :

```javascript
// Configuration
cfg.members[2].tags = { workload: "analytics", dc: "east" }
cfg.members[3].tags = { workload: "reporting", dc: "west" }

// Application Analytics
db.getMongo().setReadPref("secondary", [
  { workload: "analytics" }  // Uniquement les Secondaries analytics
])

// Application Reporting
db.getMongo().setReadPref("secondary", [
  { workload: "reporting" }
])
```

**Avantages** :
- Requ√™tes OLAP lourdes isol√©es des op√©rations OLTP
- Plusieurs pipelines analytics sans interf√©rence
- Optimisation g√©ographique (lecture locale)

#### Consid√©rations de Coh√©rence

**Probl√®me : Lectures Obsol√®tes**

```
T0: Primary √©crit doc { x: 1 }
T1: Application lit sur Secondary ‚Üí { x: 0 } (lag de r√©plication)
T2: Secondary applique l'op√©ration ‚Üí { x: 1 }
```

L'application a lu une valeur obsol√®te √† T1.

**Solutions** :

1. **Read Concern "majority"** :
```javascript
db.collection.find({ ... }).readConcern("majority")
```
Garantit que les donn√©es lues sont durables (r√©pliqu√©es sur la majorit√©).

2. **Causal Consistency** :
```javascript
const session = db.getMongo().startSession({ causalConsistency: true })
session.startTransaction()
// Toutes les lectures voient les √©critures pr√©c√©dentes de la session
```

3. **Read from Primary** :
```javascript
db.getMongo().setReadPref("primary")  // Coh√©rence forte
```

## √âtats et Transitions

Un Secondary traverse plusieurs √©tats au cours de son cycle de vie :

### √âtats Possibles

#### STARTUP

√âtat initial au d√©marrage du processus `mongod` :

- Chargement de la configuration
- Initialisation du moteur de stockage
- Pas encore connect√© au Replica Set

**Dur√©e** : Quelques secondes typiquement

**Transition** : STARTUP ‚Üí STARTUP2 (d√®s connexion aux autres membres)

#### STARTUP2

Phase d'initialisation de la r√©plication :

- D√©tection du Primary et des autres membres
- D√©termination de la source de r√©plication
- D√©cision entre initial sync complet ou rattrapage Oplog

**Sous-√©tats** :
- **STARTUP2** avec initial sync : Copie compl√®te du dataset
- **STARTUP2** avec Oplog catchup : Application des op√©rations manqu√©es

**Dur√©e** : Variable (secondes √† heures selon la taille du dataset)

**Transition** : STARTUP2 ‚Üí SECONDARY (quand √† jour)

#### SECONDARY

√âtat normal d'un Secondary op√©rationnel :

**Caract√©ristiques** :
- R√©plique continuellement depuis sa source
- Peut servir les lectures (selon Read Preference)
- Participe aux √©lections (si votant)
- Peut devenir Primary (si √©ligible)

**Activit√©s** :
- Application des op√©rations Oplog
- Envoi/r√©ception de heartbeats
- R√©ponse aux requ√™tes de lecture

#### RECOVERING

√âtat temporaire de r√©cup√©ration ou maintenance :

**Causes** :
- Rattrapage apr√®s un retard important
- Rollback en cours (apr√®s rejoindre un nouveau Primary)
- Reconstruction d'indexes
- Op√©rations de maintenance (compact, repair)

**Restrictions** :
- Ne peut pas servir les lectures
- Participe toujours aux √©lections (si votant)
- Continue de r√©pliquer en arri√®re-plan

**Transition** : RECOVERING ‚Üí SECONDARY (quand r√©cup√©ration termin√©e)

#### ROLLBACK

√âtat critique lors d'un rollback d'op√©rations :

**Sc√©nario** :
```
1. Ancien Primary √©crit op1, op2 (non r√©pliqu√©es)
2. Ancien Primary crash
3. Secondary devient Primary
4. Ancien Primary revient, d√©tecte divergence
5. Entre en √©tat ROLLBACK
6. Annule op1, op2 (sauvegarde dans rollback/)
7. R√©plique depuis le nouveau Primary
8. Retour √† SECONDARY
```

**Dur√©e** : Secondes √† minutes (selon volume de rollback)

**Impact** : Service interrompu sur ce membre

### Diagramme de Transitions d'√âtats

```
                    [STARTUP]
                        ‚Üì
                  [STARTUP2]
                    /     \
         (initial sync) (oplog catchup)
                  /         \
                 ‚Üì           ‚Üì
            [SECONDARY] ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                 ‚Üì  ‚Üë
                 |  | (recovery complete)
                 ‚Üì  |
            [RECOVERING]
                 ‚Üì
            [ROLLBACK]
                 ‚Üì
            [SECONDARY]
                 ‚Üì
         (election won)
                 ‚Üì
             [PRIMARY]
```

### Initial Sync : Synchronisation Initiale

L'initial sync est un processus complet de copie du dataset lorsqu'un Secondary rejoint ou est trop en retard :

#### Phases de l'Initial Sync

**Phase 1 : Clone des Collections**

1. **√ânum√©ration des bases** :
   - Le Secondary liste toutes les bases de donn√©es (sauf `local`)
   - Pour chaque base, liste toutes les collections

2. **Clone parall√®le** :
   - Ouverture de cursors sur chaque collection de la source
   - Copie de tous les documents en parall√®le (multi-threaded)
   - Utilisation de batches pour optimiser le transfert r√©seau

```
Source                           Destination (Secondary)
  ‚Üì                                     ‚Üì
db.users ‚Üí [cursor] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Copie users (Thread 1)
db.orders ‚Üí [cursor] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Copie orders (Thread 2)
db.products ‚Üí [cursor] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Copie products (Thread 3)
```

**Phase 2 : Application Oplog**

Pendant le clone, de nouvelles op√©rations s'accumulent. Apr√®s le clone :

1. **R√©cup√©ration de l'Oplog** depuis le d√©but du clone
2. **Application des op√©rations** manqu√©es
3. Peut n√©cessiter plusieurs it√©rations si le taux d'√©criture est √©lev√©

**Phase 3 : Synchronisation des Index**

1. **Construction des index** sur toutes les collections
2. Parall√©lisation pour acc√©l√©rer le processus
3. Peut √™tre tr√®s co√ªteux en CPU/disque

**Phase 4 : Final Catchup**

1. Application des derni√®res op√©rations Oplog
2. R√©duction de l'√©cart jusqu'√† ce qu'il soit minimal (< 1 seconde)
3. Transition vers l'√©tat SECONDARY

#### Optimisations de l'Initial Sync

**Resumable Initial Sync (MongoDB 4.4+)** :

Permet de reprendre un initial sync interrompu sans tout recommencer :

```javascript
// M√©canisme interne
{
  "initialSyncId": UUID("..."),  // Identifiant de la tentative
  "clonedDatabases": ["db1", "db2"],  // Bases d√©j√† clon√©es
  "clonedCollections": ["db3.coll1"],  // Collections partielles
  "lastOplogEntry": Timestamp(...)  // Position Oplog
}
```

Si le processus est interrompu, il reprend √† partir de la derni√®re collection/base compl√©t√©e.

**Logical Initial Sync** :

Depuis MongoDB 4.4, l'initial sync utilise des cursors logiques plut√¥t que la copie physique des fichiers :

**Avantages** :
- Meilleure efficacit√© r√©seau (compression)
- R√©silience aux interruptions
- Pas de blocage des op√©rations sur la source

**File Copy Based Initial Sync** (Legacy) :

Ancienne m√©thode (pr√©-4.0) : copie physique des fichiers de donn√©es.

- Obsol√®te et non recommand√©e
- Risque de corruption si fichiers modifi√©s pendant la copie

### Rollback : Gestion des Divergences

Le rollback survient lorsqu'un ancien Primary rejoint le cluster et d√©tecte que ses op√©rations r√©centes ne sont pas sur le nouveau Primary.

#### Sc√©nario de Rollback

**S√©quence d'√©v√©nements** :

```
T0: Replica Set normal
    Primary (P) : OpTime = 100
    Sec1 (S1)   : OpTime = 100
    Sec2 (S2)   : OpTime = 100

T1: √âcriture sur Primary avec w:1
    P  : OpTime = 101, 102, 103  (ops non r√©pliqu√©es)
    S1 : OpTime = 100
    S2 : OpTime = 100

T2: Primary P crash avant r√©plication

T3: √âlection ‚Üí S1 devient nouveau Primary
    P  : DOWN
    S1 : PRIMARY, OpTime = 100
    S2 : SECONDARY, OpTime = 100

T4: √âcritures sur nouveau Primary S1
    P  : DOWN
    S1 : OpTime = 100, 101', 102'  (ops diff√©rentes)
    S2 : OpTime = 100, 101', 102'

T5: Ancien Primary P revient
    P d√©tecte divergence : ses ops 101,102,103 != ops 101',102' de S1
    ‚Üí Rollback requis
```

#### Processus de Rollback

**√âtapes** :

1. **D√©tection de la Divergence** :
   - P compare son Oplog avec celui du nouveau Primary S1
   - Identifie le point de divergence (OpTime = 100)

2. **√âtat ROLLBACK** :
   - P passe en √©tat ROLLBACK
   - Ne peut plus servir de lectures ou d'√©critures

3. **Annulation des Op√©rations** :
   - P identifie les op√©rations √† annuler (101, 102, 103)
   - G√©n√®re des op√©rations inverses pour restaurer l'√©tat √† OpTime = 100

4. **Sauvegarde des Donn√©es Rollback** :
   - Les documents affect√©s sont sauvegard√©s dans `<dbpath>/rollback/`
   - Format : BSON files avec m√©tadonn√©es

```bash
/var/lib/mongodb/rollback/
  ‚îú‚îÄ‚îÄ mydb.users.2024-12-02T15-30-00.0.bson
  ‚îú‚îÄ‚îÄ mydb.orders.2024-12-02T15-30-00.0.bson
  ‚îî‚îÄ‚îÄ rollback.json  # M√©tadonn√©es
```

5. **R√©plication depuis S1** :
   - P r√©plique les op√©rations depuis OpTime = 100
   - Applique les ops 101', 102' du nouveau Primary

6. **Retour √† SECONDARY** :
   - P revient en √©tat SECONDARY
   - Rejoint le flux de r√©plication normal

#### Limites du Rollback

**Limite de 300 MB** (MongoDB 4.0+) :

Si le volume de donn√©es √† rollback d√©passe 300 MB :
- Le rollback automatique √©choue
- Le membre reste en √©tat ROLLBACK ind√©finiment
- **Intervention manuelle requise** : resync complet

**Avant MongoDB 4.0 : Limite de 30 minutes** :

Si les op√©rations √† rollback s'√©talent sur plus de 30 minutes d'Oplog :
- Rollback impossible
- Resync manuel n√©cessaire

#### Pr√©vention du Rollback

**Write Concern "majority"** :

```javascript
db.collection.insertOne(
  { data: "critical" },
  { writeConcern: { w: "majority", j: true } }
)
```

**Garantie** : Les op√©rations committ√©es avec `w: "majority"` ne seront **jamais rollback**.

**M√©canisme** :
- L'ACK n'est renvoy√© au client que lorsque la majorit√© a r√©pliqu√©
- Si le Primary crash, les op√©rations committ√©es sont sur la majorit√©
- Le nouveau Primary √©lu aura n√©cessairement ces op√©rations

## M√©canismes de R√©plication

### S√©lection de la Source de R√©plication

Un Secondary doit d√©terminer depuis quel membre r√©pliquer :

#### Algorithme de S√©lection

**Crit√®res** (par ordre de priorit√©) :

1. **Chaining autoris√©** :
   - Si `chainingAllowed: false`, source = Primary uniquement
   - Si `chainingAllowed: true`, continuer l'√©valuation

2. **Proximit√© r√©seau** :
   - S√©lectionner le membre avec le ping le plus faible
   - Mesure via les heartbeats (champ `pingMs`)

3. **Fra√Æcheur des donn√©es** :
   - Le membre source doit √™tre √† jour (pas trop de lag)
   - √âvite de r√©pliquer depuis un Secondary tr√®s en retard

4. **Disponibilit√©** :
   - Le membre doit √™tre en √©tat SECONDARY ou PRIMARY
   - Exclure les membres en RECOVERING, ROLLBACK, DOWN

**Exemple de s√©lection** :

```javascript
// √âtat du Replica Set
Primary : ping = 50ms,  OpTime = 1000
Sec1    : ping = 10ms,  OpTime = 998   // Proche mais l√©g√®rement en retard
Sec2    : ping = 100ms, OpTime = 1000  // Loin mais √† jour

// Si chainingAllowed: true
‚Üí S√©lection de Sec1 (proximit√© prioritaire, lag acceptable)

// Si chainingAllowed: false
‚Üí S√©lection obligatoire du Primary
```

#### Changement de Source Dynamique

Le Secondary r√©√©value sa source de r√©plication p√©riodiquement :

**D√©clencheurs** :
- La source actuelle devient inaccessible
- Un membre plus proche devient disponible
- La source accumule trop de lag
- Reconfiguration du Replica Set

**Transition** :

```
1. D√©tection qu'une meilleure source existe
2. Fermeture du cursor actuel
3. Ouverture d'un nouveau cursor sur la nouvelle source
4. D√©termination du point de reprise (lastAppliedOpTime)
5. Reprise de la r√©plication sans perte de donn√©es
```

### Replication Chaining

Le **chaining** permet √† un Secondary de r√©pliquer depuis un autre Secondary :

#### Topologie en Cha√Æne

```
Configuration avec chaining:
  [Primary] ‚Üí [Sec1] ‚Üí [Sec2]
                 ‚Üì
             [Sec3]
```

**Avantages** :

1. **R√©duction de la charge r√©seau du Primary** :
   - Le Primary n'envoie l'Oplog qu'√† Sec1
   - Sec1 redistribue √† Sec2 et Sec3

2. **Optimisation g√©ographique** :
   - Primary (DC-A) ‚Üí Sec1 (DC-B, proche de DC-A)
   - Sec1 (DC-B) ‚Üí Sec2 (DC-C, proche de DC-B)
   - √âvite le trafic DC-A ‚Üí DC-C direct

3. **Flexibilit√© topologique** :
   - Adaptation automatique aux changements de latence
   - R√©silience aux pannes de membres interm√©diaires

**Inconv√©nients** :

1. **Augmentation du replication lag** :
   - Lag cumulatif : lag(Primary‚ÜíSec1) + lag(Sec1‚ÜíSec2)
   - Peut devenir significatif dans les cha√Ænes longues

2. **Complexit√© de diagnostic** :
   - Plus difficile de tracer la source d'un probl√®me
   - N√©cessite de suivre la cha√Æne de r√©plication

#### Configuration

**Activ√© par d√©faut** :

```javascript
rs.conf().settings.chainingAllowed  // true par d√©faut
```

**D√©sactivation** :

```javascript
cfg = rs.conf()
cfg.settings.chainingAllowed = false
rs.reconfig(cfg)

// Tous les Secondaries r√©pliquent maintenant directement depuis le Primary
```

**Cas d'usage pour d√©sactivation** :
- Minimiser le replication lag (au prix de la charge r√©seau du Primary)
- Topologies simples (petit nombre de membres)
- Besoin de coh√©rence maximale

### Application Parall√®le des Op√©rations

Depuis MongoDB 4.0, les Secondaries appliquent les op√©rations Oplog en parall√®le :

#### D√©tection de D√©pendances

MongoDB analyse les op√©rations pour d√©tecter les conflits :

**Crit√®res de parall√©lisation** :

- Op√©rations sur des **documents diff√©rents** : Parall√©lisables
- Op√©rations sur la **m√™me collection** mais documents diff√©rents : Parall√©lisables
- Op√©rations sur le **m√™me document** : Doivent √™tre s√©rialis√©es

**Exemple** :

```javascript
// Batch d'op√©rations Oplog
[
  { op: "i", ns: "db.users", o: { _id: 1, name: "Alice" } },   // Op1
  { op: "i", ns: "db.users", o: { _id: 2, name: "Bob" } },     // Op2
  { op: "u", ns: "db.orders", o2: { _id: 100 }, o: {...} },    // Op3
  { op: "u", ns: "db.users", o2: { _id: 1 }, o: {...} },       // Op4
]

Parall√©lisation:
  Thread 1: Op1, Op4 (m√™me document _id:1, s√©rialis√©es)
  Thread 2: Op2
  Thread 3: Op3
```

#### Throughput Am√©lioration

**Gains de performance** :

- **Avant MongoDB 4.0** : Application s√©quentielle (~10,000 ops/sec)
- **MongoDB 4.0+** : Application parall√®le (~50,000-100,000 ops/sec sur hardware moderne)

**Facteurs** :
- Nombre de CPU cores disponibles
- Niveau de parall√©lisme des op√©rations (documents distincts)
- Vitesse du stockage (IOPS)

### Compression de l'Oplog

Le trafic de r√©plication peut √™tre compress√© pour √©conomiser la bande passante :

#### Algorithmes de Compression

**snappy** (d√©faut) :
- Compression rapide, ratio mod√©r√© (~50-70% r√©duction)
- Faible overhead CPU
- Recommand√© pour la plupart des cas

**zstd** (depuis MongoDB 4.2) :
- Meilleur ratio de compression (~60-80% r√©duction)
- Overhead CPU l√©g√®rement sup√©rieur
- Recommand√© pour liens WAN (haute latence, bande passante limit√©e)

**zlib** :
- Tr√®s bon ratio de compression (~70-85% r√©duction)
- Overhead CPU significatif
- Rarement utilis√© en pratique

#### Configuration

```yaml
# mongod.conf
net:
  compression:
    compressors: "snappy,zstd,zlib"  # Par ordre de pr√©f√©rence
```

**N√©gociation** :
- Le Secondary et sa source n√©gocient le meilleur compresseur support√© par les deux
- Fallback automatique si compression non support√©e

**Impact** :

```
Sans compression:
  Taux d'√©criture : 100 MB/s
  Bande passante r√©seau : 100 MB/s √ó 3 Secondaries = 300 MB/s

Avec snappy (60% r√©duction):
  Bande passante r√©seau : 100 MB/s √ó 0.4 √ó 3 = 120 MB/s
  √âconomie : 60%
```

## Performance et Optimisations

### Replication Lag : Causes et Mitigation

Le **replication lag** est l'√©cart temporel entre le Primary et un Secondary :

#### Causes du Lag

**1. Charge du Secondary** :

- **CPU satur√©** : Trop de lectures simultan√©es sur le Secondary
- **Disque satur√© (IOPS)** : Application de l'Oplog bloqu√©e par les I/O
- **RAM insuffisante** : Page faults, swap

**Diagnostic** :

```javascript
db.serverStatus().wiredTiger.cache
// V√©rifier cache miss rate, evictions

db.currentOp({ op: { $in: ["query", "getmore"] } })
// Identifier les requ√™tes lourdes
```

**2. Volume d'√âcritures √âlev√©** :

Si le taux d'√©criture sur le Primary d√©passe la capacit√© d'application du Secondary :

```
Primary √©crit     : 50,000 ops/sec
Secondary applique: 30,000 ops/sec
‚Üí Lag augmente de 20,000 ops/sec (divergence croissante)
```

**Mitigation** :
- Augmenter les ressources du Secondary (CPU, disque SSD)
- R√©duire les lectures sur le Secondary (Read Preference)
- Optimiser les indexes (acc√©l√©rer l'application des updates)

**3. Latence R√©seau** :

Liens WAN entre Primary et Secondary :

```
Latence r√©seau : 100 ms (inter-continental)
Batch Oplog    : 1000 op√©rations
‚Üí Temps de transfert : 100 ms par batch
‚Üí Lag minimum : ~100 ms (incompressible)
```

**Mitigation** :
- Replication chaining (r√©plica interm√©diaire g√©ographiquement proche)
- Compression de l'Oplog (zstd pour WAN)
- Augmenter la taille des batches (si latence > bande passante)

**4. Initial Sync ou Op√©rations Lourdes** :

- Reconstruction d'index sur le Secondary
- Op√©rations bulk (import massif de donn√©es)
- Compaction ou r√©paration de base

**Mitigation** :
- Planifier les op√©rations de maintenance en heures creuses
- Utiliser des membres Hidden d√©di√©s pour les op√©rations lourdes

#### Mesure du Lag

**Via rs.status()** :

```javascript
rs.status().members.forEach(m => {
  if (m.stateStr === "SECONDARY") {
    const primary = rs.status().members.find(p => p.stateStr === "PRIMARY")
    const lag = (primary.optimeDate - m.optimeDate) / 1000
    print(`${m.name}: ${lag.toFixed(2)}s behind primary`)
  }
})
```

**Via serverStatus** :

```javascript
db.serverStatus().metrics.repl.apply.batches
// Nombre de batches appliqu√©s

db.serverStatus().metrics.repl.apply.ops
// Nombre d'op√©rations appliqu√©es
```

#### Alerting sur Lag

**Seuils recommand√©s** :

- **Warning** : Lag > 30 secondes
- **Critical** : Lag > 60 secondes
- **Urgent** : Lag croissant de mani√®re monotone (ne rattrape pas)

**Scripts de monitoring** :

```bash
#!/bin/bash
# check_replication_lag.sh

LAG=$(mongo --quiet --eval "
  rs.status().members
    .filter(m => m.stateStr === 'SECONDARY')
    .map(m => {
      const primary = rs.status().members.find(p => p.stateStr === 'PRIMARY')
      return (primary.optimeDate - m.optimeDate) / 1000
    })
    .reduce((max, curr) => Math.max(max, curr), 0)
")

if (( $(echo "$LAG > 60" | bc -l) )); then
  echo "CRITICAL: Max replication lag ${LAG}s"
  exit 2
elif (( $(echo "$LAG > 30" | bc -l) )); then
  echo "WARNING: Max replication lag ${LAG}s"
  exit 1
else
  echo "OK: Max replication lag ${LAG}s"
  exit 0
fi
```

### Optimisation du Stockage

#### Cache WiredTiger

Configuration de la m√©moire allou√©e au cache :

```yaml
# mongod.conf
storage:
  wiredTiger:
    engineConfig:
      cacheSizeGB: 64  # Exemple pour serveur 128 GB RAM
```

**Formule** :
```
cacheSizeGB = 0.5 √ó (RAM_total - 1 GB - RAM_OS)

Exemple :
  RAM total : 128 GB
  RAM OS    : 4 GB
‚Üí cacheSizeGB = 0.5 √ó (128 - 1 - 4) = 61.5 GB
```

**Monitoring** :

```javascript
db.serverStatus().wiredTiger.cache
{
  "bytes currently in the cache": 65432198765,
  "tracked dirty bytes in the cache": 1234567890,
  "pages evicted by application threads": 12345,
  "pages read into cache": 234567,
  "pages written from cache": 123456
}
```

**Indicateurs de probl√®me** :
- `pages evicted` √©lev√© ‚Üí Cache trop petit
- `tracked dirty bytes` proche de `bytes in cache` ‚Üí Flush disque lent

#### Index Optimisation

Les Secondaries b√©n√©ficient des m√™mes indexes que le Primary :

**Strat√©gie** :
- Indexes optimis√©s pour les requ√™tes de lecture sur Secondaries
- √âviter les indexes inutilis√©s (consomment ressources lors de l'application Oplog)

**Analyse** :

```javascript
// Identifier les indexes non utilis√©s
db.collection.aggregate([
  { $indexStats: {} }
])

// Chercher les indexes avec accesses.ops == 0
```

**Suppression d'indexes inutilis√©s** :

```javascript
db.collection.dropIndex("unusedIndex")
// R√©duit la charge d'application de l'Oplog
```

### Scaling en Lecture

#### Ajout de Secondaries

Pour augmenter la capacit√© de lecture :

```javascript
// Ajouter un nouveau Secondary
cfg = rs.conf()
cfg.members.push({
  _id: 4,
  host: "mongo5.example.com:27017",
  priority: 1,
  tags: { workload: "read_replica" }
})
rs.reconfig(cfg)
```

**Consid√©rations** :
- Chaque Secondary suppl√©mentaire augmente la charge r√©seau du Primary
- Diminishing returns au-del√† de 5-7 Secondaries (sans chaining)

#### Read Preference Optimis√©e

Distribution intelligente des lectures :

```javascript
// Mode nearest avec tags
db.getMongo().setReadPref("nearest", [
  { dc: "us-east", rack: "A" },  // Pr√©f√©rence locale
  { dc: "us-east" },              // Fallback datacenter
  {}                              // Fallback global
])
```

**M√©triques** :
- Surveiller la distribution des lectures entre membres
- √âquilibrer la charge pour √©viter la saturation d'un Secondary

## Types Sp√©cialis√©s de Secondaries

### Hidden Members

Les **Hidden members** sont des Secondaries invisibles aux applications :

#### Configuration

```javascript
cfg = rs.conf()
cfg.members[3] = {
  _id: 3,
  host: "hidden.example.com:27017",
  priority: 0,    // Obligatoire
  hidden: true,
  votes: 1        // Peut voter (optionnel)
}
rs.reconfig(cfg)
```

#### Cas d'Usage

**1. Reporting et Analytics** :

```javascript
// Secondary d√©di√© aux requ√™tes OLAP
{
  _id: 4,
  host: "analytics.example.com:27017",
  priority: 0,
  hidden: true,
  votes: 0,
  tags: { usage: "analytics" }
}
```

Connexion directe pour requ√™tes lourdes :
```javascript
const client = new MongoClient("mongodb://analytics.example.com:27017")
// Pas de routing via Read Preference, connexion directe
```

**2. Backup Continus** :

```javascript
// Secondary pour backups sans impact
{
  _id: 5,
  host: "backup.example.com:27017",
  priority: 0,
  hidden: true,
  votes: 0,
  tags: { usage: "backup" }
}
```

Ex√©cution de mongodump sans perturber les op√©rations :
```bash
mongodump --host backup.example.com --port 27017 --out /backups/
```

**3. Tests et D√©veloppement** :

- Copie de production pour tests destructifs
- Validation de nouvelles versions MongoDB
- Exp√©rimentation sans risque

#### Limitations

- Ne peut jamais devenir Primary (`priority: 0`)
- Invisible dans `isMaster`/`hello` (applications ne le voient pas)
- Doit toujours avoir `priority: 0`

### Delayed Members

Les **Delayed members** maintiennent un √©tat historique des donn√©es :

#### Configuration

```javascript
cfg = rs.conf()
cfg.members[4] = {
  _id: 4,
  host: "delayed.example.com:27017",
  priority: 0,     // Obligatoire
  hidden: true,    // Obligatoire
  slaveDelay: 14400,  // 4 heures (en secondes)
  votes: 0         // Recommand√©
}
rs.reconfig(cfg)
```

#### M√©canisme

Le Secondary applique les op√©rations Oplog avec un d√©lai fixe :

```
Primary √©crit √† T0     : doc { x: 1 }
Primary √©crit √† T1 (1h): doc { x: 2 }
Primary √©crit √† T2 (2h): doc { x: 3 }

Delayed member (delay: 4h) :
  √Ä T4 : applique op de T0 ‚Üí doc { x: 1 }
  √Ä T5 : applique op de T1 ‚Üí doc { x: 2 }
  √Ä T6 : applique op de T2 ‚Üí doc { x: 3 }
```

Le delayed member a toujours un √©tat "4 heures dans le pass√©".

#### Cas d'Usage : Protection contre Erreurs Humaines

**Sc√©nario de r√©cup√©ration** :

```
T0 (14h00) : db.users.drop() ex√©cut√© par erreur
T1 (14h15) : Erreur d√©tect√©e
T2 (14h20) : Connexion au delayed member (delay: 4h)
             ‚Üí √âtat √† T0 - 4h = 10h00
             ‚Üí Collection users existe encore
T3 (14h25) : Dump de la collection depuis le delayed member
T4 (14h30) : Restauration sur le Primary
```

**Fen√™tre de r√©cup√©ration** : 4 heures dans cet exemple (configurable).

#### Limitations

**1. Ne Remplace Pas les Backups** :

- Le delayed member r√©plique aussi les corruptions logiques
- Suppression en cascade : si le Primary applique `db.dropDatabase()`, le delayed member l'appliquera apr√®s le d√©lai
- N√©cessite toujours des backups r√©guliers (mongodump, snapshots)

**2. Co√ªt de Stockage** :

- N√©cessite autant d'espace disque qu'un Secondary normal
- Oplog doit √™tre dimensionn√© pour couvrir delay + marge de s√©curit√©

**3. Retard Permanent** :

- Le delayed member sera toujours en retard (par design)
- Ne peut pas √™tre utilis√© pour failover rapide

### Non-Voting Members

Les **non-voting members** r√©pliquent mais ne participent pas aux √©lections :

#### Configuration

```javascript
cfg = rs.conf()
cfg.members[7] = {  // Au-del√† du 7√®me membre
  _id: 7,
  host: "mongo8.example.com:27017",
  priority: 1,     // Peut devenir Primary
  votes: 0         // Ne vote pas
}
rs.reconfig(cfg)
```

#### Cas d'Usage

**1. Replica Sets > 7 Membres** :

Limitation : Maximum 7 membres votants.

```javascript
// Configuration avec 10 membres
members[0-6] : votes: 1  // 7 votants
members[7-9] : votes: 0  // 3 non-votants
```

**2. Membres G√©ographiquement Distants** :

√âviter que des membres √† haute latence ralentissent les √©lections :

```javascript
// Membres proches (votes)
{ _id: 0, host: "us-east-1.example.com", votes: 1 }
{ _id: 1, host: "us-east-2.example.com", votes: 1 }
{ _id: 2, host: "us-west-1.example.com", votes: 1 }

// Membre distant (r√©plication uniquement)
{ _id: 3, host: "eu-west-1.example.com", votes: 0 }
```

**3. Membres Temporaires** :

Pour tests ou analytics temporaires sans affecter le quorum.

## Monitoring et Diagnostic

### M√©triques Cl√©s

#### √âtat du Secondary

```javascript
rs.status()

{
  "members": [
    {
      "_id": 1,
      "name": "mongo2.example.com:27017",
      "health": 1,
      "state": 2,              // 2 = SECONDARY
      "stateStr": "SECONDARY",
      "uptime": 345600,
      "optime": {
        "ts": Timestamp(1638360000, 42),
        "t": NumberLong(5)
      },
      "optimeDate": ISODate("2024-12-02T12:00:00Z"),
      "syncSourceHost": "mongo1.example.com:27017",  // Source de r√©plication
      "syncSourceId": 0,
      "lastHeartbeat": ISODate("2024-12-02T12:00:05Z"),
      "lastHeartbeatRecv": ISODate("2024-12-02T12:00:04Z"),
      "pingMs": NumberLong(1),  // Latence r√©seau
      "configVersion": 3
    }
  ]
}
```

**Champs critiques** :
- `stateStr` : √âtat actuel (SECONDARY, RECOVERING, etc.)
- `optime` : Position dans l'Oplog
- `syncSourceHost` : Source de r√©plication actuelle
- `pingMs` : Latence r√©seau

#### Throughput de R√©plication

```javascript
db.serverStatus().metrics.repl.apply

{
  "batches": {
    "num": 12345,            // Nombre de batches appliqu√©s
    "totalMillis": 67890     // Temps total d'application
  },
  "ops": NumberLong(1234567)  // Op√©rations totales appliqu√©es
}
```

**Calcul du throughput** :

```javascript
const current = db.serverStatus().metrics.repl.apply.ops
// Attendre 1 seconde
sleep(1000)
const next = db.serverStatus().metrics.repl.apply.ops
const opsPerSec = next - current
print(`Throughput: ${opsPerSec} ops/sec`)
```

#### Utilisation du R√©seau

```javascript
db.serverStatus().network

{
  "bytesIn": NumberLong(12345678901),    // Trafic entrant total
  "bytesOut": NumberLong(98765432109),   // Trafic sortant total
  "numRequests": NumberLong(123456)      // Nombre de requ√™tes
}
```

**Surveillance du trafic de r√©plication** :

Diff√©rence entre mesures successives pour estimer le d√©bit r√©seau.

### Commandes de Diagnostic

#### Informations de R√©plication

```javascript
// Vue depuis un Secondary
db.getReplicationInfo()

{
  "logSizeMB": 10240,
  "usedMB": 5120,
  "timeDiff": 86400,
  "timeDiffHours": 24,
  "tFirst": "Mon Dec 01 2024 00:00:00",
  "tLast": "Tue Dec 02 2024 00:00:00",
  "now": "Tue Dec 02 2024 12:00:00"
}
```

#### √âtat de la Source de R√©plication

```javascript
db.printSlaveReplicationInfo()

// Affiche des informations d√©taill√©es sur tous les Secondaries
source: mongo2.example.com:27017
  syncedTo: Tue Dec 02 2024 12:00:00
  0 secs (0 hrs) behind the primary
```

#### Op√©rations en Cours

```javascript
db.currentOp({
  $or: [
    { op: "query" },
    { op: "getmore" },
    { "command.aggregate": { $exists: true } }
  ]
})
```

Identifier les requ√™tes lourdes qui pourraient causer du lag.

### Alerting

#### Alertes Critiques

1. **Secondary DOWN** :
```javascript
rs.status().members.filter(m => m.health === 0)
// Alerte si non vide
```

2. **√âtat non-SECONDARY** :
```javascript
rs.status().members.filter(m =>
  m._id !== 0 &&  // Pas le Primary
  m.stateStr !== "SECONDARY"
)
```

3. **Lag excessif** :
```javascript
const maxLag = rs.status().members
  .filter(m => m.stateStr === "SECONDARY")
  .map(m => (primary.optimeDate - m.optimeDate) / 1000)
  .reduce((max, curr) => Math.max(max, curr), 0)

if (maxLag > 60) {
  alert("CRITICAL: Replication lag > 60s")
}
```

#### Alertes Warning

1. **Source de r√©plication sous-optimale** :
   - R√©pliquer depuis un membre avec lag √©lev√©
   - R√©pliquer depuis un membre g√©ographiquement distant

2. **Oplog window court** :
   - `timeDiffHours < 12` (ajuster selon contexte)

3. **Cache saturation** :
   - `cache evictions` √©lev√©es

## Bonnes Pratiques

### Dimensionnement

**Recommandations mat√©rielles** :

- **CPU** : √âquivalent ou l√©g√®rement inf√©rieur au Primary (8-16 cores)
- **RAM** : √âquivalent au Primary (working set + cache)
- **Disque** : SSD recommand√© (IOPS similaires au Primary)
- **R√©seau** : 1-10 Gbps selon le taux de r√©plication

**Formule** :
```
Si Primary a :
  - 16 cores CPU
  - 128 GB RAM
  - 1 TB SSD NVMe

Secondary doit avoir minimum :
  - 12-16 cores CPU (pour rattrapage et lectures)
  - 128 GB RAM (m√™me working set)
  - 1 TB SSD (minimum, id√©alement NVMe aussi)
```

### Oplog Sizing sur Secondaries

Les Secondaries ont le m√™me Oplog que le Primary :

**V√©rification** :
```javascript
// Sur chaque membre
db.getReplicationInfo().timeDiffHours
// Doit √™tre similaire sur tous les membres
```

Si un Secondary a un Oplog plus petit (rare, erreur de configuration) :
- Risque de ne pas pouvoir rattraper apr√®s une panne prolong√©e
- N√©cessite initial sync plus fr√©quemment

### Distribution G√©ographique

**Architecture multi-datacenter** :

```
DC-A (Principal)          DC-B (Secondaire)      DC-C (DR)
[Primary, priority: 2]    [Sec, priority: 1]     [Sec, priority: 0.5]
[Sec, priority: 2]        [Sec, priority: 1]

Quorum : 3/5
Tol√©rance : Perte d'un DC entier
```

**Avantages** :
- Survie √† la perte d'un datacenter complet
- √âlection possible apr√®s panne de DC
- Lecture locale (Read Preference: nearest)

### Isolation des Charges

**Strat√©gie avec tags** :

```javascript
// Production OLTP
cfg.members[0].tags = { workload: "oltp", dc: "east" }
cfg.members[1].tags = { workload: "oltp", dc: "east" }

// Analytics OLAP
cfg.members[2].tags = { workload: "analytics", dc: "east", hidden: true }

// Backup
cfg.members[3].tags = { workload: "backup", dc: "west", hidden: true }

// Delayed (protection erreurs)
cfg.members[4].tags = { workload: "delayed", dc: "west", hidden: true, slaveDelay: 14400 }
```

**Routage applicatif** :
```javascript
// App principale
readPreference: "primaryPreferred"
readPreferenceTags: [{ workload: "oltp" }]

// App analytics
readPreference: "secondary"
readPreferenceTags: [{ workload: "analytics" }]
```

### S√©curit√©

**Chiffrement inter-membres** :

```yaml
# mongod.conf (sur tous les membres)
net:
  tls:
    mode: requireTLS
    certificateKeyFile: /etc/ssl/mongodb.pem
    CAFile: /etc/ssl/ca.pem
    allowConnectionsWithoutCertificates: false

security:
  clusterAuthMode: x509
```

**Firewall** :

```bash
# Autoriser uniquement les autres membres du Replica Set
iptables -A INPUT -p tcp --dport 27017 -s 10.0.1.10 -j ACCEPT  # Primary
iptables -A INPUT -p tcp --dport 27017 -s 10.0.1.11 -j ACCEPT  # Sec1
iptables -A INPUT -p tcp --dport 27017 -s 10.0.1.12 -j ACCEPT  # Sec2
iptables -A INPUT -p tcp --dport 27017 -j DROP  # Bloquer tout le reste
```

## Conclusion

Les **Secondaries** sont des composants essentiels d'un Replica Set MongoDB, offrant :

1. **Redondance** : Protection contre la perte de donn√©es via r√©plication
2. **Haute disponibilit√©** : Capacit√© √† devenir Primary lors d'une √©lection
3. **Scalabilit√© en lecture** : Distribution de la charge de lecture
4. **Isolation des charges** : Membres sp√©cialis√©s (Hidden, Delayed) pour cas d'usage sp√©cifiques

**Points cl√©s √† retenir** :

- Les Secondaries **r√©pliquent continuellement** depuis le Primary ou d'autres Secondaries (chaining)
- L'**application parall√®le** des op√©rations (MongoDB 4.0+) am√©liore drastiquement le throughput
- Le **replication lag** est in√©vitable mais doit √™tre surveill√© et minimis√©
- Les **membres sp√©cialis√©s** (Hidden, Delayed, Non-Voting) offrent une flexibilit√© architecturale
- Le **dimensionnement appropri√©** des Secondaries est crucial pour garantir les performances
- Le **monitoring continu** du lag et de l'√©tat des Secondaries est imp√©ratif

La ma√Ætrise des Secondaries permet de construire des architectures MongoDB robustes, performantes et adapt√©es aux exigences de production les plus strictes.

‚è≠Ô∏è [Arbiter](/09-replication/03.3-arbiter.md)
