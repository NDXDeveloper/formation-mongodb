ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 16.1.1 Principes et cas d'usage des Change Streams

## Introduction

Les Change Streams reprÃ©sentent bien plus qu'un simple mÃ©canisme de notification. Ils incarnent un changement de paradigme dans la conception d'applications modernes, permettant de passer d'architectures synchrones et basÃ©es sur le polling Ã  des systÃ¨mes vÃ©ritablement rÃ©actifs et Ã©vÃ©nementiels. Cette section explore les principes fondamentaux et les cas d'usage avancÃ©s qui tirent parti de cette capacitÃ© unique de MongoDB.

---

## Principes architecturaux fondamentaux

### 1. Event-Driven Architecture (EDA)

Les Change Streams sont la pierre angulaire d'une architecture orientÃ©e Ã©vÃ©nements avec MongoDB. Ils permettent de propager les changements d'Ã©tat de maniÃ¨re asynchrone vers des systÃ¨mes consommateurs sans couplage fort.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Event-Driven Architecture                  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Orders    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Change       â”‚                    â”‚
â”‚  â”‚ Collection  â”‚  write   â”‚ Streams      â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                  â”‚                            â”‚
â”‚                                  â”‚ Events                     â”‚
â”‚                                  â–¼                            â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                         â”‚  Event Router  â”‚                    â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                  â”‚                            â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                  â”‚               â”‚               â”‚            â”‚
â”‚                  â–¼               â–¼               â–¼            â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚         â”‚  Inventory â”‚  â”‚   Email    â”‚  â”‚  Analytics â”‚        â”‚
â”‚         â”‚  Service   â”‚  â”‚  Service   â”‚  â”‚  Service   â”‚        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Principes clÃ©s :**
- **DÃ©couplage** : Les producteurs et consommateurs ne se connaissent pas
- **Asynchronisme** : Traitement non-bloquant
- **ScalabilitÃ©** : Ajout de consommateurs sans impacter les producteurs
- **RÃ©silience** : Ã‰chec d'un consommateur n'affecte pas les autres

### 2. Reactive Systems (Manifeste RÃ©actif)

Les Change Streams permettent de construire des systÃ¨mes rÃ©actifs selon les quatre piliers du Reactive Manifesto :

| Pilier | ImplÃ©mentation avec Change Streams |
|--------|-----------------------------------|
| **Responsive** | RÃ©ponse en temps rÃ©el (< 200ms) aux changements |
| **Resilient** | Resume tokens permettent la reprise aprÃ¨s panne |
| **Elastic** | ScalabilitÃ© horizontale via sharding |
| **Message-Driven** | Communication asynchrone par Ã©vÃ©nements |

### 3. Eventually Consistent Systems

Les Change Streams s'intÃ¨grent naturellement dans des architectures Ã  cohÃ©rence Ã©ventuelle :

```javascript
// Exemple : Synchronisation multi-bases avec cohÃ©rence Ã©ventuelle
class EventuallyConsistentSync {
  constructor(mongoPrimary, mongoReadReplicas) {
    this.primary = mongoPrimary;
    this.replicas = mongoReadReplicas;
    this.syncQueue = [];
  }

  async initialize() {
    // Surveiller la base primaire
    const changeStream = this.primary.db('main').watch();

    changeStream.on('change', async (change) => {
      // Propager aux rÃ©plicas avec tolÃ©rance aux pannes
      const syncPromises = this.replicas.map(replica =>
        this.syncToReplica(replica, change).catch(err => {
          console.error(`Sync failed to replica: ${err.message}`);
          this.syncQueue.push({ replica, change });
          return null;
        })
      );

      await Promise.allSettled(syncPromises);
    });

    // Retry pÃ©riodique des syncs Ã©chouÃ©s
    setInterval(() => this.retryFailedSyncs(), 30000);
  }

  async syncToReplica(replica, change) {
    const collection = replica.db(change.ns.db).collection(change.ns.coll);

    switch (change.operationType) {
      case 'insert':
        await collection.insertOne(change.fullDocument);
        break;
      case 'update':
        await collection.updateOne(
          { _id: change.documentKey._id },
          { $set: change.updateDescription.updatedFields }
        );
        break;
      case 'delete':
        await collection.deleteOne({ _id: change.documentKey._id });
        break;
    }
  }

  async retryFailedSyncs() {
    const toRetry = [...this.syncQueue];
    this.syncQueue = [];

    for (const item of toRetry) {
      try {
        await this.syncToReplica(item.replica, item.change);
      } catch (err) {
        this.syncQueue.push(item); // Re-queue si Ã©chec
      }
    }
  }
}
```

---

## Cas d'usage avancÃ©s

### Cas d'usage 1 : Event Sourcing

**Principe :** Stocker tous les changements d'Ã©tat comme une sÃ©quence d'Ã©vÃ©nements immuables plutÃ´t que l'Ã©tat actuel.

**Architecture :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Event Sourcing                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Commands   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Aggregates  â”‚                   â”‚
â”‚  â”‚  (Write)    â”‚          â”‚  (Domain)    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                  â”‚                           â”‚
â”‚                                  â”‚ Events                    â”‚
â”‚                                  â–¼                           â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                         â”‚  Event Store   â”‚                   â”‚
â”‚                         â”‚   (MongoDB)    â”‚                   â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                  â”‚                           â”‚
â”‚                                  â”‚ Change Streams            â”‚
â”‚                                  â–¼                           â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                         â”‚  Projections   â”‚                   â”‚
â”‚                         â”‚  (Read Models) â”‚                   â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplÃ©mentation complÃ¨te :**

```javascript
// 1. Event Store
class EventStore {
  constructor(db) {
    this.collection = db.collection('events');
    // Index pour performance
    this.collection.createIndex({ aggregateId: 1, version: 1 });
    this.collection.createIndex({ eventType: 1, timestamp: 1 });
  }

  async append(aggregateId, aggregateType, events) {
    // RÃ©cupÃ©rer la version actuelle
    const lastEvent = await this.collection
      .find({ aggregateId })
      .sort({ version: -1 })
      .limit(1)
      .toArray();

    const currentVersion = lastEvent.length > 0 ? lastEvent[0].version : 0;

    // PrÃ©parer les Ã©vÃ©nements
    const eventsToStore = events.map((event, index) => ({
      _id: new ObjectId(),
      aggregateId,
      aggregateType,
      eventType: event.type,
      eventData: event.data,
      version: currentVersion + index + 1,
      timestamp: new Date(),
      metadata: {
        userId: event.metadata?.userId,
        correlationId: event.metadata?.correlationId,
        causationId: event.metadata?.causationId
      }
    }));

    // InsÃ©rer atomiquement
    const result = await this.collection.insertMany(eventsToStore);
    return result.insertedIds;
  }

  async getEvents(aggregateId, fromVersion = 0) {
    return await this.collection
      .find({
        aggregateId,
        version: { $gt: fromVersion }
      })
      .sort({ version: 1 })
      .toArray();
  }

  async getAllEvents(eventTypes = null, fromTimestamp = null) {
    const filter = {};
    if (eventTypes) filter.eventType = { $in: eventTypes };
    if (fromTimestamp) filter.timestamp = { $gte: fromTimestamp };

    return await this.collection
      .find(filter)
      .sort({ timestamp: 1 })
      .toArray();
  }
}

// 2. Aggregate (exemple : commande e-commerce)
class OrderAggregate {
  constructor(orderId) {
    this.orderId = orderId;
    this.version = 0;
    this.items = [];
    this.status = 'draft';
    this.totalAmount = 0;
  }

  // Charger depuis l'event store
  static async load(eventStore, orderId) {
    const events = await eventStore.getEvents(orderId);
    const order = new OrderAggregate(orderId);

    events.forEach(event => {
      order.apply(event);
    });

    return order;
  }

  // Appliquer un Ã©vÃ©nement (replay)
  apply(event) {
    switch (event.eventType) {
      case 'OrderCreated':
        this.status = 'pending';
        break;

      case 'ItemAdded':
        this.items.push(event.eventData.item);
        this.totalAmount += event.eventData.item.price;
        break;

      case 'ItemRemoved':
        const index = this.items.findIndex(i => i.id === event.eventData.itemId);
        if (index > -1) {
          this.totalAmount -= this.items[index].price;
          this.items.splice(index, 1);
        }
        break;

      case 'OrderConfirmed':
        this.status = 'confirmed';
        break;

      case 'OrderShipped':
        this.status = 'shipped';
        this.shippingInfo = event.eventData.shippingInfo;
        break;

      case 'OrderCancelled':
        this.status = 'cancelled';
        break;
    }

    this.version = event.version;
  }

  // Commandes (gÃ©nÃ¨rent des Ã©vÃ©nements)
  addItem(item, userId) {
    if (this.status !== 'draft' && this.status !== 'pending') {
      throw new Error('Cannot add items to confirmed order');
    }

    return [{
      type: 'ItemAdded',
      data: { item },
      metadata: { userId }
    }];
  }

  confirm(userId) {
    if (this.items.length === 0) {
      throw new Error('Cannot confirm empty order');
    }
    if (this.status !== 'pending') {
      throw new Error('Order already confirmed or cancelled');
    }

    return [{
      type: 'OrderConfirmed',
      data: { totalAmount: this.totalAmount, itemCount: this.items.length },
      metadata: { userId }
    }];
  }
}

// 3. Projection Builder (construit les read models)
class OrderProjectionBuilder {
  constructor(db, eventStore) {
    this.readModels = db.collection('orders_read_model');
    this.eventStore = eventStore;
  }

  async initialize() {
    // CrÃ©er les index
    await this.readModels.createIndex({ orderId: 1 }, { unique: true });
    await this.readModels.createIndex({ customerId: 1 });
    await this.readModels.createIndex({ status: 1 });

    // Ã‰couter les nouveaux Ã©vÃ©nements via Change Streams
    const changeStream = this.eventStore.collection.watch([
      { $match: { operationType: 'insert' } }
    ]);

    changeStream.on('change', async (change) => {
      const event = change.fullDocument;
      await this.projectEvent(event);
    });
  }

  async projectEvent(event) {
    const { aggregateId, eventType, eventData, timestamp } = event;

    switch (eventType) {
      case 'OrderCreated':
        await this.readModels.insertOne({
          orderId: aggregateId,
          customerId: eventData.customerId,
          status: 'pending',
          items: [],
          totalAmount: 0,
          createdAt: timestamp,
          updatedAt: timestamp
        });
        break;

      case 'ItemAdded':
        await this.readModels.updateOne(
          { orderId: aggregateId },
          {
            $push: { items: eventData.item },
            $inc: { totalAmount: eventData.item.price },
            $set: { updatedAt: timestamp }
          }
        );
        break;

      case 'OrderConfirmed':
        await this.readModels.updateOne(
          { orderId: aggregateId },
          {
            $set: {
              status: 'confirmed',
              confirmedAt: timestamp,
              updatedAt: timestamp
            }
          }
        );
        break;

      case 'OrderShipped':
        await this.readModels.updateOne(
          { orderId: aggregateId },
          {
            $set: {
              status: 'shipped',
              shippingInfo: eventData.shippingInfo,
              shippedAt: timestamp,
              updatedAt: timestamp
            }
          }
        );
        break;
    }
  }

  // Rebuild complet des projections (replay)
  async rebuildAll() {
    await this.readModels.deleteMany({});

    const events = await this.eventStore.getAllEvents();

    for (const event of events) {
      await this.projectEvent(event);
    }

    console.log(`Rebuilt ${events.length} events`);
  }
}

// 4. Utilisation
const db = client.db('ecommerce');
const eventStore = new EventStore(db);
const projectionBuilder = new OrderProjectionBuilder(db, eventStore);

// Initialiser les projections
await projectionBuilder.initialize();

// CrÃ©er une commande
const order = new OrderAggregate('order-123');
const events = [
  {
    type: 'OrderCreated',
    data: { customerId: 'cust-456' },
    metadata: { userId: 'user-789' }
  }
];

await eventStore.append('order-123', 'Order', events);

// Ajouter des items
const loadedOrder = await OrderAggregate.load(eventStore, 'order-123');
const itemEvents = loadedOrder.addItem(
  { id: 'item-1', name: 'Product A', price: 29.99 },
  'user-789'
);
await eventStore.append('order-123', 'Order', itemEvents);

// Les projections sont automatiquement mises Ã  jour via Change Streams
```

**Avantages de ce pattern :**
- âœ… Audit trail complet et immuable
- âœ… PossibilitÃ© de replay pour debugging
- âœ… Multiples projections optimisÃ©es par cas d'usage
- âœ… Ã‰volution du schÃ©ma simplifiÃ©e (ajout de projections)
- âœ… Time travel (Ã©tat Ã  un instant donnÃ©)

---

### Cas d'usage 2 : CQRS (Command Query Responsibility Segregation)

**Principe :** SÃ©parer les modÃ¨les de lecture et d'Ã©criture, synchronisÃ©s via Change Streams.

**Architecture :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CQRS Pattern                        â”‚
â”‚                                                              â”‚
â”‚  Commands (Write Side)              Queries (Read Side)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Write Database  â”‚              â”‚  Read Database   â”‚      â”‚
â”‚  â”‚   (Normalized)   â”‚              â”‚  (Denormalized)  â”‚      â”‚
â”‚  â”‚                  â”‚              â”‚                  â”‚      â”‚
â”‚  â”‚ - users          â”‚              â”‚ - user_profiles  â”‚      â”‚
â”‚  â”‚ - orders         â”‚              â”‚ - order_summary  â”‚      â”‚
â”‚  â”‚ - payments       â”‚              â”‚ - dashboard_data â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚           â”‚                                   â–²              â”‚
â”‚           â”‚                                   â”‚              â”‚
â”‚           â”‚ Change Streams                    â”‚              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                    Synchronization                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplÃ©mentation :**

```javascript
// 1. Write Model (Command Side)
class WriteModel {
  constructor(db) {
    this.users = db.collection('users');
    this.orders = db.collection('orders');
    this.payments = db.collection('payments');
  }

  async createOrder(orderData) {
    // Normalisation : une commande rÃ©fÃ©rence un user
    const order = {
      _id: new ObjectId(),
      userId: orderData.userId,
      items: orderData.items,
      totalAmount: orderData.items.reduce((sum, item) => sum + item.price, 0),
      status: 'pending',
      createdAt: new Date()
    };

    await this.orders.insertOne(order);
    return order._id;
  }

  async processPayment(orderId, paymentData) {
    const payment = {
      _id: new ObjectId(),
      orderId,
      amount: paymentData.amount,
      method: paymentData.method,
      status: 'completed',
      processedAt: new Date()
    };

    const session = this.orders.client.startSession();

    try {
      await session.withTransaction(async () => {
        await this.payments.insertOne(payment, { session });
        await this.orders.updateOne(
          { _id: orderId },
          { $set: { status: 'paid', paidAt: new Date() } },
          { session }
        );
      });
    } finally {
      await session.endSession();
    }

    return payment._id;
  }
}

// 2. Read Model Builder (Query Side)
class ReadModelBuilder {
  constructor(writeDb, readDb) {
    this.writeDb = writeDb;
    this.readDb = readDb;

    // Collections dÃ©normalisÃ©es pour lecture rapide
    this.orderSummary = readDb.collection('order_summary');
    this.userDashboard = readDb.collection('user_dashboard');
  }

  async initialize() {
    // Index optimisÃ©s pour requÃªtes
    await this.orderSummary.createIndex({ userId: 1, status: 1 });
    await this.orderSummary.createIndex({ createdAt: -1 });
    await this.userDashboard.createIndex({ userId: 1 }, { unique: true });

    // Ã‰couter les changements sur le write side
    this.watchOrders();
    this.watchPayments();
    this.watchUsers();
  }

  watchOrders() {
    const changeStream = this.writeDb.collection('orders').watch();

    changeStream.on('change', async (change) => {
      switch (change.operationType) {
        case 'insert':
          await this.handleOrderCreated(change.fullDocument);
          break;
        case 'update':
          await this.handleOrderUpdated(change);
          break;
      }
    });
  }

  watchPayments() {
    const changeStream = this.writeDb.collection('payments').watch([
      { $match: { operationType: 'insert' } }
    ]);

    changeStream.on('change', async (change) => {
      await this.handlePaymentProcessed(change.fullDocument);
    });
  }

  watchUsers() {
    const changeStream = this.writeDb.collection('users').watch();

    changeStream.on('change', async (change) => {
      if (change.operationType === 'insert') {
        await this.handleUserCreated(change.fullDocument);
      }
    });
  }

  async handleOrderCreated(order) {
    // RÃ©cupÃ©rer les infos utilisateur (jointure)
    const user = await this.writeDb.collection('users')
      .findOne({ _id: order.userId });

    // CrÃ©er un rÃ©sumÃ© dÃ©normalisÃ© pour lecture rapide
    await this.orderSummary.insertOne({
      _id: order._id,
      orderId: order._id,
      userId: order.userId,
      userName: user?.name || 'Unknown',
      userEmail: user?.email,
      itemCount: order.items.length,
      totalAmount: order.totalAmount,
      status: order.status,
      createdAt: order.createdAt,
      // Champs prÃ©-calculÃ©s pour analytics
      year: order.createdAt.getFullYear(),
      month: order.createdAt.getMonth() + 1,
      dayOfWeek: order.createdAt.getDay()
    });

    // Mettre Ã  jour le dashboard utilisateur
    await this.userDashboard.updateOne(
      { userId: order.userId },
      {
        $inc: {
          totalOrders: 1,
          pendingOrders: 1
        },
        $set: { lastOrderDate: order.createdAt }
      },
      { upsert: true }
    );
  }

  async handleOrderUpdated(change) {
    const orderId = change.documentKey._id;
    const updatedFields = change.updateDescription?.updatedFields || {};

    // Mettre Ã  jour le rÃ©sumÃ©
    if (updatedFields.status) {
      await this.orderSummary.updateOne(
        { orderId },
        { $set: { status: updatedFields.status } }
      );

      // Mettre Ã  jour le dashboard si passage Ã  "paid"
      if (updatedFields.status === 'paid') {
        const order = await this.orderSummary.findOne({ orderId });

        await this.userDashboard.updateOne(
          { userId: order.userId },
          {
            $inc: {
              pendingOrders: -1,
              completedOrders: 1,
              totalSpent: order.totalAmount
            }
          }
        );
      }
    }
  }

  async handlePaymentProcessed(payment) {
    // Enrichir le rÃ©sumÃ© de commande avec info paiement
    await this.orderSummary.updateOne(
      { orderId: payment.orderId },
      {
        $set: {
          paymentId: payment._id,
          paymentMethod: payment.method,
          paidAt: payment.processedAt
        }
      }
    );
  }

  async handleUserCreated(user) {
    // Initialiser le dashboard utilisateur
    await this.userDashboard.insertOne({
      userId: user._id,
      userName: user.name,
      userEmail: user.email,
      totalOrders: 0,
      pendingOrders: 0,
      completedOrders: 0,
      totalSpent: 0,
      joinedAt: user.createdAt
    });
  }
}

// 3. Query Service (Read Side)
class QueryService {
  constructor(readDb) {
    this.orderSummary = readDb.collection('order_summary');
    this.userDashboard = readDb.collection('user_dashboard');
  }

  // RequÃªtes ultra-rapides sur donnÃ©es dÃ©normalisÃ©es
  async getUserOrders(userId, options = {}) {
    const { status, limit = 20, skip = 0 } = options;

    const filter = { userId };
    if (status) filter.status = status;

    return await this.orderSummary
      .find(filter)
      .sort({ createdAt: -1 })
      .skip(skip)
      .limit(limit)
      .toArray();
  }

  async getUserDashboard(userId) {
    return await this.userDashboard.findOne({ userId });
  }

  async getRecentOrders(limit = 50) {
    return await this.orderSummary
      .find({})
      .sort({ createdAt: -1 })
      .limit(limit)
      .toArray();
  }

  async getOrderAnalytics(year, month) {
    return await this.orderSummary.aggregate([
      {
        $match: { year, month }
      },
      {
        $group: {
          _id: '$dayOfWeek',
          totalOrders: { $sum: 1 },
          totalRevenue: { $sum: '$totalAmount' },
          avgOrderValue: { $avg: '$totalAmount' }
        }
      },
      {
        $sort: { _id: 1 }
      }
    ]).toArray();
  }
}

// 4. Utilisation
const writeDb = client.db('ecommerce_write');
const readDb = client.db('ecommerce_read');

const writeModel = new WriteModel(writeDb);
const readModelBuilder = new ReadModelBuilder(writeDb, readDb);
const queryService = new QueryService(readDb);

// Initialiser la synchronisation
await readModelBuilder.initialize();

// Commandes (write side)
const orderId = await writeModel.createOrder({
  userId: new ObjectId('...'),
  items: [
    { name: 'Product A', price: 29.99 },
    { name: 'Product B', price: 49.99 }
  ]
});

await writeModel.processPayment(orderId, {
  amount: 79.98,
  method: 'credit_card'
});

// Queries (read side) - instantanÃ©es car donnÃ©es prÃ©-calculÃ©es
const userOrders = await queryService.getUserOrders(userId);
const dashboard = await queryService.getUserDashboard(userId);
const analytics = await queryService.getOrderAnalytics(2024, 12);
```

**Avantages du pattern CQRS :**
- âœ… Optimisation indÃ©pendante des lectures et Ã©critures
- âœ… Scaling diffÃ©renciÃ© (plus de read replicas)
- âœ… SchÃ©mas adaptÃ©s aux cas d'usage (dÃ©normalisation)
- âœ… Performances de lecture excellentes (donnÃ©es prÃ©-calculÃ©es)
- âœ… Ã‰volution du read model sans impact sur write model

---

### Cas d'usage 3 : Cache Invalidation et Synchronisation

**Principe :** Maintenir automatiquement un cache (Redis, Memcached) synchronisÃ© avec MongoDB.

**ImplÃ©mentation avec stratÃ©gies multiples :**

```javascript
class IntelligentCacheSync {
  constructor(mongoClient, redisClient) {
    this.mongo = mongoClient;
    this.redis = redisClient;
    this.db = mongoClient.db('myapp');

    // Statistiques pour optimisation
    this.stats = {
      cacheHits: 0,
      cacheMisses: 0,
      invalidations: 0,
      updates: 0
    };
  }

  async initialize() {
    // Surveiller les collections avec stratÃ©gies diffÃ©rentes
    this.watchWithInvalidation('users'); // Invalidation simple
    this.watchWithUpdate('products'); // Mise Ã  jour du cache
    this.watchWithTTL('sessions', 3600); // TTL dynamique
  }

  // StratÃ©gie 1 : Cache Invalidation (supprime du cache)
  watchWithInvalidation(collectionName) {
    const changeStream = this.db.collection(collectionName).watch([
      {
        $match: {
          operationType: { $in: ['update', 'replace', 'delete'] }
        }
      }
    ]);

    changeStream.on('change', async (change) => {
      const cacheKey = `${collectionName}:${change.documentKey._id}`;

      await this.redis.del(cacheKey);
      this.stats.invalidations++;

      console.log(`Cache invalidated: ${cacheKey}`);
    });
  }

  // StratÃ©gie 2 : Cache Update (met Ã  jour le cache)
  watchWithUpdate(collectionName) {
    const changeStream = this.db.collection(collectionName).watch([
      {
        $match: {
          operationType: { $in: ['insert', 'update', 'replace'] }
        }
      }
    ], {
      fullDocument: 'updateLookup' // RÃ©cupÃ©rer le document complet
    });

    changeStream.on('change', async (change) => {
      const cacheKey = `${collectionName}:${change.documentKey._id}`;
      const document = change.fullDocument;

      if (document) {
        // Calculer TTL basÃ© sur la frÃ©quence d'accÃ¨s
        const accessCount = await this.redis.get(`${cacheKey}:access_count`);
        const ttl = this.calculateTTL(accessCount);

        await this.redis.setex(
          cacheKey,
          ttl,
          JSON.stringify(document)
        );

        this.stats.updates++;
        console.log(`Cache updated: ${cacheKey} (TTL: ${ttl}s)`);
      }
    });
  }

  // StratÃ©gie 3 : TTL dynamique selon usage
  watchWithTTL(collectionName, baseTTL) {
    const changeStream = this.db.collection(collectionName).watch();

    changeStream.on('change', async (change) => {
      const cacheKey = `${collectionName}:${change.documentKey._id}`;

      switch (change.operationType) {
        case 'insert':
          await this.redis.setex(
            cacheKey,
            baseTTL,
            JSON.stringify(change.fullDocument)
          );
          break;

        case 'update':
        case 'replace':
          // RÃ©cupÃ©rer le TTL restant et le prÃ©server
          const ttl = await this.redis.ttl(cacheKey);
          if (ttl > 0) {
            await this.redis.setex(
              cacheKey,
              ttl,
              JSON.stringify(change.fullDocument)
            );
          }
          break;

        case 'delete':
          await this.redis.del(cacheKey);
          break;
      }
    });
  }

  calculateTTL(accessCount) {
    // TTL adaptatif : plus c'est accÃ©dÃ©, plus le TTL est long
    const count = parseInt(accessCount) || 0;

    if (count > 1000) return 3600; // 1h pour donnÃ©es trÃ¨s accÃ©dÃ©es
    if (count > 100) return 1800;  // 30min
    if (count > 10) return 900;    // 15min
    return 300; // 5min par dÃ©faut
  }

  // MÃ©thode read-through avec mise Ã  jour des stats
  async get(collection, id) {
    const cacheKey = `${collection}:${id}`;

    // IncrÃ©menter le compteur d'accÃ¨s
    await this.redis.incr(`${cacheKey}:access_count`);

    // Tenter de rÃ©cupÃ©rer du cache
    const cached = await this.redis.get(cacheKey);

    if (cached) {
      this.stats.cacheHits++;
      return JSON.parse(cached);
    }

    // Cache miss : lire depuis MongoDB
    this.stats.cacheMisses++;
    const doc = await this.db.collection(collection).findOne({ _id: id });

    if (doc) {
      // Peupler le cache
      const ttl = this.calculateTTL(
        await this.redis.get(`${cacheKey}:access_count`)
      );
      await this.redis.setex(cacheKey, ttl, JSON.stringify(doc));
    }

    return doc;
  }

  // Monitoring
  getStats() {
    const hitRate = this.stats.cacheHits /
      (this.stats.cacheHits + this.stats.cacheMisses);

    return {
      ...this.stats,
      hitRate: `${(hitRate * 100).toFixed(2)}%`
    };
  }
}

// Utilisation
const cacheSync = new IntelligentCacheSync(mongoClient, redisClient);
await cacheSync.initialize();

// Lecture avec cache automatique
const user = await cacheSync.get('users', userId);

// Statistiques
setInterval(() => {
  console.log('Cache stats:', cacheSync.getStats());
}, 60000);
```

---

### Cas d'usage 4 : Data Replication Multi-Cloud

**Principe :** RÃ©pliquer les donnÃ©es vers plusieurs clouds ou rÃ©gions pour disaster recovery et compliance.

```javascript
class MultiCloudReplicator {
  constructor(primaryMongo, replicaConfigs) {
    this.primary = primaryMongo;
    this.replicas = replicaConfigs; // [{ client, region, priority }]
    this.replicationLag = new Map();
  }

  async initialize() {
    const changeStream = this.primary.db().watch([], {
      fullDocument: 'updateLookup'
    });

    changeStream.on('change', async (change) => {
      // RÃ©plication parallÃ¨le vers toutes les destinations
      const replicationPromises = this.replicas.map(replica =>
        this.replicateToRegion(replica, change)
      );

      // Attendre toutes les rÃ©plications avec gestion d'erreur
      const results = await Promise.allSettled(replicationPromises);

      // Logger les Ã©checs
      results.forEach((result, index) => {
        if (result.status === 'rejected') {
          console.error(
            `Replication failed to ${this.replicas[index].region}:`,
            result.reason
          );
        }
      });
    });

    // Monitoring du lag de rÃ©plication
    setInterval(() => this.monitorReplicationLag(), 30000);
  }

  async replicateToRegion(replica, change) {
    const startTime = Date.now();
    const { client, region } = replica;

    try {
      const db = client.db(change.ns.db);
      const collection = db.collection(change.ns.coll);

      switch (change.operationType) {
        case 'insert':
          await collection.insertOne(change.fullDocument);
          break;

        case 'update':
          await collection.updateOne(
            { _id: change.documentKey._id },
            {
              $set: change.updateDescription.updatedFields,
              $unset: change.updateDescription.removedFields || {}
            },
            { upsert: true }
          );
          break;

        case 'replace':
          await collection.replaceOne(
            { _id: change.documentKey._id },
            change.fullDocument,
            { upsert: true }
          );
          break;

        case 'delete':
          await collection.deleteOne({ _id: change.documentKey._id });
          break;

        case 'drop':
          await collection.drop().catch(() => {}); // Ignorer si n'existe pas
          break;
      }

      // Enregistrer le lag
      const lag = Date.now() - startTime;
      this.replicationLag.set(region, lag);

    } catch (error) {
      console.error(`Replication error to ${region}:`, error);
      throw error;
    }
  }

  async monitorReplicationLag() {
    console.log('Replication Lag Report:');
    this.replicationLag.forEach((lag, region) => {
      const status = lag < 1000 ? 'âœ“' : lag < 5000 ? 'âš ' : 'âœ—';
      console.log(`  ${status} ${region}: ${lag}ms`);
    });

    // Alerter si lag excessif
    const maxLag = Math.max(...Array.from(this.replicationLag.values()));
    if (maxLag > 10000) {
      await this.sendAlert(`High replication lag detected: ${maxLag}ms`);
    }
  }

  async sendAlert(message) {
    // IntÃ©gration avec systÃ¨me d'alerting (PagerDuty, Slack, etc.)
    console.error(`ALERT: ${message}`);
  }

  // VÃ©rification de consistance pÃ©riodique
  async verifyConsistency(collection, sampleSize = 100) {
    const primaryDocs = await this.primary
      .db()
      .collection(collection)
      .aggregate([{ $sample: { size: sampleSize } }])
      .toArray();

    for (const replica of this.replicas) {
      const replicaDocs = await Promise.all(
        primaryDocs.map(doc =>
          replica.client
            .db()
            .collection(collection)
            .findOne({ _id: doc._id })
        )
      );

      const mismatches = primaryDocs.filter((primaryDoc, index) => {
        const replicaDoc = replicaDocs[index];
        return JSON.stringify(primaryDoc) !== JSON.stringify(replicaDoc);
      });

      if (mismatches.length > 0) {
        console.warn(
          `Consistency check: ${mismatches.length}/${sampleSize} ` +
          `documents differ in ${replica.region}`
        );
      }
    }
  }
}

// Configuration multi-cloud
const replicator = new MultiCloudReplicator(
  primaryMongoClient,
  [
    {
      client: awsMongoClient,
      region: 'us-east-1',
      priority: 1
    },
    {
      client: gcpMongoClient,
      region: 'europe-west1',
      priority: 2
    },
    {
      client: azureMongoClient,
      region: 'westeurope',
      priority: 3
    }
  ]
);

await replicator.initialize();

// VÃ©rification pÃ©riodique de consistance
setInterval(async () => {
  await replicator.verifyConsistency('critical_data');
}, 3600000); // Toutes les heures
```

---

### Cas d'usage 5 : Real-time Analytics Pipeline

**Principe :** Pipeline analytics en temps rÃ©el avec agrÃ©gations incrÃ©mentales.

```javascript
class RealtimeAnalyticsPipeline {
  constructor(db) {
    this.db = db;
    this.events = db.collection('events');
    this.analytics = db.collection('analytics');
  }

  async initialize() {
    // Index pour performance
    await this.analytics.createIndex({ metric: 1, timestamp: -1 });
    await this.analytics.createIndex({ metric: 1, dimensions: 1 });

    // Pipeline de transformation pour analytics
    const pipeline = [
      {
        $match: {
          operationType: 'insert',
          'fullDocument.type': { $in: ['page_view', 'purchase', 'signup'] }
        }
      },
      {
        $project: {
          eventType: '$fullDocument.type',
          userId: '$fullDocument.userId',
          timestamp: '$fullDocument.timestamp',
          value: '$fullDocument.value',
          properties: '$fullDocument.properties'
        }
      }
    ];

    const changeStream = this.events.watch(pipeline);

    changeStream.on('change', async (change) => {
      await this.processAnalyticsEvent(change);
    });
  }

  async processAnalyticsEvent(change) {
    const event = {
      type: change.eventType,
      userId: change.userId,
      timestamp: change.timestamp,
      value: change.value,
      properties: change.properties
    };

    // AgrÃ©gations incrÃ©mentales multiples
    await Promise.all([
      this.updateHourlyMetrics(event),
      this.updateDailyMetrics(event),
      this.updateUserMetrics(event),
      this.updateRealtimeDashboard(event)
    ]);
  }

  async updateHourlyMetrics(event) {
    const hourBucket = new Date(event.timestamp);
    hourBucket.setMinutes(0, 0, 0);

    await this.analytics.updateOne(
      {
        metric: 'hourly',
        eventType: event.type,
        timestamp: hourBucket
      },
      {
        $inc: {
          count: 1,
          totalValue: event.value || 0
        },
        $push: {
          samples: {
            $each: [{ userId: event.userId, timestamp: event.timestamp }],
            $slice: -100 // Garder les 100 derniers
          }
        },
        $setOnInsert: {
          createdAt: new Date()
        }
      },
      { upsert: true }
    );
  }

  async updateDailyMetrics(event) {
    const dayBucket = new Date(event.timestamp);
    dayBucket.setHours(0, 0, 0, 0);

    await this.analytics.updateOne(
      {
        metric: 'daily',
        eventType: event.type,
        timestamp: dayBucket
      },
      {
        $inc: { count: 1, totalValue: event.value || 0 },
        $min: { minValue: event.value },
        $max: { maxValue: event.value }
      },
      { upsert: true }
    );
  }

  async updateUserMetrics(event) {
    await this.analytics.updateOne(
      {
        metric: 'user',
        userId: event.userId
      },
      {
        $inc: { [`events.${event.type}`]: 1 },
        $set: { lastSeen: event.timestamp },
        $setOnInsert: { firstSeen: event.timestamp }
      },
      { upsert: true }
    );
  }

  async updateRealtimeDashboard(event) {
    // FenÃªtre glissante des 5 derniÃ¨res minutes
    const fiveMinutesAgo = new Date(Date.now() - 5 * 60 * 1000);

    await this.analytics.updateOne(
      {
        metric: 'realtime',
        window: '5min'
      },
      {
        $push: {
          events: {
            $each: [event],
            $slice: -1000, // Max 1000 Ã©vÃ©nements
            $position: 0
          }
        },
        $inc: { [`counts.${event.type}`]: 1 },
        $set: { updatedAt: new Date() }
      },
      { upsert: true }
    );

    // Nettoyer les vieux Ã©vÃ©nements
    await this.analytics.updateOne(
      { metric: 'realtime', window: '5min' },
      {
        $pull: {
          events: { timestamp: { $lt: fiveMinutesAgo } }
        }
      }
    );
  }

  // API pour requÃªtes analytics
  async getHourlyStats(eventType, hoursBack = 24) {
    const since = new Date(Date.now() - hoursBack * 3600 * 1000);

    return await this.analytics
      .find({
        metric: 'hourly',
        eventType,
        timestamp: { $gte: since }
      })
      .sort({ timestamp: 1 })
      .toArray();
  }

  async getUserActivity(userId) {
    return await this.analytics.findOne({
      metric: 'user',
      userId
    });
  }

  async getRealtimeDashboard() {
    const dashboard = await this.analytics.findOne({
      metric: 'realtime',
      window: '5min'
    });

    return {
      counts: dashboard?.counts || {},
      recentEvents: dashboard?.events || [],
      updatedAt: dashboard?.updatedAt
    };
  }
}

// Utilisation
const analytics = new RealtimeAnalyticsPipeline(db);
await analytics.initialize();

// Insertion d'Ã©vÃ©nements (automatiquement agrÃ©gÃ©s)
await db.collection('events').insertOne({
  type: 'purchase',
  userId: 'user-123',
  value: 149.99,
  timestamp: new Date(),
  properties: { productId: 'prod-456' }
});

// RequÃªtes analytics instantanÃ©es
const hourlyStats = await analytics.getHourlyStats('purchase', 24);
const userActivity = await analytics.getUserActivity('user-123');
const dashboard = await analytics.getRealtimeDashboard();
```

---

## Anti-patterns Ã  Ã©viter

### âŒ Anti-pattern 1 : Traitement synchrone bloquant

```javascript
// MAUVAIS : Traitement synchrone qui bloque le stream
changeStream.on('change', async (change) => {
  await heavyOperation(change); // Peut prendre plusieurs secondes
  await anotherSlowOperation(change);
  // Le stream est bloquÃ© pendant ce temps
});

// BON : Traitement asynchrone non-bloquant
const queue = [];

changeStream.on('change', (change) => {
  queue.push(change); // Mise en queue immÃ©diate
});

// Worker sÃ©parÃ©
setInterval(async () => {
  const batch = queue.splice(0, 100);
  await Promise.all(batch.map(change => processChange(change)));
}, 1000);
```

### âŒ Anti-pattern 2 : Oublier la gestion des erreurs

```javascript
// MAUVAIS : Pas de gestion d'erreur
changeStream.on('change', async (change) => {
  await processChange(change); // Peut Ã©chouer et crash l'app
});

// BON : Gestion robuste des erreurs
changeStream.on('change', async (change) => {
  try {
    await processChange(change);
  } catch (error) {
    console.error('Processing error:', error);
    await logErrorToMonitoring(error, change);
    // Continuer malgrÃ© l'erreur
  }
});

changeStream.on('error', async (error) => {
  console.error('Stream error:', error);
  await reconnectWithBackoff();
});
```

### âŒ Anti-pattern 3 : Ne pas sauvegarder les resume tokens

```javascript
// MAUVAIS : Perte de resume token
let changeStream = collection.watch();

// BON : Persistance du resume token
let resumeToken;
const changeStream = collection.watch([], {
  resumeAfter: await loadResumeToken() // Charger depuis DB
});

changeStream.on('change', async (change) => {
  resumeToken = change._id;
  await saveResumeToken(resumeToken); // Sauvegarder pÃ©riodiquement
  await processChange(change);
});
```

---

## Bonnes pratiques de production

### 1. Monitoring et alerting

```javascript
class ProductionChangeStreamMonitor {
  constructor(collection) {
    this.metrics = {
      eventsProcessed: 0,
      errors: 0,
      lastEventTime: null,
      avgProcessingTime: 0
    };

    this.collection = collection;
  }

  async start() {
    const changeStream = this.collection.watch();

    changeStream.on('change', async (change) => {
      const startTime = Date.now();

      try {
        await this.processWithTimeout(change, 5000);

        // MÃ©triques
        this.metrics.eventsProcessed++;
        this.metrics.lastEventTime = new Date();

        const processingTime = Date.now() - startTime;
        this.metrics.avgProcessingTime =
          (this.metrics.avgProcessingTime * 0.9) + (processingTime * 0.1);

      } catch (error) {
        this.metrics.errors++;
        await this.alertOnError(error, change);
      }
    });

    // Watchdog : alerter si pas d'Ã©vÃ©nements depuis longtemps
    setInterval(() => {
      const timeSinceLastEvent = Date.now() -
        (this.metrics.lastEventTime?.getTime() || 0);

      if (timeSinceLastEvent > 300000) { // 5 minutes
        this.alertSilence();
      }
    }, 60000);
  }

  async processWithTimeout(change, timeout) {
    return Promise.race([
      this.processChange(change),
      new Promise((_, reject) =>
        setTimeout(() => reject(new Error('Timeout')), timeout)
      )
    ]);
  }

  async alertOnError(error, change) {
    // IntÃ©gration avec systÃ¨me d'alerting
    console.error('Alert: Change Stream error', {
      error: error.message,
      operationType: change.operationType,
      documentId: change.documentKey._id
    });
  }

  alertSilence() {
    console.warn('Alert: No events received for 5 minutes');
  }
}
```

### 2. Testing

```javascript
// Test unitaire avec mock
describe('Change Stream Handler', () => {
  it('should process insert event', async () => {
    const mockChange = {
      operationType: 'insert',
      fullDocument: { _id: '123', name: 'Test' },
      documentKey: { _id: '123' }
    };

    const handler = new ChangeStreamHandler();
    await handler.handleChange(mockChange);

    expect(handler.processed).toContain('123');
  });
});

// Test d'intÃ©gration
describe('Change Stream Integration', () => {
  it('should sync to cache', async (done) => {
    const collection = db.collection('test');
    const changeStream = collection.watch();

    changeStream.on('change', async (change) => {
      expect(change.operationType).toBe('insert');
      await changeStream.close();
      done();
    });

    await collection.insertOne({ test: true });
  });
});
```

---

## Conclusion

Les Change Streams sont un outil puissant pour construire des architectures modernes, mais leur utilisation nÃ©cessite une comprÃ©hension approfondie des patterns et bonnes pratiques. Les cas d'usage prÃ©sentÃ©s dÃ©montrent comment ils peuvent transformer MongoDB en une plateforme Ã©vÃ©nementielle complÃ¨te, supportant Event Sourcing, CQRS, rÃ©plication multi-cloud, et analytics en temps rÃ©el.

**Points clÃ©s Ã  retenir :**
- âœ… Choisir le bon pattern selon le cas d'usage
- âœ… ImplÃ©menter une gestion d'erreur robuste
- âœ… Monitorer les performances et le lag
- âœ… Sauvegarder les resume tokens pour rÃ©silience
- âœ… Tester thoroughly en dÃ©veloppement

---


â­ï¸ [Configuration et filtres](/16-fonctionnalites-avancees/01.2-configuration-filtres.md)
