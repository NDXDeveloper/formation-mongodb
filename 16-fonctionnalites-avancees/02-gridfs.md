üîù Retour au [Sommaire](/SOMMAIRE.md)

# 16.2 GridFS (Stockage de fichiers volumineux)

## Introduction

**GridFS** est une sp√©cification MongoDB pour stocker et r√©cup√©rer des fichiers qui d√©passent la limite BSON de 16 m√©gaoctets. Plut√¥t qu'une collection MongoDB traditionnelle, GridFS divise les fichiers en chunks (morceaux) et les stocke dans deux collections distinctes : `fs.files` pour les m√©tadonn√©es et `fs.chunks` pour les donn√©es binaires.

GridFS est particuli√®rement utile lorsque vous souhaitez stocker des fichiers volumineux tout en b√©n√©ficiant des avantages de MongoDB : r√©plication automatique, sharding, requ√™tes sur m√©tadonn√©es, et int√©gration native avec votre base de donn√©es.

---

## Architecture et fonctionnement

### Structure de GridFS

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        GridFS                            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ           Collection: fs.files                   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (M√©tadonn√©es des fichiers)                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  {                                               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    _id: ObjectId("..."),                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    length: 5242880,      // Taille totale        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    chunkSize: 261120,    // Taille d'un chunk    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    uploadDate: ISODate,                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    filename: "video.mp4",                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    contentType: "video/mp4",                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    metadata: { /* custom */ }                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  }                                               ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                          ‚îÇ                               ‚îÇ
‚îÇ                          ‚îÇ _id reference                 ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ           Collection: fs.chunks                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (Donn√©es binaires fragment√©es)                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  {                                               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    _id: ObjectId("..."),                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    files_id: ObjectId("..."), // Ref to fs.files ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    n: 0,                      // Num√©ro chunk    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    data: BinData(...)         // Donn√©es         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  }                                               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  { files_id: ..., n: 1, data: ... }              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  { files_id: ..., n: 2, data: ... }              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ...                                             ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Taille des chunks

- **Par d√©faut** : 255 Ko (261120 bytes)
- **Configurable** : Peut √™tre ajust√© selon les besoins
- **Compromis** :
  - Chunks plus petits : Plus de documents, overhead plus important
  - Chunks plus grands : Moins de documents, transferts moins granulaires

### Index automatiques

GridFS cr√©e automatiquement des index pour optimiser les performances :

```javascript
// Sur fs.files
{ _id: 1 }
{ filename: 1, uploadDate: 1 }

// Sur fs.chunks
{ files_id: 1, n: 1 }  // Compos√© et unique
```

---

## API GridFS et op√©rations de base

### Initialisation

```javascript
const { MongoClient, GridFSBucket } = require('mongodb');

const client = await MongoClient.connect('mongodb://localhost:27017');
const db = client.db('myapp');

// Cr√©er un bucket GridFS (collection fs.files et fs.chunks)
const bucket = new GridFSBucket(db, {
  bucketName: 'fs',        // Pr√©fixe des collections (d√©faut: 'fs')
  chunkSizeBytes: 261120   // Taille des chunks (d√©faut: 255KB)
});
```

### Upload de fichiers

```javascript
const fs = require('fs');
const path = require('path');

// M√©thode 1 : Upload depuis un stream
async function uploadFromStream(filePath) {
  const filename = path.basename(filePath);

  // Cr√©er un stream de lecture
  const readStream = fs.createReadStream(filePath);

  // Cr√©er un stream d'upload GridFS
  const uploadStream = bucket.openUploadStream(filename, {
    contentType: 'video/mp4',
    metadata: {
      uploadedBy: 'user-123',
      category: 'media',
      tags: ['important', 'demo'],
      originalPath: filePath
    }
  });

  // Pipe et attendre la fin
  return new Promise((resolve, reject) => {
    readStream.pipe(uploadStream)
      .on('error', reject)
      .on('finish', () => {
        console.log('File uploaded:', uploadStream.id);
        resolve(uploadStream.id);
      });
  });
}

// Utilisation
const fileId = await uploadFromStream('./video.mp4');
console.log('File ID:', fileId);


// M√©thode 2 : Upload depuis un buffer
async function uploadFromBuffer(buffer, filename, options = {}) {
  const uploadStream = bucket.openUploadStream(filename, {
    contentType: options.contentType || 'application/octet-stream',
    metadata: options.metadata || {}
  });

  return new Promise((resolve, reject) => {
    uploadStream.end(buffer, (error) => {
      if (error) {
        reject(error);
      } else {
        resolve(uploadStream.id);
      }
    });
  });
}

// Utilisation
const imageBuffer = await fs.promises.readFile('./image.jpg');
const imageId = await uploadFromBuffer(imageBuffer, 'profile.jpg', {
  contentType: 'image/jpeg',
  metadata: { userId: 'user-456' }
});
```

### Download de fichiers

```javascript
// M√©thode 1 : Download vers un stream
async function downloadToStream(fileId, outputPath) {
  const downloadStream = bucket.openDownloadStream(fileId);
  const writeStream = fs.createWriteStream(outputPath);

  return new Promise((resolve, reject) => {
    downloadStream.pipe(writeStream)
      .on('error', reject)
      .on('finish', resolve);
  });
}

// Utilisation
await downloadToStream(fileId, './downloaded-video.mp4');


// M√©thode 2 : Download vers un buffer
async function downloadToBuffer(fileId) {
  const downloadStream = bucket.openDownloadStream(fileId);
  const chunks = [];

  return new Promise((resolve, reject) => {
    downloadStream
      .on('data', (chunk) => chunks.push(chunk))
      .on('error', reject)
      .on('end', () => resolve(Buffer.concat(chunks)));
  });
}

// Utilisation
const fileBuffer = await downloadToBuffer(fileId);


// M√©thode 3 : Download par nom de fichier
async function downloadByName(filename, outputPath) {
  const downloadStream = bucket.openDownloadStreamByName(filename, {
    revision: -1  // -1 = plus r√©cent, 0 = plus ancien, n = n-i√®me version
  });

  const writeStream = fs.createWriteStream(outputPath);

  return new Promise((resolve, reject) => {
    downloadStream.pipe(writeStream)
      .on('error', reject)
      .on('finish', resolve);
  });
}
```

### Suppression de fichiers

```javascript
// Supprimer par ID (supprime le document fs.files ET tous les chunks)
async function deleteFile(fileId) {
  try {
    await bucket.delete(fileId);
    console.log('File deleted:', fileId);
  } catch (error) {
    if (error.code === 'ENOENT') {
      console.log('File not found');
    } else {
      throw error;
    }
  }
}

// Suppression multiple (par requ√™te)
async function deleteFilesByQuery(query) {
  const files = await db.collection('fs.files')
    .find(query)
    .project({ _id: 1 })
    .toArray();

  for (const file of files) {
    await bucket.delete(file._id);
  }

  console.log(`Deleted ${files.length} files`);
}

// Utilisation
await deleteFilesByQuery({
  'metadata.userId': 'user-123',
  uploadDate: { $lt: new Date('2024-01-01') }
});
```

### Requ√™tes sur m√©tadonn√©es

```javascript
// Rechercher des fichiers par m√©tadonn√©es
async function findFiles(query, options = {}) {
  return await db.collection('fs.files')
    .find(query)
    .sort(options.sort || { uploadDate: -1 })
    .limit(options.limit || 20)
    .toArray();
}

// Exemples de requ√™tes
const userFiles = await findFiles({ 'metadata.userId': 'user-123' });

const recentVideos = await findFiles({
  contentType: 'video/mp4',
  uploadDate: { $gte: new Date('2024-12-01') }
});

const largeFiles = await findFiles({
  length: { $gt: 100 * 1024 * 1024 } // > 100 MB
});

// Requ√™te complexe avec agr√©gation
const fileStats = await db.collection('fs.files').aggregate([
  {
    $match: { 'metadata.category': 'media' }
  },
  {
    $group: {
      _id: '$contentType',
      count: { $sum: 1 },
      totalSize: { $sum: '$length' },
      avgSize: { $avg: '$length' }
    }
  }
]).toArray();
```

---

## Cas d'usage avanc√©s

### Cas 1 : Syst√®me de gestion de m√©dias avec streaming

```javascript
class MediaStorageService {
  constructor(db) {
    this.db = db;
    this.bucket = new GridFSBucket(db, {
      bucketName: 'media',
      chunkSizeBytes: 512 * 1024 // 512 KB pour streaming vid√©o
    });
  }

  async uploadMedia(filePath, metadata) {
    const stats = await fs.promises.stat(filePath);
    const filename = path.basename(filePath);

    // D√©tection du type MIME
    const contentType = this.detectContentType(filename);

    // Upload avec m√©tadonn√©es enrichies
    const uploadStream = this.bucket.openUploadStream(filename, {
      contentType,
      metadata: {
        ...metadata,
        originalName: filename,
        fileSize: stats.size,
        uploadedAt: new Date(),
        processingStatus: 'pending'
      }
    });

    const readStream = fs.createReadStream(filePath);

    return new Promise((resolve, reject) => {
      readStream.pipe(uploadStream)
        .on('error', reject)
        .on('finish', async () => {
          // Post-processing asynchrone
          await this.scheduleProcessing(uploadStream.id, contentType);
          resolve({
            fileId: uploadStream.id,
            filename,
            contentType,
            size: stats.size
          });
        });
    });
  }

  async streamToClient(fileId, response, options = {}) {
    try {
      // R√©cup√©rer les m√©tadonn√©es
      const file = await this.db.collection('media.files')
        .findOne({ _id: fileId });

      if (!file) {
        response.status(404).send('File not found');
        return;
      }

      // Support du Range header pour streaming vid√©o
      const range = options.range;
      let downloadStream;

      if (range) {
        // Parse du range header
        const parts = range.replace(/bytes=/, '').split('-');
        const start = parseInt(parts[0], 10);
        const end = parts[1] ? parseInt(parts[1], 10) : file.length - 1;
        const chunksize = (end - start) + 1;

        // Headers pour partial content
        response.status(206);
        response.set({
          'Content-Range': `bytes ${start}-${end}/${file.length}`,
          'Accept-Ranges': 'bytes',
          'Content-Length': chunksize,
          'Content-Type': file.contentType
        });

        // Stream avec range
        downloadStream = this.bucket.openDownloadStream(fileId, {
          start,
          end: end + 1
        });
      } else {
        // Stream complet
        response.set({
          'Content-Type': file.contentType,
          'Content-Length': file.length,
          'Content-Disposition': `inline; filename="${file.filename}"`,
          'Cache-Control': 'public, max-age=31536000' // 1 an
        });

        downloadStream = this.bucket.openDownloadStream(fileId);
      }

      // Pipe vers la r√©ponse
      downloadStream.pipe(response);

      downloadStream.on('error', (error) => {
        console.error('Stream error:', error);
        if (!response.headersSent) {
          response.status(500).send('Stream error');
        }
      });

    } catch (error) {
      console.error('Streaming error:', error);
      response.status(500).send('Internal server error');
    }
  }

  async scheduleProcessing(fileId, contentType) {
    // Marquer pour processing
    await this.db.collection('media.files').updateOne(
      { _id: fileId },
      {
        $set: {
          'metadata.processingStatus': 'queued',
          'metadata.queuedAt': new Date()
        }
      }
    );

    // Ajouter √† une queue de traitement (ex: pour g√©n√©ration de thumbnails)
    if (contentType.startsWith('image/')) {
      await this.queueThumbnailGeneration(fileId);
    } else if (contentType.startsWith('video/')) {
      await this.queueVideoTranscoding(fileId);
    }
  }

  async queueThumbnailGeneration(fileId) {
    // Int√©gration avec syst√®me de queue (Bull, BullMQ, etc.)
    console.log('Queued thumbnail generation for:', fileId);
  }

  async queueVideoTranscoding(fileId) {
    console.log('Queued video transcoding for:', fileId);
  }

  detectContentType(filename) {
    const ext = path.extname(filename).toLowerCase();
    const mimeTypes = {
      '.jpg': 'image/jpeg',
      '.jpeg': 'image/jpeg',
      '.png': 'image/png',
      '.gif': 'image/gif',
      '.mp4': 'video/mp4',
      '.webm': 'video/webm',
      '.pdf': 'application/pdf',
      '.zip': 'application/zip'
    };
    return mimeTypes[ext] || 'application/octet-stream';
  }

  async getMediaInfo(fileId) {
    const file = await this.db.collection('media.files')
      .findOne({ _id: fileId });

    if (!file) {
      return null;
    }

    return {
      id: file._id,
      filename: file.filename,
      contentType: file.contentType,
      size: file.length,
      uploadDate: file.uploadDate,
      metadata: file.metadata
    };
  }

  async deleteMedia(fileId) {
    // Supprimer les fichiers d√©riv√©s (thumbnails, transcodes, etc.)
    await this.deleteDerivatives(fileId);

    // Supprimer le fichier principal
    await this.bucket.delete(fileId);
  }

  async deleteDerivatives(fileId) {
    // Supprimer thumbnails, versions transcod√©es, etc.
    const derivatives = await this.db.collection('media.files')
      .find({ 'metadata.originalFileId': fileId })
      .toArray();

    for (const derivative of derivatives) {
      await this.bucket.delete(derivative._id);
    }
  }
}

// Utilisation avec Express
const express = require('express');
const app = express();

const mediaService = new MediaStorageService(db);

// Upload endpoint
app.post('/media/upload', upload.single('file'), async (req, res) => {
  try {
    const result = await mediaService.uploadMedia(req.file.path, {
      userId: req.user.id,
      category: req.body.category,
      tags: req.body.tags
    });

    res.json(result);
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Streaming endpoint
app.get('/media/:fileId', async (req, res) => {
  await mediaService.streamToClient(
    new ObjectId(req.params.fileId),
    res,
    { range: req.headers.range }
  );
});

// Info endpoint
app.get('/media/:fileId/info', async (req, res) => {
  const info = await mediaService.getMediaInfo(
    new ObjectId(req.params.fileId)
  );

  if (!info) {
    return res.status(404).json({ error: 'File not found' });
  }

  res.json(info);
});
```

### Cas 2 : Versioning de fichiers

```javascript
class VersionedFileStorage {
  constructor(db) {
    this.db = db;
    this.bucket = new GridFSBucket(db, { bucketName: 'documents' });
  }

  async uploadVersion(filePath, documentId, version, metadata = {}) {
    const filename = path.basename(filePath);

    // Nom versionn√©
    const versionedFilename = `${documentId}_v${version}_${filename}`;

    const uploadStream = this.bucket.openUploadStream(versionedFilename, {
      contentType: this.detectContentType(filename),
      metadata: {
        documentId,
        version,
        originalFilename: filename,
        uploadedAt: new Date(),
        uploadedBy: metadata.userId,
        changeDescription: metadata.changeDescription || '',
        ...metadata
      }
    });

    const readStream = fs.createReadStream(filePath);

    return new Promise((resolve, reject) => {
      readStream.pipe(uploadStream)
        .on('error', reject)
        .on('finish', async () => {
          // Enregistrer dans l'historique
          await this.recordVersion({
            documentId,
            version,
            fileId: uploadStream.id,
            filename: versionedFilename,
            uploadedBy: metadata.userId,
            changeDescription: metadata.changeDescription
          });

          resolve({
            fileId: uploadStream.id,
            documentId,
            version
          });
        });
    });
  }

  async recordVersion(versionInfo) {
    await this.db.collection('document_versions').insertOne({
      ...versionInfo,
      createdAt: new Date()
    });
  }

  async getVersionHistory(documentId) {
    return await this.db.collection('document_versions')
      .find({ documentId })
      .sort({ version: -1 })
      .toArray();
  }

  async downloadVersion(documentId, version, outputPath) {
    const versionInfo = await this.db.collection('document_versions')
      .findOne({ documentId, version });

    if (!versionInfo) {
      throw new Error('Version not found');
    }

    const downloadStream = this.bucket.openDownloadStream(versionInfo.fileId);
    const writeStream = fs.createWriteStream(outputPath);

    return new Promise((resolve, reject) => {
      downloadStream.pipe(writeStream)
        .on('error', reject)
        .on('finish', resolve);
    });
  }

  async getLatestVersion(documentId) {
    const versions = await this.getVersionHistory(documentId);
    return versions[0]; // Premier = plus r√©cent
  }

  async compareVersions(documentId, version1, version2) {
    const [v1, v2] = await Promise.all([
      this.db.collection('document_versions')
        .findOne({ documentId, version: version1 }),
      this.db.collection('document_versions')
        .findOne({ documentId, version: version2 })
    ]);

    if (!v1 || !v2) {
      throw new Error('Version not found');
    }

    return {
      version1: {
        version: v1.version,
        uploadedAt: v1.createdAt,
        uploadedBy: v1.uploadedBy,
        changeDescription: v1.changeDescription
      },
      version2: {
        version: v2.version,
        uploadedAt: v2.createdAt,
        uploadedBy: v2.uploadedBy,
        changeDescription: v2.changeDescription
      }
    };
  }

  async revertToVersion(documentId, targetVersion, userId) {
    const versionInfo = await this.db.collection('document_versions')
      .findOne({ documentId, version: targetVersion });

    if (!versionInfo) {
      throw new Error('Target version not found');
    }

    // R√©cup√©rer le fichier de la version cible
    const buffer = await this.downloadToBuffer(versionInfo.fileId);

    // Cr√©er une nouvelle version (pas d'√©crasement)
    const latestVersion = await this.getLatestVersion(documentId);
    const newVersion = (latestVersion?.version || 0) + 1;

    // Upload comme nouvelle version
    const uploadStream = this.bucket.openUploadStream(
      `${documentId}_v${newVersion}_${versionInfo.originalFilename}`,
      {
        contentType: versionInfo.contentType,
        metadata: {
          documentId,
          version: newVersion,
          originalFilename: versionInfo.originalFilename,
          uploadedBy: userId,
          changeDescription: `Reverted to version ${targetVersion}`,
          revertedFrom: targetVersion
        }
      }
    );

    return new Promise((resolve, reject) => {
      uploadStream.end(buffer, async (error) => {
        if (error) {
          reject(error);
        } else {
          await this.recordVersion({
            documentId,
            version: newVersion,
            fileId: uploadStream.id,
            filename: `${documentId}_v${newVersion}_${versionInfo.originalFilename}`,
            uploadedBy: userId,
            changeDescription: `Reverted to version ${targetVersion}`
          });

          resolve({
            fileId: uploadStream.id,
            version: newVersion,
            revertedFrom: targetVersion
          });
        }
      });
    });
  }

  async downloadToBuffer(fileId) {
    const downloadStream = this.bucket.openDownloadStream(fileId);
    const chunks = [];

    return new Promise((resolve, reject) => {
      downloadStream
        .on('data', (chunk) => chunks.push(chunk))
        .on('error', reject)
        .on('end', () => resolve(Buffer.concat(chunks)));
    });
  }

  async deleteOldVersions(documentId, keepLast = 5) {
    const versions = await this.getVersionHistory(documentId);

    // Garder seulement les N derni√®res versions
    const toDelete = versions.slice(keepLast);

    for (const version of toDelete) {
      await this.bucket.delete(version.fileId);
      await this.db.collection('document_versions').deleteOne({
        _id: version._id
      });
    }

    console.log(`Deleted ${toDelete.length} old versions of ${documentId}`);
  }

  detectContentType(filename) {
    // M√™me logique que pr√©c√©demment
    const ext = path.extname(filename).toLowerCase();
    const mimeTypes = {
      '.pdf': 'application/pdf',
      '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
      '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    };
    return mimeTypes[ext] || 'application/octet-stream';
  }
}

// Utilisation
const versionedStorage = new VersionedFileStorage(db);

// Upload version 1
await versionedStorage.uploadVersion(
  './contract.pdf',
  'DOC-123',
  1,
  { userId: 'user-456', changeDescription: 'Initial version' }
);

// Upload version 2
await versionedStorage.uploadVersion(
  './contract-v2.pdf',
  'DOC-123',
  2,
  { userId: 'user-456', changeDescription: 'Added signature page' }
);

// Historique
const history = await versionedStorage.getVersionHistory('DOC-123');

// Revert
await versionedStorage.revertToVersion('DOC-123', 1, 'user-456');

// Nettoyage
await versionedStorage.deleteOldVersions('DOC-123', 5);
```

### Cas 3 : CDN-like avec cache et compression

```javascript
const zlib = require('zlib');
const { createHash } = require('crypto');

class GridFSCDN {
  constructor(db, redisClient) {
    this.db = db;
    this.redis = redisClient;
    this.bucket = new GridFSBucket(db, { bucketName: 'cdn' });
  }

  async uploadWithCompression(filePath, options = {}) {
    const filename = path.basename(filePath);
    const stats = await fs.promises.stat(filePath);

    // Lire le fichier
    const buffer = await fs.promises.readFile(filePath);

    // Calculer le hash pour d√©duplication
    const hash = createHash('sha256').update(buffer).digest('hex');

    // V√©rifier si d√©j√† upload√© (d√©duplication)
    const existing = await this.db.collection('cdn.files')
      .findOne({ 'metadata.sha256': hash });

    if (existing && options.deduplicate !== false) {
      console.log('File already exists, reusing:', existing._id);
      return {
        fileId: existing._id,
        deduplicated: true,
        existingFileId: existing._id
      };
    }

    // Compresser si fichier texte/JSON/HTML
    let finalBuffer = buffer;
    let compressed = false;
    const compressibleTypes = ['text/', 'application/json', 'application/javascript'];

    if (compressibleTypes.some(type => options.contentType?.startsWith(type))) {
      finalBuffer = await this.compressBuffer(buffer);
      compressed = true;
      console.log(`Compressed: ${buffer.length} -> ${finalBuffer.length} bytes`);
    }

    // Upload
    const uploadStream = this.bucket.openUploadStream(filename, {
      contentType: options.contentType || 'application/octet-stream',
      metadata: {
        originalSize: stats.size,
        compressedSize: finalBuffer.length,
        compressed,
        sha256: hash,
        uploadedAt: new Date(),
        ...options.metadata
      }
    });

    return new Promise((resolve, reject) => {
      uploadStream.end(finalBuffer, (error) => {
        if (error) {
          reject(error);
        } else {
          resolve({
            fileId: uploadStream.id,
            compressed,
            originalSize: stats.size,
            storedSize: finalBuffer.length,
            compressionRatio: compressed
              ? ((1 - finalBuffer.length / stats.size) * 100).toFixed(2) + '%'
              : '0%'
          });
        }
      });
    });
  }

  async serveWithCache(fileId, response) {
    try {
      // 1. V√©rifier le cache Redis
      const cacheKey = `cdn:${fileId}`;
      const cached = await this.redis.get(cacheKey);

      if (cached) {
        console.log('Cache HIT:', fileId);
        const data = JSON.parse(cached);

        response.set({
          'Content-Type': data.contentType,
          'Content-Length': data.content.length,
          'Cache-Control': 'public, max-age=31536000',
          'X-Cache': 'HIT'
        });

        return response.send(Buffer.from(data.content, 'base64'));
      }

      console.log('Cache MISS:', fileId);

      // 2. R√©cup√©rer depuis GridFS
      const file = await this.db.collection('cdn.files')
        .findOne({ _id: fileId });

      if (!file) {
        return response.status(404).send('File not found');
      }

      // 3. Download
      const buffer = await this.downloadToBuffer(fileId);

      // 4. D√©compresser si n√©cessaire
      let finalBuffer = buffer;
      if (file.metadata.compressed) {
        finalBuffer = await this.decompressBuffer(buffer);
      }

      // 5. Mettre en cache
      await this.redis.setex(
        cacheKey,
        86400, // 24h
        JSON.stringify({
          contentType: file.contentType,
          content: finalBuffer.toString('base64')
        })
      );

      // 6. R√©pondre
      response.set({
        'Content-Type': file.contentType,
        'Content-Length': finalBuffer.length,
        'Cache-Control': 'public, max-age=31536000',
        'X-Cache': 'MISS',
        'X-Original-Size': file.metadata.originalSize,
        'X-Compressed': file.metadata.compressed
      });

      response.send(finalBuffer);

    } catch (error) {
      console.error('CDN serve error:', error);
      response.status(500).send('Internal server error');
    }
  }

  async compressBuffer(buffer) {
    return new Promise((resolve, reject) => {
      zlib.gzip(buffer, { level: 9 }, (error, result) => {
        if (error) reject(error);
        else resolve(result);
      });
    });
  }

  async decompressBuffer(buffer) {
    return new Promise((resolve, reject) => {
      zlib.gunzip(buffer, (error, result) => {
        if (error) reject(error);
        else resolve(result);
      });
    });
  }

  async downloadToBuffer(fileId) {
    const downloadStream = this.bucket.openDownloadStream(fileId);
    const chunks = [];

    return new Promise((resolve, reject) => {
      downloadStream
        .on('data', (chunk) => chunks.push(chunk))
        .on('error', reject)
        .on('end', () => resolve(Buffer.concat(chunks)));
    });
  }

  async invalidateCache(fileId) {
    const cacheKey = `cdn:${fileId}`;
    await this.redis.del(cacheKey);
    console.log('Cache invalidated:', fileId);
  }

  async getStatistics() {
    const stats = await this.db.collection('cdn.files').aggregate([
      {
        $group: {
          _id: null,
          totalFiles: { $sum: 1 },
          totalOriginalSize: { $sum: '$metadata.originalSize' },
          totalStoredSize: { $sum: '$metadata.compressedSize' },
          compressedFiles: {
            $sum: { $cond: ['$metadata.compressed', 1, 0] }
          }
        }
      }
    ]).toArray();

    if (stats.length === 0) {
      return null;
    }

    const s = stats[0];
    return {
      totalFiles: s.totalFiles,
      totalOriginalSize: s.totalOriginalSize,
      totalStoredSize: s.totalStoredSize,
      compressionRatio: ((1 - s.totalStoredSize / s.totalOriginalSize) * 100).toFixed(2) + '%',
      compressedFiles: s.compressedFiles,
      savedSpace: s.totalOriginalSize - s.totalStoredSize
    };
  }
}

// Utilisation
const cdn = new GridFSCDN(db, redisClient);

// Upload avec compression
const result = await cdn.uploadWithCompression(
  './app.bundle.js',
  {
    contentType: 'application/javascript',
    metadata: { version: '1.2.3' }
  }
);

console.log('Upload result:', result);

// Endpoint Express
app.get('/cdn/:fileId', async (req, res) => {
  await cdn.serveWithCache(new ObjectId(req.params.fileId), res);
});

// Statistiques
const stats = await cdn.getStatistics();
console.log('CDN Statistics:', stats);
```

### Cas 4 : Backup incr√©mental avec change streams

```javascript
class GridFSBackupService {
  constructor(sourceDb, backupDb) {
    this.sourceBucket = new GridFSBucket(sourceDb, { bucketName: 'production' });
    this.backupBucket = new GridFSBucket(backupDb, { bucketName: 'backup' });
    this.sourceDb = sourceDb;
    this.backupDb = backupDb;
  }

  async initializeIncrementalBackup() {
    // Surveiller les changements sur fs.files
    const changeStream = this.sourceDb
      .collection('production.files')
      .watch([
        {
          $match: {
            operationType: { $in: ['insert', 'update', 'delete'] }
          }
        }
      ], {
        fullDocument: 'updateLookup'
      });

    changeStream.on('change', async (change) => {
      await this.handleFileChange(change);
    });

    changeStream.on('error', (error) => {
      console.error('Backup stream error:', error);
      // R√©initialiser le stream
      setTimeout(() => this.initializeIncrementalBackup(), 5000);
    });

    console.log('Incremental backup initialized');
  }

  async handleFileChange(change) {
    try {
      switch (change.operationType) {
        case 'insert':
          await this.backupFile(change.fullDocument);
          break;

        case 'update':
          await this.updateBackup(change.fullDocument);
          break;

        case 'delete':
          await this.deleteBackup(change.documentKey._id);
          break;
      }

      // Logger le backup
      await this.logBackupOperation(change);
    } catch (error) {
      console.error('Backup operation failed:', error);
      await this.logBackupError(change, error);
    }
  }

  async backupFile(fileDoc) {
    console.log('Backing up file:', fileDoc._id);

    // V√©rifier si d√©j√† sauvegard√©
    const existing = await this.backupDb
      .collection('backup.files')
      .findOne({ 'metadata.sourceId': fileDoc._id.toString() });

    if (existing) {
      console.log('File already backed up');
      return;
    }

    // Download depuis source
    const buffer = await this.downloadToBuffer(
      this.sourceBucket,
      fileDoc._id
    );

    // Upload vers backup
    const uploadStream = this.backupBucket.openUploadStream(
      fileDoc.filename,
      {
        contentType: fileDoc.contentType,
        metadata: {
          sourceId: fileDoc._id.toString(),
          backedUpAt: new Date(),
          originalMetadata: fileDoc.metadata
        }
      }
    );

    return new Promise((resolve, reject) => {
      uploadStream.end(buffer, (error) => {
        if (error) {
          reject(error);
        } else {
          console.log('Backup complete:', fileDoc._id);
          resolve(uploadStream.id);
        }
      });
    });
  }

  async updateBackup(fileDoc) {
    // Supprimer l'ancienne version
    const existing = await this.backupDb
      .collection('backup.files')
      .findOne({ 'metadata.sourceId': fileDoc._id.toString() });

    if (existing) {
      await this.backupBucket.delete(existing._id);
    }

    // Re-backup
    await this.backupFile(fileDoc);
  }

  async deleteBackup(fileId) {
    const backup = await this.backupDb
      .collection('backup.files')
      .findOne({ 'metadata.sourceId': fileId.toString() });

    if (backup) {
      await this.backupBucket.delete(backup._id);
      console.log('Backup deleted:', fileId);
    }
  }

  async downloadToBuffer(bucket, fileId) {
    const downloadStream = bucket.openDownloadStream(fileId);
    const chunks = [];

    return new Promise((resolve, reject) => {
      downloadStream
        .on('data', (chunk) => chunks.push(chunk))
        .on('error', reject)
        .on('end', () => resolve(Buffer.concat(chunks)));
    });
  }

  async logBackupOperation(change) {
    await this.backupDb.collection('backup_log').insertOne({
      operationType: change.operationType,
      fileId: change.documentKey._id,
      timestamp: new Date(),
      status: 'success'
    });
  }

  async logBackupError(change, error) {
    await this.backupDb.collection('backup_log').insertOne({
      operationType: change.operationType,
      fileId: change.documentKey._id,
      timestamp: new Date(),
      status: 'error',
      error: error.message
    });
  }

  async performFullBackup() {
    console.log('Starting full backup...');

    const files = await this.sourceDb
      .collection('production.files')
      .find()
      .toArray();

    let backedUp = 0;
    let errors = 0;

    for (const file of files) {
      try {
        await this.backupFile(file);
        backedUp++;
      } catch (error) {
        console.error(`Failed to backup ${file._id}:`, error);
        errors++;
      }
    }

    console.log(`Full backup complete: ${backedUp} files, ${errors} errors`);
    return { backedUp, errors, total: files.length };
  }

  async verifyBackup() {
    const sourceFiles = await this.sourceDb
      .collection('production.files')
      .find()
      .project({ _id: 1, length: 1 })
      .toArray();

    const backupFiles = await this.backupDb
      .collection('backup.files')
      .find()
      .project({ 'metadata.sourceId': 1, length: 1 })
      .toArray();

    const backupMap = new Map(
      backupFiles.map(f => [f.metadata.sourceId, f.length])
    );

    const missing = [];
    const sizeMismatch = [];

    for (const source of sourceFiles) {
      const sourceId = source._id.toString();
      const backupLength = backupMap.get(sourceId);

      if (!backupLength) {
        missing.push(sourceId);
      } else if (backupLength !== source.length) {
        sizeMismatch.push({
          sourceId,
          sourceLength: source.length,
          backupLength
        });
      }
    }

    return {
      totalSource: sourceFiles.length,
      totalBackup: backupFiles.length,
      missing,
      sizeMismatch,
      valid: missing.length === 0 && sizeMismatch.length === 0
    };
  }
}

// Utilisation
const sourceClient = await MongoClient.connect('mongodb://source:27017');
const backupClient = await MongoClient.connect('mongodb://backup:27017');

const backupService = new GridFSBackupService(
  sourceClient.db('production'),
  backupClient.db('backup')
);

// Full backup initial
const fullBackupResult = await backupService.performFullBackup();
console.log('Full backup:', fullBackupResult);

// D√©marrer backup incr√©mental
await backupService.initializeIncrementalBackup();

// V√©rification p√©riodique
setInterval(async () => {
  const verification = await backupService.verifyBackup();
  console.log('Backup verification:', verification);

  if (!verification.valid) {
    console.error('Backup integrity issues detected!');
  }
}, 3600000); // Toutes les heures
```

---

## Optimisations et performances

### 1. Taille optimale des chunks

```javascript
// Pour streaming vid√©o : chunks plus larges (512 KB - 1 MB)
const videoBucket = new GridFSBucket(db, {
  bucketName: 'videos',
  chunkSizeBytes: 1024 * 1024 // 1 MB
});

// Pour images : chunks par d√©faut (255 KB)
const imageBucket = new GridFSBucket(db, {
  bucketName: 'images'
  // chunkSizeBytes par d√©faut: 255 KB
});

// Pour petits fichiers : chunks plus petits (128 KB)
const docsBucket = new GridFSBucket(db, {
  bucketName: 'docs',
  chunkSizeBytes: 128 * 1024 // 128 KB
});
```

### 2. Index personnalis√©s pour requ√™tes fr√©quentes

```javascript
// Index sur m√©tadonn√©es
await db.collection('fs.files').createIndex({
  'metadata.userId': 1,
  uploadDate: -1
});

await db.collection('fs.files').createIndex({
  contentType: 1,
  'metadata.category': 1
});

// Index texte pour recherche
await db.collection('fs.files').createIndex({
  filename: 'text',
  'metadata.tags': 'text',
  'metadata.description': 'text'
});

// Index pour taille de fichiers
await db.collection('fs.files').createIndex({
  length: 1
});
```

### 3. Monitoring des performances

```javascript
class GridFSMonitor {
  constructor(db) {
    this.db = db;
  }

  async getStorageStatistics() {
    const [filesStats, chunksStats] = await Promise.all([
      this.db.collection('fs.files').stats(),
      this.db.collection('fs.chunks').stats()
    ]);

    return {
      files: {
        count: filesStats.count,
        size: filesStats.size,
        storageSize: filesStats.storageSize,
        avgObjSize: filesStats.avgObjSize,
        indexes: filesStats.nindexes,
        indexSize: filesStats.totalIndexSize
      },
      chunks: {
        count: chunksStats.count,
        size: chunksStats.size,
        storageSize: chunksStats.storageSize,
        avgObjSize: chunksStats.avgObjSize
      },
      totalStorageSize: filesStats.storageSize + chunksStats.storageSize
    };
  }

  async getPerformanceMetrics() {
    // Temps moyen de recherche
    const searchStart = Date.now();
    await this.db.collection('fs.files')
      .find({ length: { $gt: 1000000 } })
      .limit(10)
      .toArray();
    const searchTime = Date.now() - searchStart;

    // Distribution par taille
    const sizeDistribution = await this.db.collection('fs.files').aggregate([
      {
        $bucket: {
          groupBy: '$length',
          boundaries: [0, 100000, 1000000, 10000000, 100000000, Infinity],
          default: 'Other',
          output: {
            count: { $sum: 1 },
            totalSize: { $sum: '$length' }
          }
        }
      }
    ]).toArray();

    return {
      searchTime,
      sizeDistribution
    };
  }

  async identifyLargeFiles(limit = 10) {
    return await this.db.collection('fs.files')
      .find()
      .sort({ length: -1 })
      .limit(limit)
      .project({
        filename: 1,
        length: 1,
        uploadDate: 1,
        contentType: 1
      })
      .toArray();
  }

  async identifyOrphanedChunks() {
    // Chunks sans fichier parent
    const orphaned = await this.db.collection('fs.chunks').aggregate([
      {
        $lookup: {
          from: 'fs.files',
          localField: 'files_id',
          foreignField: '_id',
          as: 'parent'
        }
      },
      {
        $match: { parent: { $size: 0 } }
      },
      {
        $group: {
          _id: '$files_id',
          chunkCount: { $sum: 1 }
        }
      }
    ]).toArray();

    return orphaned;
  }

  async cleanupOrphanedChunks() {
    const orphaned = await this.identifyOrphanedChunks();

    let deletedCount = 0;
    for (const orphan of orphaned) {
      const result = await this.db.collection('fs.chunks').deleteMany({
        files_id: orphan._id
      });
      deletedCount += result.deletedCount;
    }

    console.log(`Cleaned up ${deletedCount} orphaned chunks`);
    return deletedCount;
  }
}

// Utilisation
const monitor = new GridFSMonitor(db);

// Statistiques
const stats = await monitor.getStorageStatistics();
console.log('Storage:', stats);

// M√©triques performance
const metrics = await monitor.getPerformanceMetrics();
console.log('Performance:', metrics);

// Plus gros fichiers
const largeFiles = await monitor.identifyLargeFiles();
console.log('Largest files:', largeFiles);

// Nettoyage
const cleaned = await monitor.cleanupOrphanedChunks();
```

---

## GridFS vs Alternatives

### Comparaison avec stockage cloud (S3, GCS, Azure Blob)

| Crit√®re | GridFS | S3/GCS/Blob |
|---------|--------|-------------|
| **Int√©gration** | Native MongoDB | API externe |
| **Requ√™tes m√©tadonn√©es** | ‚úÖ MongoDB queries | ‚ùå API limit√©es |
| **Transactions** | ‚úÖ Support√©es | ‚ùå Non |
| **Co√ªt** | Inclus dans MongoDB | Facturation s√©par√©e |
| **Scalabilit√©** | Limit√©e par cluster | Illimit√©e |
| **Performances** | Bonnes (<100MB) | Excellentes (tous fichiers) |
| **CDN** | N√©cessite config | ‚úÖ Int√©gr√© (CloudFront, etc.) |
| **Durabilit√©** | D√©pend du cluster | 99.999999999% |

### Quand utiliser GridFS ?

‚úÖ **Utiliser GridFS quand :**
- Fichiers < 100 MB majoritairement
- Requ√™tes complexes sur m√©tadonn√©es n√©cessaires
- Transactions sur fichiers + donn√©es requises
- Infrastructure MongoDB d√©j√† en place
- Co√ªt d'infrastructure cloud √† minimiser
- Besoin de versioning int√©gr√©

‚ùå **√âviter GridFS quand :**
- Fichiers > 500 MB fr√©quents
- Tr√®s haute volum√©trie (To par jour)
- CDN global requis imm√©diatement
- Streaming haute performance critique
- Besoin de fonctionnalit√©s cloud avanc√©es (lifecycle policies, etc.)

### Pattern hybride recommand√©

```javascript
class HybridStorageService {
  constructor(db, s3Client) {
    this.db = db;
    this.s3 = s3Client;
    this.gridfs = new GridFSBucket(db);
    this.sizeThreshold = 50 * 1024 * 1024; // 50 MB
  }

  async upload(filePath, metadata = {}) {
    const stats = await fs.promises.stat(filePath);

    if (stats.size < this.sizeThreshold) {
      // Petits fichiers : GridFS
      return await this.uploadToGridFS(filePath, metadata);
    } else {
      // Gros fichiers : S3
      return await this.uploadToS3(filePath, metadata);
    }
  }

  async uploadToGridFS(filePath, metadata) {
    const filename = path.basename(filePath);
    const uploadStream = this.gridfs.openUploadStream(filename, {
      metadata: {
        ...metadata,
        storage: 'gridfs'
      }
    });

    const readStream = fs.createReadStream(filePath);

    return new Promise((resolve, reject) => {
      readStream.pipe(uploadStream)
        .on('error', reject)
        .on('finish', () => resolve({
          fileId: uploadStream.id,
          storage: 'gridfs'
        }));
    });
  }

  async uploadToS3(filePath, metadata) {
    const filename = path.basename(filePath);
    const fileStream = fs.createReadStream(filePath);

    const uploadParams = {
      Bucket: 'my-bucket',
      Key: filename,
      Body: fileStream,
      Metadata: metadata
    };

    const result = await this.s3.upload(uploadParams).promise();

    // Sauvegarder r√©f√©rence dans MongoDB
    const fileDoc = await this.db.collection('files').insertOne({
      filename,
      s3Key: result.Key,
      s3Bucket: result.Bucket,
      s3Location: result.Location,
      storage: 's3',
      uploadDate: new Date(),
      metadata
    });

    return {
      fileId: fileDoc.insertedId,
      storage: 's3',
      url: result.Location
    };
  }

  async download(fileId) {
    // D√©terminer le storage
    const fileDoc = await this.db.collection('fs.files').findOne({ _id: fileId });

    if (fileDoc) {
      // GridFS
      return await this.downloadFromGridFS(fileId);
    } else {
      // S3
      const s3Doc = await this.db.collection('files').findOne({ _id: fileId });
      if (s3Doc) {
        return await this.downloadFromS3(s3Doc.s3Key);
      }
    }

    throw new Error('File not found');
  }

  async downloadFromGridFS(fileId) {
    const downloadStream = this.gridfs.openDownloadStream(fileId);
    const chunks = [];

    return new Promise((resolve, reject) => {
      downloadStream
        .on('data', (chunk) => chunks.push(chunk))
        .on('error', reject)
        .on('end', () => resolve(Buffer.concat(chunks)));
    });
  }

  async downloadFromS3(s3Key) {
    const params = {
      Bucket: 'my-bucket',
      Key: s3Key
    };

    const data = await this.s3.getObject(params).promise();
    return data.Body;
  }
}
```

---

## Bonnes pratiques de production

### ‚úÖ DO (√Ä faire)

```javascript
// 1. Cr√©er des index personnalis√©s sur m√©tadonn√©es
await db.collection('fs.files').createIndex({
  'metadata.userId': 1,
  uploadDate: -1
});

// 2. Configurer la taille des chunks selon l'usage
const videoBucket = new GridFSBucket(db, {
  chunkSizeBytes: 1024 * 1024 // 1 MB pour vid√©os
});

// 3. Toujours g√©rer les erreurs de stream
uploadStream.on('error', (error) => {
  console.error('Upload failed:', error);
  // Cleanup et retry
});

// 4. Impl√©menter une strat√©gie de nettoyage
async function cleanupOldFiles() {
  const threshold = new Date();
  threshold.setDate(threshold.getDate() - 90);

  const oldFiles = await db.collection('fs.files')
    .find({ uploadDate: { $lt: threshold } })
    .toArray();

  for (const file of oldFiles) {
    await bucket.delete(file._id);
  }
}

// 5. Monitorer l'utilisation du stockage
setInterval(async () => {
  const stats = await db.collection('fs.files').stats();
  console.log('GridFS storage:', stats.storageSize);
}, 3600000);
```

### ‚ùå DON'T (√Ä √©viter)

```javascript
// 1. Ne pas stocker des fichiers √©normes (>500MB)
// ‚ùå Performance d√©grad√©e
await bucket.uploadFromStream('huge-file.iso');

// 2. Ne pas oublier de supprimer les chunks
// ‚ùå MAUVAIS : supprime seulement fs.files
await db.collection('fs.files').deleteOne({ _id: fileId });
// ‚úÖ BON : utiliser bucket.delete()
await bucket.delete(fileId);

// 3. Ne pas charger tout le fichier en m√©moire si pas n√©cessaire
// ‚ùå MAUVAIS pour gros fichiers
const buffer = await downloadToBuffer(fileId);
// ‚úÖ BON : utiliser des streams
downloadStream.pipe(response);

// 4. Ne pas utiliser GridFS pour nombreux petits fichiers (<1KB)
// ‚ùå Overhead important
// ‚úÖ Stocker directement dans documents MongoDB

// 5. Ne pas oublier la validation
// ‚ùå Accepter n'importe quel fichier
// ‚úÖ Valider type, taille, contenu
if (stats.size > MAX_FILE_SIZE) {
  throw new Error('File too large');
}
```

---

## Conclusion

GridFS est une solution puissante pour le stockage de fichiers volumineux dans MongoDB, particuli√®rement adapt√©e aux cas o√π :
- ‚úÖ L'int√©gration native avec MongoDB est cruciale
- ‚úÖ Les requ√™tes complexes sur m√©tadonn√©es sont n√©cessaires
- ‚úÖ Les transactions impliquant fichiers et donn√©es sont requises
- ‚úÖ La taille des fichiers reste raisonnable (< 100 MB g√©n√©ralement)

**Points cl√©s √† retenir :**
1. Comprendre l'architecture interne (fs.files + fs.chunks)
2. Configurer la taille des chunks selon le cas d'usage
3. Utiliser des streams pour √©viter les probl√®mes de m√©moire
4. Impl√©menter des index personnalis√©s sur m√©tadonn√©es
5. Monitorer l'utilisation du stockage
6. Consid√©rer une approche hybride (GridFS + cloud storage)

**Alternatives √† consid√©rer :**
- **S3/GCS/Azure Blob** : Pour tr√®s gros fichiers ou volum√©trie massive
- **CDN** : Pour distribution mondiale haute performance
- **Hybrid** : GridFS pour petits/moyens + cloud pour gros fichiers

---


‚è≠Ô∏è [Capped Collections](/16-fonctionnalites-avancees/03-capped-collections.md)
