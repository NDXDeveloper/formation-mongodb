ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 19.4 StratÃ©gies de Migration IncrÃ©mentale

## Introduction

La migration incrÃ©mentale reprÃ©sente l'approche privilÃ©giÃ©e pour les systÃ¨mes critiques nÃ©cessitant une disponibilitÃ© continue (24/7) et une rÃ©duction des risques. Contrairement aux migrations "big bang" qui basculent l'ensemble du systÃ¨me en une seule opÃ©ration, la migration incrÃ©mentale permet une transition progressive, module par module, avec validation continue et possibilitÃ© de rollback granulaire.

Cette section explore les patterns architecturaux, les stratÃ©gies de synchronisation et les dÃ©fis techniques de la migration incrÃ©mentale vers MongoDB.

---

## ğŸ¯ Principes Fondamentaux

### Objectifs de la migration incrÃ©mentale

**Business**
- **Zero downtime** : Service ininterrompu pendant toute la migration
- **RÃ©duction des risques** : Validation continue, rollback possible par module
- **ROI progressif** : BÃ©nÃ©fices immÃ©diats sur modules migrÃ©s
- **Apprentissage** : Ã‰quipes formÃ©es progressivement

**Techniques**
- **Isolation des modules** : Migrations indÃ©pendantes par domaine mÃ©tier
- **Coexistence** : SQL et MongoDB opÃ¨rent simultanÃ©ment (semaines/mois)
- **Synchronisation** : CohÃ©rence des donnÃ©es entre les deux systÃ¨mes
- **Validation continue** : Tests automatisÃ©s et monitoring temps rÃ©el

### Compromis et trade-offs

| Aspect | Migration Big Bang | Migration IncrÃ©mentale |
|--------|-------------------|------------------------|
| **DurÃ©e totale** | Courte (heures/jours) | Longue (semaines/mois) |
| **Downtime** | Ã‰levÃ© (heures) | Zero |
| **ComplexitÃ©** | Faible | Ã‰levÃ©e |
| **Risque** | Ã‰levÃ© (rollback difficile) | Faible (rollback granulaire) |
| **CoÃ»t infrastructure** | Faible | Ã‰levÃ© (double) |
| **Maintenance** | Simple (un systÃ¨me) | Complexe (deux systÃ¨mes) |
| **Validation** | Post-migration | Continue |
| **Apprentissage Ã©quipe** | Brutal | Progressif |

**Quand choisir l'incrÃ©mental ?**
- âœ… Applications critiques (SLA stricts, 24/7)
- âœ… SystÃ¨mes complexes (nombreux modules interdÃ©pendants)
- âœ… Ã‰quipes larges (besoin de formation progressive)
- âœ… Budget disponible (double infrastructure temporaire)
- âŒ Applications simples, non-critiques
- âŒ Budget contraint
- âŒ Downtime acceptable

---

## ğŸ—ï¸ Patterns Architecturaux

### 1. Strangler Fig Pattern

**Principe**
Pattern inspirÃ© de la figue Ã©trangleuse : un nouveau systÃ¨me "Ã©trangle" progressivement l'ancien en lui substituant ses fonctionnalitÃ©s.

**Architecture**

```
Phase 1 : Ã‰tat Initial
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application                â”‚
â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    Business Logic           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    SQL Database             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 2 : Migration du Module Catalogue
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application                       â”‚
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Catalogue  â”‚    â”‚  Orders     â”‚       â”‚
â”‚  â”‚  Module     â”‚    â”‚  Module     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â†“                  â†“              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  MongoDB    â”‚    â”‚  SQL        â”‚       â”‚
â”‚  â”‚  (NEW)      â”‚    â”‚  (Legacy)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 3 : Migration Progressive
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application                       â”‚
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Cata â”‚  â”‚Orderâ”‚  â”‚User â”‚  â”‚Invenâ”‚       â”‚
â”‚  â”‚logueâ”‚  â”‚     â”‚  â”‚     â”‚  â”‚tory â”‚       â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜       â”‚
â”‚     â†“        â†“        â†“        â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  MongoDB     â”‚  â”‚  SQL         â”‚       â”‚
â”‚  â”‚  (Growing)   â”‚  â”‚  (Shrinking) â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 4 : Migration ComplÃ¨te
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application                â”‚
â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    Business Logic           â”‚   â”‚
â”‚  â”‚    (All modules)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚             â†“                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    MongoDB                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚
â”‚  [SQL archived/decommissioned]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplÃ©mentation avec API Gateway**

```javascript
// api-gateway/router.js
const express = require('express');
const router = express.Router();

// Configuration des routes par module
const ROUTING_CONFIG = {
  '/api/products': {
    target: 'mongodb',
    service: 'product-service-mongo',
    migrated: true,
    migratedAt: '2024-01-15'
  },
  '/api/categories': {
    target: 'mongodb',
    service: 'product-service-mongo',
    migrated: true,
    migratedAt: '2024-01-15'
  },
  '/api/orders': {
    target: 'sql',
    service: 'order-service-sql',
    migrated: false,
    plannedMigration: '2024-02-01'
  },
  '/api/customers': {
    target: 'sql',
    service: 'customer-service-sql',
    migrated: false,
    plannedMigration: '2024-02-15'
  }
};

// Middleware de routage dynamique
router.use((req, res, next) => {
  const path = req.path;
  const route = Object.keys(ROUTING_CONFIG).find(r => path.startsWith(r));

  if (!route) {
    return res.status(404).json({ error: 'Route not found' });
  }

  const config = ROUTING_CONFIG[route];

  // Headers pour debugging/monitoring
  res.setHeader('X-Data-Source', config.target);
  res.setHeader('X-Service', config.service);
  res.setHeader('X-Migrated', config.migrated);

  // Logger pour analytics
  logRequest({
    path,
    target: config.target,
    migrated: config.migrated,
    timestamp: new Date()
  });

  // Proxy vers le service appropriÃ©
  req.targetService = config.service;
  next();
});

// Exemple de service Product (MongoDB)
router.get('/api/products/:id', async (req, res) => {
  if (req.targetService !== 'product-service-mongo') {
    return proxyToLegacy(req, res);
  }

  try {
    const product = await mongoDb.collection('products').findOne({
      _id: new ObjectId(req.params.id)
    });

    if (!product) {
      return res.status(404).json({ error: 'Product not found' });
    }

    res.json(product);
  } catch (error) {
    console.error('MongoDB error:', error);

    // Fallback to SQL if MongoDB fails (during migration)
    if (FALLBACK_ENABLED) {
      console.warn('Falling back to SQL');
      return proxyToLegacy(req, res);
    }

    res.status(500).json({ error: 'Internal server error' });
  }
});
```

**Migration progressive d'un module**

```typescript
// migration-orchestrator.ts
interface ModuleMigration {
  name: string;
  status: 'planned' | 'in_progress' | 'completed' | 'rolled_back';
  startDate?: Date;
  completionDate?: Date;
  phases: MigrationPhase[];
}

interface MigrationPhase {
  phase: number;
  name: string;
  actions: string[];
  validations: string[];
  rollbackProcedure: string;
}

class StranglerMigrationOrchestrator {
  private modules: Map<string, ModuleMigration>;

  async migrateModule(moduleName: string): Promise<void> {
    const module = this.modules.get(moduleName);

    for (const phase of module.phases) {
      console.log(`Starting ${moduleName} - ${phase.name}`);

      try {
        // ExÃ©cuter les actions de la phase
        for (const action of phase.actions) {
          await this.executeAction(action);
        }

        // Valider
        const validationResults = await this.runValidations(phase.validations);

        if (!validationResults.allPassed) {
          throw new Error(`Validation failed: ${validationResults.errors}`);
        }

        // Checkpoint
        await this.saveCheckpoint(moduleName, phase.phase);

        console.log(`âœ“ Phase ${phase.phase} completed`);

        // Attendre validation manuelle si critique
        if (phase.requiresManualApproval) {
          await this.waitForApproval(moduleName, phase.phase);
        }

      } catch (error) {
        console.error(`âœ— Phase ${phase.phase} failed: ${error.message}`);

        // Rollback automatique
        await this.rollbackPhase(moduleName, phase);

        throw error;
      }
    }

    module.status = 'completed';
    module.completionDate = new Date();
  }
}

// Configuration exemple : Migration module Catalogue
const catalogueMigration: ModuleMigration = {
  name: 'Catalogue',
  status: 'planned',
  phases: [
    {
      phase: 1,
      name: 'Schema creation & initial data load',
      actions: [
        'create_mongodb_collections',
        'create_indexes',
        'migrate_products_initial',
        'migrate_categories_initial'
      ],
      validations: [
        'validate_row_counts',
        'validate_sample_data'
      ],
      rollbackProcedure: 'drop_collections'
    },
    {
      phase: 2,
      name: 'Setup CDC for real-time sync',
      actions: [
        'deploy_debezium_connector',
        'start_cdc_streaming',
        'validate_lag_acceptable'
      ],
      validations: [
        'validate_cdc_lag_under_1s',
        'validate_no_data_loss'
      ],
      rollbackProcedure: 'stop_cdc'
    },
    {
      phase: 3,
      name: 'Dual-read (SQL primary, MongoDB shadow)',
      actions: [
        'enable_dual_read',
        'compare_results_continuously'
      ],
      validations: [
        'validate_read_accuracy_99_9_percent',
        'validate_performance_acceptable'
      ],
      rollbackProcedure: 'disable_dual_read'
    },
    {
      phase: 4,
      name: 'Switch reads to MongoDB (SQL fallback)',
      actions: [
        'update_routing_config',
        'deploy_new_app_version',
        'monitor_error_rates'
      ],
      validations: [
        'validate_error_rate_under_0_1_percent',
        'validate_performance_improvement'
      ],
      rollbackProcedure: 'revert_routing_to_sql'
    },
    {
      phase: 5,
      name: 'Dual-write (both systems)',
      actions: [
        'enable_dual_write',
        'validate_write_consistency'
      ],
      validations: [
        'validate_data_consistency',
        'validate_no_conflicts'
      ],
      rollbackProcedure: 'disable_dual_write'
    },
    {
      phase: 6,
      name: 'Switch writes to MongoDB (SQL read-only)',
      actions: [
        'stop_cdc',
        'mongodb_becomes_primary',
        'sql_becomes_readonly'
      ],
      validations: [
        'validate_all_writes_to_mongodb',
        'validate_data_integrity'
      ],
      rollbackProcedure: 'revert_to_sql_primary'
    },
    {
      phase: 7,
      name: 'SQL decommissioning',
      actions: [
        'remove_sql_fallback',
        'archive_sql_data',
        'update_documentation'
      ],
      validations: [
        'validate_no_sql_dependencies',
        'validate_backup_exists'
      ],
      rollbackProcedure: 'restore_from_archive'
    }
  ]
};
```

**Timeline rÃ©elle : Migration module Catalogue (6 semaines)**

```
Semaine 1 (5 jours) : Phase 1-2 - Setup & CDC
â”œâ”€ Jour 1-2 : CrÃ©ation schema MongoDB, indexes
â”œâ”€ Jour 3 : Migration initiale (snapshot)
â”œâ”€ Jour 4 : Setup Debezium CDC
â””â”€ Jour 5 : Validation, lag monitoring

Semaine 2 (5 jours) : Phase 3 - Dual-read shadow
â”œâ”€ Jour 1 : DÃ©ploiement dual-read code
â”œâ”€ Jour 2-5 : Monitoring comparaisons, fix divergences

Semaine 3 (5 jours) : Phase 4 - MongoDB primary reads
â”œâ”€ Jour 1 : Bascule reads vers MongoDB (10% traffic)
â”œâ”€ Jour 2 : 50% traffic
â”œâ”€ Jour 3 : 100% traffic
â”œâ”€ Jour 4-5 : Stabilisation, performance tuning

Semaine 4 (5 jours) : Phase 5 - Dual-write
â”œâ”€ Jour 1-2 : ImplÃ©mentation dual-write
â”œâ”€ Jour 3-5 : Validation consistency

Semaine 5 (5 jours) : Phase 6 - MongoDB primary writes
â”œâ”€ Jour 1 : Bascule writes vers MongoDB
â”œâ”€ Jour 2-5 : Validation extensive, monitoring

Semaine 6 (5 jours) : Phase 7 - Decommissioning SQL
â”œâ”€ Jour 1-2 : Archive SQL
â”œâ”€ Jour 3 : Remove fallback code
â””â”€ Jour 4-5 : Documentation, post-mortem
```

---

### 2. Parallel Run Pattern

**Principe**
ExÃ©cuter simultanÃ©ment l'ancien et le nouveau systÃ¨me, en comparant continuellement les rÃ©sultats pour garantir l'Ã©quivalence fonctionnelle.

**Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Application Layer                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          Dual-Execution Framework                â”‚ â”‚
â”‚  â”‚                                                  â”‚ â”‚
â”‚  â”‚  1. Execute on SQL                               â”‚ â”‚
â”‚  â”‚  2. Execute on MongoDB (parallel)                â”‚ â”‚
â”‚  â”‚  3. Compare results                              â”‚ â”‚
â”‚  â”‚  4. Return SQL result (if SQL is primary)        â”‚ â”‚
â”‚  â”‚  5. Log discrepancies                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â†“                           â†“               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  SQL Database   â”‚         â”‚  MongoDB        â”‚      â”‚
â”‚  â”‚  (Primary)      â”‚         â”‚  (Shadow)       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          Comparison & Logging Service            â”‚ â”‚
â”‚  â”‚  â€¢ Real-time diff detection                      â”‚ â”‚
â”‚  â”‚  â€¢ Alert on high divergence rate                 â”‚ â”‚
â”‚  â”‚  â€¢ Metrics dashboard                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ImplÃ©mentation Dual-Execution**

```typescript
// dual-execution-framework.ts
interface ExecutionResult<T> {
  data: T;
  duration: number;
  errors?: Error[];
}

interface ComparisonResult {
  match: boolean;
  differences?: string[];
  sqlDuration: number;
  mongoDuration: number;
}

class DualExecutionService {
  private sqlClient: SQLClient;
  private mongoClient: MongoClient;
  private comparisonService: ComparisonService;
  private metricsCollector: MetricsCollector;

  async executeQuery<T>(
    operation: string,
    params: any,
    options: {
      primary: 'sql' | 'mongodb',
      compareResults: boolean,
      timeout: number
    }
  ): Promise<T> {

    const startTime = Date.now();

    // ExÃ©cuter en parallÃ¨le
    const [sqlResult, mongoResult] = await Promise.allSettled([
      this.executeSQLQuery<T>(operation, params, options.timeout),
      this.executeMongoQuery<T>(operation, params, options.timeout)
    ]);

    // Collecter mÃ©triques
    const sqlDuration = sqlResult.status === 'fulfilled' ?
      sqlResult.value.duration : null;
    const mongoDuration = mongoResult.status === 'fulfilled' ?
      mongoResult.value.duration : null;

    this.metricsCollector.record({
      operation,
      sqlDuration,
      mongoDuration,
      sqlSuccess: sqlResult.status === 'fulfilled',
      mongoSuccess: mongoResult.status === 'fulfilled',
      timestamp: new Date()
    });

    // Comparaison si demandÃ©e
    if (options.compareResults &&
        sqlResult.status === 'fulfilled' &&
        mongoResult.status === 'fulfilled') {

      const comparison = await this.comparisonService.compare(
        sqlResult.value.data,
        mongoResult.value.data,
        operation
      );

      if (!comparison.match) {
        await this.handleDivergence({
          operation,
          params,
          sqlResult: sqlResult.value.data,
          mongoResult: mongoResult.value.data,
          differences: comparison.differences
        });
      }

      // Alerter si taux de divergence Ã©levÃ©
      const divergenceRate = await this.metricsCollector.getDivergenceRate(
        operation,
        '5m'
      );

      if (divergenceRate > 0.01) {  // > 1%
        await this.alertOps({
          severity: 'high',
          message: `High divergence rate for ${operation}: ${divergenceRate}%`,
          operation,
          divergenceRate
        });
      }
    }

    // Retourner rÃ©sultat du primary
    if (options.primary === 'sql') {
      if (sqlResult.status === 'fulfilled') {
        return sqlResult.value.data;
      } else {
        // Fallback to MongoDB if SQL fails
        if (mongoResult.status === 'fulfilled') {
          console.warn(`SQL failed, falling back to MongoDB for ${operation}`);
          return mongoResult.value.data;
        }
        throw sqlResult.reason;
      }
    } else {
      if (mongoResult.status === 'fulfilled') {
        return mongoResult.value.data;
      } else {
        // Fallback to SQL if MongoDB fails
        if (sqlResult.status === 'fulfilled') {
          console.warn(`MongoDB failed, falling back to SQL for ${operation}`);
          return sqlResult.value.data;
        }
        throw mongoResult.reason;
      }
    }
  }

  private async executeSQLQuery<T>(
    operation: string,
    params: any,
    timeout: number
  ): Promise<ExecutionResult<T>> {
    const start = Date.now();

    try {
      const data = await this.sqlClient.query(
        this.getSQLQuery(operation),
        params,
        { timeout }
      );

      return {
        data: data as T,
        duration: Date.now() - start
      };
    } catch (error) {
      throw error;
    }
  }

  private async executeMongoQuery<T>(
    operation: string,
    params: any,
    timeout: number
  ): Promise<ExecutionResult<T>> {
    const start = Date.now();

    try {
      const data = await this.mongoClient
        .db()
        .collection(this.getCollectionName(operation))
        .find(this.buildMongoQuery(operation, params))
        .maxTimeMS(timeout)
        .toArray();

      return {
        data: data as T,
        duration: Date.now() - start
      };
    } catch (error) {
      throw error;
    }
  }

  private async handleDivergence(divergence: any): Promise<void> {
    // Logger divergence
    await this.divergenceLogger.log({
      timestamp: new Date(),
      operation: divergence.operation,
      params: divergence.params,
      sqlResult: divergence.sqlResult,
      mongoResult: divergence.mongoResult,
      differences: divergence.differences
    });

    // Stocker pour analyse
    await this.divergenceDb.insertOne({
      ...divergence,
      analyzed: false,
      created_at: new Date()
    });
  }
}

// Exemple d'utilisation
class ProductService {
  constructor(private dualExec: DualExecutionService) {}

  async getProduct(productId: string): Promise<Product> {
    return await this.dualExec.executeQuery<Product>(
      'get_product',
      { id: productId },
      {
        primary: 'mongodb',  // MongoDB est maintenant primary
        compareResults: true,
        timeout: 5000
      }
    );
  }

  async searchProducts(query: string): Promise<Product[]> {
    return await this.dualExec.executeQuery<Product[]>(
      'search_products',
      { query },
      {
        primary: 'mongodb',
        compareResults: true,
        timeout: 10000
      }
    );
  }
}
```

**Service de Comparaison**

```typescript
// comparison-service.ts
interface DifferenceDetail {
  field: string;
  sqlValue: any;
  mongoValue: any;
  type: 'missing' | 'mismatch' | 'extra' | 'type_difference';
}

class ComparisonService {
  compare(
    sqlData: any,
    mongoData: any,
    operation: string
  ): ComparisonResult {

    // Normaliser les donnÃ©es (types, formats)
    const normalizedSql = this.normalize(sqlData, 'sql');
    const normalizedMongo = this.normalize(mongoData, 'mongodb');

    // Comparaison profonde
    const differences = this.deepCompare(normalizedSql, normalizedMongo);

    return {
      match: differences.length === 0,
      differences: differences.map(d => this.formatDifference(d)),
      sqlData: normalizedSql,
      mongoData: normalizedMongo
    };
  }

  private normalize(data: any, source: 'sql' | 'mongodb'): any {
    if (Array.isArray(data)) {
      return data.map(item => this.normalize(item, source));
    }

    if (data === null || typeof data !== 'object') {
      return data;
    }

    const normalized: any = {};

    for (const [key, value] of Object.entries(data)) {
      // Ignorer champs techniques
      if (this.isInternalField(key, source)) {
        continue;
      }

      // Normaliser noms de champs (snake_case vs camelCase)
      const normalizedKey = this.normalizeFieldName(key);

      // Normaliser valeurs
      normalized[normalizedKey] = this.normalizeValue(value, source);
    }

    return normalized;
  }

  private normalizeValue(value: any, source: string): any {
    // Dates
    if (value instanceof Date || this.isDateString(value)) {
      const date = new Date(value);
      return date.toISOString();
    }

    // Nombres (BigInt, Decimal)
    if (typeof value === 'number' || this.isNumericString(value)) {
      return parseFloat(value.toString());
    }

    // BoolÃ©ens (SQL peut retourner 0/1)
    if (value === 0 || value === 1 || typeof value === 'boolean') {
      return Boolean(value);
    }

    // Null vs undefined
    if (value === null || value === undefined) {
      return null;
    }

    // Objets imbriquÃ©s
    if (typeof value === 'object') {
      return this.normalize(value, source);
    }

    return value;
  }

  private deepCompare(
    obj1: any,
    obj2: any,
    path: string = ''
  ): DifferenceDetail[] {
    const differences: DifferenceDetail[] = [];

    // Comparer les clÃ©s
    const keys1 = new Set(Object.keys(obj1 || {}));
    const keys2 = new Set(Object.keys(obj2 || {}));

    // Champs manquants dans obj2
    for (const key of keys1) {
      if (!keys2.has(key)) {
        differences.push({
          field: `${path}.${key}`,
          sqlValue: obj1[key],
          mongoValue: undefined,
          type: 'missing'
        });
      }
    }

    // Champs extra dans obj2
    for (const key of keys2) {
      if (!keys1.has(key)) {
        differences.push({
          field: `${path}.${key}`,
          sqlValue: undefined,
          mongoValue: obj2[key],
          type: 'extra'
        });
      }
    }

    // Comparer valeurs communes
    for (const key of keys1) {
      if (keys2.has(key)) {
        const val1 = obj1[key];
        const val2 = obj2[key];

        if (typeof val1 !== typeof val2) {
          differences.push({
            field: `${path}.${key}`,
            sqlValue: val1,
            mongoValue: val2,
            type: 'type_difference'
          });
        } else if (typeof val1 === 'object' && val1 !== null) {
          // Comparaison rÃ©cursive
          differences.push(
            ...this.deepCompare(val1, val2, `${path}.${key}`)
          );
        } else if (val1 !== val2) {
          // TolÃ©rance pour nombres flottants
          if (typeof val1 === 'number' && typeof val2 === 'number') {
            if (Math.abs(val1 - val2) > 0.0001) {
              differences.push({
                field: `${path}.${key}`,
                sqlValue: val1,
                mongoValue: val2,
                type: 'mismatch'
              });
            }
          } else {
            differences.push({
              field: `${path}.${key}`,
              sqlValue: val1,
              mongoValue: val2,
              type: 'mismatch'
            });
          }
        }
      }
    }

    return differences;
  }
}
```

**Dashboard de Monitoring**

```typescript
// metrics-dashboard.ts
interface DivergenceMetrics {
  totalOperations: number;
  divergentOperations: number;
  divergenceRate: number;
  topDivergentOperations: Array<{
    operation: string;
    count: number;
    rate: number;
  }>;
  performanceComparison: {
    sqlAvgMs: number;
    mongoAvgMs: number;
    improvement: number;
  };
}

class MetricsCollector {
  async getDivergenceReport(timeRange: string): Promise<DivergenceMetrics> {
    const operations = await this.getOperations(timeRange);

    const totalOperations = operations.length;
    const divergentOperations = operations.filter(op => !op.match).length;
    const divergenceRate = (divergentOperations / totalOperations) * 100;

    // Grouper par type d'opÃ©ration
    const operationGroups = this.groupBy(operations, 'operation');

    const topDivergent = Object.entries(operationGroups)
      .map(([operation, ops]: [string, any[]]) => ({
        operation,
        count: ops.filter(op => !op.match).length,
        rate: (ops.filter(op => !op.match).length / ops.length) * 100
      }))
      .sort((a, b) => b.rate - a.rate)
      .slice(0, 10);

    // Performance comparison
    const sqlDurations = operations
      .filter(op => op.sqlDuration)
      .map(op => op.sqlDuration);
    const mongoDurations = operations
      .filter(op => op.mongoDuration)
      .map(op => op.mongoDuration);

    const sqlAvg = this.average(sqlDurations);
    const mongoAvg = this.average(mongoDurations);
    const improvement = ((sqlAvg - mongoAvg) / sqlAvg) * 100;

    return {
      totalOperations,
      divergentOperations,
      divergenceRate,
      topDivergentOperations: topDivergent,
      performanceComparison: {
        sqlAvgMs: sqlAvg,
        mongoAvgMs: mongoAvg,
        improvement
      }
    };
  }
}
```

---

### 3. Feature Toggle Pattern

**Principe**
Utiliser des feature flags pour activer/dÃ©sactiver dynamiquement MongoDB par module, utilisateur ou pourcentage de traffic.

**Architecture**

```typescript
// feature-toggle-service.ts
interface FeatureConfig {
  name: string;
  enabled: boolean;
  rolloutPercentage: number;
  enabledForUsers?: string[];
  enabledForTenants?: string[];
  conditions?: FeatureCondition[];
}

interface FeatureCondition {
  type: 'user_attribute' | 'tenant_attribute' | 'time_window' | 'custom';
  attribute?: string;
  operator: '=' | '!=' | '>' | '<' | 'in' | 'not_in';
  value: any;
}

class FeatureToggleService {
  private config: Map<string, FeatureConfig>;
  private redis: RedisClient;

  async isEnabled(
    featureName: string,
    context: {
      userId?: string;
      tenantId?: string;
      userAttributes?: Record<string, any>;
      tenantAttributes?: Record<string, any>;
    }
  ): Promise<boolean> {

    const feature = this.config.get(featureName);

    if (!feature) {
      return false;
    }

    if (!feature.enabled) {
      return false;
    }

    // VÃ©rifier whitelist utilisateurs
    if (feature.enabledForUsers && context.userId) {
      if (feature.enabledForUsers.includes(context.userId)) {
        return true;
      }
    }

    // VÃ©rifier whitelist tenants
    if (feature.enabledForTenants && context.tenantId) {
      if (feature.enabledForTenants.includes(context.tenantId)) {
        return true;
      }
    }

    // VÃ©rifier conditions
    if (feature.conditions) {
      const conditionsMet = await this.evaluateConditions(
        feature.conditions,
        context
      );
      if (conditionsMet) {
        return true;
      }
    }

    // Rollout percentage (consistent hashing)
    if (feature.rolloutPercentage > 0) {
      const hash = this.hashContext(context);
      const bucket = hash % 100;
      return bucket < feature.rolloutPercentage;
    }

    return false;
  }

  private hashContext(context: any): number {
    // Consistent hash basÃ© sur userId ou tenantId
    const key = context.userId || context.tenantId || 'anonymous';
    let hash = 0;
    for (let i = 0; i < key.length; i++) {
      hash = ((hash << 5) - hash) + key.charCodeAt(i);
      hash = hash & hash;
    }
    return Math.abs(hash);
  }
}

// Exemple d'utilisation dans un service
class OrderService {
  constructor(
    private featureToggle: FeatureToggleService,
    private sqlClient: SQLClient,
    private mongoClient: MongoClient
  ) {}

  async getOrder(orderId: string, context: any): Promise<Order> {
    // Feature flag : utiliser MongoDB pour ce module ?
    const useMongo = await this.featureToggle.isEnabled(
      'orders_mongodb',
      context
    );

    if (useMongo) {
      try {
        return await this.getOrderFromMongo(orderId);
      } catch (error) {
        console.error('MongoDB error, falling back to SQL:', error);
        // Fallback automatique
        return await this.getOrderFromSQL(orderId);
      }
    } else {
      return await this.getOrderFromSQL(orderId);
    }
  }

  private async getOrderFromMongo(orderId: string): Promise<Order> {
    const order = await this.mongoClient
      .db()
      .collection('orders')
      .findOne({ _id: new ObjectId(orderId) });

    if (!order) {
      throw new Error('Order not found');
    }

    return order as Order;
  }

  private async getOrderFromSQL(orderId: string): Promise<Order> {
    const result = await this.sqlClient.query(
      'SELECT * FROM orders WHERE id = ?',
      [orderId]
    );

    if (result.length === 0) {
      throw new Error('Order not found');
    }

    return result[0] as Order;
  }
}
```

**Configuration feature flags (LaunchDarkly style)**

```yaml
# feature-flags.yml
features:
  # Catalogue produits : 100% MongoDB
  - name: products_mongodb
    enabled: true
    rollout_percentage: 100
    conditions: []

  # Orders : Rollout progressif
  - name: orders_mongodb
    enabled: true
    rollout_percentage: 50  # 50% des utilisateurs
    conditions:
      - type: tenant_attribute
        attribute: tier
        operator: in
        value: ['enterprise', 'premium']  # PrioritÃ© aux gros clients

  # Customers : Beta testing
  - name: customers_mongodb
    enabled: true
    rollout_percentage: 0
    enabled_for_users:
      - user_123  # Beta testers
      - user_456
      - user_789

  # Analytics : Time window (nuit uniquement)
  - name: analytics_mongodb
    enabled: true
    rollout_percentage: 100
    conditions:
      - type: time_window
        start_hour: 22  # 22h
        end_hour: 6     # 6h
```

**Progression du rollout**

```
Semaine 1 : 1% â†’ 10 beta testers
Semaine 2 : 5% â†’ ~500 users
Semaine 3 : 10%
Semaine 4 : 25%
Semaine 5 : 50%
Semaine 6 : 75%
Semaine 7 : 100% â†’ Suppression feature flag
```

---

## ğŸ”„ StratÃ©gies de Synchronisation

### 1. Dual-Write Pattern

**Principe**
Application Ã©crit simultanÃ©ment dans SQL et MongoDB. Garantit cohÃ©rence immÃ©diate mais complexe Ã  gÃ©rer.

**Architecture**

```typescript
// dual-write-service.ts
class DualWriteService {
  async createOrder(order: Order): Promise<Order> {
    const session = await this.mongoClient.startSession();
    const sqlTransaction = await this.sqlClient.beginTransaction();

    try {
      // Write to SQL (primary)
      const sqlOrder = await sqlTransaction.query(
        'INSERT INTO orders (...) VALUES (...)',
        order
      );

      // Write to MongoDB (secondary)
      const mongoOrder = await this.mongoClient
        .db()
        .collection('orders')
        .insertOne(this.transformToMongo(order), { session });

      // Commit both
      await sqlTransaction.commit();
      await session.commitTransaction();

      return sqlOrder;

    } catch (error) {
      // Rollback both
      await sqlTransaction.rollback();
      await session.abortTransaction();

      throw error;
    } finally {
      await session.endSession();
    }
  }
}
```

**ProblÃ¨mes et solutions**

**ProblÃ¨me 1 : Inconsistency (un write rÃ©ussit, l'autre Ã©choue)**

```typescript
// Solution : Retry avec compensation
class ResilientDualWriteService {
  async createOrder(order: Order): Promise<Order> {
    let sqlSuccess = false;
    let mongoSuccess = false;
    let sqlOrder: any;
    let mongoOrder: any;

    try {
      // Write SQL
      sqlOrder = await this.writeSQLWithRetry(order, 3);
      sqlSuccess = true;

      // Write MongoDB
      mongoOrder = await this.writeMongoWithRetry(order, 3);
      mongoSuccess = true;

      return sqlOrder;

    } catch (error) {
      // Compensation
      if (sqlSuccess && !mongoSuccess) {
        // SQL succeeded but MongoDB failed
        // Option 1: Rollback SQL (si possible)
        await this.compensateSQLWrite(sqlOrder.id);

        // Option 2: Queue for retry
        await this.queueMongoWrite(order);

        // Option 3: Alert and manual intervention
        await this.alertInconsistency({
          type: 'sql_success_mongo_fail',
          orderId: sqlOrder.id,
          order
        });
      }

      if (mongoSuccess && !sqlSuccess) {
        // MongoDB succeeded but SQL failed (rare)
        await this.compensateMongoWrite(mongoOrder.insertedId);
      }

      throw error;
    }
  }

  private async queueMongoWrite(data: any): Promise<void> {
    await this.queue.publish('mongo_writes', {
      operation: 'insert',
      collection: 'orders',
      data,
      retries: 0,
      timestamp: new Date()
    });
  }
}

// Worker pour rattrapage asynchrone
class MongoWriteRetryWorker {
  async processQueue(): Promise<void> {
    const message = await this.queue.consume('mongo_writes');

    try {
      await this.mongoClient
        .db()
        .collection(message.collection)
        .insertOne(message.data);

      await this.queue.ack(message);

    } catch (error) {
      if (message.retries < 5) {
        // Re-queue avec backoff exponentiel
        message.retries++;
        await this.queue.publish('mongo_writes', message, {
          delay: Math.pow(2, message.retries) * 1000
        });
      } else {
        // Dead letter queue aprÃ¨s 5 tentatives
        await this.queue.publish('mongo_writes_dlq', message);
        await this.alertOps({
          severity: 'high',
          message: 'Failed to sync write to MongoDB after 5 retries',
          data: message
        });
      }
    }
  }
}
```

**ProblÃ¨me 2 : Performance (double latence)**

```typescript
// Solution : Write asynchrone Ã  MongoDB
class AsyncDualWriteService {
  async createOrder(order: Order): Promise<Order> {
    // Write synchrone Ã  SQL (primary)
    const sqlOrder = await this.sqlClient.query(
      'INSERT INTO orders (...) VALUES (...)',
      order
    );

    // Write asynchrone Ã  MongoDB (non-bloquant)
    this.writeMongoAsync(order).catch(error => {
      console.error('Async MongoDB write failed:', error);
      this.queueMongoWrite(order);
    });

    return sqlOrder;
  }

  private async writeMongoAsync(order: Order): Promise<void> {
    // Fire-and-forget (avec retry)
    setImmediate(async () => {
      try {
        await this.mongoClient
          .db()
          .collection('orders')
          .insertOne(this.transformToMongo(order));
      } catch (error) {
        await this.queueMongoWrite(order);
      }
    });
  }
}
```

---

### 2. CDC (Change Data Capture) Pattern

**Architecture complÃ¨te de production**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Production CDC Pipeline                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  [Application] â”€â”€writesâ”€â”€â–¶ [PostgreSQL Primary]            â”‚
â”‚                                  â”‚                         â”‚
â”‚                                  â”‚ WAL/Binlog              â”‚
â”‚                                  â†“                         â”‚
â”‚                            [Debezium]                      â”‚
â”‚                                  â”‚                         â”‚
â”‚                                  â”‚ Change events           â”‚
â”‚                                  â†“                         â”‚
â”‚                            [Kafka Cluster]                 â”‚
â”‚                            3 brokers, RF=3                 â”‚
â”‚                                  â”‚                         â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                    â†“             â†“             â†“           â”‚
â”‚              [Consumer 1]  [Consumer 2]  [Consumer 3]      â”‚
â”‚              (Partition 0) (Partition 1) (Partition 2)     â”‚
â”‚                    â”‚             â”‚             â”‚           â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                  â†“                         â”‚
â”‚                    [MongoDB Sink Connector]                â”‚
â”‚                                  â”‚                         â”‚
â”‚                                  â†“                         â”‚
â”‚                    [MongoDB Replica Set]                   â”‚
â”‚                    Primary + 2 Secondaries                 â”‚
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Monitoring Stack                                    â”‚  â”‚
â”‚  â”‚  â€¢ Kafka lag (consumer group)                        â”‚  â”‚
â”‚  â”‚  â€¢ Debezium metrics (streaming lag, throughput)      â”‚  â”‚
â”‚  â”‚  â€¢ Data quality alerts (divergence detection)        â”‚  â”‚
â”‚  â”‚  â€¢ Performance metrics (latency P50, P95, P99)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration Debezium optimisÃ©e**

```json
{
  "name": "postgres-cdc-orders",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "tasks.max": "1",

    "database.hostname": "postgres-primary.prod.internal",
    "database.port": "5432",
    "database.user": "debezium_user",
    "database.password": "${file:/secrets/db-password}",
    "database.dbname": "ecommerce",
    "database.server.name": "prod-ecommerce",

    "table.include.list": "public.orders,public.order_items",

    "plugin.name": "pgoutput",
    "publication.name": "dbz_publication",
    "publication.autocreate.mode": "filtered",

    "slot.name": "debezium_orders_slot",
    "slot.drop.on.stop": "false",

    "snapshot.mode": "initial",
    "snapshot.locking.mode": "none",

    "poll.interval.ms": "100",
    "max.batch.size": "2048",
    "max.queue.size": "8192",

    "heartbeat.interval.ms": "10000",
    "heartbeat.action.query": "INSERT INTO heartbeat (ts) VALUES (NOW())",

    "tombstones.on.delete": "true",

    "transforms": "unwrap,addSource",
    "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "rewrite",
    "transforms.unwrap.add.fields": "op,source.ts_ms",

    "transforms.addSource.type": "org.apache.kafka.connect.transforms.InsertField$Value",
    "transforms.addSource.static.field": "_cdc_source",
    "transforms.addSource.static.value": "postgresql",

    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
  }
}
```

**Consumer MongoDB avec transformation**

```typescript
// cdc-consumer.ts
import { Kafka, Consumer, EachMessagePayload } from 'kafkajs';
import { MongoClient, Db } from 'mongodb';

interface CDCEvent {
  op: 'c' | 'u' | 'd' | 'r';  // create, update, delete, read (snapshot)
  source: {
    ts_ms: number;
    db: string;
    table: string;
  };
  before?: any;
  after?: any;
  _cdc_source: string;
}

class CDCConsumer {
  private kafka: Kafka;
  private consumer: Consumer;
  private mongoClient: MongoClient;
  private db: Db;

  async start(): Promise<void> {
    this.kafka = new Kafka({
      clientId: 'mongodb-cdc-consumer',
      brokers: ['kafka1:9092', 'kafka2:9092', 'kafka3:9092'],
      ssl: true,
      sasl: {
        mechanism: 'scram-sha-256',
        username: process.env.KAFKA_USERNAME,
        password: process.env.KAFKA_PASSWORD
      }
    });

    this.consumer = this.kafka.consumer({
      groupId: 'mongodb-sink',
      sessionTimeout: 30000,
      heartbeatInterval: 3000,
      maxBytesPerPartition: 1048576,  // 1 MB
      retry: {
        retries: 5,
        initialRetryTime: 300,
        multiplier: 2
      }
    });

    await this.consumer.connect();
    await this.consumer.subscribe({
      topics: ['prod-ecommerce.public.orders', 'prod-ecommerce.public.order_items'],
      fromBeginning: false
    });

    this.mongoClient = new MongoClient(process.env.MONGODB_URI);
    await this.mongoClient.connect();
    this.db = this.mongoClient.db('ecommerce');

    await this.consumer.run({
      autoCommit: false,
      eachMessage: this.processMessage.bind(this)
    });
  }

  private async processMessage(payload: EachMessagePayload): Promise<void> {
    const { topic, partition, message } = payload;

    try {
      const event: CDCEvent = JSON.parse(message.value.toString());

      // Extract table name from topic
      const tableName = topic.split('.').pop();
      const collectionName = this.getCollectionName(tableName);

      // Process based on operation type
      switch (event.op) {
        case 'c':  // CREATE
        case 'r':  // READ (snapshot)
          await this.handleInsert(collectionName, event.after);
          break;

        case 'u':  // UPDATE
          await this.handleUpdate(collectionName, event.before, event.after);
          break;

        case 'd':  // DELETE
          await this.handleDelete(collectionName, event.before);
          break;
      }

      // Commit offset aprÃ¨s traitement rÃ©ussi
      await this.consumer.commitOffsets([{
        topic,
        partition,
        offset: (parseInt(message.offset) + 1).toString()
      }]);

      // MÃ©triques
      this.recordMetrics({
        topic,
        operation: event.op,
        latency: Date.now() - event.source.ts_ms,
        timestamp: new Date()
      });

    } catch (error) {
      console.error('Error processing message:', error);

      // Dead letter queue
      await this.sendToDLQ({
        topic,
        partition,
        offset: message.offset,
        value: message.value.toString(),
        error: error.message,
        timestamp: new Date()
      });

      // Ne pas commit l'offset en cas d'erreur
      // Le message sera retraitÃ©
      throw error;
    }
  }

  private async handleInsert(collection: string, data: any): Promise<void> {
    const transformed = this.transformDocument(collection, data);

    // Upsert pour idempotence (reprocessing de messages)
    await this.db.collection(collection).updateOne(
      { legacy_id: data.id },
      { $set: transformed },
      { upsert: true }
    );
  }

  private async handleUpdate(
    collection: string,
    before: any,
    after: any
  ): Promise<void> {
    const transformed = this.transformDocument(collection, after);

    await this.db.collection(collection).updateOne(
      { legacy_id: after.id },
      { $set: transformed }
    );
  }

  private async handleDelete(collection: string, data: any): Promise<void> {
    await this.db.collection(collection).deleteOne({
      legacy_id: data.id
    });
  }

  private transformDocument(collection: string, sqlRow: any): any {
    // Transformation spÃ©cifique par collection
    if (collection === 'orders') {
      return {
        legacy_id: sqlRow.id,
        order_number: sqlRow.order_number,
        customer_id: sqlRow.customer_id,
        order_date: new Date(sqlRow.order_date),
        status: sqlRow.status,
        total: parseFloat(sqlRow.total_amount),
        _cdc_synced_at: new Date()
      };
    }

    // Default transformation
    return {
      ...sqlRow,
      legacy_id: sqlRow.id,
      _cdc_synced_at: new Date()
    };
  }
}
```

**Monitoring CDC**

```typescript
// cdc-monitoring.ts
interface CDCMetrics {
  kafkaLag: number;
  streamingLag: number;
  throughput: number;
  errorRate: number;
  p95Latency: number;
}

class CDCMonitor {
  async getMetrics(): Promise<CDCMetrics> {
    const [kafkaLag, debeziumMetrics] = await Promise.all([
      this.getKafkaLag(),
      this.getDebeziumMetrics()
    ]);

    return {
      kafkaLag,
      streamingLag: debeziumMetrics.streamingLagMs,
      throughput: debeziumMetrics.eventsPerSecond,
      errorRate: debeziumMetrics.errorRate,
      p95Latency: debeziumMetrics.p95LatencyMs
    };
  }

  private async getKafkaLag(): Promise<number> {
    const admin = this.kafka.admin();
    await admin.connect();

    const groupDescription = await admin.describeGroups(['mongodb-sink']);
    const offsets = await admin.fetchOffsets({
      groupId: 'mongodb-sink',
      topics: ['prod-ecommerce.public.orders']
    });

    // Calculer lag total
    let totalLag = 0;
    for (const topic of offsets) {
      for (const partition of topic.partitions) {
        totalLag += partition.lag;
      }
    }

    await admin.disconnect();
    return totalLag;
  }

  async checkHealth(): Promise<{healthy: boolean, issues: string[]}> {
    const metrics = await this.getMetrics();
    const issues: string[] = [];

    // Alertes
    if (metrics.kafkaLag > 10000) {
      issues.push(`High Kafka lag: ${metrics.kafkaLag} messages`);
    }

    if (metrics.streamingLag > 60000) {
      issues.push(`High streaming lag: ${metrics.streamingLag}ms (> 1 minute)`);
    }

    if (metrics.errorRate > 0.01) {
      issues.push(`High error rate: ${(metrics.errorRate * 100).toFixed(2)}%`);
    }

    return {
      healthy: issues.length === 0,
      issues
    };
  }
}
```

---

## ğŸ“Š ScÃ©nario RÃ©el : Migration SaaS Banking (18 mois)

**Contexte**
- Application bancaire SaaS, 500 clients (banques rÃ©gionales)
- PostgreSQL 14, 80 tables, 5 To
- SLA : 99.95% uptime, RPO < 15 min
- Ã‰quipe : 8 devs, 2 DBAs, 1 architecte
- Objectif : Migrer vers MongoDB pour scalabilitÃ© et flexibilitÃ© schÃ©ma

### Timeline complÃ¨te (Mois par mois)

**Mois 1-2 : PrÃ©paration**
- Analyse schÃ©ma PostgreSQL
- POC migration 3 tables (comptes, transactions, clients)
- Choix architecture (Strangler + CDC)
- Formation Ã©quipe MongoDB (certification M121, M201)

**Mois 3-4 : Infrastructure & Tooling**
- Setup MongoDB Atlas (M50 clusters par environnement)
- DÃ©ploiement Kafka cluster (3 brokers)
- Configuration Debezium
- CI/CD pipelines
- Monitoring stack (Prometheus, Grafana, Datadog)

**Mois 5-8 : Migration Module 1 - Comptes (Core Banking)**

**Mois 5 : PrÃ©paration**
- ModÃ©lisation MongoDB (2 semaines)
- DÃ©veloppement services MongoDB (2 semaines)

**Mois 6 : Migration initiale + CDC**
- Migration snapshot (48h)
- Setup CDC Debezium
- Validation donnÃ©es (1 semaine)

**Mois 7 : Dual-read testing**
- DÃ©ploiement dual-read en staging
- Validation 2 semaines
- Fix 23 divergences dÃ©tectÃ©es

**Mois 8 : Production cutover**
- Rollout progressif : 1% â†’ 10% â†’ 50% â†’ 100%
- Bascule reads vers MongoDB
- Performance : AmÃ©lioration 35% latence P95

**Mois 9-12 : Migration Module 2 - Transactions**

**Mois 9-10 : DÃ©veloppement**
- ModÃ¨le hybride : Transactions rÃ©centes MongoDB, historique SQL
- Time-series collections MongoDB

**Mois 11-12 : Migration & cutover**
- Migration 2 milliards de transactions
- Rollout sur 4 semaines
- RÃ©sultat : Query analytics 10x plus rapide

**Mois 13-16 : Migration Module 3 - Clients & KYC**

**Mois 13-14 : Refactoring**
- ModÃ¨le client unifiÃ© (fusion 8 tables SQL)
- Schema validation stricte (compliance)

**Mois 15-16 : Migration & validation**
- Migration 5M clients
- Validation intensive (RGPD, audit trails)
- Performance : 50% rÃ©duction latence authentification

**Mois 17-18 : Finalisation**
- Migration modules restants (reporting, audit)
- Decommissioning progressif PostgreSQL
- Documentation & formation clients

### RÃ©sultats quantifiÃ©s

**Performance**
- Latence API : 250ms â†’ 85ms (P95)
- Throughput : 5000 â†’ 15000 req/sec
- Queries analytics : 30s â†’ 2s (moyenne)

**Business**
- Time-to-market nouvelles features : -60%
- CoÃ»ts infrastructure : -30% (consolidation)
- Satisfaction clients : +25%

**Technique**
- Uptime maintenu : 99.97% (objectif 99.95%)
- Zero incident majeur
- 0 perte de donnÃ©es

### Challenges rencontrÃ©s

**Challenge 1 : CDC lag spikes**
- **SymptÃ´me** : Lag Kafka monte Ã  2 minutes lors de bulk operations
- **Cause** : Imports clients volumineux (10K records)
- **Solution** : Rate limiting imports + augmentation partition count

**Challenge 2 : Divergences donnÃ©es**
- **SymptÃ´me** : 0.3% divergence dÃ©tectÃ©e en dual-read
- **Cause** : Race conditions dans dual-write
- **Solution** : Transactions distribuÃ©es + idempotence

**Challenge 3 : Performance dÃ©gradÃ©e post-migration (1 requÃªte)**
- **SymptÃ´me** : Query "transactions par client" 10x plus lente MongoDB
- **Cause** : Index manquant sur compound key
- **Solution** : Index { customer_id: 1, transaction_date: -1 }

---

## ğŸ¯ Bonnes Pratiques

### 1. Planification

- [ ] **Phase discovery exhaustive** (2-4 semaines)
  - Analyse complÃ¨te schÃ©ma source
  - Identification dÃ©pendances
  - Patterns d'accÃ¨s rÃ©els (logs)

- [ ] **POC avant full migration** (2-4 semaines)
  - 1-3 tables reprÃ©sentatives
  - Validation architecture end-to-end
  - Benchmarks performance

- [ ] **Timeline rÃ©aliste**
  - Budget 2-3x estimation initiale
  - Marges pour imprÃ©vus
  - Validation phases longues (30-40% du temps)

### 2. Architecture

- [ ] **Isolation modules** : Migrations indÃ©pendantes
- [ ] **Feature flags** : Rollback instantanÃ© par module
- [ ] **CDC over dual-write** : Ã‰vite complexitÃ© applicative
- [ ] **Monitoring exhaustif** : MÃ©triques temps rÃ©el
- [ ] **Fallback automatique** : SQL si MongoDB Ã©choue

### 3. Validation

- [ ] **Validation continue** : Pas seulement post-migration
- [ ] **Comparaison automatisÃ©e** : Dual-read avec alerting
- [ ] **Tests fonctionnels** : Suite complÃ¨te par module
- [ ] **Load testing** : Avant cutover production

### 4. Rollback

- [ ] **Plan dÃ©taillÃ©** : ProcÃ©dure par phase
- [ ] **Backups** : Avant chaque Ã©tape critique
- [ ] **Feature flags** : DÃ©sactivation en 1 clic
- [ ] **Rehearsal** : Test rollback en staging

---

## ğŸ“š Checklist Migration IncrÃ©mentale

**Avant de commencer**
- [ ] Business case validÃ© (ROI, durÃ©e, budget)
- [ ] Sponsor exÃ©cutif engagÃ©
- [ ] Ã‰quipe dÃ©diÃ©e (devs, DBAs, architecte)
- [ ] Architecture cible documentÃ©e
- [ ] POC rÃ©ussi
- [ ] Infrastructure provisionnÃ©e (double capacitÃ©)

**Par module migrÃ©**
- [ ] ModÃ¨le MongoDB conÃ§u et validÃ©
- [ ] Code dÃ©veloppÃ© et testÃ©
- [ ] CDC configurÃ© et validÃ©
- [ ] Dual-read testÃ© en staging
- [ ] Feature flags configurÃ©s
- [ ] Monitoring dashboards prÃªts
- [ ] Rollback plan documentÃ©
- [ ] Validation automatisÃ©e en place

**Post-migration module**
- [ ] Validation donnÃ©es (100% row counts)
- [ ] Performance validÃ©e (benchmarks)
- [ ] Monitoring actif (alertes configurÃ©es)
- [ ] Documentation mise Ã  jour
- [ ] Ã‰quipe formÃ©e
- [ ] Post-mortem rÃ©alisÃ©

**Finalisation projet**
- [ ] Tous modules migrÃ©s
- [ ] SQL decommissioned
- [ ] Documentation complÃ¨te
- [ ] Formation clients/users
- [ ] CÃ©lÃ©bration ! ğŸ‰

---

**Prochaine section** : 19.5 Synchronisation bidirectionnelle - Techniques de synchronisation complexe entre systÃ¨mes hÃ©tÃ©rogÃ¨nes.

â­ï¸ [Synchronisation bidirectionnelle](/19-migration-integration/05-synchronisation-bidirectionnelle.md)
